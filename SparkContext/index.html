<!doctype html><html lang=en class=no-js> <head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=description content="Demystifying inner-workings of Apache Spark"><link href=https://books.japila.pl/apache-spark-internals/SparkContext/ rel=canonical><meta name=author content="Jacek Laskowski"><link rel="shortcut icon" href=../assets/images/favicon.png><meta name=generator content="mkdocs-1.1.2, mkdocs-material-6.0.2"><title>SparkContext - The Internals of Apache Spark</title><link rel=stylesheet href=../assets/stylesheets/main.38780c08.min.css><link rel=stylesheet href=../assets/stylesheets/palette.3f72e892.min.css><link href=https://fonts.gstatic.com rel=preconnect crossorigin><link rel=stylesheet href="https://fonts.googleapis.com/css?family=Roboto:300,400,400i,700%7CRoboto+Mono&display=fallback"><style>body,input{font-family:"Roboto",-apple-system,BlinkMacSystemFont,Helvetica,Arial,sans-serif}code,kbd,pre{font-family:"Roboto Mono",SFMono-Regular,Consolas,Menlo,monospace}</style><script>window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)},ga.l=+new Date,ga("create","UA-151208281-5","auto"),ga("set","anonymizeIp",!0),ga("send","pageview"),document.addEventListener("DOMContentLoaded",function(){document.forms.search&&document.forms.search.query.addEventListener("blur",function(){if(this.value){var e=document.location.pathname;ga("send","pageview",e+"?q="+this.value)}})}),document.addEventListener("DOMContentSwitch",function(){ga("send","pageview",document.location.pathname)})</script><script async src=https://www.google-analytics.com/analytics.js></script></head> <body dir=ltr data-md-color-scheme data-md-color-primary=none data-md-color-accent=none> <input class=md-toggle data-md-toggle=drawer type=checkbox id=__drawer autocomplete=off> <input class=md-toggle data-md-toggle=search type=checkbox id=__search autocomplete=off> <label class=md-overlay for=__drawer></label> <div data-md-component=skip> <a href=#source-scala class=md-skip> Skip to content </a> </div> <div data-md-component=announce> </div> <header class=md-header data-md-component=header> <nav class="md-header-nav md-grid" aria-label=Header> <a href=https://books.japila.pl/apache-spark-internals title="The Internals of Apache Spark" class="md-header-nav__button md-logo" aria-label="The Internals of Apache Spark"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M19 2l-5 4.5v11l5-4.5V2M6.5 5C4.55 5 2.45 5.4 1 6.5v14.66c0 .25.25.5.5.5.1 0 .15-.07.25-.07 1.35-.65 3.3-1.09 4.75-1.09 1.95 0 4.05.4 5.5 1.5 1.35-.85 3.8-1.5 5.5-1.5 1.65 0 3.35.31 4.75 1.06.1.05.15.03.25.03.25 0 .5-.25.5-.5V6.5c-.6-.45-1.25-.75-2-1V19c-1.1-.35-2.3-.5-3.5-.5-1.7 0-4.15.65-5.5 1.5V6.5C10.55 5.4 8.45 5 6.5 5z"/></svg> </a> <label class="md-header-nav__button md-icon" for=__drawer> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M3 6h18v2H3V6m0 5h18v2H3v-2m0 5h18v2H3v-2z"/></svg> </label> <div class=md-header-nav__title data-md-component=header-title> <div class=md-header-nav__ellipsis> <span class="md-header-nav__topic md-ellipsis"> The Internals of Apache Spark </span> <span class="md-header-nav__topic md-ellipsis"> SparkContext </span> </div> </div> <label class="md-header-nav__button md-icon" for=__search> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0116 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 019.5 16 6.5 6.5 0 013 9.5 6.5 6.5 0 019.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5z"/></svg> </label> <div class=md-search data-md-component=search role=dialog> <label class=md-search__overlay for=__search></label> <div class=md-search__inner role=search> <form class=md-search__form name=search> <input type=text class=md-search__input name=query aria-label=Search placeholder=Search autocapitalize=off autocorrect=off autocomplete=off spellcheck=false data-md-component=search-query data-md-state=active> <label class="md-search__icon md-icon" for=__search> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0116 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 019.5 16 6.5 6.5 0 013 9.5 6.5 6.5 0 019.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5z"/></svg> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12z"/></svg> </label> <button type=reset class="md-search__icon md-icon" aria-label=Clear data-md-component=search-reset tabindex=-1> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M19 6.41L17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12 19 6.41z"/></svg> </button> </form> <div class=md-search__output> <div class=md-search__scrollwrap data-md-scrollfix> <div class=md-search-result data-md-component=search-result> <div class=md-search-result__meta> Initializing search </div> <ol class=md-search-result__list></ol> </div> </div> </div> </div> </div> <div class=md-header-nav__source> <a href=https://github.com/japila-books/apache-spark-internals/ title="Go to repository" class=md-source> <div class="md-source__icon md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 496 512"><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6 0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2z"/></svg> </div> <div class=md-source__repository> apache-spark-internals </div> </a> </div> </nav> </header> <div class=md-container data-md-component=container> <nav class="md-tabs md-tabs--active" aria-label=Tabs data-md-component=tabs> <div class="md-tabs__inner md-grid"> <ul class=md-tabs__list> <li class=md-tabs__item> <a href=.. class="md-tabs__link md-tabs__link--active"> Home </a> </li> <li class=md-tabs__item> <a href=../scheduler/DAGScheduler/ class=md-tabs__link> Scheduler </a> </li> <li class=md-tabs__item> <a href=../tools/spark-shell/ class=md-tabs__link> Tools </a> </li> <li class=md-tabs__item> <a href=../rdd/ class=md-tabs__link> RDD </a> </li> <li class=md-tabs__item> <a href=../metrics/ class=md-tabs__link> Metrics </a> </li> </ul> </div> </nav> <main class=md-main data-md-component=main> <div class="md-main__inner md-grid"> <div class="md-sidebar md-sidebar--primary" data-md-component=navigation> <div class=md-sidebar__scrollwrap> <div class=md-sidebar__inner> <nav class="md-nav md-nav--primary" aria-label=Navigation data-md-level=0> <label class=md-nav__title for=__drawer> <a href=https://books.japila.pl/apache-spark-internals title="The Internals of Apache Spark" class="md-nav__button md-logo" aria-label="The Internals of Apache Spark"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M19 2l-5 4.5v11l5-4.5V2M6.5 5C4.55 5 2.45 5.4 1 6.5v14.66c0 .25.25.5.5.5.1 0 .15-.07.25-.07 1.35-.65 3.3-1.09 4.75-1.09 1.95 0 4.05.4 5.5 1.5 1.35-.85 3.8-1.5 5.5-1.5 1.65 0 3.35.31 4.75 1.06.1.05.15.03.25.03.25 0 .5-.25.5-.5V6.5c-.6-.45-1.25-.75-2-1V19c-1.1-.35-2.3-.5-3.5-.5-1.7 0-4.15.65-5.5 1.5V6.5C10.55 5.4 8.45 5 6.5 5z"/></svg> </a> The Internals of Apache Spark </label> <div class=md-nav__source> <a href=https://github.com/japila-books/apache-spark-internals/ title="Go to repository" class=md-source> <div class="md-source__icon md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 496 512"><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6 0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2z"/></svg> </div> <div class=md-source__repository> apache-spark-internals </div> </a> </div> <ul class=md-nav__list data-md-scrollfix> <li class="md-nav__item md-nav__item--active md-nav__item--nested"> <input class="md-nav__toggle md-toggle" data-md-toggle=nav-1 type=checkbox id=nav-1 checked> <label class=md-nav__link for=nav-1> Home <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav aria-label=Home data-md-level=1> <label class=md-nav__title for=nav-1> <span class="md-nav__icon md-icon"></span> Home </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=.. class=md-nav__link> Welcome </a> </li> <li class=md-nav__item> <a href=../overview/ class=md-nav__link> Overview </a> </li> <li class=md-nav__item> <a href=../SparkEnv/ class=md-nav__link> SparkEnv </a> </li> <li class=md-nav__item> <a href=../SparkConf/ class=md-nav__link> SparkConf </a> </li> <li class="md-nav__item md-nav__item--active md-nav__item--nested"> <input class="md-nav__toggle md-toggle" data-md-toggle=nav-1-5 type=checkbox id=nav-1-5 checked> <label class=md-nav__link for=nav-1-5> SparkContext <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav aria-label=SparkContext data-md-level=2> <label class=md-nav__title for=nav-1-5> <span class="md-nav__icon md-icon"></span> SparkContext </label> <ul class=md-nav__list data-md-scrollfix> <li class="md-nav__item md-nav__item--active"> <input class="md-nav__toggle md-toggle" data-md-toggle=toc type=checkbox id=__toc> <a href=./ class="md-nav__link md-nav__link--active"> SparkContext </a> </li> <li class=md-nav__item> <a href=../spark-SparkContext-creating-instance-internals/ class=md-nav__link> Creating SparkContext </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=../spark-logging/ class=md-nav__link> Logging </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle" data-md-toggle=nav-2 type=checkbox id=nav-2> <label class=md-nav__link for=nav-2> Scheduler <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav aria-label=Scheduler data-md-level=1> <label class=md-nav__title for=nav-2> <span class="md-nav__icon md-icon"></span> Scheduler </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../scheduler/DAGScheduler/ class=md-nav__link> DAGScheduler </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle" data-md-toggle=nav-3 type=checkbox id=nav-3> <label class=md-nav__link for=nav-3> Tools <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav aria-label=Tools data-md-level=1> <label class=md-nav__title for=nav-3> <span class="md-nav__icon md-icon"></span> Tools </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../tools/spark-shell/ class=md-nav__link> spark-shell </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle" data-md-toggle=nav-4 type=checkbox id=nav-4> <label class=md-nav__link for=nav-4> RDD <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav aria-label=RDD data-md-level=1> <label class=md-nav__title for=nav-4> <span class="md-nav__icon md-icon"></span> RDD </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../rdd/ class=md-nav__link> Resilient Distributed Dataset </a> </li> <li class=md-nav__item> <a href=../rdd/RDD/ class=md-nav__link> RDD </a> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle" data-md-toggle=nav-4-3 type=checkbox id=nav-4-3> <label class=md-nav__link for=nav-4-3> RDDs <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav aria-label=RDDs data-md-level=2> <label class=md-nav__title for=nav-4-3> <span class="md-nav__icon md-icon"></span> RDDs </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../rdd/spark-rdd-CoGroupedRDD/ class=md-nav__link> CoGroupedRDD </a> </li> <li class=md-nav__item> <a href=../rdd/spark-rdd-HadoopRDD/ class=md-nav__link> HadoopRDD </a> </li> <li class=md-nav__item> <a href=../rdd/spark-rdd-MapPartitionsRDD/ class=md-nav__link> MapPartitionsRDD </a> </li> <li class=md-nav__item> <a href=../rdd/spark-rdd-NewHadoopRDD/ class=md-nav__link> NewHadoopRDD </a> </li> <li class=md-nav__item> <a href=../rdd/spark-rdd-OrderedRDDFunctions/ class=md-nav__link> OrderedRDDFunctions </a> </li> <li class=md-nav__item> <a href=../rdd/spark-rdd-ParallelCollectionRDD/ class=md-nav__link> ParallelCollectionRDD </a> </li> <li class=md-nav__item> <a href=../rdd/CheckpointRDD/ class=md-nav__link> CheckpointRDD </a> </li> <li class=md-nav__item> <a href=../rdd/ReliableCheckpointRDD/ class=md-nav__link> ReliableCheckpointRDD </a> </li> <li class=md-nav__item> <a href=../rdd/ShuffledRDD/ class=md-nav__link> ShuffledRDD </a> </li> <li class=md-nav__item> <a href=../rdd/spark-rdd-SubtractedRDD/ class=md-nav__link> SubtractedRDD </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle" data-md-toggle=nav-4-4 type=checkbox id=nav-4-4> <label class=md-nav__link for=nav-4-4> Operators <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav aria-label=Operators data-md-level=2> <label class=md-nav__title for=nav-4-4> <span class="md-nav__icon md-icon"></span> Operators </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../rdd/spark-rdd-operations/ class=md-nav__link> Operators </a> </li> <li class=md-nav__item> <a href=../rdd/spark-rdd-transformations/ class=md-nav__link> Transformations </a> </li> <li class=md-nav__item> <a href=../rdd/PairRDDFunctions/ class=md-nav__link> PairRDDFunctions </a> </li> <li class=md-nav__item> <a href=../rdd/spark-rdd-actions/ class=md-nav__link> Actions </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=../rdd/Partitioner/ class=md-nav__link> Partitioner </a> </li> <li class=md-nav__item> <a href=../rdd/spark-rdd-lineage/ class=md-nav__link> RDD Lineage </a> </li> <li class=md-nav__item> <a href=../rdd/spark-rdd-caching/ class=md-nav__link> Caching and Persistence </a> </li> <li class=md-nav__item> <a href=../rdd/spark-rdd-partitions/ class=md-nav__link> Partitions and Partitioning </a> </li> <li class=md-nav__item> <a href=../rdd/spark-rdd-Partition/ class=md-nav__link> Partition </a> </li> <li class=md-nav__item> <a href=../rdd/RDDCheckpointData/ class=md-nav__link> RDDCheckpointData </a> </li> <li class=md-nav__item> <a href=../rdd/LocalRDDCheckpointData/ class=md-nav__link> LocalRDDCheckpointData </a> </li> <li class=md-nav__item> <a href=../rdd/ReliableRDDCheckpointData/ class=md-nav__link> ReliableRDDCheckpointData </a> </li> <li class=md-nav__item> <a href=../rdd/spark-rdd-shuffle/ class=md-nav__link> Shuffling </a> </li> <li class=md-nav__item> <a href=../rdd/spark-rdd-Dependency/ class=md-nav__link> Dependencies </a> </li> <li class=md-nav__item> <a href=../rdd/spark-rdd-NarrowDependency/ class=md-nav__link> NarrowDependency </a> </li> <li class=md-nav__item> <a href=../rdd/ShuffleDependency/ class=md-nav__link> ShuffleDependency </a> </li> <li class=md-nav__item> <a href=../rdd/Aggregator/ class=md-nav__link> Aggregator </a> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle" data-md-toggle=nav-4-18 type=checkbox id=nav-4-18> <label class=md-nav__link for=nav-4-18> Partitioners <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav aria-label=Partitioners data-md-level=2> <label class=md-nav__title for=nav-4-18> <span class="md-nav__icon md-icon"></span> Partitioners </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../rdd/HashPartitioner/ class=md-nav__link> HashPartitioner </a> </li> <li class=md-nav__item> <a href=../rdd/RangePartitioner/ class=md-nav__link> RangePartitioner </a> </li> </ul> </nav> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle" data-md-toggle=nav-5 type=checkbox id=nav-5> <label class=md-nav__link for=nav-5> Metrics <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav aria-label=Metrics data-md-level=1> <label class=md-nav__title for=nav-5> <span class="md-nav__icon md-icon"></span> Metrics </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../metrics/ class=md-nav__link> Spark Metrics </a> </li> <li class=md-nav__item> <a href=../metrics/configuration-properties/ class=md-nav__link> Configuration Properties </a> </li> <li class=md-nav__item> <a href=../metrics/MetricsSystem/ class=md-nav__link> MetricsSystem </a> </li> <li class=md-nav__item> <a href=../metrics/MetricsConfig/ class=md-nav__link> MetricsConfig </a> </li> <li class=md-nav__item> <a href=../metrics/Source/ class=md-nav__link> Source </a> </li> <li class=md-nav__item> <a href=../metrics/Sink/ class=md-nav__link> Sink </a> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle" data-md-toggle=nav-5-7 type=checkbox id=nav-5-7> <label class=md-nav__link for=nav-5-7> Sources <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav aria-label=Sources data-md-level=2> <label class=md-nav__title for=nav-5-7> <span class="md-nav__icon md-icon"></span> Sources </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../metrics/JvmSource/ class=md-nav__link> JvmSource </a> </li> <li class=md-nav__item> <a href=../metrics/DAGSchedulerSource/ class=md-nav__link> DAGSchedulerSource </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle" data-md-toggle=nav-5-8 type=checkbox id=nav-5-8> <label class=md-nav__link for=nav-5-8> Sinks <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav aria-label=Sinks data-md-level=2> <label class=md-nav__title for=nav-5-8> <span class="md-nav__icon md-icon"></span> Sinks </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../metrics/MetricsServlet/ class=md-nav__link> MetricsServlet </a> </li> </ul> </nav> </li> </ul> </nav> </li> </ul> </nav> </div> </div> </div> <div class="md-sidebar md-sidebar--secondary" data-md-component=toc> <div class=md-sidebar__scrollwrap> <div class=md-sidebar__inner> <nav class="md-nav md-nav--secondary" aria-label="Table of contents"> </nav> </div> </div> </div> <div class=md-content> <article class="md-content__inner md-typeset"> <a href=https://github.com/japila-books/apache-spark-internals/edit/mkdocs-material/docs/SparkContext.md title="Edit this page" class="md-content__button md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M20.71 7.04c.39-.39.39-1.04 0-1.41l-2.34-2.34c-.37-.39-1.02-.39-1.41 0l-1.84 1.83 3.75 3.75M3 17.25V21h3.75L17.81 9.93l-3.75-3.75L3 17.25z"/></svg> </a> <p>= SparkContext</p> <p><em>SparkContext</em> is the entry point to all components of Apache Spark (execution engine) and so the heart of a Spark application. In fact, you can consider an application a Spark application only when it uses a SparkContext (directly or indirectly).</p> <p>[[methods]] .SparkContext's Developer API (Public Methods) [cols="1,3",options="header",width="100%"] |=== | Method | Description</p> <p>| &lt;<addjar-internals, addjar>&gt; a| [[addJar]]</p> <h2 id=source-scala>[source, scala]<a class=headerlink href=#source-scala title="Permanent link">&para;</a></h2> <h2 id=addjarpath-string-unit>addJar(path: String): Unit<a class=headerlink href=#addjarpath-string-unit title="Permanent link">&para;</a></h2> <p>| a| <em>More to be added soon</em></p> <p>|===</p> <p>Spark context link:spark-SparkContext-creating-instance-internals.adoc[sets up internal services] and establishes a connection to a link:spark-deployment-environments.adoc[Spark execution environment].</p> <p>Once a &lt;<creating-instance, sparkcontext is created>&gt; you can use it to &lt;<creating-rdds, create rdds>&gt;, &lt;<creating-accumulators, accumulators>&gt; and &lt;<broadcast, broadcast variables>&gt;, access Spark services and &lt;<runjob, run jobs>&gt; (until SparkContext is &lt;<stop, stopped>&gt;).</p> <p>A Spark context is essentially a client of Spark's execution environment and acts as the <em>master of your Spark application</em> (don't get confused with the other meaning of link:spark-master.adoc[Master] in Spark, though).</p> <p>.Spark context acts as the master of your Spark application image::diagrams/sparkcontext-services.png[align="center"]</p> <p>SparkContext offers the following functions:</p> <ul> <li> <p>Getting current status of a Spark application ** &lt;<env, sparkenv>&gt; ** &lt;<getconf, sparkconf>&gt; ** &lt;<master, deployment environment (as master url)>&gt; ** &lt;<appname, application name>&gt; ** &lt;<applicationattemptid, unique identifier of execution attempt>&gt; ** &lt;<deploymode, deploy mode>&gt; ** &lt;<defaultparallelism, default level of parallelism>&gt; that specifies the number of link:spark-rdd-partitions.adoc[partitions] in RDDs when they are created without specifying the number explicitly by a user. ** &lt;<sparkuser, spark user>&gt; ** &lt;<starttime, the time (in milliseconds) when sparkcontext was created>&gt; ** &lt;<uiweburl, url of web ui>&gt; ** &lt;<version, spark version>&gt; ** &lt;<getexecutorstoragestatus, storage status>&gt;</p> </li> <li> <p>Setting Configuration ** &lt;<master-url, master url>&gt; ** link:spark-sparkcontext-local-properties.adoc[Local Properties -- Creating Logical Job Groups] ** &lt;<setjobgroup, setting local properties to group spark jobs>&gt; ** &lt;<setting-default-log-level, default logging level>&gt;</p> </li> <li> <p>Creating Distributed Entities ** &lt;<creating-rdds, rdds>&gt; ** &lt;<creating-accumulators, accumulators>&gt; ** &lt;<broadcast, broadcast variables>&gt;</p> </li> <li> <p>Accessing services, e.g. &lt;<statusstore, appstatusstore>&gt;, &lt;<taskscheduler, taskscheduler>&gt;, xref:scheduler:LiveListenerBus.adoc[], xref:storage:BlockManager.adoc[BlockManager], xref:scheduler:SchedulerBackend.adoc[SchedulerBackends], xref:shuffle:ShuffleManager.adoc[ShuffleManager] and the &lt;<cleaner, optional contextcleaner>&gt;.</p> </li> <li> <p>&lt;<runjob, running jobs synchronously>&gt;</p> </li> <li>&lt;<submitjob, submitting jobs asynchronously>&gt;</li> <li>&lt;<canceljob, cancelling a job>&gt;</li> <li>&lt;<cancelstage, cancelling a stage>&gt;</li> <li>&lt;<custom-schedulers, assigning custom scheduler backend, taskscheduler and dagscheduler>&gt;</li> <li>&lt;<closure-cleaning, closure cleaning>&gt;</li> <li>&lt;<getpersistentrdds, accessing persistent rdds>&gt;</li> <li>&lt;<unpersist, unpersisting rdds, i.e. marking rdds as non-persistent>&gt;</li> <li>&lt;<addsparklistener, registering sparklistener>&gt;</li> <li>&lt;<dynamic-allocation, programmable dynamic allocation>&gt;</li> </ul> <p>TIP: Read the scaladoc of <a href=http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.SparkContext[org.apache.spark.SparkContext>http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.SparkContext[org.apache.spark.SparkContext</a>].</p> <p>== [[addFile]] addFile Method</p> <h2 id=source-scala_1>[source, scala]<a class=headerlink href=#source-scala_1 title="Permanent link">&para;</a></h2> <p>addFile( path: String): Unit // &lt;1&gt; addFile( path: String, recursive: Boolean): Unit</p> <hr> <p>&lt;1&gt; <code>recursive</code> flag is off</p> <p><code>addFile</code> adds the <code>path</code> file to be downloaded...FIXME</p> <h1 id=note>[NOTE]<a class=headerlink href=#note title="Permanent link">&para;</a></h1> <p><code>addFile</code> is used when:</p> <ul> <li> <p>SparkContext is link:spark-SparkContext-creating-instance-internals.adoc#files[initialized] (and <code>files</code> were defined)</p> </li> <li> <p>Spark SQL's <code>AddFileCommand</code> is executed</p> </li> </ul> <h1 id=spark-sqls-sessionresourceloader-is-requested-to-load-a-file-resource>* Spark SQL's <code>SessionResourceLoader</code> is requested to load a file resource<a class=headerlink href=#spark-sqls-sessionresourceloader-is-requested-to-load-a-file-resource title="Permanent link">&para;</a></h1> <p>== [[unpersistRDD]] Removing RDD Blocks from BlockManagerMaster -- <code>unpersistRDD</code> Internal Method</p> <h2 id=source-scala_2>[source, scala]<a class=headerlink href=#source-scala_2 title="Permanent link">&para;</a></h2> <h2 id=unpersistrddrddid-int-blocking-boolean-true-unit>unpersistRDD(rddId: Int, blocking: Boolean = true): Unit<a class=headerlink href=#unpersistrddrddid-int-blocking-boolean-true-unit title="Permanent link">&para;</a></h2> <p><code>unpersistRDD</code> requests <code>BlockManagerMaster</code> to xref:storage:BlockManagerMaster.adoc#removeRdd[remove the blocks for the RDD] (given <code>rddId</code>).</p> <p>NOTE: <code>unpersistRDD</code> uses <code>SparkEnv</code> xref:core:SparkEnv.adoc#blockManager[to access the current <code>BlockManager</code>] that is in turn used to xref:storage:BlockManager.adoc#master[access the current <code>BlockManagerMaster</code>].</p> <p><code>unpersistRDD</code> removes <code>rddId</code> from &lt;<persistentrdds, persistentrdds>&gt; registry.</p> <p>In the end, <code>unpersistRDD</code> posts a xref:ROOT:SparkListener.adoc#SparkListenerUnpersistRDD[SparkListenerUnpersistRDD] (with <code>rddId</code>) to &lt;<listenerbus, livelistenerbus event bus>&gt;.</p> <h1 id=note_1>[NOTE]<a class=headerlink href=#note_1 title="Permanent link">&para;</a></h1> <p><code>unpersistRDD</code> is used when:</p> <ul> <li><code>ContextCleaner</code> does xref:core:ContextCleaner.adoc#doCleanupRDD[doCleanupRDD]</li> <li> <h1 id=sparkcontext-ie-marks-an-rdd-as-non-persistent>SparkContext &lt;<unpersist, unpersists an rdd>&gt; (i.e. marks an RDD as non-persistent)<a class=headerlink href=#sparkcontext-ie-marks-an-rdd-as-non-persistent title="Permanent link">&para;</a></h1> </li> </ul> <p>== [[applicationId]] Unique Identifier of Spark Application -- <code>applicationId</code> Method</p> <p>CAUTION: FIXME</p> <p>== [[postApplicationStart]] <code>postApplicationStart</code> Internal Method</p> <h2 id=source-scala_3>[source, scala]<a class=headerlink href=#source-scala_3 title="Permanent link">&para;</a></h2> <h2 id=postapplicationstart-unit>postApplicationStart(): Unit<a class=headerlink href=#postapplicationstart-unit title="Permanent link">&para;</a></h2> <p><code>postApplicationStart</code>...FIXME</p> <p>NOTE: <code>postApplicationStart</code> is used exclusively while SparkContext is being &lt;<spark-sparkcontext-creating-instance-internals.adoc#postapplicationstart, created>&gt;</p> <p>== [[postApplicationEnd]] <code>postApplicationEnd</code> Method</p> <p>CAUTION: FIXME</p> <p>== [[clearActiveContext]] <code>clearActiveContext</code> Method</p> <p>CAUTION: FIXME</p> <p>== [[getPersistentRDDs]] Accessing persistent RDDs -- <code>getPersistentRDDs</code> Method</p> <h2 id=source-scala_4>[source, scala]<a class=headerlink href=#source-scala_4 title="Permanent link">&para;</a></h2> <h2 id=getpersistentrdds-mapint-rdd_>getPersistentRDDs: Map[Int, RDD[_]]<a class=headerlink href=#getpersistentrdds-mapint-rdd_ title="Permanent link">&para;</a></h2> <p><code>getPersistentRDDs</code> returns the collection of RDDs that have marked themselves as persistent via link:spark-rdd-caching.adoc#cache[cache].</p> <p>Internally, <code>getPersistentRDDs</code> returns &lt;<persistentrdds, persistentrdds>&gt; internal registry.</p> <p>== [[cancelJob]] Cancelling Job -- <code>cancelJob</code> Method</p> <h2 id=source-scala_5>[source, scala]<a class=headerlink href=#source-scala_5 title="Permanent link">&para;</a></h2> <h2 id=canceljobjobid-int>cancelJob(jobId: Int)<a class=headerlink href=#canceljobjobid-int title="Permanent link">&para;</a></h2> <p><code>cancelJob</code> requests <code>DAGScheduler</code> xref:scheduler:DAGScheduler.adoc#cancelJob[to cancel a Spark job].</p> <p>== [[cancelStage]] Cancelling Stage -- <code>cancelStage</code> Methods</p> <h2 id=source-scala_6>[source, scala]<a class=headerlink href=#source-scala_6 title="Permanent link">&para;</a></h2> <p>cancelStage(stageId: Int): Unit cancelStage(stageId: Int, reason: String): Unit</p> <hr> <p><code>cancelStage</code> simply requests <code>DAGScheduler</code> xref:scheduler:DAGScheduler.adoc#cancelJob[to cancel a Spark stage] (with an optional <code>reason</code>).</p> <p>NOTE: <code>cancelStage</code> is used when <code>StagesTab</code> link:spark-webui-StagesTab.adoc#handleKillRequest[handles a kill request] (from a user in web UI).</p> <p>== [[dynamic-allocation]] Programmable Dynamic Allocation</p> <p>SparkContext offers the following methods as the developer API for xref:ROOT:spark-dynamic-allocation.adoc[]:</p> <ul> <li>&lt;<requestexecutors, requestexecutors>&gt;</li> <li>&lt;<killexecutors, killexecutors>&gt;</li> <li>&lt;<requesttotalexecutors, requesttotalexecutors>&gt;</li> <li>(private!) &lt;<getexecutorids, getexecutorids>&gt;</li> </ul> <p>=== [[requestExecutors]] Requesting New Executors -- <code>requestExecutors</code> Method</p> <h2 id=source-scala_7>[source, scala]<a class=headerlink href=#source-scala_7 title="Permanent link">&para;</a></h2> <h2 id=requestexecutorsnumadditionalexecutors-int-boolean>requestExecutors(numAdditionalExecutors: Int): Boolean<a class=headerlink href=#requestexecutorsnumadditionalexecutors-int-boolean title="Permanent link">&para;</a></h2> <p><code>requestExecutors</code> requests <code>numAdditionalExecutors</code> executors from xref:scheduler:CoarseGrainedSchedulerBackend.adoc[CoarseGrainedSchedulerBackend].</p> <p>=== [[killExecutors]] Requesting to Kill Executors -- <code>killExecutors</code> Method</p> <h2 id=source-scala_8>[source, scala]<a class=headerlink href=#source-scala_8 title="Permanent link">&para;</a></h2> <h2 id=killexecutorsexecutorids-seqstring-boolean>killExecutors(executorIds: Seq[String]): Boolean<a class=headerlink href=#killexecutorsexecutorids-seqstring-boolean title="Permanent link">&para;</a></h2> <p>CAUTION: FIXME</p> <p>=== [[requestTotalExecutors]] Requesting Total Executors -- <code>requestTotalExecutors</code> Method</p> <h2 id=source-scala_9>[source, scala]<a class=headerlink href=#source-scala_9 title="Permanent link">&para;</a></h2> <p>requestTotalExecutors( numExecutors: Int, localityAwareTasks: Int, hostToLocalTaskCount: Map[String, Int]): Boolean</p> <hr> <p><code>requestTotalExecutors</code> is a <code>private[spark]</code> method that xref:scheduler:CoarseGrainedSchedulerBackend.adoc#requestTotalExecutors[requests the exact number of executors from a coarse-grained scheduler backend].</p> <p>NOTE: It works for xref:scheduler:CoarseGrainedSchedulerBackend.adoc[coarse-grained scheduler backends] only.</p> <p>When called for other scheduler backends you should see the following WARN message in the logs:</p> <div class=highlight><pre><span></span><code>WARN Requesting executors is only supported in coarse-grained mode
</code></pre></div> <p>=== [[getExecutorIds]] Getting Executor Ids -- <code>getExecutorIds</code> Method</p> <p><code>getExecutorIds</code> is a <code>private[spark]</code> method that is part of link:spark-service-ExecutorAllocationClient.adoc[ExecutorAllocationClient contract]. It simply xref:scheduler:CoarseGrainedSchedulerBackend.adoc#getExecutorIds[passes the call on to the current coarse-grained scheduler backend, i.e. calls <code>getExecutorIds</code>].</p> <p>NOTE: It works for xref:scheduler:CoarseGrainedSchedulerBackend.adoc[coarse-grained scheduler backends] only.</p> <p>When called for other scheduler backends you should see the following WARN message in the logs:</p> <div class=highlight><pre><span></span><code>WARN Requesting executors is only supported in coarse-grained mode
</code></pre></div> <p>CAUTION: FIXME Why does SparkContext implement the method for coarse-grained scheduler backends? Why doesn't SparkContext throw an exception when the method is called? Nobody seems to be using it (!)</p> <p>== [[creating-instance]] Creating SparkContext Instance</p> <p>You can create a SparkContext instance with or without creating a xref:ROOT:SparkConf.adoc[SparkConf] object first.</p> <p>NOTE: You may want to read link:spark-SparkContext-creating-instance-internals.adoc[Inside Creating SparkContext] to learn what happens behind the scenes when SparkContext is created.</p> <p>=== [[getOrCreate]] Getting Existing or Creating New SparkContext -- <code>getOrCreate</code> Methods</p> <h2 id=source-scala_10>[source, scala]<a class=headerlink href=#source-scala_10 title="Permanent link">&para;</a></h2> <p>getOrCreate(): SparkContext getOrCreate(conf: SparkConf): SparkContext</p> <hr> <p><code>getOrCreate</code> methods allow you to get the existing SparkContext or create a new one.</p> <h2 id=source-scala_11>[source, scala]<a class=headerlink href=#source-scala_11 title="Permanent link">&para;</a></h2> <p>import org.apache.spark.SparkContext val sc = SparkContext.getOrCreate()</p> <p>// Using an explicit SparkConf object import org.apache.spark.SparkConf val conf = new SparkConf() .setMaster("local[*]") .setAppName("SparkMe App") val sc = SparkContext.getOrCreate(conf)</p> <hr> <p>The no-param <code>getOrCreate</code> method requires that the two mandatory Spark settings - &lt;<master, master>&gt; and &lt;<appname, application name>&gt; - are specified using link:spark-submit.adoc[spark-submit].</p> <p>=== [[constructors]] Constructors</p> <h2 id=source-scala_12>[source, scala]<a class=headerlink href=#source-scala_12 title="Permanent link">&para;</a></h2> <p>SparkContext() SparkContext(conf: SparkConf) SparkContext(master: String, appName: String, conf: SparkConf) SparkContext( master: String, appName: String, sparkHome: String = null, jars: Seq[String] = Nil, environment: Map[String, String] = Map())</p> <hr> <p>You can create a SparkContext instance using the four constructors.</p> <h2 id=source-scala_13>[source, scala]<a class=headerlink href=#source-scala_13 title="Permanent link">&para;</a></h2> <p>import org.apache.spark.SparkConf val conf = new SparkConf() .setMaster("local[*]") .setAppName("SparkMe App")</p> <p>import org.apache.spark.SparkContext val sc = new SparkContext(conf)</p> <hr> <p>When a Spark context starts up you should see the following INFO in the logs (amongst the other messages that come from the Spark services):</p> <div class=highlight><pre><span></span><code>INFO SparkContext: Running Spark version 2.0.0-SNAPSHOT
</code></pre></div> <p>NOTE: Only one SparkContext may be running in a single JVM (check out <a href=https://issues.apache.org/jira/browse/SPARK-2243[SPARK-2243>https://issues.apache.org/jira/browse/SPARK-2243[SPARK-2243</a> Support multiple SparkContexts in the same JVM]). Sharing access to a SparkContext in the JVM is the solution to share data within Spark (without relying on other means of data sharing using external data stores).</p> <p>== [[env]] Accessing Current SparkEnv -- <code>env</code> Method</p> <p>CAUTION: FIXME</p> <p>== [[getConf]] Getting Current SparkConf -- <code>getConf</code> Method</p> <h2 id=source-scala_14>[source, scala]<a class=headerlink href=#source-scala_14 title="Permanent link">&para;</a></h2> <h2 id=getconf-sparkconf>getConf: SparkConf<a class=headerlink href=#getconf-sparkconf title="Permanent link">&para;</a></h2> <p><code>getConf</code> returns the current xref:ROOT:SparkConf.adoc[SparkConf].</p> <p>NOTE: Changing the <code>SparkConf</code> object does not change the current configuration (as the method returns a copy).</p> <p>== [[master]][[master-url]] Deployment Environment -- <code>master</code> Method</p> <h2 id=source-scala_15>[source, scala]<a class=headerlink href=#source-scala_15 title="Permanent link">&para;</a></h2> <h2 id=master-string>master: String<a class=headerlink href=#master-string title="Permanent link">&para;</a></h2> <p><code>master</code> method returns the current value of xref:ROOT:configuration-properties.adoc#spark.master[spark.master] which is the link:spark-deployment-environments.adoc[deployment environment] in use.</p> <p>== [[appName]] Application Name -- <code>appName</code> Method</p> <h2 id=source-scala_16>[source, scala]<a class=headerlink href=#source-scala_16 title="Permanent link">&para;</a></h2> <h2 id=appname-string>appName: String<a class=headerlink href=#appname-string title="Permanent link">&para;</a></h2> <p><code>appName</code> gives the value of the mandatory xref:ROOT:SparkConf.adoc#spark.app.name[spark.app.name] setting.</p> <p>NOTE: <code>appName</code> is used when link:spark-standalone.adoc#SparkDeploySchedulerBackend[<code>SparkDeploySchedulerBackend</code> starts], link:spark-webui-SparkUI.adoc#createLiveUI[<code>SparkUI</code> creates a web UI], when <code>postApplicationStart</code> is executed, and for Mesos and checkpointing in Spark Streaming.</p> <p>== [[applicationAttemptId]] Unique Identifier of Execution Attempt -- <code>applicationAttemptId</code> Method</p> <h2 id=source-scala_17>[source, scala]<a class=headerlink href=#source-scala_17 title="Permanent link">&para;</a></h2> <h2 id=applicationattemptid-optionstring>applicationAttemptId: Option[String]<a class=headerlink href=#applicationattemptid-optionstring title="Permanent link">&para;</a></h2> <p><code>applicationAttemptId</code> gives the unique identifier of the execution attempt of a Spark application.</p> <h1 id=note_2>[NOTE]<a class=headerlink href=#note_2 title="Permanent link">&para;</a></h1> <p><code>applicationAttemptId</code> is used when:</p> <ul> <li>xref:scheduler:ShuffleMapTask.adoc#creating-instance[ShuffleMapTask] and xref:scheduler:ResultTask.adoc#creating-instance[ResultTask] are created</li> </ul> <h1 id=sparkcontext>* SparkContext &lt;<postapplicationstart, announces that a spark application has started>&gt;<a class=headerlink href=#sparkcontext title="Permanent link">&para;</a></h1> <p>== [[getExecutorStorageStatus]] Storage Status (of All BlockManagers) -- <code>getExecutorStorageStatus</code> Method</p> <h2 id=source-scala_18>[source, scala]<a class=headerlink href=#source-scala_18 title="Permanent link">&para;</a></h2> <h2 id=getexecutorstoragestatus-arraystoragestatus>getExecutorStorageStatus: Array[StorageStatus]<a class=headerlink href=#getexecutorstoragestatus-arraystoragestatus title="Permanent link">&para;</a></h2> <p><code>getExecutorStorageStatus</code> xref:storage:BlockManagerMaster.adoc#getStorageStatus[requests <code>BlockManagerMaster</code> for storage status] (of all xref:storage:BlockManager.adoc[BlockManagers]).</p> <p>NOTE: <code>getExecutorStorageStatus</code> is a developer API.</p> <h1 id=note_3>[NOTE]<a class=headerlink href=#note_3 title="Permanent link">&para;</a></h1> <p><code>getExecutorStorageStatus</code> is used when:</p> <ul> <li>SparkContext &lt;<getrddstorageinfo, is requested for storage status of cached rdds>&gt;</li> </ul> <h1 id=sparkstatustracker-linkspark-sparkcontext-sparkstatustrackeradocgetexecutorinfosis-requested-for-information-about-all-known-executors>* <code>SparkStatusTracker</code> link:spark-sparkcontext-SparkStatusTracker.adoc#getExecutorInfos[is requested for information about all known executors]<a class=headerlink href=#sparkstatustracker-linkspark-sparkcontext-sparkstatustrackeradocgetexecutorinfosis-requested-for-information-about-all-known-executors title="Permanent link">&para;</a></h1> <p>== [[deployMode]] Deploy Mode -- <code>deployMode</code> Method</p> <h2 id=sourcescala>[source,scala]<a class=headerlink href=#sourcescala title="Permanent link">&para;</a></h2> <h2 id=deploymode-string>deployMode: String<a class=headerlink href=#deploymode-string title="Permanent link">&para;</a></h2> <p><code>deployMode</code> returns the current value of link:spark-deploy-mode.adoc[spark.submit.deployMode] setting or <code>client</code> if not set.</p> <p>== [[getSchedulingMode]] Scheduling Mode -- <code>getSchedulingMode</code> Method</p> <h2 id=source-scala_19>[source, scala]<a class=headerlink href=#source-scala_19 title="Permanent link">&para;</a></h2> <h2 id=getschedulingmode-schedulingmodeschedulingmode>getSchedulingMode: SchedulingMode.SchedulingMode<a class=headerlink href=#getschedulingmode-schedulingmodeschedulingmode title="Permanent link">&para;</a></h2> <p><code>getSchedulingMode</code> returns the current link:spark-scheduler-SchedulingMode.adoc[Scheduling Mode].</p> <p>== [[getPoolForName]] Schedulable (Pool) by Name -- <code>getPoolForName</code> Method</p> <h2 id=source-scala_20>[source, scala]<a class=headerlink href=#source-scala_20 title="Permanent link">&para;</a></h2> <h2 id=getpoolfornamepool-string-optionschedulable>getPoolForName(pool: String): Option[Schedulable]<a class=headerlink href=#getpoolfornamepool-string-optionschedulable title="Permanent link">&para;</a></h2> <p><code>getPoolForName</code> returns a link:spark-scheduler-Schedulable.adoc[Schedulable] by the <code>pool</code> name, if one exists.</p> <p>NOTE: <code>getPoolForName</code> is part of the Developer's API and may change in the future.</p> <p>Internally, it requests the xref:scheduler:TaskScheduler.adoc#rootPool[TaskScheduler for the root pool] and link:spark-scheduler-Pool.adoc#schedulableNameToSchedulable[looks up the <code>Schedulable</code> by the <code>pool</code> name].</p> <p>It is exclusively used to link:spark-webui-PoolPage.adoc[show pool details in web UI (for a stage)].</p> <p>== [[getAllPools]] All Schedulable Pools -- <code>getAllPools</code> Method</p> <h2 id=source-scala_21>[source, scala]<a class=headerlink href=#source-scala_21 title="Permanent link">&para;</a></h2> <h2 id=getallpools-seqschedulable>getAllPools: Seq[Schedulable]<a class=headerlink href=#getallpools-seqschedulable title="Permanent link">&para;</a></h2> <p><code>getAllPools</code> collects the link:spark-scheduler-Pool.adoc[Pools] in xref:scheduler:TaskScheduler.adoc#contract[TaskScheduler.rootPool].</p> <p>NOTE: <code>TaskScheduler.rootPool</code> is part of the xref:scheduler:TaskScheduler.adoc#contract[TaskScheduler Contract].</p> <p>NOTE: <code>getAllPools</code> is part of the Developer's API.</p> <p>CAUTION: FIXME Where is the method used?</p> <p>NOTE: <code>getAllPools</code> is used to calculate pool names for link:spark-webui-AllStagesPage.adoc#pool-names[Stages tab in web UI] with FAIR scheduling mode used.</p> <p>== [[defaultParallelism]] Default Level of Parallelism</p> <h2 id=source-scala_22>[source, scala]<a class=headerlink href=#source-scala_22 title="Permanent link">&para;</a></h2> <h2 id=defaultparallelism-int>defaultParallelism: Int<a class=headerlink href=#defaultparallelism-int title="Permanent link">&para;</a></h2> <p><code>defaultParallelism</code> requests &lt;<taskscheduler, taskscheduler>&gt; for the xref:scheduler:TaskScheduler.adoc#defaultParallelism[default level of parallelism].</p> <p>NOTE: <em>Default level of parallelism</em> specifies the number of link:spark-rdd-partitions.adoc[partitions] in RDDs when created without specifying them explicitly by a user.</p> <h1 id=note_4>[NOTE]<a class=headerlink href=#note_4 title="Permanent link">&para;</a></h1> <p><code>defaultParallelism</code> is used in &lt;<parallelize, sparkcontext.parallelize>&gt;, <code>SparkContext.range</code> and &lt;<makerdd, sparkcontext.makerdd>&gt; (as well as Spark Streaming's <code>DStream.countByValue</code> and <code>DStream.countByValueAndWindow</code> et al.).</p> <h1 id=defaultparallelism-is-also-used-to-instantiate-xrefrddhashpartitioneradochashpartitioner-and-for-the-minimum-number-of-partitions-in-xrefrddspark-rdd-hadooprddadochadooprdds><code>defaultParallelism</code> is also used to instantiate xref:rdd:HashPartitioner.adoc[HashPartitioner] and for the minimum number of partitions in xref:rdd:spark-rdd-HadoopRDD.adoc[HadoopRDDs].<a class=headerlink href=#defaultparallelism-is-also-used-to-instantiate-xrefrddhashpartitioneradochashpartitioner-and-for-the-minimum-number-of-partitions-in-xrefrddspark-rdd-hadooprddadochadooprdds title="Permanent link">&para;</a></h1> <p>== [[taskScheduler]] Current Spark Scheduler (aka TaskScheduler) -- <code>taskScheduler</code> Property</p> <h2 id=source-scala_23>[source, scala]<a class=headerlink href=#source-scala_23 title="Permanent link">&para;</a></h2> <p>taskScheduler: TaskScheduler taskScheduler_=(ts: TaskScheduler): Unit</p> <hr> <p><code>taskScheduler</code> manages (i.e. reads or writes) &lt;&lt;_taskScheduler, _taskScheduler&gt;&gt; internal property.</p> <p>== [[version]] Getting Spark Version -- <code>version</code> Property</p> <h2 id=source-scala_24>[source, scala]<a class=headerlink href=#source-scala_24 title="Permanent link">&para;</a></h2> <h2 id=version-string>version: String<a class=headerlink href=#version-string title="Permanent link">&para;</a></h2> <p><code>version</code> returns the Spark version this SparkContext uses.</p> <p>== [[makeRDD]] <code>makeRDD</code> Method</p> <p>CAUTION: FIXME</p> <p>== [[submitJob]] Submitting Jobs Asynchronously -- <code>submitJob</code> Method</p> <h2 id=source-scala_25>[source, scala]<a class=headerlink href=#source-scala_25 title="Permanent link">&para;</a></h2> <p>submitJob<a href="rdd: RDD[T],
  processPartition: Iterator[T] => U,
  partitions: Seq[Int],
  resultHandler: (Int, U) => Unit,
  resultFunc: => R">T, U, R</a>: SimpleFutureAction[R]</p> <hr> <p><code>submitJob</code> submits a job in an asynchronous, non-blocking way to xref:scheduler:DAGScheduler.adoc#submitJob[DAGScheduler].</p> <p>It cleans the <code>processPartition</code> input function argument and returns an instance of link:spark-rdd-actions.adoc#FutureAction[SimpleFutureAction] that holds the xref:scheduler:spark-scheduler-JobWaiter.adoc[JobWaiter] instance.</p> <p>CAUTION: FIXME What are <code>resultFunc</code>?</p> <p>It is used in:</p> <ul> <li>link:spark-rdd-actions.adoc#AsyncRDDActions[AsyncRDDActions] methods</li> <li>link:spark-streaming/spark-streaming.adoc[Spark Streaming] for link:spark-streaming/spark-streaming-receivertracker.adoc#ReceiverTrackerEndpoint-startReceiver[ReceiverTrackerEndpoint.startReceiver]</li> </ul> <p>== [[spark-configuration]] Spark Configuration</p> <p>CAUTION: FIXME</p> <p>== [[sparkcontext-and-rdd]] SparkContext and RDDs</p> <p>You use a Spark context to create RDDs (see &lt;<creating-rdds, creating rdd>&gt;).</p> <p>When an RDD is created, it belongs to and is completely owned by the Spark context it originated from. RDDs can't by design be shared between SparkContexts.</p> <p>.A Spark context creates a living space for RDDs. image::diagrams/sparkcontext-rdds.png[align="center"]</p> <p>== [[creating-rdds]][[parallelize]] Creating RDD -- <code>parallelize</code> Method</p> <p>SparkContext allows you to create many different RDDs from input sources like:</p> <ul> <li>Scala's collections, i.e. <code>sc.parallelize(0 to 100)</code></li> <li>local or remote filesystems, i.e. <code>sc.textFile("README.md")</code></li> <li>Any Hadoop <code>InputSource</code> using <code>sc.newAPIHadoopFile</code></li> </ul> <p>Read xref:rdd:index.adoc#creating-rdds[Creating RDDs] in xref:rdd:index.adoc[RDD - Resilient Distributed Dataset].</p> <p>== [[unpersist]] Unpersisting RDD (Marking RDD as Non-Persistent) -- <code>unpersist</code> Method</p> <p>CAUTION: FIXME</p> <p><code>unpersist</code> removes an RDD from the master's xref:storage:BlockManager.adoc[Block Manager] (calls <code>removeRdd(rddId: Int, blocking: Boolean)</code>) and the internal &lt;<persistentrdds, persistentrdds>&gt; mapping.</p> <p>It finally posts xref:ROOT:SparkListener.adoc#SparkListenerUnpersistRDD[SparkListenerUnpersistRDD] message to <code>listenerBus</code>.</p> <p>== [[setCheckpointDir]] Setting Checkpoint Directory -- <code>setCheckpointDir</code> Method</p> <h2 id=source-scala_26>[source, scala]<a class=headerlink href=#source-scala_26 title="Permanent link">&para;</a></h2> <h2 id=setcheckpointdirdirectory-string>setCheckpointDir(directory: String)<a class=headerlink href=#setcheckpointdirdirectory-string title="Permanent link">&para;</a></h2> <p><code>setCheckpointDir</code> method is used to set up the checkpoint directory...FIXME</p> <p>CAUTION: FIXME</p> <p>== [[register]] Registering Accumulator -- <code>register</code> Methods</p> <h2 id=source-scala_27>[source, scala]<a class=headerlink href=#source-scala_27 title="Permanent link">&para;</a></h2> <p>register(acc: AccumulatorV2[<em>, _]): Unit register(acc: AccumulatorV2[</em>, _], name: String): Unit</p> <hr> <p><code>register</code> registers the <code>acc</code> link:spark-accumulators.adoc[accumulator]. You can optionally give an accumulator a <code>name</code>.</p> <p>TIP: You can create built-in accumulators for longs, doubles, and collection types using &lt;<creating-accumulators, specialized methods>&gt;.</p> <p>Internally, <code>register</code> link:spark-accumulators.adoc#register[registers <code>acc</code> accumulator] (with the current SparkContext).</p> <p>== [[creating-accumulators]][[longAccumulator]][[doubleAccumulator]][[collectionAccumulator]] Creating Built-In Accumulators</p> <h2 id=source-scala_28>[source, scala]<a class=headerlink href=#source-scala_28 title="Permanent link">&para;</a></h2> <p>longAccumulator: LongAccumulator longAccumulator(name: String): LongAccumulator doubleAccumulator: DoubleAccumulator doubleAccumulator(name: String): DoubleAccumulator collectionAccumulator[T]: CollectionAccumulator[T] collectionAccumulator<a href="name: String">T</a>: CollectionAccumulator[T]</p> <hr> <p>You can use <code>longAccumulator</code>, <code>doubleAccumulator</code> or <code>collectionAccumulator</code> to create and register link:spark-accumulators.adoc[accumulators] for simple and collection values.</p> <p><code>longAccumulator</code> returns link:spark-accumulators.adoc#LongAccumulator[LongAccumulator] with the zero value <code>0</code>.</p> <p><code>doubleAccumulator</code> returns link:spark-accumulators.adoc#DoubleAccumulator[DoubleAccumulator] with the zero value <code>0.0</code>.</p> <p><code>collectionAccumulator</code> returns link:spark-accumulators.adoc#CollectionAccumulator[CollectionAccumulator] with the zero value <code>java.util.List[T]</code>.</p> <h2 id=source-scala_29>[source, scala]<a class=headerlink href=#source-scala_29 title="Permanent link">&para;</a></h2> <p>scala&gt; val acc = sc.longAccumulator acc: org.apache.spark.util.LongAccumulator = LongAccumulator(id: 0, name: None, value: 0)</p> <p>scala&gt; val counter = sc.longAccumulator("counter") counter: org.apache.spark.util.LongAccumulator = LongAccumulator(id: 1, name: Some(counter), value: 0)</p> <p>scala&gt; counter.value res0: Long = 0</p> <p>scala&gt; sc.parallelize(0 to 9).foreach(n =&gt; counter.add(n))</p> <p>scala&gt; counter.value res3: Long = 45</p> <hr> <p>The <code>name</code> input parameter allows you to give a name to an accumulator and have it displayed in link:spark-webui-StagePage.adoc#accumulators[Spark UI] (under Stages tab for a given stage).</p> <p>.Accumulators in the Spark UI image::spark-webui-accumulators.png[align="center"]</p> <p>TIP: You can register custom accumulators using &lt;<register, register>&gt; methods.</p> <p>== [[broadcast]] Creating Broadcast Variable -- broadcast Method</p> <h2 id=source-scala_30>[source, scala]<a class=headerlink href=#source-scala_30 title="Permanent link">&para;</a></h2> <p>broadcast<a href="value: T">T</a>: Broadcast[T]</p> <hr> <p>broadcast method creates a xref:ROOT:Broadcast.adoc[]. It is a shared memory with <code>value</code> (as broadcast blocks) on the driver and later on all Spark executors.</p> <h2 id=sourceplaintext>[source,plaintext]<a class=headerlink href=#sourceplaintext title="Permanent link">&para;</a></h2> <p>val sc: SparkContext = ??? scala&gt; val hello = sc.broadcast("hello") hello: org.apache.spark.broadcast.Broadcast[String] = Broadcast(0)</p> <hr> <p>Spark transfers the value to Spark executors <em>once</em>, and tasks can share it without incurring repetitive network transmissions when the broadcast variable is used multiple times.</p> <p>.Broadcasting a value to executors image::sparkcontext-broadcast-executors.png[align="center"]</p> <p>Internally, broadcast requests BroadcastManager for a xref:core:BroadcastManager.adoc#newBroadcast[new broadcast variable].</p> <p>NOTE: The current <code>BroadcastManager</code> is available using xref:core:SparkEnv.adoc#broadcastManager[<code>SparkEnv.broadcastManager</code>] attribute and is always xref:core:BroadcastManager.adoc[BroadcastManager] (with few internal configuration changes to reflect where it runs, i.e. inside the driver or executors).</p> <p>You should see the following INFO message in the logs:</p> <div class=highlight><pre><span></span><code>Created broadcast [id] from [callSite]
</code></pre></div> <p>If <code>ContextCleaner</code> is defined, the xref:core:ContextCleaner.adoc#[new broadcast variable is registered for cleanup].</p> <h1 id=note_5>[NOTE]<a class=headerlink href=#note_5 title="Permanent link">&para;</a></h1> <p>Spark does not support broadcasting RDDs.</p> <h1 id=scala-scbroadcastscrange0-10-javalangillegalargumentexception-requirement-failed-can-not-directly-broadcast-rdds-instead-call-collect-and-broadcast-the-result-at-scalapredefrequirepredefscala224-at-orgapachesparksparkcontextbroadcastsparkcontextscala1392-48-elided><div class=highlight><pre><span></span><code>scala&gt; sc.broadcast(sc.range(0, 10))
java.lang.IllegalArgumentException: requirement failed: Can not directly broadcast RDDs; instead, call collect() and broadcast the result.
  at scala.Predef$.require(Predef.scala:224)
  at org.apache.spark.SparkContext.broadcast(SparkContext.scala:1392)
  ... 48 elided
</code></pre></div><a class=headerlink href=#scala-scbroadcastscrange0-10-javalangillegalargumentexception-requirement-failed-can-not-directly-broadcast-rdds-instead-call-collect-and-broadcast-the-result-at-scalapredefrequirepredefscala224-at-orgapachesparksparkcontextbroadcastsparkcontextscala1392-48-elided title="Permanent link">&para;</a></h1> <p>Once created, the broadcast variable (and other blocks) are displayed per executor and the driver in web UI (under link:spark-webui-executors.adoc[Executors tab]).</p> <p>.Broadcast Variables In web UI's Executors Tab image::spark-broadcast-webui-executors-rdd-blocks.png[align="center"]</p> <p>== [[jars]] Distribute JARs to workers</p> <p>The jar you specify with <code>SparkContext.addJar</code> will be copied to all the worker nodes.</p> <p>The configuration setting <code>spark.jars</code> is a comma-separated list of jar paths to be included in all tasks executed from this SparkContext. A path can either be a local file, a file in HDFS (or other Hadoop-supported filesystems), an HTTP, HTTPS or FTP URI, or <code>local:/path</code> for a file on every worker node.</p> <div class=highlight><pre><span></span><code>scala&gt; sc.addJar(&quot;build.sbt&quot;)
15/11/11 21:54:54 INFO SparkContext: Added JAR build.sbt at http://192.168.1.4:49427/jars/build.sbt with timestamp 1447275294457
</code></pre></div> <p>CAUTION: FIXME Why is HttpFileServer used for addJar?</p> <p>=== SparkContext as Application-Wide Counter</p> <p>SparkContext keeps track of:</p> <p>[[nextShuffleId]] * shuffle ids using <code>nextShuffleId</code> internal counter for xref:scheduler:ShuffleMapStage.adoc[registering shuffle dependencies] to xref:shuffle:ShuffleManager.adoc[Shuffle Service].</p> <p>== [[runJob]] Running Job Synchronously</p> <p>xref:rdd:index.adoc#actions[RDD actions] run link:spark-scheduler-ActiveJob.adoc[jobs] using one of <code>runJob</code> methods.</p> <h2 id=source-scala_31>[source, scala]<a class=headerlink href=#source-scala_31 title="Permanent link">&para;</a></h2> <p>runJob<a href="rdd: RDD[T],
  func: (TaskContext, Iterator[T]) => U,
  partitions: Seq[Int],
  resultHandler: (Int, U) => Unit">T, U</a>: Unit runJob<a href="rdd: RDD[T],
  func: (TaskContext, Iterator[T]) => U,
  partitions: Seq[Int]">T, U</a>: Array[U] runJob<a href="rdd: RDD[T],
  func: Iterator[T] => U,
  partitions: Seq[Int]">T, U</a>: Array[U] runJob<a href="rdd: RDD[T], func: (TaskContext, Iterator[T]) => U">T, U</a>: Array[U] runJob<a href="rdd: RDD[T], func: Iterator[T] => U">T, U</a>: Array[U] runJob<a href="rdd: RDD[T],
  processPartition: (TaskContext, Iterator[T]) => U,
  resultHandler: (Int, U) => Unit">T, U</a> runJob<a href="rdd: RDD[T],
  processPartition: Iterator[T] => U,
  resultHandler: (Int, U) => Unit">T, U: ClassTag</a></p> <hr> <p><code>runJob</code> executes a function on one or many partitions of a RDD (in a SparkContext space) to produce a collection of values per partition.</p> <p>NOTE: <code>runJob</code> can only work when a SparkContext is <em>not</em> &lt;<stop, stopped>&gt;.</p> <p>Internally, <code>runJob</code> first makes sure that the SparkContext is not &lt;<stop, stopped>&gt;. If it is, you should see the following <code>IllegalStateException</code> exception in the logs:</p> <div class=highlight><pre><span></span><code>java.lang.IllegalStateException: SparkContext has been shutdown
  at org.apache.spark.SparkContext.runJob(SparkContext.scala:1893)
  at org.apache.spark.SparkContext.runJob(SparkContext.scala:1914)
  at org.apache.spark.SparkContext.runJob(SparkContext.scala:1934)
  ... 48 elided
</code></pre></div> <p><code>runJob</code> then &lt;<getcallsite, calculates the call site>&gt; and &lt;<clean, cleans a &lt;code>func</code> closure>&gt;.</p> <p>You should see the following INFO message in the logs:</p> <div class=highlight><pre><span></span><code>INFO SparkContext: Starting job: [callSite]
</code></pre></div> <p>With link:spark-rdd-lineage.adoc#spark_logLineage[spark.logLineage] enabled (which is not by default), you should see the following INFO message with link:spark-rdd-lineage.adoc#toDebugString[toDebugString] (executed on <code>rdd</code>):</p> <div class=highlight><pre><span></span><code>INFO SparkContext: RDD&#39;s recursive dependencies:
[toDebugString]
</code></pre></div> <p><code>runJob</code> requests xref:scheduler:DAGScheduler.adoc#runJob[<code>DAGScheduler</code> to run a job].</p> <p>TIP: <code>runJob</code> just prepares input parameters for xref:scheduler:DAGScheduler.adoc#runJob[<code>DAGScheduler</code> to run a job].</p> <p>After <code>DAGScheduler</code> is done and the job has finished, <code>runJob</code> link:spark-sparkcontext-ConsoleProgressBar.adoc#finishAll[stops <code>ConsoleProgressBar</code>] and xref:ROOT:rdd-checkpointing.adoc#doCheckpoint[performs RDD checkpointing of <code>rdd</code>].</p> <p>TIP: For some actions, e.g. <code>first()</code> and <code>lookup()</code>, there is no need to compute all the partitions of the RDD in a job. And Spark knows it.</p> <h2 id=sourcescala_1>[source,scala]<a class=headerlink href=#sourcescala_1 title="Permanent link">&para;</a></h2> <p>// RDD to work with val lines = sc.parallelize(Seq("hello world", "nice to see you"))</p> <p>import org.apache.spark.TaskContext scala&gt; sc.runJob(lines, (t: TaskContext, i: Iterator[String]) =&gt; 1) // &lt;1&gt; res0: Array[Int] = Array(1, 1) // &lt;2&gt;</p> <hr> <p>&lt;1&gt; Run a job using <code>runJob</code> on <code>lines</code> RDD with a function that returns 1 for every partition (of <code>lines</code> RDD). &lt;2&gt; What can you say about the number of partitions of the <code>lines</code> RDD? Is your result <code>res0</code> different than mine? Why?</p> <p>TIP: Read link:spark-TaskContext.adoc[TaskContext].</p> <p>Running a job is essentially executing a <code>func</code> function on all or a subset of partitions in an <code>rdd</code> RDD and returning the result as an array (with elements being the results per partition).</p> <p>.Executing action image::spark-runjob.png[align="center"]</p> <p>== [[stop]][[stopping]] Stopping SparkContext -- <code>stop</code> Method</p> <h2 id=source-scala_32>[source, scala]<a class=headerlink href=#source-scala_32 title="Permanent link">&para;</a></h2> <h2 id=stop-unit>stop(): Unit<a class=headerlink href=#stop-unit title="Permanent link">&para;</a></h2> <p><code>stop</code> stops the SparkContext.</p> <p>Internally, <code>stop</code> enables <code>stopped</code> internal flag. If already stopped, you should see the following INFO message in the logs:</p> <div class=highlight><pre><span></span><code>INFO SparkContext: SparkContext already stopped.
</code></pre></div> <p><code>stop</code> then does the following:</p> <ol> <li>Removes <code>_shutdownHookRef</code> from <code>ShutdownHookManager</code></li> <li>&lt;<postapplicationend, posts a &lt;code>SparkListenerApplicationEnd</code>>&gt; (to &lt;<listenerbus, livelistenerbus event bus>&gt;)</li> <li>link:spark-webui-SparkUI.adoc#stop[Stops web UI]</li> <li>link:spark-metrics-MetricsSystem.adoc#report[Requests <code>MetricSystem</code> to report metrics] (from all registered sinks)</li> <li>xref:core:ContextCleaner.adoc#stop[Stops <code>ContextCleaner</code>]</li> <li>link:spark-ExecutorAllocationManager.adoc#stop[Requests <code>ExecutorAllocationManager</code> to stop]</li> <li>If <code>LiveListenerBus</code> was started, xref:scheduler:LiveListenerBus.adoc#stop[requests <code>LiveListenerBus</code> to stop]</li> <li>Requests xref:spark-history-server:EventLoggingListener.adoc#stop[<code>EventLoggingListener</code> to stop]</li> <li>Requests xref:scheduler:DAGScheduler.adoc#stop[<code>DAGScheduler</code> to stop]</li> <li>Requests xref:rpc:index.adoc#stop[RpcEnv to stop <code>HeartbeatReceiver</code> endpoint]</li> <li>Requests link:spark-sparkcontext-ConsoleProgressBar.adoc#stop[<code>ConsoleProgressBar</code> to stop]</li> <li>Clears the reference to <code>TaskScheduler</code>, i.e. <code>_taskScheduler</code> is <code>null</code></li> <li>Requests xref:core:SparkEnv.adoc#stop[<code>SparkEnv</code> to stop] and clears <code>SparkEnv</code></li> <li>Clears link:yarn/spark-yarn-client.adoc#SPARK_YARN_MODE[<code>SPARK_YARN_MODE</code> flag]</li> <li>&lt;<clearactivecontext, clears an active sparkcontext>&gt;</li> </ol> <p>Ultimately, you should see the following INFO message in the logs:</p> <div class=highlight><pre><span></span><code>INFO SparkContext: Successfully stopped SparkContext
</code></pre></div> <p>== [[addSparkListener]] Registering SparkListener -- <code>addSparkListener</code> Method</p> <h2 id=source-scala_33>[source, scala]<a class=headerlink href=#source-scala_33 title="Permanent link">&para;</a></h2> <h2 id=addsparklistenerlistener-sparklistenerinterface-unit>addSparkListener(listener: SparkListenerInterface): Unit<a class=headerlink href=#addsparklistenerlistener-sparklistenerinterface-unit title="Permanent link">&para;</a></h2> <p>You can register a custom xref:ROOT:SparkListener.adoc#SparkListenerInterface[SparkListenerInterface] using <code>addSparkListener</code> method</p> <p>NOTE: You can also register custom listeners using xref:ROOT:configuration-properties.adoc#spark.extraListeners[spark.extraListeners] configuration property.</p> <p>== [[custom-schedulers]] Custom SchedulerBackend, TaskScheduler and DAGScheduler</p> <p>By default, SparkContext uses (<code>private[spark]</code> class) <code>org.apache.spark.scheduler.DAGScheduler</code>, but you can develop your own custom DAGScheduler implementation, and use (<code>private[spark]</code>) <code>SparkContext.dagScheduler_=(ds: DAGScheduler)</code> method to assign yours.</p> <p>It is also applicable to <code>SchedulerBackend</code> and <code>TaskScheduler</code> using <code>schedulerBackend_=(sb: SchedulerBackend)</code> and <code>taskScheduler_=(ts: TaskScheduler)</code> methods, respectively.</p> <p>CAUTION: FIXME Make it an advanced exercise.</p> <p>== [[events]] Events</p> <p>When a Spark context starts, it triggers xref:ROOT:SparkListener.adoc#SparkListenerEnvironmentUpdate[SparkListenerEnvironmentUpdate] and xref:ROOT:SparkListener.adoc#SparkListenerApplicationStart[SparkListenerApplicationStart] messages.</p> <p>Refer to the section &lt;<creating-instance, sparkcontext&#x27;s initialization>&gt;.</p> <p>== [[setLogLevel]][[setting-default-log-level]] Setting Default Logging Level -- <code>setLogLevel</code> Method</p> <h2 id=source-scala_34>[source, scala]<a class=headerlink href=#source-scala_34 title="Permanent link">&para;</a></h2> <h2 id=setloglevelloglevel-string>setLogLevel(logLevel: String)<a class=headerlink href=#setloglevelloglevel-string title="Permanent link">&para;</a></h2> <p><code>setLogLevel</code> allows you to set the root logging level in a Spark application, e.g. link:spark-shell.adoc[Spark shell].</p> <p>Internally, <code>setLogLevel</code> calls link:++<a href=http://logging.apache.org/log4j/2.x/log4j-api/apidocs/org/apache/logging/log4j/Level.html#toLevel(java.lang.String)++[org.apache.log4j.Level.toLevel(logLevel>http://logging.apache.org/log4j/2.x/log4j-api/apidocs/org/apache/logging/log4j/Level.html#toLevel(java.lang.String)++[org.apache.log4j.Level.toLevel(logLevel</a>)] that it then uses to set using link:++<a href=http://logging.apache.org/log4j/2.x/log4j-api/apidocs/org/apache/logging/log4j/LogManager.html#getRootLogger()++[org.apache.log4j.LogManager.getRootLogger().setLevel(level>http://logging.apache.org/log4j/2.x/log4j-api/apidocs/org/apache/logging/log4j/LogManager.html#getRootLogger()++[org.apache.log4j.LogManager.getRootLogger().setLevel(level</a>)].</p> <h1 id=tip>[TIP]<a class=headerlink href=#tip title="Permanent link">&para;</a></h1> <p>You can directly set the logging level using link:++<a href=http://logging.apache.org/log4j/2.x/log4j-api/apidocs/org/apache/logging/log4j/LogManager.html#getLogger()++[org.apache.log4j.LogManager.getLogger>http://logging.apache.org/log4j/2.x/log4j-api/apidocs/org/apache/logging/log4j/LogManager.html#getLogger()++[org.apache.log4j.LogManager.getLogger</a>()].</p> <h2 id=source-scala_35>[source, scala]<a class=headerlink href=#source-scala_35 title="Permanent link">&para;</a></h2> <h2 id=logmanagergetloggerorgsetlevelleveloff>LogManager.getLogger("org").setLevel(Level.OFF)<a class=headerlink href=#logmanagergetloggerorgsetlevelleveloff title="Permanent link">&para;</a></h2> <p>====</p> <p>== [[clean]][[closure-cleaning]] Closure Cleaning -- <code>clean</code> Method</p> <h2 id=source-scala_36>[source, scala]<a class=headerlink href=#source-scala_36 title="Permanent link">&para;</a></h2> <h2 id=cleanf-f-checkserializable-boolean-true-f>clean(f: F, checkSerializable: Boolean = true): F<a class=headerlink href=#cleanf-f-checkserializable-boolean-true-f title="Permanent link">&para;</a></h2> <p>Every time an action is called, Spark cleans up the closure, i.e. the body of the action, before it is serialized and sent over the wire to executors.</p> <p>SparkContext comes with <code>clean(f: F, checkSerializable: Boolean = true)</code> method that does this. It in turn calls <code>ClosureCleaner.clean</code> method.</p> <p>Not only does <code>ClosureCleaner.clean</code> method clean the closure, but also does it transitively, i.e. referenced closures are cleaned transitively.</p> <p>A closure is considered serializable as long as it does not explicitly reference unserializable objects. It does so by traversing the hierarchy of enclosing closures and null out any references that are not actually used by the starting closure.</p> <h1 id=tip_1>[TIP]<a class=headerlink href=#tip_1 title="Permanent link">&para;</a></h1> <p>Enable <code>DEBUG</code> logging level for <code>org.apache.spark.util.ClosureCleaner</code> logger to see what happens inside the class.</p> <p>Add the following line to <code>conf/log4j.properties</code>:</p> <div class=highlight><pre><span></span><code>log4j.logger.org.apache.spark.util.ClosureCleaner=DEBUG
</code></pre></div> <h1 id=refer-to-linkspark-loggingadoclogging>Refer to link:spark-logging.adoc[Logging].<a class=headerlink href=#refer-to-linkspark-loggingadoclogging title="Permanent link">&para;</a></h1> <p>With <code>DEBUG</code> logging level you should see the following messages in the logs:</p> <div class=highlight><pre><span></span><code>+++ Cleaning closure [func] ([func.getClass.getName]) +++
 + declared fields: [declaredFields.size]
     [field]
 ...
+++ closure [func] ([func.getClass.getName]) is now cleaned +++
</code></pre></div> <p>Serialization is verified using a new instance of <code>Serializer</code> (as xref:core:SparkEnv.adoc#closureSerializer[closure Serializer]). Refer to link:spark-serialization.adoc[Serialization].</p> <p>CAUTION: FIXME an example, please.</p> <p>== [[hadoopConfiguration]] Hadoop Configuration</p> <p>While a &lt;<creating-instance, sparkcontext is being created>&gt;, so is a Hadoop configuration (as an instance of <a href=https://hadoop.apache.org/docs/current/api/org/apache/hadoop/conf/Configuration.html[org.apache.hadoop.conf.Configuration>https://hadoop.apache.org/docs/current/api/org/apache/hadoop/conf/Configuration.html[org.apache.hadoop.conf.Configuration</a>] that is available as <code>_hadoopConfiguration</code>).</p> <p>NOTE: link:spark-SparkHadoopUtil.adoc#newConfiguration[SparkHadoopUtil.get.newConfiguration] is used.</p> <p>If a SparkConf is provided it is used to build the configuration as described. Otherwise, the default <code>Configuration</code> object is returned.</p> <p>If <code>AWS_ACCESS_KEY_ID</code> and <code>AWS_SECRET_ACCESS_KEY</code> are both available, the following settings are set for the Hadoop configuration:</p> <ul> <li><code>fs.s3.awsAccessKeyId</code>, <code>fs.s3n.awsAccessKeyId</code>, <code>fs.s3a.access.key</code> are set to the value of <code>AWS_ACCESS_KEY_ID</code></li> <li><code>fs.s3.awsSecretAccessKey</code>, <code>fs.s3n.awsSecretAccessKey</code>, and <code>fs.s3a.secret.key</code> are set to the value of <code>AWS_SECRET_ACCESS_KEY</code></li> </ul> <p>Every <code>spark.hadoop.</code> setting becomes a setting of the configuration with the prefix <code>spark.hadoop.</code> removed for the key.</p> <p>The value of <code>spark.buffer.size</code> (default: <code>65536</code>) is used as the value of <code>io.file.buffer.size</code>.</p> <p>== [[listenerBus]] <code>listenerBus</code> -- <code>LiveListenerBus</code> Event Bus</p> <p><code>listenerBus</code> is a xref:scheduler:LiveListenerBus.adoc[] object that acts as a mechanism to announce events to other services on the link:spark-driver.adoc[driver].</p> <p>NOTE: It is created and started when link:spark-SparkContext-creating-instance-internals.adoc[SparkContext starts] and, since it is a single-JVM event bus, is exclusively used on the driver.</p> <p>NOTE: <code>listenerBus</code> is a <code>private[spark]</code> value in SparkContext.</p> <p>== [[startTime]] Time when SparkContext was Created -- <code>startTime</code> Property</p> <h2 id=source-scala_37>[source, scala]<a class=headerlink href=#source-scala_37 title="Permanent link">&para;</a></h2> <h2 id=starttime-long>startTime: Long<a class=headerlink href=#starttime-long title="Permanent link">&para;</a></h2> <p><code>startTime</code> is the time in milliseconds when &lt;<creating-instance, sparkcontext was created>&gt;.</p> <h2 id=source-scala_38>[source, scala]<a class=headerlink href=#source-scala_38 title="Permanent link">&para;</a></h2> <p>scala&gt; sc.startTime res0: Long = 1464425605653</p> <hr> <p>== [[sparkUser]] Spark User -- <code>sparkUser</code> Property</p> <h2 id=source-scala_39>[source, scala]<a class=headerlink href=#source-scala_39 title="Permanent link">&para;</a></h2> <h2 id=sparkuser-string>sparkUser: String<a class=headerlink href=#sparkuser-string title="Permanent link">&para;</a></h2> <p><code>sparkUser</code> is the user who started the SparkContext instance.</p> <p>NOTE: It is computed when link:spark-SparkContext-creating-instance-internals.adoc#sparkUser[SparkContext is created] using link:spark-SparkContext-creating-instance-internals.adoc#[Utils.getCurrentUserName].</p> <p>== [[submitMapStage]] Submitting <code>ShuffleDependency</code> for Execution -- <code>submitMapStage</code> Internal Method</p> <h2 id=source-scala_40>[source, scala]<a class=headerlink href=#source-scala_40 title="Permanent link">&para;</a></h2> <p>submitMapStage<a href="dependency: ShuffleDependency[K, V, C]">K, V, C</a>: SimpleFutureAction[MapOutputStatistics]</p> <hr> <p><code>submitMapStage</code> xref:scheduler:DAGScheduler.adoc#submitMapStage[submits the input <code>ShuffleDependency</code> to <code>DAGScheduler</code> for execution] and returns a <code>SimpleFutureAction</code>.</p> <p>Internally, <code>submitMapStage</code> &lt;<getcallsite, calculates the call site>&gt; first and submits it with <code>localProperties</code>.</p> <p>NOTE: Interestingly, <code>submitMapStage</code> is used exclusively when Spark SQL's link:spark-sql-SparkPlan-ShuffleExchange.adoc[ShuffleExchange] physical operator is executed.</p> <p>NOTE: <code>submitMapStage</code> <em>seems</em> related to xref:scheduler:DAGScheduler.adoc#adaptive-query-planning[Adaptive Query Planning / Adaptive Scheduling].</p> <p>== [[getCallSite]] Calculating Call Site -- <code>getCallSite</code> Method</p> <p>CAUTION: FIXME</p> <p>== [[cancelJobGroup]] Cancelling Job Group -- <code>cancelJobGroup</code> Method</p> <h2 id=source-scala_41>[source, scala]<a class=headerlink href=#source-scala_41 title="Permanent link">&para;</a></h2> <h2 id=canceljobgroupgroupid-string>cancelJobGroup(groupId: String)<a class=headerlink href=#canceljobgroupgroupid-string title="Permanent link">&para;</a></h2> <p><code>cancelJobGroup</code> requests <code>DAGScheduler</code> xref:scheduler:DAGScheduler.adoc#cancelJobGroup[to cancel a group of active Spark jobs].</p> <p>NOTE: <code>cancelJobGroup</code> is used exclusively when <code>SparkExecuteStatementOperation</code> does <code>cancel</code>.</p> <p>== [[cancelAllJobs]] Cancelling All Running and Scheduled Jobs -- <code>cancelAllJobs</code> Method</p> <p>CAUTION: FIXME</p> <p>NOTE: <code>cancelAllJobs</code> is used when link:spark-shell.adoc[spark-shell] is terminated (e.g. using Ctrl+C, so it can in turn terminate all active Spark jobs) or <code>SparkSQLCLIDriver</code> is terminated.</p> <p>== [[setJobGroup]] Setting Local Properties to Group Spark Jobs -- <code>setJobGroup</code> Method</p> <h2 id=source-scala_42>[source, scala]<a class=headerlink href=#source-scala_42 title="Permanent link">&para;</a></h2> <p>setJobGroup( groupId: String, description: String, interruptOnCancel: Boolean = false): Unit</p> <hr> <p><code>setJobGroup</code> link:spark-sparkcontext-local-properties.adoc#setLocalProperty[sets local properties]:</p> <ul> <li><code>spark.jobGroup.id</code> as <code>groupId</code></li> <li><code>spark.job.description</code> as <code>description</code></li> <li><code>spark.job.interruptOnCancel</code> as <code>interruptOnCancel</code></li> </ul> <h1 id=note_6>[NOTE]<a class=headerlink href=#note_6 title="Permanent link">&para;</a></h1> <p><code>setJobGroup</code> is used when:</p> <ul> <li>Spark Thrift Server's <code>SparkExecuteStatementOperation</code> runs a query</li> <li> <h1 id=structured-streamings-streamexecution-runs-batches>Structured Streaming's <code>StreamExecution</code> runs batches<a class=headerlink href=#structured-streamings-streamexecution-runs-batches title="Permanent link">&para;</a></h1> </li> </ul> <p>== [[cleaner]] ContextCleaner</p> <h2 id=source-scala_43>[source, scala]<a class=headerlink href=#source-scala_43 title="Permanent link">&para;</a></h2> <h2 id=cleaner-optioncontextcleaner>cleaner: Option[ContextCleaner]<a class=headerlink href=#cleaner-optioncontextcleaner title="Permanent link">&para;</a></h2> <p>SparkContext may have a xref:core:ContextCleaner.adoc[ContextCleaner] defined.</p> <p>ContextCleaner is created when xref:ROOT:spark-SparkContext-creating-instance-internals.adoc#_cleaner[SparkContext is created] with xref:ROOT:configuration-properties.adoc#spark.cleaner.referenceTracking[spark.cleaner.referenceTracking] configuration property enabled.</p> <p>== [[getPreferredLocs]] Finding Preferred Locations (Placement Preferences) for RDD Partition</p> <h2 id=source-scala_44>[source, scala]<a class=headerlink href=#source-scala_44 title="Permanent link">&para;</a></h2> <p>getPreferredLocs( rdd: RDD[_], partition: Int): Seq[TaskLocation]</p> <hr> <p>getPreferredLocs simply xref:scheduler:DAGScheduler.adoc#getPreferredLocs[requests <code>DAGScheduler</code> for the preferred locations for <code>partition</code>].</p> <p>NOTE: Preferred locations of a partition of a RDD are also called <em>placement preferences</em> or <em>locality preferences</em>.</p> <p>getPreferredLocs is used in CoalescedRDDPartition, DefaultPartitionCoalescer and PartitionerAwareUnionRDD.</p> <p>== [[persistRDD]] Registering RDD in persistentRdds Internal Registry -- <code>persistRDD</code> Internal Method</p> <h2 id=source-scala_45>[source, scala]<a class=headerlink href=#source-scala_45 title="Permanent link">&para;</a></h2> <h2 id=persistrddrdd-rdd_-unit>persistRDD(rdd: RDD[_]): Unit<a class=headerlink href=#persistrddrdd-rdd_-unit title="Permanent link">&para;</a></h2> <p><code>persistRDD</code> registers <code>rdd</code> in &lt;<persistentrdds, persistentrdds>&gt; internal registry.</p> <p>NOTE: <code>persistRDD</code> is used exclusively when <code>RDD</code> is xref:rdd:index.adoc#persist-internal[persisted or locally checkpointed].</p> <p>== [[getRDDStorageInfo]] Getting Storage Status of Cached RDDs (as RDDInfos) -- <code>getRDDStorageInfo</code> Methods</p> <h2 id=source-scala_46>[source, scala]<a class=headerlink href=#source-scala_46 title="Permanent link">&para;</a></h2> <p>getRDDStorageInfo: Array[RDDInfo] // &lt;1&gt; getRDDStorageInfo(filter: RDD[_] =&gt; Boolean): Array[RDDInfo] // &lt;2&gt;</p> <hr> <p>&lt;1&gt; Part of Spark's Developer API that uses &lt;2&gt; filtering no RDDs</p> <p><code>getRDDStorageInfo</code> takes all the RDDs (from &lt;<persistentrdds, persistentrdds>&gt; registry) that match <code>filter</code> and creates a collection of xref:storage:RDDInfo.adoc[RDDInfo] instances.</p> <p><code>getRDDStorageInfo</code> then link:spark-webui-StorageListener.adoc#StorageUtils.updateRddInfo[updates the RDDInfos] with the &lt;<getexecutorstoragestatus, current status of all blockmanagers>&gt; (in a Spark application).</p> <p>In the end, <code>getRDDStorageInfo</code> gives only the RDD that are cached (i.e. the sum of memory and disk sizes as well as the number of partitions cached are greater than <code>0</code>).</p> <p>NOTE: <code>getRDDStorageInfo</code> is used when <code>RDD</code> link:spark-rdd-lineage.adoc#toDebugString[is requested for RDD lineage graph].</p> <p>== [[settings]] Settings</p> <p>=== [[spark.driver.allowMultipleContexts]] spark.driver.allowMultipleContexts</p> <p>Quoting the scaladoc of <a href=http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.SparkContext[org.apache.spark.SparkContext>http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.SparkContext[org.apache.spark.SparkContext</a>]:</p> <blockquote> <p>Only one SparkContext may be active per JVM. You must <code>stop()</code> the active SparkContext before creating a new one.</p> </blockquote> <p>You can however control the behaviour using <code>spark.driver.allowMultipleContexts</code> flag.</p> <p>It is disabled, i.e. <code>false</code>, by default.</p> <p>If enabled (i.e. <code>true</code>), Spark prints the following WARN message to the logs:</p> <div class=highlight><pre><span></span><code>WARN Multiple running SparkContexts detected in the same JVM!
</code></pre></div> <p>If disabled (default), it will throw an <code>SparkException</code> exception:</p> <div class=highlight><pre><span></span><code>Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
[ctx.creationSite.longForm]
</code></pre></div> <p>When creating an instance of SparkContext, Spark marks the current thread as having it being created (very early in the instantiation process).</p> <p>CAUTION: It's not guaranteed that Spark will work properly with two or more SparkContexts. Consider the feature a work in progress.</p> <p>== [[statusStore]] Accessing AppStatusStore</p> <h2 id=source-scala_47>[source, scala]<a class=headerlink href=#source-scala_47 title="Permanent link">&para;</a></h2> <h2 id=statusstore-appstatusstore>statusStore: AppStatusStore<a class=headerlink href=#statusstore-appstatusstore title="Permanent link">&para;</a></h2> <p>statusStore gives the current xref:core:AppStatusStore.adoc[].</p> <p>statusStore is used when:</p> <ul> <li> <p>SparkContext is requested to &lt;<getrddstorageinfo, getrddstorageinfo>&gt;</p> </li> <li> <p>ConsoleProgressBar is requested to xref:ROOT:spark-sparkcontext-ConsoleProgressBar.adoc#refresh[refresh]</p> </li> <li> <p>SharedState (Spark SQL) is requested for a SQLAppStatusStore</p> </li> </ul> <p>== [[uiWebUrl]] Requesting URL of web UI -- <code>uiWebUrl</code> Method</p> <h2 id=source-scala_48>[source, scala]<a class=headerlink href=#source-scala_48 title="Permanent link">&para;</a></h2> <h2 id=uiweburl-optionstring>uiWebUrl: Option[String]<a class=headerlink href=#uiweburl-optionstring title="Permanent link">&para;</a></h2> <p><code>uiWebUrl</code> requests the link:spark-SparkContext-creating-instance-internals.adoc#_ui[SparkUI] for link:spark-webui-WebUI.adoc#webUrl[webUrl].</p> <p>== [[maxNumConcurrentTasks]] <code>maxNumConcurrentTasks</code> Method</p> <h2 id=source-scala_49>[source, scala]<a class=headerlink href=#source-scala_49 title="Permanent link">&para;</a></h2> <h2 id=maxnumconcurrenttasks-int>maxNumConcurrentTasks(): Int<a class=headerlink href=#maxnumconcurrenttasks-int title="Permanent link">&para;</a></h2> <p><code>maxNumConcurrentTasks</code> simply requests the &lt;<schedulerbackend, schedulerbackend>&gt; for the xref:scheduler:SchedulerBackend.adoc#maxNumConcurrentTasks[maximum number of tasks that can be launched concurrently].</p> <p>NOTE: <code>maxNumConcurrentTasks</code> is used exclusively when <code>DAGScheduler</code> is requested to xref:scheduler:DAGScheduler.adoc#checkBarrierStageWithNumSlots[checkBarrierStageWithNumSlots].</p> <p>== [[createTaskScheduler]] Creating SchedulerBackend and TaskScheduler -- <code>createTaskScheduler</code> Internal Factory Method</p> <h2 id=source-scala_50>[source, scala]<a class=headerlink href=#source-scala_50 title="Permanent link">&para;</a></h2> <p>createTaskScheduler( sc: SparkContext, master: String, deployMode: String): (SchedulerBackend, TaskScheduler)</p> <hr> <p><code>createTaskScheduler</code> creates the xref:scheduler:SchedulerBackend.adoc[SchedulerBackend] and the xref:scheduler:TaskScheduler.adoc[TaskScheduler] for the given master URL and deployment mode.</p> <p>.SparkContext creates Task Scheduler and Scheduler Backend image::diagrams/sparkcontext-createtaskscheduler.png[align="center"]</p> <p>Internally, <code>createTaskScheduler</code> branches off per the given master URL (link:spark-deployment-environments.adoc#master-urls[master URL]) to select the requested implementations.</p> <p><code>createTaskScheduler</code> understands the following master URLs:</p> <ul> <li><code>local</code> - local mode with 1 thread only</li> <li><code>local[n]</code> or <code>local[*]</code> - local mode with <code>n</code> threads</li> <li><code>local[n, m]</code> or <code>local[*, m]</code> -- local mode with <code>n</code> threads and <code>m</code> number of failures</li> <li><code>spark://hostname:port</code> for Spark Standalone</li> <li><code>local-cluster[n, m, z]</code> -- local cluster with <code>n</code> workers, <code>m</code> cores per worker, and <code>z</code> memory per worker</li> <li>any other URL is passed to &lt;<getclustermanager, &lt;code>getClusterManager</code> to load an external cluster manager>&gt;.</li> </ul> <p>CAUTION: FIXME</p> <p>== [[environment-variables]] Environment Variables</p> <p>.Environment Variables [cols="1,1,2",options="header",width="100%"] |=== | Environment Variable | Default Value | Description</p> <p>| [[SPARK_EXECUTOR_MEMORY]] <code>SPARK_EXECUTOR_MEMORY</code> | <code>1024</code> | Amount of memory to allocate for a Spark executor in MB.</p> <p>See xref:executor:Executor.adoc#memory[Executor Memory].</p> <table> <thead> <tr> <th>[[SPARK_USER]] <code>SPARK_USER</code></th> </tr> </thead> <tbody> <tr> <td>The user who is running SparkContext. Available later as &lt;<sparkuser, sparkuser>&gt;.</td> </tr> <tr> <td>===</td> </tr> </tbody> </table> <p>== [[postEnvironmentUpdate]] Posting SparkListenerEnvironmentUpdate Event</p> <h2 id=source-scala_51>[source, scala]<a class=headerlink href=#source-scala_51 title="Permanent link">&para;</a></h2> <h2 id=postenvironmentupdate-unit>postEnvironmentUpdate(): Unit<a class=headerlink href=#postenvironmentupdate-unit title="Permanent link">&para;</a></h2> <p><code>postEnvironmentUpdate</code>...FIXME</p> <p>NOTE: <code>postEnvironmentUpdate</code> is used when SparkContext is &lt;<spark-sparkcontext-creating-instance-internals.adoc#postenvironmentupdate, created>&gt;, and requested to &lt;<addfile, addfile>&gt; and &lt;<addjar, addjar>&gt;.</p> <p>== [[addJar-internals]] <code>addJar</code> Method</p> <h2 id=source-scala_52>[source, scala]<a class=headerlink href=#source-scala_52 title="Permanent link">&para;</a></h2> <h2 id=addjarpath-string-unit_1>addJar(path: String): Unit<a class=headerlink href=#addjarpath-string-unit_1 title="Permanent link">&para;</a></h2> <p><code>addJar</code>...FIXME</p> <p>NOTE: <code>addJar</code> is used when...FIXME</p> <p>== [[runApproximateJob]] Running Approximate Job</p> <h2 id=source-scala_53>[source, scala]<a class=headerlink href=#source-scala_53 title="Permanent link">&para;</a></h2> <p>runApproximateJob<a href="rdd: RDD[T],
  func: (TaskContext, Iterator[T]) => U,
  evaluator: ApproximateEvaluator[U, R],
  timeout: Long">T, U, R</a>: PartialResult[R]</p> <hr> <p>runApproximateJob...FIXME</p> <p>runApproximateJob is used when:</p> <ul> <li> <p>DoubleRDDFunctions is requested to meanApprox and sumApprox</p> </li> <li> <p>RDD is requested to countApprox and countByValueApprox</p> </li> </ul> <p>== [[killTaskAttempt]] Killing Task</p> <h2 id=source-scala_54>[source, scala]<a class=headerlink href=#source-scala_54 title="Permanent link">&para;</a></h2> <p>killTaskAttempt( taskId: Long, interruptThread: Boolean = true, reason: String = "killed via SparkContext.killTaskAttempt"): Boolean</p> <hr> <p>killTaskAttempt requests the &lt;<dagscheduler, dagscheduler>&gt; to xref:scheduler:DAGScheduler.adoc#killTaskAttempt[kill a task].</p> <p>== [[checkpointFile]] checkpointFile Internal Method</p> <h2 id=source-scala_55>[source, scala]<a class=headerlink href=#source-scala_55 title="Permanent link">&para;</a></h2> <p>checkpointFile<a href="path: String">T: ClassTag</a>: RDD[T]</p> <hr> <p>checkpointFile...FIXME</p> <p>== [[logging]] Logging</p> <p>Enable <code>ALL</code> logging level for <code>org.apache.spark.SparkContext</code> logger to see what happens inside.</p> <p>Add the following line to <code>conf/log4j.properties</code>:</p> <h2 id=sourceplaintext_1>[source,plaintext]<a class=headerlink href=#sourceplaintext_1 title="Permanent link">&para;</a></h2> <h2 id=log4jloggerorgapachesparksparkcontextall>log4j.logger.org.apache.spark.SparkContext=ALL<a class=headerlink href=#log4jloggerorgapachesparksparkcontextall title="Permanent link">&para;</a></h2> <p>Refer to xref:ROOT:spark-logging.adoc[Logging].</p> <p>== [[internal-properties]] Internal Properties</p> <p>=== [[checkpointDir]] Checkpoint Directory</p> <h2 id=sourcescala_2>[source,scala]<a class=headerlink href=#sourcescala_2 title="Permanent link">&para;</a></h2> <h2 id=checkpointdir-optionstring-none>checkpointDir: Option[String] = None<a class=headerlink href=#checkpointdir-optionstring-none title="Permanent link">&para;</a></h2> <p>checkpointDir is...FIXME</p> <p>=== [[persistentRdds]] persistentRdds Lookup Table</p> <p>Lookup table of persistent/cached RDDs per their ids.</p> <p>Used when SparkContext is requested to:</p> <ul> <li>&lt;<persistrdd, persistrdd>&gt;</li> <li>&lt;<getrddstorageinfo, getrddstorageinfo>&gt;</li> <li>&lt;<getpersistentrdds, getpersistentrdds>&gt;</li> <li>&lt;<unpersistrdd, unpersistrdd>&gt;</li> </ul> <p>=== [[stopped]] stopped Flag</p> <p>Flag that says whether...FIXME (<code>true</code>) or not (<code>false</code>)</p> <p>=== [[_taskScheduler]] TaskScheduler</p> <p>xref:scheduler:TaskScheduler.adoc[TaskScheduler]</p> <hr> <div class=md-source-date> <small> Last update: 2020-10-05 </small> </div> </article> </div> </div> </main> <footer class=md-footer> <div class=md-footer-nav> <nav class="md-footer-nav__inner md-grid" aria-label=Footer> <a href=../SparkConf/ class="md-footer-nav__link md-footer-nav__link--prev" rel=prev> <div class="md-footer-nav__button md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12z"/></svg> </div> <div class=md-footer-nav__title> <div class=md-ellipsis> <span class=md-footer-nav__direction> Previous </span> SparkConf </div> </div> </a> <a href=../spark-SparkContext-creating-instance-internals/ class="md-footer-nav__link md-footer-nav__link--next" rel=next> <div class=md-footer-nav__title> <div class=md-ellipsis> <span class=md-footer-nav__direction> Next </span> Creating SparkContext </div> </div> <div class="md-footer-nav__button md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M4 11v2h12l-5.5 5.5 1.42 1.42L19.84 12l-7.92-7.92L10.5 5.5 16 11H4z"/></svg> </div> </a> </nav> </div> <div class="md-footer-meta md-typeset"> <div class="md-footer-meta__inner md-grid"> <div class=md-footer-copyright> <div class=md-footer-copyright__highlight> Copyright &copy; 2020 <a href=https://twitter.com/jaceklaskowski target=_blank rel=noopener>Jacek Laskowski</a> </div> Made with <a href=https://squidfunk.github.io/mkdocs-material/ target=_blank rel=noopener> Material for MkDocs </a> </div> <div class=md-footer-social> <a href=https://github.com/jaceklaskowski target=_blank rel=noopener title=github.com class=md-footer-social__link> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 496 512"><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6 0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2z"/></svg> </a> <a href=https://twitter.com/jaceklaskowski target=_blank rel=noopener title=twitter.com class=md-footer-social__link> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 512 512"><path d="M459.37 151.716c.325 4.548.325 9.097.325 13.645 0 138.72-105.583 298.558-298.558 298.558-59.452 0-114.68-17.219-161.137-47.106 8.447.974 16.568 1.299 25.34 1.299 49.055 0 94.213-16.568 130.274-44.832-46.132-.975-84.792-31.188-98.112-72.772 6.498.974 12.995 1.624 19.818 1.624 9.421 0 18.843-1.3 27.614-3.573-48.081-9.747-84.143-51.98-84.143-102.985v-1.299c13.969 7.797 30.214 12.67 47.431 13.319-28.264-18.843-46.781-51.005-46.781-87.391 0-19.492 5.197-37.36 14.294-52.954 51.655 63.675 129.3 105.258 216.365 109.807-1.624-7.797-2.599-15.918-2.599-24.04 0-57.828 46.782-104.934 104.934-104.934 30.213 0 57.502 12.67 76.67 33.137 23.715-4.548 46.456-13.32 66.599-25.34-7.798 24.366-24.366 44.833-46.132 57.827 21.117-2.273 41.584-8.122 60.426-16.243-14.292 20.791-32.161 39.308-52.628 54.253z"/></svg> </a> <a href=https://linkedin.com/in/jaceklaskowski target=_blank rel=noopener title=linkedin.com class=md-footer-social__link> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 448 512"><path d="M416 32H31.9C14.3 32 0 46.5 0 64.3v383.4C0 465.5 14.3 480 31.9 480H416c17.6 0 32-14.5 32-32.3V64.3c0-17.8-14.4-32.3-32-32.3zM135.4 416H69V202.2h66.5V416zm-33.2-243c-21.3 0-38.5-17.3-38.5-38.5S80.9 96 102.2 96c21.2 0 38.5 17.3 38.5 38.5 0 21.3-17.2 38.5-38.5 38.5zm282.1 243h-66.4V312c0-24.8-.5-56.7-34.5-56.7-34.6 0-39.9 27-39.9 54.9V416h-66.4V202.2h63.7v29.2h.9c8.9-16.8 30.6-34.5 62.9-34.5 67.2 0 79.7 44.3 79.7 101.9V416z"/></svg> </a> </div> </div> </div> </footer> </div> <script src=../assets/javascripts/vendor.77e55a48.min.js></script> <script src=../assets/javascripts/bundle.9554a270.min.js></script><script id=__lang type=application/json>{"clipboard.copy": "Copy to clipboard", "clipboard.copied": "Copied to clipboard", "search.config.lang": "en", "search.config.pipeline": "trimmer, stopWordFilter", "search.config.separator": "[\\s\\-]+", "search.result.placeholder": "Type to start searching", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.term.missing": "Missing"}</script> <script>
        app = initialize({
          base: "..",
          features: ['navigation.tabs', 'navigation.instant'],
          search: Object.assign({
            worker: "../assets/javascripts/worker/search.4ac00218.min.js"
          }, typeof search !== "undefined" && search)
        })
      </script> </body> </html>