{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"The Internals of Apache Spark 3.0.1 \u00b6 Welcome to The Internals of Apache Spark online book! I'm Jacek Laskowski , a Seasoned IT Professional specializing in Apache Spark , Delta Lake , Apache Kafka and Kafka Streams . I'm very excited to have you here and hope you will enjoy exploring the internals of Apache Spark as much as I have. Flannery O'Connor I write to discover what I know. \"The Internals Of\" series I'm also writing other online books in the \"The Internals Of\" series. Please visit \"The Internals Of\" Online Books home page. Expect text and code snippets from a variety of public sources. Attribution follows. Now, let me introduce you to Apache Spark .","title":"Welcome"},{"location":"#the-internals-of-apache-spark-301","text":"Welcome to The Internals of Apache Spark online book! I'm Jacek Laskowski , a Seasoned IT Professional specializing in Apache Spark , Delta Lake , Apache Kafka and Kafka Streams . I'm very excited to have you here and hope you will enjoy exploring the internals of Apache Spark as much as I have. Flannery O'Connor I write to discover what I know. \"The Internals Of\" series I'm also writing other online books in the \"The Internals Of\" series. Please visit \"The Internals Of\" Online Books home page. Expect text and code snippets from a variety of public sources. Attribution follows. Now, let me introduce you to Apache Spark .","title":"The Internals of Apache Spark 3.0.1"},{"location":"overview/","text":"= Apache Spark http://spark.apache.org/[Apache Spark] is an open-source distributed general-purpose cluster computing framework with (mostly) in-memory data processing engine that can do ETL, analytics, machine learning and graph processing on large volumes of data at rest (batch processing) or in motion (streaming processing) with < > for the programming languages: Scala, Python, Java, R, and SQL. .The Spark Platform image::diagrams/spark-platform.png[align=\"center\"] You could also describe Spark as a distributed, data processing engine for batch and streaming modes featuring SQL queries, graph processing, and machine learning. In contrast to Hadoop\u2019s two-stage disk-based MapReduce computation engine, Spark's multi-stage (mostly) in-memory computing engine allows for running most computations in memory, and hence most of the time provides better performance for certain applications, e.g. iterative algorithms or interactive data mining (read https://databricks.com/blog/2014/11/05/spark-officially-sets-a-new-record-in-large-scale-sorting.html[Spark officially sets a new record in large-scale sorting]). Spark aims at speed, ease of use, extensibility and interactive analytics. Spark is often called cluster computing engine or simply execution engine . Spark is a distributed platform for executing complex multi-stage applications , like machine learning algorithms , and interactive ad hoc queries . Spark provides an efficient abstraction for in-memory cluster computing called xref:rdd:index.adoc[Resilient Distributed Dataset]. Using Spark Application Frameworks, Spark simplifies access to machine learning and predictive analytics at scale. Spark is mainly written in http://scala-lang.org/[Scala ], but provides developer API for languages like Java, Python, and R. NOTE: Microsoft's https://github.com/Microsoft/Mobius[Mobius project] provides C# API for Spark \"enabling the implementation of Spark driver program and data processing operations in the languages supported in the .NET framework like C# or F#.\" If you have large amounts of data that requires low latency processing that a typical MapReduce program cannot provide, Spark is a viable alternative. Access any data type across any data source. Huge demand for storage and data processing. The Apache Spark project is an umbrella for https://jaceklaskowski.gitbooks.io/mastering-spark-sql/[SQL ] (with Datasets), https://jaceklaskowski.gitbooks.io/spark-structured-streaming/[streaming ], http://spark.apache.org/mllib/[machine learning] (pipelines) and http://spark.apache.org/graphx/[graph ] processing engines built on top of the Spark Core. You can run them all in a single application using a consistent API. Spark runs locally as well as in clusters, on-premises or in cloud. It runs on top of Hadoop YARN, Apache Mesos, standalone or in the cloud (Amazon EC2 or IBM Bluemix). Apache Spark's https://jaceklaskowski.gitbooks.io/spark-structured-streaming/[Structured Streaming] and https://jaceklaskowski.gitbooks.io/mastering-spark-sql/[SQL ] programming models with MLlib and GraphX make it easier for developers and data scientists to build applications that exploit machine learning and graph analytics. At a high level, any Spark application creates RDDs out of some input, run xref:rdd:index.adoc[(lazy) transformations] of these RDDs to some other form (shape), and finally perform xref:rdd:index.adoc[actions] to collect or store data. Not much, huh? You can look at Spark from programmer's, data engineer's and administrator's point of view. And to be honest, all three types of people will spend quite a lot of their time with Spark to finally reach the point where they exploit all the available features. Programmers use language-specific APIs (and work at the level of RDDs using transformations and actions), data engineers use higher-level abstractions like DataFrames or Pipelines APIs or external tools (that connect to Spark), and finally it all can only be possible to run because administrators set up Spark clusters to deploy Spark applications to. It is Spark's goal to be a general-purpose computing platform with various specialized applications frameworks on top of a single unified engine. NOTE: When you hear \"Apache Spark\" it can be two things -- the Spark engine aka Spark Core or the Apache Spark open source project which is an \"umbrella\" term for Spark Core and the accompanying Spark Application Frameworks, i.e. Spark SQL, link:spark-streaming/spark-streaming.adoc[Spark Streaming], link:spark-mllib/spark-mllib.adoc[Spark MLlib] and link:spark-graphx.adoc[Spark GraphX] that sit on top of Spark Core and the main data abstraction in Spark called xref:rdd:index.adoc[RDD - Resilient Distributed Dataset]. == [[why-spark]] Why Spark Let's list a few of the many reasons for Spark. We are doing it first, and then comes the overview that lends a more technical helping hand. === Easy to Get Started Spark offers link:spark-shell.adoc[spark-shell] that makes for a very easy head start to writing and running Spark applications on the command line on your laptop. You could then use link:spark-standalone.adoc[Spark Standalone] built-in cluster manager to deploy your Spark applications to a production-grade cluster to run on a full dataset. === Unified Engine for Diverse Workloads As said by Matei Zaharia - the author of Apache Spark - in https://youtu.be/49Hr5xZyTEA[Introduction to AmpLab Spark Internals video] (quoting with few changes): One of the Spark project goals was to deliver a platform that supports a very wide array of diverse workflows - not only MapReduce batch jobs (there were available in Hadoop already at that time), but also iterative computations like graph algorithms or Machine Learning. And also different scales of workloads from sub-second interactive jobs to jobs that run for many hours. Spark combines batch, interactive, and streaming workloads under one rich concise API. Spark supports near real-time streaming workloads via link:spark-streaming/spark-streaming.adoc[Spark Streaming] application framework. ETL workloads and Analytics workloads are different, however Spark attempts to offer a unified platform for a wide variety of workloads. Graph and Machine Learning algorithms are iterative by nature and less saves to disk or transfers over network means better performance. There is also support for interactive workloads using Spark shell. You should watch the video https://youtu.be/SxAxAhn-BDU[What is Apache Spark?] by Mike Olson, Chief Strategy Officer and Co-Founder at Cloudera, who provides a very exceptional overview of Apache Spark, its rise in popularity in the open source community, and how Spark is primed to replace MapReduce as the general processing engine in Hadoop. === Leverages the Best in distributed batch data processing When you think about distributed batch data processing , link:varia/spark-hadoop.adoc[Hadoop] naturally comes to mind as a viable solution. Spark draws many ideas out of Hadoop MapReduce. They work together well - Spark on YARN and HDFS - while improving on the performance and simplicity of the distributed computing engine. For many, Spark is Hadoop++, i.e. MapReduce done in a better way. And it should not come as a surprise, without Hadoop MapReduce (its advances and deficiencies), Spark would not have been born at all. === RDD - Distributed Parallel Scala Collections As a Scala developer, you may find Spark's RDD API very similar (if not identical) to http://www.scala-lang.org/docu/files/collections-api/collections.html[Scala's Collections API]. It is also exposed in Java, Python and R (as well as SQL, i.e. SparkSQL, in a sense). So, when you have a need for distributed Collections API in Scala, Spark with RDD API should be a serious contender. === [[rich-standard-library]] Rich Standard Library Not only can you use map and reduce (as in Hadoop MapReduce jobs) in Spark, but also a vast array of other higher-level operators to ease your Spark queries and application development. It expanded on the available computation styles beyond the only map-and-reduce available in Hadoop MapReduce. === Unified development and deployment environment for all Regardless of the Spark tools you use - the Spark API for the many programming languages supported - Scala, Java, Python, R, or link:spark-shell.adoc[the Spark shell], or the many Spark Application Frameworks leveraging the concept of xref:rdd:index.adoc[RDD], i.e. Spark SQL, link:spark-streaming/spark-streaming.adoc[Spark Streaming], link:spark-mllib/spark-mllib.adoc[Spark MLlib] and link:spark-graphx.adoc[Spark GraphX], you still use the same development and deployment environment to for large data sets to yield a result, be it a prediction (link:spark-mllib/spark-mllib.adoc[Spark MLlib]), a structured data queries (Spark SQL) or just a large distributed batch (Spark Core) or streaming (Spark Streaming) computation. It's also very productive of Spark that teams can exploit the different skills the team members have acquired so far. Data analysts, data scientists, Python programmers, or Java, or Scala, or R, can all use the same Spark platform using tailor-made API. It makes for bringing skilled people with their expertise in different programming languages together to a Spark project. === Interactive Exploration / Exploratory Analytics It is also called ad hoc queries . Using link:spark-shell.adoc[the Spark shell] you can execute computations to process large amount of data ( The Big Data ). It's all interactive and very useful to explore the data before final production release. Also, using the Spark shell you can access any link:spark-cluster.adoc[Spark cluster] as if it was your local machine. Just point the Spark shell to a 20-node of 10TB RAM memory in total (using --master ) and use all the components (and their abstractions) like Spark SQL, Spark MLlib, link:spark-streaming/spark-streaming.adoc[Spark Streaming], and Spark GraphX. Depending on your needs and skills, you may see a better fit for SQL vs programming APIs or apply machine learning algorithms (Spark MLlib) from data in graph data structures (Spark GraphX). === Single Environment Regardless of which programming language you are good at, be it Scala, Java, Python, R or SQL, you can use the same single clustered runtime environment for prototyping, ad hoc queries, and deploying your applications leveraging the many ingestion data points offered by the Spark platform. You can be as low-level as using RDD API directly or leverage higher-level APIs of Spark SQL (Datasets), Spark MLlib (ML Pipelines), Spark GraphX (Graphs) or link:spark-streaming/spark-streaming.adoc[Spark Streaming] (DStreams). Or use them all in a single application. The single programming model and execution engine for different kinds of workloads simplify development and deployment architectures. === Data Integration Toolkit with Rich Set of Supported Data Sources Spark can read from many types of data sources -- relational, NoSQL, file systems, etc. -- using many types of data formats - Parquet, Avro, CSV, JSON. Both, input and output data sources, allow programmers and data engineers use Spark as the platform with the large amount of data that is read from or saved to for processing, interactively (using Spark shell) or in applications. === Tools unavailable then, at your fingertips now As much and often as it's recommended http://c2.com/cgi/wiki?PickTheRightToolForTheJob[to pick the right tool for the job], it's not always feasible. Time, personal preference, operating system you work on are all factors to decide what is right at a time (and using a hammer can be a reasonable choice). Spark embraces many concepts in a single unified development and runtime environment. Machine learning that is so tool- and feature-rich in Python, e.g. SciKit library, can now be used by Scala developers (as Pipeline API in Spark MLlib or calling pipe() ). DataFrames from R are available in Scala, Java, Python, R APIs. Single node computations in machine learning algorithms are migrated to their distributed versions in Spark MLlib. This single platform gives plenty of opportunities for Python, Scala, Java, and R programmers as well as data engineers (SparkR) and scientists (using proprietary enterprise data warehouses with link:spark-sql-thrift-server.adoc[Thrift JDBC/ODBC Server] in Spark SQL). Mind the proverb https://en.wiktionary.org/wiki/if_all_you_have_is_a_hammer,_everything_looks_like_a_nail[if all you have is a hammer, everything looks like a nail], too. === Low-level Optimizations Apache Spark uses a xref:scheduler:DAGScheduler.adoc[directed acyclic graph (DAG) of computation stages] (aka execution DAG ). It postpones any processing until really required for actions. Spark's lazy evaluation gives plenty of opportunities to induce low-level optimizations (so users have to know less to do more). Mind the proverb https://en.wiktionary.org/wiki/less_is_more[less is more]. === Excels at low-latency iterative workloads Spark supports diverse workloads, but successfully targets low-latency iterative ones. They are often used in Machine Learning and graph algorithms. Many Machine Learning algorithms require plenty of iterations before the result models get optimal, like logistic regression. The same applies to graph algorithms to traverse all the nodes and edges when needed. Such computations can increase their performance when the interim partial results are stored in memory or at very fast solid state drives. Spark can link:spark-rdd-caching.adoc[cache intermediate data in memory for faster model building and training]. Once the data is loaded to memory (as an initial step), reusing it multiple times incurs no performance slowdowns. Also, graph algorithms can traverse graphs one connection per iteration with the partial result in memory. Less disk access and network can make a huge difference when you need to process lots of data, esp. when it is a BIG Data. === ETL done easier Spark gives Extract, Transform and Load (ETL) a new look with the many programming languages supported - Scala, Java, Python (less likely R). You can use them all or pick the best for a problem. Scala in Spark, especially, makes for a much less boiler-plate code (comparing to other languages and approaches like MapReduce in Java). === [[unified-api]] Unified Concise High-Level API Spark offers a unified, concise, high-level APIs for batch analytics (RDD API), SQL queries (Dataset API), real-time analysis (DStream API), machine learning (ML Pipeline API) and graph processing (Graph API). Developers no longer have to learn many different processing engines and platforms, and let the time be spent on mastering framework APIs per use case (atop a single computation engine Spark). === Different kinds of data processing using unified API Spark offers three kinds of data processing using batch , interactive , and stream processing with the unified API and data structures. === Little to no disk use for better performance In the no-so-long-ago times, when the most prevalent distributed computing framework was link:varia/spark-hadoop.adoc[Hadoop MapReduce], you could reuse a data between computation (even partial ones!) only after you've written it to an external storage like link:varia/spark-hadoop.adoc[Hadoop Distributed Filesystem (HDFS)]. It can cost you a lot of time to compute even very basic multi-stage computations. It simply suffers from IO (and perhaps network) overhead. One of the many motivations to build Spark was to have a framework that is good at data reuse. Spark cuts it out in a way to keep as much data as possible in memory and keep it there until a job is finished. It doesn't matter how many stages belong to a job. What does matter is the available memory and how effective you are in using Spark API (so xref:rdd:index.adoc[no shuffle occur]). The less network and disk IO, the better performance, and Spark tries hard to find ways to minimize both. === Fault Tolerance included Faults are not considered a special case in Spark, but obvious consequence of being a parallel and distributed system. Spark handles and recovers from faults by default without particularly complex logic to deal with them. === Small Codebase Invites Contributors Spark's design is fairly simple and the code that comes out of it is not huge comparing to the features it offers. The reasonably small codebase of Spark invites project contributors - programmers who extend the platform and fix bugs in a more steady pace. == [[i-want-more]] Further reading or watching (video) https://youtu.be/L029ZNBG7bk[Keynote : Spark 2.0 - Matei Zaharia, Apache Spark Creator and CTO of Databricks]","title":"Overview"},{"location":"metrics/","text":"Spark Metrics \u00b6 Spark Metrics gives you execution metrics of Spark subsystems ( metrics instances ), e.g. the driver of a Spark application or the master of a Spark Standalone cluster. Spark Metrics uses Dropwizard Metrics 3.1.0 Java library for the metrics infrastructure. Metrics is a Java library which gives you unparalleled insight into what your code does in production. Metrics provides a powerful toolkit of ways to measure the behavior of critical components in your production environment . MetricsSystem \u00b6 The main part of Spark Metrics is MetricsSystem which is a registry of metrics link:spark-metrics-Source.adoc[sources] and link:spark-metrics-Sink.adoc[sinks] of a Spark subsystem. MetricsSystem uses Dropwizard Metrics' link:spark-metrics-MetricsSystem.adoc#registry[MetricRegistry] that acts as the integration point between Spark and the metrics library. A Spark subsystem can access the MetricsSystem through the xref:core:SparkEnv.adoc#metricsSystem[SparkEnv.metricsSystem] property. val metricsSystem = SparkEnv.get.metricsSystem MetricsConfig \u00b6 MetricsConfig is the configuration of the link:spark-metrics-MetricsSystem.adoc[MetricsSystem] (i.e. metrics link:spark-metrics-Source.adoc[sources] and link:spark-metrics-Sink.adoc[sinks]). metrics.properties is the default metrics configuration file. It is configured using link:spark-metrics-properties.adoc#spark.metrics.conf[spark.metrics.conf] configuration property. The file is first loaded from the path directly before using Spark's CLASSPATH. MetricsConfig also accepts a metrics configuration using spark.metrics.conf. -prefixed configuration properties. Spark comes with conf/metrics.properties.template file that is a template of metrics configuration. MetricsServlet Metrics Sink \u00b6 Among the metrics sinks is link:spark-metrics-MetricsServlet.adoc[MetricsServlet] that is used when sink.servlet metrics sink is configured in link:spark-metrics-MetricsConfig.adoc[metrics configuration]. CAUTION: FIXME Describe configuration files and properties JmxSink Metrics Sink \u00b6 Enable org.apache.spark.metrics.sink.JmxSink in link:spark-metrics-MetricsConfig.adoc[metrics configuration]. You can then use jconsole to access Spark metrics through JMX. *.sink.jmx.class=org.apache.spark.metrics.sink.JmxSink JSON URI Path \u00b6 Metrics System is available at http://localhost:4040/metrics/json (for the default setup of a Spark application). $ http --follow http://localhost:4040/metrics/json HTTP/1.1 200 OK Cache-Control: no-cache, no-store, must-revalidate Content-Length: 2200 Content-Type: text/json;charset=utf-8 Date: Sat, 25 Feb 2017 14:14:16 GMT Server: Jetty(9.2.z-SNAPSHOT) X-Frame-Options: SAMEORIGIN { \"counters\": { \"app-20170225151406-0000.driver.HiveExternalCatalog.fileCacheHits\": { \"count\": 0 }, \"app-20170225151406-0000.driver.HiveExternalCatalog.filesDiscovered\": { \"count\": 0 }, \"app-20170225151406-0000.driver.HiveExternalCatalog.hiveClientCalls\": { \"count\": 2 }, \"app-20170225151406-0000.driver.HiveExternalCatalog.parallelListingJobCount\": { \"count\": 0 }, \"app-20170225151406-0000.driver.HiveExternalCatalog.partitionsFetched\": { \"count\": 0 } }, \"gauges\": { ... \"timers\": { \"app-20170225151406-0000.driver.DAGScheduler.messageProcessingTime\": { \"count\": 0, \"duration_units\": \"milliseconds\", \"m15_rate\": 0.0, \"m1_rate\": 0.0, \"m5_rate\": 0.0, \"max\": 0.0, \"mean\": 0.0, \"mean_rate\": 0.0, \"min\": 0.0, \"p50\": 0.0, \"p75\": 0.0, \"p95\": 0.0, \"p98\": 0.0, \"p99\": 0.0, \"p999\": 0.0, \"rate_units\": \"calls/second\", \"stddev\": 0.0 } }, \"version\": \"3.0.0\" } NOTE: You can access a Spark subsystem's MetricsSystem using its corresponding \"leading\" port, e.g. 4040 for the driver , 8080 for Spark Standalone's master and applications . NOTE: You have to use the trailing slash ( / ) to have the output. Spark Standalone Master \u00b6 $ http http://192.168.1.4:8080/metrics/master/json/path HTTP/1.1 200 OK Cache-Control: no-cache, no-store, must-revalidate Content-Length: 207 Content-Type: text/json;charset=UTF-8 Server: Jetty(8.y.z-SNAPSHOT) X-Frame-Options: SAMEORIGIN { \"counters\": {}, \"gauges\": { \"master.aliveWorkers\": { \"value\": 0 }, \"master.apps\": { \"value\": 0 }, \"master.waitingApps\": { \"value\": 0 }, \"master.workers\": { \"value\": 0 } }, \"histograms\": {}, \"meters\": {}, \"timers\": {}, \"version\": \"3.0.0\" }","title":"Spark Metrics"},{"location":"metrics/#spark-metrics","text":"Spark Metrics gives you execution metrics of Spark subsystems ( metrics instances ), e.g. the driver of a Spark application or the master of a Spark Standalone cluster. Spark Metrics uses Dropwizard Metrics 3.1.0 Java library for the metrics infrastructure. Metrics is a Java library which gives you unparalleled insight into what your code does in production. Metrics provides a powerful toolkit of ways to measure the behavior of critical components in your production environment .","title":"Spark Metrics"},{"location":"metrics/#metricssystem","text":"The main part of Spark Metrics is MetricsSystem which is a registry of metrics link:spark-metrics-Source.adoc[sources] and link:spark-metrics-Sink.adoc[sinks] of a Spark subsystem. MetricsSystem uses Dropwizard Metrics' link:spark-metrics-MetricsSystem.adoc#registry[MetricRegistry] that acts as the integration point between Spark and the metrics library. A Spark subsystem can access the MetricsSystem through the xref:core:SparkEnv.adoc#metricsSystem[SparkEnv.metricsSystem] property. val metricsSystem = SparkEnv.get.metricsSystem","title":" MetricsSystem"},{"location":"metrics/#metricsconfig","text":"MetricsConfig is the configuration of the link:spark-metrics-MetricsSystem.adoc[MetricsSystem] (i.e. metrics link:spark-metrics-Source.adoc[sources] and link:spark-metrics-Sink.adoc[sinks]). metrics.properties is the default metrics configuration file. It is configured using link:spark-metrics-properties.adoc#spark.metrics.conf[spark.metrics.conf] configuration property. The file is first loaded from the path directly before using Spark's CLASSPATH. MetricsConfig also accepts a metrics configuration using spark.metrics.conf. -prefixed configuration properties. Spark comes with conf/metrics.properties.template file that is a template of metrics configuration.","title":" MetricsConfig"},{"location":"metrics/#metricsservlet-metrics-sink","text":"Among the metrics sinks is link:spark-metrics-MetricsServlet.adoc[MetricsServlet] that is used when sink.servlet metrics sink is configured in link:spark-metrics-MetricsConfig.adoc[metrics configuration]. CAUTION: FIXME Describe configuration files and properties","title":" MetricsServlet Metrics Sink"},{"location":"metrics/#jmxsink-metrics-sink","text":"Enable org.apache.spark.metrics.sink.JmxSink in link:spark-metrics-MetricsConfig.adoc[metrics configuration]. You can then use jconsole to access Spark metrics through JMX. *.sink.jmx.class=org.apache.spark.metrics.sink.JmxSink","title":" JmxSink Metrics Sink"},{"location":"metrics/#json-uri-path","text":"Metrics System is available at http://localhost:4040/metrics/json (for the default setup of a Spark application). $ http --follow http://localhost:4040/metrics/json HTTP/1.1 200 OK Cache-Control: no-cache, no-store, must-revalidate Content-Length: 2200 Content-Type: text/json;charset=utf-8 Date: Sat, 25 Feb 2017 14:14:16 GMT Server: Jetty(9.2.z-SNAPSHOT) X-Frame-Options: SAMEORIGIN { \"counters\": { \"app-20170225151406-0000.driver.HiveExternalCatalog.fileCacheHits\": { \"count\": 0 }, \"app-20170225151406-0000.driver.HiveExternalCatalog.filesDiscovered\": { \"count\": 0 }, \"app-20170225151406-0000.driver.HiveExternalCatalog.hiveClientCalls\": { \"count\": 2 }, \"app-20170225151406-0000.driver.HiveExternalCatalog.parallelListingJobCount\": { \"count\": 0 }, \"app-20170225151406-0000.driver.HiveExternalCatalog.partitionsFetched\": { \"count\": 0 } }, \"gauges\": { ... \"timers\": { \"app-20170225151406-0000.driver.DAGScheduler.messageProcessingTime\": { \"count\": 0, \"duration_units\": \"milliseconds\", \"m15_rate\": 0.0, \"m1_rate\": 0.0, \"m5_rate\": 0.0, \"max\": 0.0, \"mean\": 0.0, \"mean_rate\": 0.0, \"min\": 0.0, \"p50\": 0.0, \"p75\": 0.0, \"p95\": 0.0, \"p98\": 0.0, \"p99\": 0.0, \"p999\": 0.0, \"rate_units\": \"calls/second\", \"stddev\": 0.0 } }, \"version\": \"3.0.0\" } NOTE: You can access a Spark subsystem's MetricsSystem using its corresponding \"leading\" port, e.g. 4040 for the driver , 8080 for Spark Standalone's master and applications . NOTE: You have to use the trailing slash ( / ) to have the output.","title":"JSON URI Path"},{"location":"metrics/#spark-standalone-master","text":"$ http http://192.168.1.4:8080/metrics/master/json/path HTTP/1.1 200 OK Cache-Control: no-cache, no-store, must-revalidate Content-Length: 207 Content-Type: text/json;charset=UTF-8 Server: Jetty(8.y.z-SNAPSHOT) X-Frame-Options: SAMEORIGIN { \"counters\": {}, \"gauges\": { \"master.aliveWorkers\": { \"value\": 0 }, \"master.apps\": { \"value\": 0 }, \"master.waitingApps\": { \"value\": 0 }, \"master.workers\": { \"value\": 0 } }, \"histograms\": {}, \"meters\": {}, \"timers\": {}, \"version\": \"3.0.0\" }","title":"Spark Standalone Master"},{"location":"metrics/DAGSchedulerSource/","text":"DAGSchedulerSource \u00b6 DAGSchedulerSource is the metrics source of DAGScheduler . DAGScheduler uses Spark Metrics System to report metrics about internal status. The name of the source is DAGScheduler . DAGSchedulerSource emits the following metrics: stage.failedStages - the number of failed stages stage.runningStages - the number of running stages stage.waitingStages - the number of waiting stages job.allJobs - the number of all jobs job.activeJobs - the number of active jobs","title":"DAGSchedulerSource"},{"location":"metrics/DAGSchedulerSource/#dagschedulersource","text":"DAGSchedulerSource is the metrics source of DAGScheduler . DAGScheduler uses Spark Metrics System to report metrics about internal status. The name of the source is DAGScheduler . DAGSchedulerSource emits the following metrics: stage.failedStages - the number of failed stages stage.runningStages - the number of running stages stage.waitingStages - the number of waiting stages job.allJobs - the number of all jobs job.activeJobs - the number of active jobs","title":"DAGSchedulerSource"},{"location":"metrics/JvmSource/","text":"JvmSource \u00b6 JvmSource is a metrics source . The name of the source is jvm . JvmSource registers the build-in Codehale metrics: GarbageCollectorMetricSet MemoryUsageGaugeSet BufferPoolMetricSet Among the metrics is total.committed (from MemoryUsageGaugeSet ) that describes the current usage of the heap and non-heap memories.","title":"JvmSource"},{"location":"metrics/JvmSource/#jvmsource","text":"JvmSource is a metrics source . The name of the source is jvm . JvmSource registers the build-in Codehale metrics: GarbageCollectorMetricSet MemoryUsageGaugeSet BufferPoolMetricSet Among the metrics is total.committed (from MemoryUsageGaugeSet ) that describes the current usage of the heap and non-heap memories.","title":"JvmSource"},{"location":"metrics/MetricsConfig/","text":"MetricsConfig \u00b6 MetricsConfig is the configuration of the MetricsSystem (i.e. metrics sources and sinks ). MetricsConfig is < > when link:spark-metrics-MetricsSystem.adoc#creating-instance[MetricsSystem] is. MetricsConfig uses metrics.properties as the default metrics configuration file. It is configured using link:spark-metrics-properties.adoc#spark.metrics.conf[spark.metrics.conf] configuration property. The file is first loaded from the path directly before using Spark's CLASSPATH. MetricsConfig accepts a metrics configuration using spark.metrics.conf. -prefixed configuration properties. Spark comes with conf/metrics.properties.template file that is a template of metrics configuration. MetricsConfig < > that the < > are always defined. [[default-properties]] .MetricsConfig's Default Metrics Properties [cols=\"1,2\",options=\"header\",width=\"100%\"] |=== | Name | Description | *.sink.servlet.class | org.apache.spark.metrics.sink.MetricsServlet | *.sink.servlet.path | /metrics/json | master.sink.servlet.path | /metrics/master/json | applications.sink.servlet.path | /metrics/applications/json |=== [NOTE] \u00b6 The order of precedence of metrics configuration settings is as follows: . < > . link:spark-metrics-properties.adoc#spark.metrics.conf[spark.metrics.conf] configuration property or metrics.properties configuration file . spark.metrics.conf. -prefixed Spark properties ==== [[creating-instance]] [[conf]] MetricsConfig takes a xref:ROOT:SparkConf.adoc[SparkConf] when created. [[internal-registries]] .MetricsConfig's Internal Registries and Counters [cols=\"1,2\",options=\"header\",width=\"100%\"] |=== | Name | Description | [[properties]] properties | https://docs.oracle.com/javase/8/docs/api/java/util/Properties.html[java.util.Properties ] with metrics properties Used to < > per-subsystem's < >. | [[perInstanceSubProperties]] perInstanceSubProperties | Lookup table of metrics properties per subsystem |=== === [[initialize]] Initializing MetricsConfig -- initialize Method [source, scala] \u00b6 initialize(): Unit \u00b6 initialize < > and < > (that is defined using link:spark-metrics-properties.adoc#spark.metrics.conf[spark.metrics.conf] configuration property). initialize takes all Spark properties that start with spark.metrics.conf. prefix from < > and adds them to < > (without the prefix). In the end, initialize splits < > with the default configuration (denoted as * ) assigned to all subsystems afterwards. NOTE: initialize accepts * (star) for the default configuration or any combination of lower- and upper-case letters for Spark subsystem names. NOTE: initialize is used exclusively when MetricsSystem is link:spark-metrics-MetricsSystem.adoc#creating-instance[created]. === [[setDefaultProperties]] setDefaultProperties Internal Method [source, scala] \u00b6 setDefaultProperties(prop: Properties): Unit \u00b6 setDefaultProperties sets the < > (in the input prop ). NOTE: setDefaultProperties is used exclusively when MetricsConfig < >. === [[loadPropertiesFromFile]] Loading Custom Metrics Configuration File or metrics.properties -- loadPropertiesFromFile Method [source, scala] \u00b6 loadPropertiesFromFile(path: Option[String]): Unit \u00b6 loadPropertiesFromFile tries to open the input path file (if defined) or the default metrics configuration file metrics.properties (on CLASSPATH). If either file is available, loadPropertiesFromFile loads the properties (to < > registry). In case of exceptions, you should see the following ERROR message in the logs followed by the exception. ERROR Error loading configuration file [file] NOTE: loadPropertiesFromFile is used exclusively when MetricsConfig < >. === [[subProperties]] Grouping Properties Per Subsystem -- subProperties Method [source, scala] \u00b6 subProperties(prop: Properties, regex: Regex): mutable.HashMap[String, Properties] \u00b6 subProperties takes prop properties and destructures keys given regex . subProperties takes the matching prefix (of a key per regex ) and uses it as a new key with the value(s) being the matching suffix(es). [source, scala] \u00b6 driver.hello.world => (driver, (hello.world)) \u00b6 NOTE: subProperties is used when MetricsConfig < > (to apply the default metrics configuration) and when MetricsSystem link:spark-metrics-MetricsSystem.adoc#registerSources[registers metrics sources] and link:spark-metrics-MetricsSystem.adoc#registerSinks[sinks]. === [[getInstance]] getInstance Method [source, scala] \u00b6 getInstance(inst: String): Properties \u00b6 getInstance ...FIXME NOTE: getInstance is used when...FIXME","title":"MetricsConfig"},{"location":"metrics/MetricsConfig/#metricsconfig","text":"MetricsConfig is the configuration of the MetricsSystem (i.e. metrics sources and sinks ). MetricsConfig is < > when link:spark-metrics-MetricsSystem.adoc#creating-instance[MetricsSystem] is. MetricsConfig uses metrics.properties as the default metrics configuration file. It is configured using link:spark-metrics-properties.adoc#spark.metrics.conf[spark.metrics.conf] configuration property. The file is first loaded from the path directly before using Spark's CLASSPATH. MetricsConfig accepts a metrics configuration using spark.metrics.conf. -prefixed configuration properties. Spark comes with conf/metrics.properties.template file that is a template of metrics configuration. MetricsConfig < > that the < > are always defined. [[default-properties]] .MetricsConfig's Default Metrics Properties [cols=\"1,2\",options=\"header\",width=\"100%\"] |=== | Name | Description | *.sink.servlet.class | org.apache.spark.metrics.sink.MetricsServlet | *.sink.servlet.path | /metrics/json | master.sink.servlet.path | /metrics/master/json | applications.sink.servlet.path | /metrics/applications/json |===","title":"MetricsConfig"},{"location":"metrics/MetricsConfig/#note","text":"The order of precedence of metrics configuration settings is as follows: . < > . link:spark-metrics-properties.adoc#spark.metrics.conf[spark.metrics.conf] configuration property or metrics.properties configuration file . spark.metrics.conf. -prefixed Spark properties ==== [[creating-instance]] [[conf]] MetricsConfig takes a xref:ROOT:SparkConf.adoc[SparkConf] when created. [[internal-registries]] .MetricsConfig's Internal Registries and Counters [cols=\"1,2\",options=\"header\",width=\"100%\"] |=== | Name | Description | [[properties]] properties | https://docs.oracle.com/javase/8/docs/api/java/util/Properties.html[java.util.Properties ] with metrics properties Used to < > per-subsystem's < >. | [[perInstanceSubProperties]] perInstanceSubProperties | Lookup table of metrics properties per subsystem |=== === [[initialize]] Initializing MetricsConfig -- initialize Method","title":"[NOTE]"},{"location":"metrics/MetricsConfig/#source-scala","text":"","title":"[source, scala]"},{"location":"metrics/MetricsConfig/#initialize-unit","text":"initialize < > and < > (that is defined using link:spark-metrics-properties.adoc#spark.metrics.conf[spark.metrics.conf] configuration property). initialize takes all Spark properties that start with spark.metrics.conf. prefix from < > and adds them to < > (without the prefix). In the end, initialize splits < > with the default configuration (denoted as * ) assigned to all subsystems afterwards. NOTE: initialize accepts * (star) for the default configuration or any combination of lower- and upper-case letters for Spark subsystem names. NOTE: initialize is used exclusively when MetricsSystem is link:spark-metrics-MetricsSystem.adoc#creating-instance[created]. === [[setDefaultProperties]] setDefaultProperties Internal Method","title":"initialize(): Unit"},{"location":"metrics/MetricsConfig/#source-scala_1","text":"","title":"[source, scala]"},{"location":"metrics/MetricsConfig/#setdefaultpropertiesprop-properties-unit","text":"setDefaultProperties sets the < > (in the input prop ). NOTE: setDefaultProperties is used exclusively when MetricsConfig < >. === [[loadPropertiesFromFile]] Loading Custom Metrics Configuration File or metrics.properties -- loadPropertiesFromFile Method","title":"setDefaultProperties(prop: Properties): Unit"},{"location":"metrics/MetricsConfig/#source-scala_2","text":"","title":"[source, scala]"},{"location":"metrics/MetricsConfig/#loadpropertiesfromfilepath-optionstring-unit","text":"loadPropertiesFromFile tries to open the input path file (if defined) or the default metrics configuration file metrics.properties (on CLASSPATH). If either file is available, loadPropertiesFromFile loads the properties (to < > registry). In case of exceptions, you should see the following ERROR message in the logs followed by the exception. ERROR Error loading configuration file [file] NOTE: loadPropertiesFromFile is used exclusively when MetricsConfig < >. === [[subProperties]] Grouping Properties Per Subsystem -- subProperties Method","title":"loadPropertiesFromFile(path: Option[String]): Unit"},{"location":"metrics/MetricsConfig/#source-scala_3","text":"","title":"[source, scala]"},{"location":"metrics/MetricsConfig/#subpropertiesprop-properties-regex-regex-mutablehashmapstring-properties","text":"subProperties takes prop properties and destructures keys given regex . subProperties takes the matching prefix (of a key per regex ) and uses it as a new key with the value(s) being the matching suffix(es).","title":"subProperties(prop: Properties, regex: Regex): mutable.HashMap[String, Properties]"},{"location":"metrics/MetricsConfig/#source-scala_4","text":"","title":"[source, scala]"},{"location":"metrics/MetricsConfig/#driverhelloworld-driver-helloworld","text":"NOTE: subProperties is used when MetricsConfig < > (to apply the default metrics configuration) and when MetricsSystem link:spark-metrics-MetricsSystem.adoc#registerSources[registers metrics sources] and link:spark-metrics-MetricsSystem.adoc#registerSinks[sinks]. === [[getInstance]] getInstance Method","title":"driver.hello.world =&gt; (driver, (hello.world))"},{"location":"metrics/MetricsConfig/#source-scala_5","text":"","title":"[source, scala]"},{"location":"metrics/MetricsConfig/#getinstanceinst-string-properties","text":"getInstance ...FIXME NOTE: getInstance is used when...FIXME","title":"getInstance(inst: String): Properties"},{"location":"metrics/MetricsServlet/","text":"MetricsServlet JSON Metrics Sink \u00b6 MetricsServlet is a metrics sink that gives metrics snapshots in JSON format. MetricsServlet is a \"special\" sink as it is only available to the metrics instances with a web UI: Driver of a Spark application Spark Standalone's Master and Worker You can access the metrics from MetricsServlet at /metrics/json URI by default. The entire URL depends on a metrics instance, e.g. http://localhost:4040/metrics/json/ for a running Spark application. $ http http://localhost:4040/metrics/json/ HTTP/1.1 200 OK Cache-Control: no-cache, no-store, must-revalidate Content-Length: 5005 Content-Type: text/json;charset=utf-8 Date: Mon, 11 Jun 2018 06:29:03 GMT Server: Jetty(9.3.z-SNAPSHOT) X-Content-Type-Options: nosniff X-Frame-Options: SAMEORIGIN X-XSS-Protection: 1; mode=block { \"counters\": { \"local-1528698499919.driver.HiveExternalCatalog.fileCacheHits\": { \"count\": 0 }, \"local-1528698499919.driver.HiveExternalCatalog.filesDiscovered\": { \"count\": 0 }, \"local-1528698499919.driver.HiveExternalCatalog.hiveClientCalls\": { \"count\": 0 }, \"local-1528698499919.driver.HiveExternalCatalog.parallelListingJobCount\": { \"count\": 0 }, \"local-1528698499919.driver.HiveExternalCatalog.partitionsFetched\": { \"count\": 0 }, \"local-1528698499919.driver.LiveListenerBus.numEventsPosted\": { \"count\": 7 }, \"local-1528698499919.driver.LiveListenerBus.queue.appStatus.numDroppedEvents\": { \"count\": 0 }, \"local-1528698499919.driver.LiveListenerBus.queue.executorManagement.numDroppedEvents\": { \"count\": 0 } }, ... MetricsServlet is < > exclusively when MetricsSystem is link:spark-metrics-MetricsSystem.adoc#start[started] (and requested to link:spark-metrics-MetricsSystem.adoc#registerSinks[register metrics sinks]). MetricsServlet can be configured using configuration properties with sink.servlet prefix (in link:spark-metrics-MetricsConfig.adoc[metrics configuration]). That is not required since MetricsConfig link:spark-metrics-MetricsConfig.adoc#setDefaultProperties[makes sure] that MetricsServlet is always configured. MetricsServlet uses https://fasterxml.github.io/jackson-databind/[jackson-databind ], the general data-binding package for Jackson (as < >) with https://metrics.dropwizard.io/3.1.0/[Dropwizard Metrics] library (i.e. registering a Coda Hale MetricsModule ). [[properties]] .MetricsServlet's Configuration Properties [cols=\"1,1,2\",options=\"header\",width=\"100%\"] |=== | Name | Default | Description | path | /metrics/json/ | [[path]] Path URI prefix to bind to | sample | false | [[sample]] Whether to show entire set of samples for histograms |=== [[internal-registries]] .MetricsServlet's Internal Properties (e.g. Registries, Counters and Flags) [cols=\"1,2\",options=\"header\",width=\"100%\"] |=== | Name | Description | mapper | [[mapper]] Jaxson's https://fasterxml.github.io/jackson-databind/javadoc/2.6/com/fasterxml/jackson/databind/ObjectMapper.html[com.fasterxml.jackson.databind.ObjectMapper ] that \"provides functionality for reading and writing JSON, either to and from basic POJOs (Plain Old Java Objects), or to and from a general-purpose JSON Tree Model (JsonNode), as well as related functionality for performing conversions.\" When created, mapper is requested to register a Coda Hale https://metrics.dropwizard.io/3.1.0/apidocs/com/codahale/metrics/json/MetricsModule.html[com.codahale.metrics.json.MetricsModule ]. Used exclusively when MetricsServlet is requested to < >. | servletPath | [[servletPath]] Value of < > configuration property | servletShowSample | [[servletShowSample]] Flag to control whether to show samples ( true ) or not ( false ). servletShowSample is the value of < > configuration property (if defined) or false . Used when < > is requested to register a Coda Hale https://metrics.dropwizard.io/3.1.0/apidocs/com/codahale/metrics/json/MetricsModule.html[com.codahale.metrics.json.MetricsModule ]. |=== === [[creating-instance]] Creating MetricsServlet Instance MetricsServlet takes the following when created: [[property]] Configuration Properties (as Java Properties ) [[registry]] Dropwizard Metrics' https://metrics.dropwizard.io/3.1.0/apidocs/com/codahale/metrics/MetricRegistry.html[MetricRegistry ] [[securityMgr]] SecurityManager MetricsServlet initializes the < >. === [[getMetricsSnapshot]] Requesting Metrics Snapshot -- getMetricsSnapshot Method [source, scala] \u00b6 getMetricsSnapshot(request: HttpServletRequest): String \u00b6 getMetricsSnapshot simply requests the < > to serialize the < > to a JSON string (using link:++ https://fasterxml.github.io/jackson-databind/javadoc/2.6/com/fasterxml/jackson/databind/ObjectMapper.html#writeValueAsString-java.lang.Object-++[ObjectMapper.writeValueAsString ]). NOTE: getMetricsSnapshot is used exclusively when MetricsServlet is requested to < >. === [[getHandlers]] Requesting JSON Servlet Handler -- getHandlers Method [source, scala] \u00b6 getHandlers(conf: SparkConf): Array[ServletContextHandler] \u00b6 getHandlers returns just a single ServletContextHandler (in a collection) that gives < > in JSON format at every request at < > URI path. NOTE: getHandlers is used exclusively when MetricsSystem is requested for link:spark-metrics-MetricsSystem.adoc#getServletHandlers[metrics ServletContextHandlers].","title":"MetricsServlet"},{"location":"metrics/MetricsServlet/#metricsservlet-json-metrics-sink","text":"MetricsServlet is a metrics sink that gives metrics snapshots in JSON format. MetricsServlet is a \"special\" sink as it is only available to the metrics instances with a web UI: Driver of a Spark application Spark Standalone's Master and Worker You can access the metrics from MetricsServlet at /metrics/json URI by default. The entire URL depends on a metrics instance, e.g. http://localhost:4040/metrics/json/ for a running Spark application. $ http http://localhost:4040/metrics/json/ HTTP/1.1 200 OK Cache-Control: no-cache, no-store, must-revalidate Content-Length: 5005 Content-Type: text/json;charset=utf-8 Date: Mon, 11 Jun 2018 06:29:03 GMT Server: Jetty(9.3.z-SNAPSHOT) X-Content-Type-Options: nosniff X-Frame-Options: SAMEORIGIN X-XSS-Protection: 1; mode=block { \"counters\": { \"local-1528698499919.driver.HiveExternalCatalog.fileCacheHits\": { \"count\": 0 }, \"local-1528698499919.driver.HiveExternalCatalog.filesDiscovered\": { \"count\": 0 }, \"local-1528698499919.driver.HiveExternalCatalog.hiveClientCalls\": { \"count\": 0 }, \"local-1528698499919.driver.HiveExternalCatalog.parallelListingJobCount\": { \"count\": 0 }, \"local-1528698499919.driver.HiveExternalCatalog.partitionsFetched\": { \"count\": 0 }, \"local-1528698499919.driver.LiveListenerBus.numEventsPosted\": { \"count\": 7 }, \"local-1528698499919.driver.LiveListenerBus.queue.appStatus.numDroppedEvents\": { \"count\": 0 }, \"local-1528698499919.driver.LiveListenerBus.queue.executorManagement.numDroppedEvents\": { \"count\": 0 } }, ... MetricsServlet is < > exclusively when MetricsSystem is link:spark-metrics-MetricsSystem.adoc#start[started] (and requested to link:spark-metrics-MetricsSystem.adoc#registerSinks[register metrics sinks]). MetricsServlet can be configured using configuration properties with sink.servlet prefix (in link:spark-metrics-MetricsConfig.adoc[metrics configuration]). That is not required since MetricsConfig link:spark-metrics-MetricsConfig.adoc#setDefaultProperties[makes sure] that MetricsServlet is always configured. MetricsServlet uses https://fasterxml.github.io/jackson-databind/[jackson-databind ], the general data-binding package for Jackson (as < >) with https://metrics.dropwizard.io/3.1.0/[Dropwizard Metrics] library (i.e. registering a Coda Hale MetricsModule ). [[properties]] .MetricsServlet's Configuration Properties [cols=\"1,1,2\",options=\"header\",width=\"100%\"] |=== | Name | Default | Description | path | /metrics/json/ | [[path]] Path URI prefix to bind to | sample | false | [[sample]] Whether to show entire set of samples for histograms |=== [[internal-registries]] .MetricsServlet's Internal Properties (e.g. Registries, Counters and Flags) [cols=\"1,2\",options=\"header\",width=\"100%\"] |=== | Name | Description | mapper | [[mapper]] Jaxson's https://fasterxml.github.io/jackson-databind/javadoc/2.6/com/fasterxml/jackson/databind/ObjectMapper.html[com.fasterxml.jackson.databind.ObjectMapper ] that \"provides functionality for reading and writing JSON, either to and from basic POJOs (Plain Old Java Objects), or to and from a general-purpose JSON Tree Model (JsonNode), as well as related functionality for performing conversions.\" When created, mapper is requested to register a Coda Hale https://metrics.dropwizard.io/3.1.0/apidocs/com/codahale/metrics/json/MetricsModule.html[com.codahale.metrics.json.MetricsModule ]. Used exclusively when MetricsServlet is requested to < >. | servletPath | [[servletPath]] Value of < > configuration property | servletShowSample | [[servletShowSample]] Flag to control whether to show samples ( true ) or not ( false ). servletShowSample is the value of < > configuration property (if defined) or false . Used when < > is requested to register a Coda Hale https://metrics.dropwizard.io/3.1.0/apidocs/com/codahale/metrics/json/MetricsModule.html[com.codahale.metrics.json.MetricsModule ]. |=== === [[creating-instance]] Creating MetricsServlet Instance MetricsServlet takes the following when created: [[property]] Configuration Properties (as Java Properties ) [[registry]] Dropwizard Metrics' https://metrics.dropwizard.io/3.1.0/apidocs/com/codahale/metrics/MetricRegistry.html[MetricRegistry ] [[securityMgr]] SecurityManager MetricsServlet initializes the < >. === [[getMetricsSnapshot]] Requesting Metrics Snapshot -- getMetricsSnapshot Method","title":"MetricsServlet JSON Metrics Sink"},{"location":"metrics/MetricsServlet/#source-scala","text":"","title":"[source, scala]"},{"location":"metrics/MetricsServlet/#getmetricssnapshotrequest-httpservletrequest-string","text":"getMetricsSnapshot simply requests the < > to serialize the < > to a JSON string (using link:++ https://fasterxml.github.io/jackson-databind/javadoc/2.6/com/fasterxml/jackson/databind/ObjectMapper.html#writeValueAsString-java.lang.Object-++[ObjectMapper.writeValueAsString ]). NOTE: getMetricsSnapshot is used exclusively when MetricsServlet is requested to < >. === [[getHandlers]] Requesting JSON Servlet Handler -- getHandlers Method","title":"getMetricsSnapshot(request: HttpServletRequest): String"},{"location":"metrics/MetricsServlet/#source-scala_1","text":"","title":"[source, scala]"},{"location":"metrics/MetricsServlet/#gethandlersconf-sparkconf-arrayservletcontexthandler","text":"getHandlers returns just a single ServletContextHandler (in a collection) that gives < > in JSON format at every request at < > URI path. NOTE: getHandlers is used exclusively when MetricsSystem is requested for link:spark-metrics-MetricsSystem.adoc#getServletHandlers[metrics ServletContextHandlers].","title":"getHandlers(conf: SparkConf): Array[ServletContextHandler]"},{"location":"metrics/MetricsSystem/","text":"MetricsSystem \u00b6 MetricsSystem is a < > of metrics < > and < > of a < >, e.g. the driver of a Spark application. .Creating MetricsSystem for Driver image::spark-metrics-MetricsSystem-driver.png[align=\"center\"] MetricsSystem may have at most one < > (which is link:spark-metrics-MetricsConfig.adoc#setDefaultProperties[registered by default]). When < >, MetricsSystem requests < > to link:spark-metrics-MetricsConfig.adoc#initialize[initialize]. .Creating MetricsSystem image::spark-metrics-MetricsSystem.png[align=\"center\"] [[metrics-instances]] [[subsystems]] .Metrics Instances (Subsystems) and MetricsSystems [cols=\"1,2\",options=\"header\",width=\"100%\"] |=== | Name | When Created | applications | Spark Standalone's Master is link:spark-standalone-Master.adoc#creating-instance[created]. | driver | SparkEnv is xref:core:SparkEnv.adoc#create[created] for the driver. | executor | SparkEnv is xref:core:SparkEnv.adoc#create[created] for an executor. | master | Spark Standalone's Master is link:spark-standalone-Master.adoc#creating-instance[created]. | mesos_cluster | Spark on Mesos' MesosClusterScheduler is created. | shuffleService | ExternalShuffleService is xref:deploy:ExternalShuffleService.adoc#creating-instance[created]. | worker | Spark Standalone's Worker is link:spark-standalone-worker.adoc#creating-instance[created]. |=== MetricsSystem uses < > as the integration point to Dropwizard Metrics library. [[internal-registries]] .MetricsSystem's Internal Registries and Counters [cols=\"1,2\",options=\"header\",width=\"100%\"] |=== | Name | Description | [[metricsConfig]] metricsConfig | link:spark-metrics-MetricsConfig.adoc[MetricsConfig] Initialized when MetricsSystem is < >. Used when MetricsSystem registers < > and < >. | [[metricsServlet]] metricsServlet | link:spark-metrics-MetricsServlet.adoc[MetricsServlet JSON metrics sink] that is only available for the < > with a web UI, i.e. the driver of a Spark application and Spark Standalone's Master . Initialized when MetricsSystem registers < > (and finds a configuration entry with servlet sink name). Used exclusively when MetricsSystem is requested for a < >. | [[registry]] registry a| Dropwizard Metrics' https://metrics.dropwizard.io/3.1.0/apidocs/com/codahale/metrics/MetricRegistry.html[MetricRegistry ] Used when MetricsSystem is requested to: < > < > < > (that in turn < >) | [[running]] running | Flag that indicates whether MetricsSystem has been < > ( true ) or not ( false ) Default: false | [[sinks]] sinks | link:spark-metrics-Sink.adoc[Metrics sinks] in a Spark application. Used when MetricsSystem < > and < >. | [[sources]] sources | link:spark-metrics-Source.adoc[Metrics sources] in a Spark application. Used when MetricsSystem < >. |=== [TIP] \u00b6 Enable WARN or ERROR logging levels for org.apache.spark.metrics.MetricsSystem logger to see what happens in MetricsSystem. Add the following line to conf/log4j.properties : log4j.logger.org.apache.spark.metrics.MetricsSystem=WARN Refer to link:spark-logging.adoc[Logging]. \u00b6 == [[StaticSources]] \"Static\" Metrics Sources for Spark SQL -- StaticSources CAUTION: FIXME == [[registerSource]] Registering Metrics Source -- registerSource Method [source, scala] \u00b6 registerSource(source: Source): Unit \u00b6 registerSource adds source to < > internal registry. registerSource < > for the metrics source and registers it with < >. NOTE: registerSource uses Metrics' link:++ http://metrics.dropwizard.io/3.1.0/apidocs/com/codahale/metrics/MetricRegistry.html#register-java.lang.String-T-++[MetricRegistry.register ] to register a metrics source under a given name. When registerSource tries to register a name more than once, you should see the following INFO message in the logs: INFO Metrics already registered [NOTE] \u00b6 registerSource is used when: SparkContext link:spark-SparkContext-creating-instance-internals.adoc#registerSource[registers metrics sources] for: ** xref:scheduler:DAGScheduler.adoc#metricsSource[DAGScheduler] ** link:spark-BlockManager-BlockManagerSource.adoc[BlockManager] ** link:spark-ExecutorAllocationManager.adoc#executorAllocationManagerSource[ExecutorAllocationManager] (for xref:ROOT:spark-dynamic-allocation.adoc[]) MetricsSystem < > (and registers the \"static\" metrics sources -- CodegenMetrics and HiveCatalogMetrics ) and does < >. Executor xref:executor:Executor.adoc#creating-instance[is created] (and registers a xref:executor:ExecutorSource.adoc[]) ExternalShuffleService xref:deploy:ExternalShuffleService.adoc#start[is started] (and registers ExternalShuffleServiceSource ) Spark Structured Streaming's StreamExecution runs batches as data arrives (when metrics are enabled). Spark Streaming's StreamingContext is started (and registers StreamingSource ) Spark Standalone's Master and Worker start (and register their MasterSource and WorkerSource , respectively) Spark Standalone's Master registers a Spark application (and registers a ApplicationSource ) Spark on Mesos' MesosClusterScheduler is started (and registers a MesosClusterSchedulerSource ) \u00b6 == [[buildRegistryName]] Building Metrics Source Identifier -- buildRegistryName Method [source, scala] \u00b6 buildRegistryName(source: Source): String \u00b6 NOTE: buildRegistryName is used to build the metrics source identifiers for a Spark application's driver and executors, but also for other Spark framework's components (e.g. Spark Standalone's master and workers). NOTE: buildRegistryName uses link:spark-metrics-properties.adoc#spark.metrics.namespace[spark.metrics.namespace] and xref:executor:Executor.adoc#spark.executor.id[spark.executor.id] Spark properties to differentiate between a Spark application's driver and executors, and the other Spark framework's components. (only when < > is driver or executor ) buildRegistryName builds metrics source name that is made up of link:spark-metrics-properties.adoc#spark.metrics.namespace[spark.metrics.namespace], xref:executor:Executor.adoc#spark.executor.id[spark.executor.id] and the name of the source . NOTE: buildRegistryName uses Dropwizard Metrics' https://metrics.dropwizard.io/3.1.0/apidocs/com/codahale/metrics/MetricRegistry.html[MetricRegistry ] to build metrics source identifiers. CAUTION: FIXME Finish for the other components. NOTE: buildRegistryName is used when MetricsSystem < > or < > a metrics source. == [[registerSources]] Registering Metrics Sources for Spark Instance -- registerSources Internal Method [source, scala] \u00b6 registerSources(): Unit \u00b6 registerSources finds < > configuration for the < >. NOTE: instance is defined when MetricsSystem < >. registerSources finds the configuration of all the link:spark-metrics-Source.adoc[metrics sources] for the subsystem (as described with source. prefix). For every metrics source, registerSources finds class property, creates an instance, and in the end < >. When registerSources fails, you should see the following ERROR message in the logs followed by the exception. ERROR Source class [classPath] cannot be instantiated NOTE: registerSources is used exclusively when MetricsSystem is < >. == [[getServletHandlers]] Requesting JSON Servlet Handler -- getServletHandlers Method [source, scala] \u00b6 getServletHandlers: Array[ServletContextHandler] \u00b6 If the MetricsSystem is < > and the < > is defined for the metrics system, getServletHandlers simply requests the < > for the link:spark-metrics-MetricsServlet.adoc#getHandlers[JSON servlet handler]. When MetricsSystem is not < > getServletHandlers throws an IllegalArgumentException . Can only call getServletHandlers on a running MetricsSystem [NOTE] \u00b6 getServletHandlers is used when: SparkContext is link:spark-SparkContext-creating-instance-internals.adoc#MetricsSystem-getServletHandlers[created] * Spark Standalone's Master and Worker are requested to start (as onStart ) \u00b6 == [[registerSinks]] Registering Metrics Sinks -- registerSinks Internal Method [source, scala] \u00b6 registerSinks(): Unit \u00b6 registerSinks requests the < > for the link:spark-metrics-MetricsConfig.adoc#getInstance[configuration] of the < >. registerSinks requests the < > for the link:spark-metrics-MetricsConfig.adoc#subProperties[configuration] of all metrics sinks (i.e. configuration entries that match ^sink\\\\.(.+)\\\\.(.+) regular expression). For every metrics sink configuration, registerSinks takes class property and (if defined) creates an instance of the metric sink using an constructor that takes the configuration, < > and < >. For a single servlet metrics sink, registerSinks converts the sink to a link:spark-metrics-MetricsServlet.adoc[MetricsServlet] and sets the < > internal registry. For all other metrics sinks, registerSinks adds the sink to the < > internal registry. In case of an Exception , registerSinks prints out the following ERROR message to the logs: Sink class [classPath] cannot be instantiated NOTE: registerSinks is used exclusively when MetricsSystem is requested to < >. == [[stop]] stop Method [source, scala] \u00b6 stop(): Unit \u00b6 stop ...FIXME NOTE: stop is used when...FIXME == [[getSourcesByName]] getSourcesByName Method [source, scala] \u00b6 getSourcesByName(sourceName: String): Seq[Source] \u00b6 getSourcesByName ...FIXME NOTE: getSourcesByName is used when...FIXME == [[removeSource]] removeSource Method [source, scala] \u00b6 removeSource(source: Source): Unit \u00b6 removeSource ...FIXME NOTE: removeSource is used when...FIXME == [[creating-instance]] Creating MetricsSystem Instance MetricsSystem takes the following when created: [[instance]] Instance name [[conf]] xref:ROOT:SparkConf.adoc[SparkConf] [[securityMgr]] SecurityManager MetricsSystem initializes the < >. When created, MetricsSystem requests < > to link:spark-metrics-MetricsConfig.adoc#initialize[initialize]. NOTE: < > is used to create a new MetricsSystems instance instead. == [[createMetricsSystem]] Creating MetricsSystem Instance For Subsystem -- createMetricsSystem Factory Method [source, scala] \u00b6 createMetricsSystem( instance: String conf: SparkConf securityMgr: SecurityManager): MetricsSystem createMetricsSystem returns a new < >. NOTE: createMetricsSystem is used when a < > is created. == [[report]] Requesting Sinks to Report Metrics -- report Method [source, scala] \u00b6 report(): Unit \u00b6 report simply requests the registered < > to link:spark-metrics-Sink.adoc#report[report metrics]. NOTE: report is used when xref:ROOT:SparkContext.adoc#stop[SparkContext], xref:executor:Executor.adoc#stop[Executor], Spark Standalone's Master and Worker , Spark on Mesos' MesosClusterScheduler are requested to stop == [[start]] Starting MetricsSystem -- start Method [source, scala] \u00b6 start(): Unit \u00b6 start turns < > flag on. NOTE: start can only be called once and < > an IllegalArgumentException when called multiple times. start < > the < > for Spark SQL, i.e. CodegenMetrics and HiveCatalogMetrics . start then registers the configured metrics < > and < > for the < >. In the end, start requests the registered < > to link:spark-metrics-Sink.adoc#start[start]. [[start-IllegalArgumentException]] start throws an IllegalArgumentException when < > flag is on. requirement failed: Attempting to start a MetricsSystem that is already running [NOTE] \u00b6 start is used when: SparkContext is link:spark-SparkContext-creating-instance-internals.adoc#MetricsSystem-start[created] SparkEnv is xref:core:SparkEnv.adoc#create[created] (on executors) ExternalShuffleService is requested to xref:deploy:ExternalShuffleService.adoc#start[start] * Spark Standalone's Master and Worker , and Spark on Mesos' MesosClusterScheduler are requested to start \u00b6","title":"MetricsSystem"},{"location":"metrics/MetricsSystem/#metricssystem","text":"MetricsSystem is a < > of metrics < > and < > of a < >, e.g. the driver of a Spark application. .Creating MetricsSystem for Driver image::spark-metrics-MetricsSystem-driver.png[align=\"center\"] MetricsSystem may have at most one < > (which is link:spark-metrics-MetricsConfig.adoc#setDefaultProperties[registered by default]). When < >, MetricsSystem requests < > to link:spark-metrics-MetricsConfig.adoc#initialize[initialize]. .Creating MetricsSystem image::spark-metrics-MetricsSystem.png[align=\"center\"] [[metrics-instances]] [[subsystems]] .Metrics Instances (Subsystems) and MetricsSystems [cols=\"1,2\",options=\"header\",width=\"100%\"] |=== | Name | When Created | applications | Spark Standalone's Master is link:spark-standalone-Master.adoc#creating-instance[created]. | driver | SparkEnv is xref:core:SparkEnv.adoc#create[created] for the driver. | executor | SparkEnv is xref:core:SparkEnv.adoc#create[created] for an executor. | master | Spark Standalone's Master is link:spark-standalone-Master.adoc#creating-instance[created]. | mesos_cluster | Spark on Mesos' MesosClusterScheduler is created. | shuffleService | ExternalShuffleService is xref:deploy:ExternalShuffleService.adoc#creating-instance[created]. | worker | Spark Standalone's Worker is link:spark-standalone-worker.adoc#creating-instance[created]. |=== MetricsSystem uses < > as the integration point to Dropwizard Metrics library. [[internal-registries]] .MetricsSystem's Internal Registries and Counters [cols=\"1,2\",options=\"header\",width=\"100%\"] |=== | Name | Description | [[metricsConfig]] metricsConfig | link:spark-metrics-MetricsConfig.adoc[MetricsConfig] Initialized when MetricsSystem is < >. Used when MetricsSystem registers < > and < >. | [[metricsServlet]] metricsServlet | link:spark-metrics-MetricsServlet.adoc[MetricsServlet JSON metrics sink] that is only available for the < > with a web UI, i.e. the driver of a Spark application and Spark Standalone's Master . Initialized when MetricsSystem registers < > (and finds a configuration entry with servlet sink name). Used exclusively when MetricsSystem is requested for a < >. | [[registry]] registry a| Dropwizard Metrics' https://metrics.dropwizard.io/3.1.0/apidocs/com/codahale/metrics/MetricRegistry.html[MetricRegistry ] Used when MetricsSystem is requested to: < > < > < > (that in turn < >) | [[running]] running | Flag that indicates whether MetricsSystem has been < > ( true ) or not ( false ) Default: false | [[sinks]] sinks | link:spark-metrics-Sink.adoc[Metrics sinks] in a Spark application. Used when MetricsSystem < > and < >. | [[sources]] sources | link:spark-metrics-Source.adoc[Metrics sources] in a Spark application. Used when MetricsSystem < >. |===","title":"MetricsSystem"},{"location":"metrics/MetricsSystem/#tip","text":"Enable WARN or ERROR logging levels for org.apache.spark.metrics.MetricsSystem logger to see what happens in MetricsSystem. Add the following line to conf/log4j.properties : log4j.logger.org.apache.spark.metrics.MetricsSystem=WARN","title":"[TIP]"},{"location":"metrics/MetricsSystem/#refer-to-linkspark-loggingadoclogging","text":"== [[StaticSources]] \"Static\" Metrics Sources for Spark SQL -- StaticSources CAUTION: FIXME == [[registerSource]] Registering Metrics Source -- registerSource Method","title":"Refer to link:spark-logging.adoc[Logging]."},{"location":"metrics/MetricsSystem/#source-scala","text":"","title":"[source, scala]"},{"location":"metrics/MetricsSystem/#registersourcesource-source-unit","text":"registerSource adds source to < > internal registry. registerSource < > for the metrics source and registers it with < >. NOTE: registerSource uses Metrics' link:++ http://metrics.dropwizard.io/3.1.0/apidocs/com/codahale/metrics/MetricRegistry.html#register-java.lang.String-T-++[MetricRegistry.register ] to register a metrics source under a given name. When registerSource tries to register a name more than once, you should see the following INFO message in the logs: INFO Metrics already registered","title":"registerSource(source: Source): Unit"},{"location":"metrics/MetricsSystem/#note","text":"registerSource is used when: SparkContext link:spark-SparkContext-creating-instance-internals.adoc#registerSource[registers metrics sources] for: ** xref:scheduler:DAGScheduler.adoc#metricsSource[DAGScheduler] ** link:spark-BlockManager-BlockManagerSource.adoc[BlockManager] ** link:spark-ExecutorAllocationManager.adoc#executorAllocationManagerSource[ExecutorAllocationManager] (for xref:ROOT:spark-dynamic-allocation.adoc[]) MetricsSystem < > (and registers the \"static\" metrics sources -- CodegenMetrics and HiveCatalogMetrics ) and does < >. Executor xref:executor:Executor.adoc#creating-instance[is created] (and registers a xref:executor:ExecutorSource.adoc[]) ExternalShuffleService xref:deploy:ExternalShuffleService.adoc#start[is started] (and registers ExternalShuffleServiceSource ) Spark Structured Streaming's StreamExecution runs batches as data arrives (when metrics are enabled). Spark Streaming's StreamingContext is started (and registers StreamingSource ) Spark Standalone's Master and Worker start (and register their MasterSource and WorkerSource , respectively) Spark Standalone's Master registers a Spark application (and registers a ApplicationSource )","title":"[NOTE]"},{"location":"metrics/MetricsSystem/#spark-on-mesos-mesosclusterscheduler-is-started-and-registers-a-mesosclusterschedulersource","text":"== [[buildRegistryName]] Building Metrics Source Identifier -- buildRegistryName Method","title":"Spark on Mesos' MesosClusterScheduler is started (and registers a MesosClusterSchedulerSource)"},{"location":"metrics/MetricsSystem/#source-scala_1","text":"","title":"[source, scala]"},{"location":"metrics/MetricsSystem/#buildregistrynamesource-source-string","text":"NOTE: buildRegistryName is used to build the metrics source identifiers for a Spark application's driver and executors, but also for other Spark framework's components (e.g. Spark Standalone's master and workers). NOTE: buildRegistryName uses link:spark-metrics-properties.adoc#spark.metrics.namespace[spark.metrics.namespace] and xref:executor:Executor.adoc#spark.executor.id[spark.executor.id] Spark properties to differentiate between a Spark application's driver and executors, and the other Spark framework's components. (only when < > is driver or executor ) buildRegistryName builds metrics source name that is made up of link:spark-metrics-properties.adoc#spark.metrics.namespace[spark.metrics.namespace], xref:executor:Executor.adoc#spark.executor.id[spark.executor.id] and the name of the source . NOTE: buildRegistryName uses Dropwizard Metrics' https://metrics.dropwizard.io/3.1.0/apidocs/com/codahale/metrics/MetricRegistry.html[MetricRegistry ] to build metrics source identifiers. CAUTION: FIXME Finish for the other components. NOTE: buildRegistryName is used when MetricsSystem < > or < > a metrics source. == [[registerSources]] Registering Metrics Sources for Spark Instance -- registerSources Internal Method","title":"buildRegistryName(source: Source): String"},{"location":"metrics/MetricsSystem/#source-scala_2","text":"","title":"[source, scala]"},{"location":"metrics/MetricsSystem/#registersources-unit","text":"registerSources finds < > configuration for the < >. NOTE: instance is defined when MetricsSystem < >. registerSources finds the configuration of all the link:spark-metrics-Source.adoc[metrics sources] for the subsystem (as described with source. prefix). For every metrics source, registerSources finds class property, creates an instance, and in the end < >. When registerSources fails, you should see the following ERROR message in the logs followed by the exception. ERROR Source class [classPath] cannot be instantiated NOTE: registerSources is used exclusively when MetricsSystem is < >. == [[getServletHandlers]] Requesting JSON Servlet Handler -- getServletHandlers Method","title":"registerSources(): Unit"},{"location":"metrics/MetricsSystem/#source-scala_3","text":"","title":"[source, scala]"},{"location":"metrics/MetricsSystem/#getservlethandlers-arrayservletcontexthandler","text":"If the MetricsSystem is < > and the < > is defined for the metrics system, getServletHandlers simply requests the < > for the link:spark-metrics-MetricsServlet.adoc#getHandlers[JSON servlet handler]. When MetricsSystem is not < > getServletHandlers throws an IllegalArgumentException . Can only call getServletHandlers on a running MetricsSystem","title":"getServletHandlers: Array[ServletContextHandler]"},{"location":"metrics/MetricsSystem/#note_1","text":"getServletHandlers is used when: SparkContext is link:spark-SparkContext-creating-instance-internals.adoc#MetricsSystem-getServletHandlers[created]","title":"[NOTE]"},{"location":"metrics/MetricsSystem/#spark-standalones-master-and-worker-are-requested-to-start-as-onstart","text":"== [[registerSinks]] Registering Metrics Sinks -- registerSinks Internal Method","title":"* Spark Standalone's Master and Worker are requested to start (as onStart)"},{"location":"metrics/MetricsSystem/#source-scala_4","text":"","title":"[source, scala]"},{"location":"metrics/MetricsSystem/#registersinks-unit","text":"registerSinks requests the < > for the link:spark-metrics-MetricsConfig.adoc#getInstance[configuration] of the < >. registerSinks requests the < > for the link:spark-metrics-MetricsConfig.adoc#subProperties[configuration] of all metrics sinks (i.e. configuration entries that match ^sink\\\\.(.+)\\\\.(.+) regular expression). For every metrics sink configuration, registerSinks takes class property and (if defined) creates an instance of the metric sink using an constructor that takes the configuration, < > and < >. For a single servlet metrics sink, registerSinks converts the sink to a link:spark-metrics-MetricsServlet.adoc[MetricsServlet] and sets the < > internal registry. For all other metrics sinks, registerSinks adds the sink to the < > internal registry. In case of an Exception , registerSinks prints out the following ERROR message to the logs: Sink class [classPath] cannot be instantiated NOTE: registerSinks is used exclusively when MetricsSystem is requested to < >. == [[stop]] stop Method","title":"registerSinks(): Unit"},{"location":"metrics/MetricsSystem/#source-scala_5","text":"","title":"[source, scala]"},{"location":"metrics/MetricsSystem/#stop-unit","text":"stop ...FIXME NOTE: stop is used when...FIXME == [[getSourcesByName]] getSourcesByName Method","title":"stop(): Unit"},{"location":"metrics/MetricsSystem/#source-scala_6","text":"","title":"[source, scala]"},{"location":"metrics/MetricsSystem/#getsourcesbynamesourcename-string-seqsource","text":"getSourcesByName ...FIXME NOTE: getSourcesByName is used when...FIXME == [[removeSource]] removeSource Method","title":"getSourcesByName(sourceName: String): Seq[Source]"},{"location":"metrics/MetricsSystem/#source-scala_7","text":"","title":"[source, scala]"},{"location":"metrics/MetricsSystem/#removesourcesource-source-unit","text":"removeSource ...FIXME NOTE: removeSource is used when...FIXME == [[creating-instance]] Creating MetricsSystem Instance MetricsSystem takes the following when created: [[instance]] Instance name [[conf]] xref:ROOT:SparkConf.adoc[SparkConf] [[securityMgr]] SecurityManager MetricsSystem initializes the < >. When created, MetricsSystem requests < > to link:spark-metrics-MetricsConfig.adoc#initialize[initialize]. NOTE: < > is used to create a new MetricsSystems instance instead. == [[createMetricsSystem]] Creating MetricsSystem Instance For Subsystem -- createMetricsSystem Factory Method","title":"removeSource(source: Source): Unit"},{"location":"metrics/MetricsSystem/#source-scala_8","text":"createMetricsSystem( instance: String conf: SparkConf securityMgr: SecurityManager): MetricsSystem createMetricsSystem returns a new < >. NOTE: createMetricsSystem is used when a < > is created. == [[report]] Requesting Sinks to Report Metrics -- report Method","title":"[source, scala]"},{"location":"metrics/MetricsSystem/#source-scala_9","text":"","title":"[source, scala]"},{"location":"metrics/MetricsSystem/#report-unit","text":"report simply requests the registered < > to link:spark-metrics-Sink.adoc#report[report metrics]. NOTE: report is used when xref:ROOT:SparkContext.adoc#stop[SparkContext], xref:executor:Executor.adoc#stop[Executor], Spark Standalone's Master and Worker , Spark on Mesos' MesosClusterScheduler are requested to stop == [[start]] Starting MetricsSystem -- start Method","title":"report(): Unit"},{"location":"metrics/MetricsSystem/#source-scala_10","text":"","title":"[source, scala]"},{"location":"metrics/MetricsSystem/#start-unit","text":"start turns < > flag on. NOTE: start can only be called once and < > an IllegalArgumentException when called multiple times. start < > the < > for Spark SQL, i.e. CodegenMetrics and HiveCatalogMetrics . start then registers the configured metrics < > and < > for the < >. In the end, start requests the registered < > to link:spark-metrics-Sink.adoc#start[start]. [[start-IllegalArgumentException]] start throws an IllegalArgumentException when < > flag is on. requirement failed: Attempting to start a MetricsSystem that is already running","title":"start(): Unit"},{"location":"metrics/MetricsSystem/#note_2","text":"start is used when: SparkContext is link:spark-SparkContext-creating-instance-internals.adoc#MetricsSystem-start[created] SparkEnv is xref:core:SparkEnv.adoc#create[created] (on executors) ExternalShuffleService is requested to xref:deploy:ExternalShuffleService.adoc#start[start]","title":"[NOTE]"},{"location":"metrics/MetricsSystem/#spark-standalones-master-and-worker-and-spark-on-mesos-mesosclusterscheduler-are-requested-to-start","text":"","title":"* Spark Standalone's Master and Worker, and Spark on Mesos' MesosClusterScheduler are requested to start"},{"location":"metrics/Sink/","text":"Sink \u00b6 Sink is a < > of metrics sinks . [[contract]] [source, scala] package org.apache.spark.metrics.sink trait Sink { def start(): Unit def stop(): Unit def report(): Unit } NOTE: Sink is a private[spark] contract. .Sink Contract [cols=\"1,2\",options=\"header\",width=\"100%\"] |=== | Method | Description | start | [[start]] Used when...FIXME | stop | [[stop]] Used when...FIXME | report | [[report]] Used when...FIXME |=== [[implementations]] .Sinks [cols=\"1,2\",options=\"header\",width=\"100%\"] |=== | Sink | Description | ConsoleSink | [[ConsoleSink]] | CsvSink | [[CsvSink]] | GraphiteSink | [[GraphiteSink]] | JmxSink | [[JmxSink]] | link:spark-metrics-MetricsServlet.adoc[MetricsServlet] | [[MetricsServlet]] | Slf4jSink | [[Slf4jSink]] | StatsdSink | [[StatsdSink]] |=== NOTE: All known < > in Spark 2.3 are in org.apache.spark.metrics.sink Scala package.","title":"Sink"},{"location":"metrics/Sink/#sink","text":"Sink is a < > of metrics sinks . [[contract]] [source, scala] package org.apache.spark.metrics.sink trait Sink { def start(): Unit def stop(): Unit def report(): Unit } NOTE: Sink is a private[spark] contract. .Sink Contract [cols=\"1,2\",options=\"header\",width=\"100%\"] |=== | Method | Description | start | [[start]] Used when...FIXME | stop | [[stop]] Used when...FIXME | report | [[report]] Used when...FIXME |=== [[implementations]] .Sinks [cols=\"1,2\",options=\"header\",width=\"100%\"] |=== | Sink | Description | ConsoleSink | [[ConsoleSink]] | CsvSink | [[CsvSink]] | GraphiteSink | [[GraphiteSink]] | JmxSink | [[JmxSink]] | link:spark-metrics-MetricsServlet.adoc[MetricsServlet] | [[MetricsServlet]] | Slf4jSink | [[Slf4jSink]] | StatsdSink | [[StatsdSink]] |=== NOTE: All known < > in Spark 2.3 are in org.apache.spark.metrics.sink Scala package.","title":"Sink"},{"location":"metrics/Source/","text":"== [[Source]] Source -- Contract of Metrics Sources Source is a < > of metrics sources . [[contract]] [source, scala] package org.apache.spark.metrics.source trait Source { def sourceName: String def metricRegistry: MetricRegistry } NOTE: Source is a private[spark] contract. .Source Contract [cols=\"1,2\",options=\"header\",width=\"100%\"] |=== | Method | Description | sourceName | [[sourceName]] Used when...FIXME | metricRegistry | [[metricRegistry]] Dropwizard Metrics' https://metrics.dropwizard.io/3.1.0/apidocs/com/codahale/metrics/MetricRegistry.html[MetricRegistry ] Used when...FIXME |=== [[implementations]] .Sources [cols=\"1,2\",options=\"header\",width=\"100%\"] |=== | Source | Description | ApplicationSource | [[ApplicationSource]] | xref:storage:spark-BlockManager-BlockManagerSource.adoc[BlockManagerSource] | [[BlockManagerSource]] | CacheMetrics | [[CacheMetrics]] | CodegenMetrics | [[CodegenMetrics]] | xref:metrics:spark-scheduler-DAGSchedulerSource.adoc[DAGSchedulerSource] | [[DAGSchedulerSource]] | xref:ROOT:spark-service-ExecutorAllocationManagerSource.adoc[ExecutorAllocationManagerSource] | [[ExecutorAllocationManagerSource]] | xref:executor:ExecutorSource.adoc[] | [[ExecutorSource]] | ExternalShuffleServiceSource | [[ExternalShuffleServiceSource]] | HiveCatalogMetrics | [[HiveCatalogMetrics]] | xref:metrics:JvmSource.adoc[JvmSource] | [[JvmSource]] | LiveListenerBusMetrics | [[LiveListenerBusMetrics]] | MasterSource | [[MasterSource]] | MesosClusterSchedulerSource | [[MesosClusterSchedulerSource]] | xref:storage:ShuffleMetricsSource.adoc[] | [[ShuffleMetricsSource]] | StreamingSource | [[StreamingSource]] | WorkerSource | [[WorkerSource]] |===","title":"Source"},{"location":"metrics/configuration-properties/","text":"Configuration Properties \u00b6 spark.metrics.conf \u00b6 The metrics configuration file Default: metrics.properties spark.metrics.namespace \u00b6 Root namespace for metrics reporting Default: Spark Application ID (i.e. spark.app.id configuration property) Since a Spark application's ID changes with every execution of a Spark application, a custom namespace can be specified for an easier metrics reporting. Used when MetricsSystem is requested for a metrics source identifier ( metrics namespace )","title":"Configuration Properties"},{"location":"metrics/configuration-properties/#configuration-properties","text":"","title":"Configuration Properties"},{"location":"metrics/configuration-properties/#sparkmetricsconf","text":"The metrics configuration file Default: metrics.properties","title":" spark.metrics.conf"},{"location":"metrics/configuration-properties/#sparkmetricsnamespace","text":"Root namespace for metrics reporting Default: Spark Application ID (i.e. spark.app.id configuration property) Since a Spark application's ID changes with every execution of a Spark application, a custom namespace can be specified for an easier metrics reporting. Used when MetricsSystem is requested for a metrics source identifier ( metrics namespace )","title":" spark.metrics.namespace"}]}