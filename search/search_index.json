{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"The Internals of Apache Spark 3.0.1 \u00b6 Welcome to The Internals of Apache Spark online book! I'm Jacek Laskowski , a Seasoned IT Professional specializing in Apache Spark , Delta Lake , Apache Kafka and Kafka Streams . I'm very excited to have you here and hope you will enjoy exploring the internals of Apache Spark as much as I have. Flannery O'Connor I write to discover what I know. \"The Internals Of\" series I'm also writing other online books in the \"The Internals Of\" series. Please visit \"The Internals Of\" Online Books home page. Expect text and code snippets from a variety of public sources. Attribution follows. Now, let me introduce you to Apache Spark .","title":"Welcome"},{"location":"#the-internals-of-apache-spark-301","text":"Welcome to The Internals of Apache Spark online book! I'm Jacek Laskowski , a Seasoned IT Professional specializing in Apache Spark , Delta Lake , Apache Kafka and Kafka Streams . I'm very excited to have you here and hope you will enjoy exploring the internals of Apache Spark as much as I have. Flannery O'Connor I write to discover what I know. \"The Internals Of\" series I'm also writing other online books in the \"The Internals Of\" series. Please visit \"The Internals Of\" Online Books home page. Expect text and code snippets from a variety of public sources. Attribution follows. Now, let me introduce you to Apache Spark .","title":"The Internals of Apache Spark 3.0.1"},{"location":"SparkConf/","text":"= SparkConf Every user program starts with creating an instance of SparkConf that holds the xref:ROOT:spark-deployment-environments.adoc#master-urls[master URL] to connect to ( spark.master ), the name for your Spark application (that is later displayed in xref:webui:index.adoc[web UI] and becomes spark.app.name ) and other Spark properties required for proper runs. The instance of SparkConf can be used to create xref:ROOT:SparkContext.adoc[SparkContext]. [TIP] \u00b6 Start xref spark-shell.adoc[Spark shell] with --conf spark.logConf=true to log the effective Spark configuration as INFO when SparkContext is started. $ ./bin/spark-shell --conf spark.logConf=true ... 15/10/19 17:13:49 INFO SparkContext: Running Spark version 1.6.0-SNAPSHOT 15/10/19 17:13:49 INFO SparkContext: Spark configuration: spark.app.name=Spark shell spark.home=/Users/jacek/dev/oss/spark spark.jars= spark.logConf=true spark.master=local[*] spark.repl.class.uri=http://10.5.10.20:64055 spark.submit.deployMode=client ... Use sc.getConf.toDebugString to have a richer output once SparkContext has finished initializing. \u00b6 You can query for the values of Spark properties in xref spark-shell.adoc[Spark shell] as follows: scala> sc.getConf.getOption(\"spark.local.dir\") res0: Option[String] = None scala> sc.getConf.getOption(\"spark.app.name\") res1: Option[String] = Some(Spark shell) scala> sc.getConf.get(\"spark.master\") res2: String = local[*] == [[setIfMissing]] setIfMissing Method CAUTION: FIXME == [[isExecutorStartupConf]] isExecutorStartupConf Method CAUTION: FIXME == [[set]] set Method CAUTION: FIXME == Setting up Spark Properties There are the following places where a Spark application looks for Spark properties (in the order of importance from the least important to the most important): conf/spark-defaults.conf - the configuration file with the default Spark properties. Read xref:ROOT:spark-properties.adoc#spark-defaults-conf[spark-defaults.conf]. --conf or -c - the command-line option used by xref spark-submit.adoc[spark-submit] (and other shell scripts that use spark-submit or spark-class under the covers, e.g. spark-shell ) SparkConf == [[default-configuration]] Default Configuration The default Spark configuration is created when you execute the following code: [source, scala] \u00b6 import org.apache.spark.SparkConf val conf = new SparkConf It simply loads spark.* system properties. You can use conf.toDebugString or conf.getAll to have the spark.* system properties loaded printed out. [source, scala] \u00b6 scala> conf.getAll res0: Array[(String, String)] = Array((spark.app.name,Spark shell), (spark.jars,\"\"), (spark.master,local[*]), (spark.submit.deployMode,client)) scala> conf.toDebugString res1: String = spark.app.name=Spark shell spark.jars= spark.master=local[*] spark.submit.deployMode=client scala> println(conf.toDebugString) spark.app.name=Spark shell spark.jars= spark.master=local[*] spark.submit.deployMode=client == [[getAppId]] Unique Identifier of Spark Application -- getAppId Method [source, scala] \u00b6 getAppId: String \u00b6 getAppId returns the value of xref:ROOT:configuration-properties.adoc#spark.app.id[spark.app.id] configuration property or throws a NoSuchElementException if not set. getAppId is used when: NettyBlockTransferService is requested to xref:storage:NettyBlockTransferService.adoc#init[init] (and creates a xref:storage:NettyBlockRpcServer.adoc#creating-instance[NettyBlockRpcServer] as well as xref:storage:NettyBlockTransferService.adoc#appId[saves the identifier for later use]). Executor xref:executor:Executor.adoc#creating-instance[is created] (in non-local mode and xref:storage:BlockManager.adoc#initialize[requests BlockManager to initialize]). == [[getAvroSchema]] getAvroSchema Method [source, scala] \u00b6 getAvroSchema: Map[Long, String] \u00b6 getAvroSchema takes all avro.schema -prefixed configuration properties from < > and...FIXME getAvroSchema is used when KryoSerializer is created (and initializes avroSchemas).","title":"SparkConf"},{"location":"SparkConf/#tip","text":"Start xref spark-shell.adoc[Spark shell] with --conf spark.logConf=true to log the effective Spark configuration as INFO when SparkContext is started. $ ./bin/spark-shell --conf spark.logConf=true ... 15/10/19 17:13:49 INFO SparkContext: Running Spark version 1.6.0-SNAPSHOT 15/10/19 17:13:49 INFO SparkContext: Spark configuration: spark.app.name=Spark shell spark.home=/Users/jacek/dev/oss/spark spark.jars= spark.logConf=true spark.master=local[*] spark.repl.class.uri=http://10.5.10.20:64055 spark.submit.deployMode=client ...","title":"[TIP]"},{"location":"SparkConf/#use-scgetconftodebugstring-to-have-a-richer-output-once-sparkcontext-has-finished-initializing","text":"You can query for the values of Spark properties in xref spark-shell.adoc[Spark shell] as follows: scala> sc.getConf.getOption(\"spark.local.dir\") res0: Option[String] = None scala> sc.getConf.getOption(\"spark.app.name\") res1: Option[String] = Some(Spark shell) scala> sc.getConf.get(\"spark.master\") res2: String = local[*] == [[setIfMissing]] setIfMissing Method CAUTION: FIXME == [[isExecutorStartupConf]] isExecutorStartupConf Method CAUTION: FIXME == [[set]] set Method CAUTION: FIXME == Setting up Spark Properties There are the following places where a Spark application looks for Spark properties (in the order of importance from the least important to the most important): conf/spark-defaults.conf - the configuration file with the default Spark properties. Read xref:ROOT:spark-properties.adoc#spark-defaults-conf[spark-defaults.conf]. --conf or -c - the command-line option used by xref spark-submit.adoc[spark-submit] (and other shell scripts that use spark-submit or spark-class under the covers, e.g. spark-shell ) SparkConf == [[default-configuration]] Default Configuration The default Spark configuration is created when you execute the following code:","title":"Use sc.getConf.toDebugString to have a richer output once SparkContext has finished initializing."},{"location":"SparkConf/#source-scala","text":"import org.apache.spark.SparkConf val conf = new SparkConf It simply loads spark.* system properties. You can use conf.toDebugString or conf.getAll to have the spark.* system properties loaded printed out.","title":"[source, scala]"},{"location":"SparkConf/#source-scala_1","text":"scala> conf.getAll res0: Array[(String, String)] = Array((spark.app.name,Spark shell), (spark.jars,\"\"), (spark.master,local[*]), (spark.submit.deployMode,client)) scala> conf.toDebugString res1: String = spark.app.name=Spark shell spark.jars= spark.master=local[*] spark.submit.deployMode=client scala> println(conf.toDebugString) spark.app.name=Spark shell spark.jars= spark.master=local[*] spark.submit.deployMode=client == [[getAppId]] Unique Identifier of Spark Application -- getAppId Method","title":"[source, scala]"},{"location":"SparkConf/#source-scala_2","text":"","title":"[source, scala]"},{"location":"SparkConf/#getappid-string","text":"getAppId returns the value of xref:ROOT:configuration-properties.adoc#spark.app.id[spark.app.id] configuration property or throws a NoSuchElementException if not set. getAppId is used when: NettyBlockTransferService is requested to xref:storage:NettyBlockTransferService.adoc#init[init] (and creates a xref:storage:NettyBlockRpcServer.adoc#creating-instance[NettyBlockRpcServer] as well as xref:storage:NettyBlockTransferService.adoc#appId[saves the identifier for later use]). Executor xref:executor:Executor.adoc#creating-instance[is created] (in non-local mode and xref:storage:BlockManager.adoc#initialize[requests BlockManager to initialize]). == [[getAvroSchema]] getAvroSchema Method","title":"getAppId: String"},{"location":"SparkConf/#source-scala_3","text":"","title":"[source, scala]"},{"location":"SparkConf/#getavroschema-maplong-string","text":"getAvroSchema takes all avro.schema -prefixed configuration properties from < > and...FIXME getAvroSchema is used when KryoSerializer is created (and initializes avroSchemas).","title":"getAvroSchema: Map[Long, String]"},{"location":"SparkContext/","text":"= SparkContext SparkContext is the entry point to all components of Apache Spark (execution engine) and so the heart of a Spark application. In fact, you can consider an application a Spark application only when it uses a SparkContext (directly or indirectly). [[methods]] .SparkContext's Developer API (Public Methods) [cols=\"1,3\",options=\"header\",width=\"100%\"] |=== | Method | Description | < > a| [[addJar]] [source, scala] \u00b6 addJar(path: String): Unit \u00b6 | a| More to be added soon |=== Spark context link:spark-SparkContext-creating-instance-internals.adoc[sets up internal services] and establishes a connection to a link:spark-deployment-environments.adoc[Spark execution environment]. Once a < > you can use it to < >, < > and < >, access Spark services and < > (until SparkContext is < >). A Spark context is essentially a client of Spark's execution environment and acts as the master of your Spark application (don't get confused with the other meaning of link:spark-master.adoc[Master] in Spark, though). .Spark context acts as the master of your Spark application image::diagrams/sparkcontext-services.png[align=\"center\"] SparkContext offers the following functions: Getting current status of a Spark application ** < > ** < > ** < > ** < > ** < > ** < > ** < > that specifies the number of link:spark-rdd-partitions.adoc[partitions] in RDDs when they are created without specifying the number explicitly by a user. ** < > ** < > ** < > ** < > ** < > Setting Configuration ** < > ** link:spark-sparkcontext-local-properties.adoc[Local Properties -- Creating Logical Job Groups] ** < > ** < > Creating Distributed Entities ** < > ** < > ** < > Accessing services, e.g. < >, < >, xref:scheduler:LiveListenerBus.adoc[], xref:storage:BlockManager.adoc[BlockManager], xref:scheduler:SchedulerBackend.adoc[SchedulerBackends], xref:shuffle:ShuffleManager.adoc[ShuffleManager] and the < >. < > < > < > < > < > < > < > < > < > < > TIP: Read the scaladoc of http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.SparkContext[org.apache.spark.SparkContext ]. == [[addFile]] addFile Method [source, scala] \u00b6 addFile( path: String): Unit // <1> addFile( path: String, recursive: Boolean): Unit <1> recursive flag is off addFile adds the path file to be downloaded...FIXME [NOTE] \u00b6 addFile is used when: SparkContext is link:spark-SparkContext-creating-instance-internals.adoc#files[initialized] (and files were defined) Spark SQL's AddFileCommand is executed * Spark SQL's SessionResourceLoader is requested to load a file resource \u00b6 == [[unpersistRDD]] Removing RDD Blocks from BlockManagerMaster -- unpersistRDD Internal Method [source, scala] \u00b6 unpersistRDD(rddId: Int, blocking: Boolean = true): Unit \u00b6 unpersistRDD requests BlockManagerMaster to xref:storage:BlockManagerMaster.adoc#removeRdd[remove the blocks for the RDD] (given rddId ). NOTE: unpersistRDD uses SparkEnv xref:core:SparkEnv.adoc#blockManager[to access the current BlockManager ] that is in turn used to xref:storage:BlockManager.adoc#master[access the current BlockManagerMaster ]. unpersistRDD removes rddId from < > registry. In the end, unpersistRDD posts a xref:ROOT:SparkListener.adoc#SparkListenerUnpersistRDD[SparkListenerUnpersistRDD] (with rddId ) to < >. [NOTE] \u00b6 unpersistRDD is used when: ContextCleaner does xref:core:ContextCleaner.adoc#doCleanupRDD[doCleanupRDD] SparkContext < > (i.e. marks an RDD as non-persistent) \u00b6 == [[applicationId]] Unique Identifier of Spark Application -- applicationId Method CAUTION: FIXME == [[postApplicationStart]] postApplicationStart Internal Method [source, scala] \u00b6 postApplicationStart(): Unit \u00b6 postApplicationStart ...FIXME NOTE: postApplicationStart is used exclusively while SparkContext is being < > == [[postApplicationEnd]] postApplicationEnd Method CAUTION: FIXME == [[clearActiveContext]] clearActiveContext Method CAUTION: FIXME == [[getPersistentRDDs]] Accessing persistent RDDs -- getPersistentRDDs Method [source, scala] \u00b6 getPersistentRDDs: Map[Int, RDD[_]] \u00b6 getPersistentRDDs returns the collection of RDDs that have marked themselves as persistent via link:spark-rdd-caching.adoc#cache[cache]. Internally, getPersistentRDDs returns < > internal registry. == [[cancelJob]] Cancelling Job -- cancelJob Method [source, scala] \u00b6 cancelJob(jobId: Int) \u00b6 cancelJob requests DAGScheduler xref:scheduler:DAGScheduler.adoc#cancelJob[to cancel a Spark job]. == [[cancelStage]] Cancelling Stage -- cancelStage Methods [source, scala] \u00b6 cancelStage(stageId: Int): Unit cancelStage(stageId: Int, reason: String): Unit cancelStage simply requests DAGScheduler xref:scheduler:DAGScheduler.adoc#cancelJob[to cancel a Spark stage] (with an optional reason ). NOTE: cancelStage is used when StagesTab link:spark-webui-StagesTab.adoc#handleKillRequest[handles a kill request] (from a user in web UI). == [[dynamic-allocation]] Programmable Dynamic Allocation SparkContext offers the following methods as the developer API for xref:ROOT:spark-dynamic-allocation.adoc[]: < > < > < > (private!) < > === [[requestExecutors]] Requesting New Executors -- requestExecutors Method [source, scala] \u00b6 requestExecutors(numAdditionalExecutors: Int): Boolean \u00b6 requestExecutors requests numAdditionalExecutors executors from xref:scheduler:CoarseGrainedSchedulerBackend.adoc[CoarseGrainedSchedulerBackend]. === [[killExecutors]] Requesting to Kill Executors -- killExecutors Method [source, scala] \u00b6 killExecutors(executorIds: Seq[String]): Boolean \u00b6 CAUTION: FIXME === [[requestTotalExecutors]] Requesting Total Executors -- requestTotalExecutors Method [source, scala] \u00b6 requestTotalExecutors( numExecutors: Int, localityAwareTasks: Int, hostToLocalTaskCount: Map[String, Int]): Boolean requestTotalExecutors is a private[spark] method that xref:scheduler:CoarseGrainedSchedulerBackend.adoc#requestTotalExecutors[requests the exact number of executors from a coarse-grained scheduler backend]. NOTE: It works for xref:scheduler:CoarseGrainedSchedulerBackend.adoc[coarse-grained scheduler backends] only. When called for other scheduler backends you should see the following WARN message in the logs: WARN Requesting executors is only supported in coarse-grained mode === [[getExecutorIds]] Getting Executor Ids -- getExecutorIds Method getExecutorIds is a private[spark] method that is part of link:spark-service-ExecutorAllocationClient.adoc[ExecutorAllocationClient contract]. It simply xref:scheduler:CoarseGrainedSchedulerBackend.adoc#getExecutorIds[passes the call on to the current coarse-grained scheduler backend, i.e. calls getExecutorIds ]. NOTE: It works for xref:scheduler:CoarseGrainedSchedulerBackend.adoc[coarse-grained scheduler backends] only. When called for other scheduler backends you should see the following WARN message in the logs: WARN Requesting executors is only supported in coarse-grained mode CAUTION: FIXME Why does SparkContext implement the method for coarse-grained scheduler backends? Why doesn't SparkContext throw an exception when the method is called? Nobody seems to be using it (!) == [[creating-instance]] Creating SparkContext Instance You can create a SparkContext instance with or without creating a xref:ROOT:SparkConf.adoc[SparkConf] object first. NOTE: You may want to read link:spark-SparkContext-creating-instance-internals.adoc[Inside Creating SparkContext] to learn what happens behind the scenes when SparkContext is created. === [[getOrCreate]] Getting Existing or Creating New SparkContext -- getOrCreate Methods [source, scala] \u00b6 getOrCreate(): SparkContext getOrCreate(conf: SparkConf): SparkContext getOrCreate methods allow you to get the existing SparkContext or create a new one. [source, scala] \u00b6 import org.apache.spark.SparkContext val sc = SparkContext.getOrCreate() // Using an explicit SparkConf object import org.apache.spark.SparkConf val conf = new SparkConf() .setMaster(\"local[*]\") .setAppName(\"SparkMe App\") val sc = SparkContext.getOrCreate(conf) The no-param getOrCreate method requires that the two mandatory Spark settings - < > and < > - are specified using link:spark-submit.adoc[spark-submit]. === [[constructors]] Constructors [source, scala] \u00b6 SparkContext() SparkContext(conf: SparkConf) SparkContext(master: String, appName: String, conf: SparkConf) SparkContext( master: String, appName: String, sparkHome: String = null, jars: Seq[String] = Nil, environment: Map[String, String] = Map()) You can create a SparkContext instance using the four constructors. [source, scala] \u00b6 import org.apache.spark.SparkConf val conf = new SparkConf() .setMaster(\"local[*]\") .setAppName(\"SparkMe App\") import org.apache.spark.SparkContext val sc = new SparkContext(conf) When a Spark context starts up you should see the following INFO in the logs (amongst the other messages that come from the Spark services): INFO SparkContext: Running Spark version 2.0.0-SNAPSHOT NOTE: Only one SparkContext may be running in a single JVM (check out https://issues.apache.org/jira/browse/SPARK-2243[SPARK-2243 Support multiple SparkContexts in the same JVM]). Sharing access to a SparkContext in the JVM is the solution to share data within Spark (without relying on other means of data sharing using external data stores). == [[env]] Accessing Current SparkEnv -- env Method CAUTION: FIXME == [[getConf]] Getting Current SparkConf -- getConf Method [source, scala] \u00b6 getConf: SparkConf \u00b6 getConf returns the current xref:ROOT:SparkConf.adoc[SparkConf]. NOTE: Changing the SparkConf object does not change the current configuration (as the method returns a copy). == [[master]][[master-url]] Deployment Environment -- master Method [source, scala] \u00b6 master: String \u00b6 master method returns the current value of xref:ROOT:configuration-properties.adoc#spark.master[spark.master] which is the link:spark-deployment-environments.adoc[deployment environment] in use. == [[appName]] Application Name -- appName Method [source, scala] \u00b6 appName: String \u00b6 appName gives the value of the mandatory xref:ROOT:SparkConf.adoc#spark.app.name[spark.app.name] setting. NOTE: appName is used when link:spark-standalone.adoc#SparkDeploySchedulerBackend[ SparkDeploySchedulerBackend starts], link:spark-webui-SparkUI.adoc#createLiveUI[ SparkUI creates a web UI], when postApplicationStart is executed, and for Mesos and checkpointing in Spark Streaming. == [[applicationAttemptId]] Unique Identifier of Execution Attempt -- applicationAttemptId Method [source, scala] \u00b6 applicationAttemptId: Option[String] \u00b6 applicationAttemptId gives the unique identifier of the execution attempt of a Spark application. [NOTE] \u00b6 applicationAttemptId is used when: xref:scheduler:ShuffleMapTask.adoc#creating-instance[ShuffleMapTask] and xref:scheduler:ResultTask.adoc#creating-instance[ResultTask] are created * SparkContext < > \u00b6 == [[getExecutorStorageStatus]] Storage Status (of All BlockManagers) -- getExecutorStorageStatus Method [source, scala] \u00b6 getExecutorStorageStatus: Array[StorageStatus] \u00b6 getExecutorStorageStatus xref:storage:BlockManagerMaster.adoc#getStorageStatus[requests BlockManagerMaster for storage status] (of all xref:storage:BlockManager.adoc[BlockManagers]). NOTE: getExecutorStorageStatus is a developer API. [NOTE] \u00b6 getExecutorStorageStatus is used when: SparkContext < > * SparkStatusTracker link:spark-sparkcontext-SparkStatusTracker.adoc#getExecutorInfos[is requested for information about all known executors] \u00b6 == [[deployMode]] Deploy Mode -- deployMode Method [source,scala] \u00b6 deployMode: String \u00b6 deployMode returns the current value of link:spark-deploy-mode.adoc[spark.submit.deployMode] setting or client if not set. == [[getSchedulingMode]] Scheduling Mode -- getSchedulingMode Method [source, scala] \u00b6 getSchedulingMode: SchedulingMode.SchedulingMode \u00b6 getSchedulingMode returns the current link:spark-scheduler-SchedulingMode.adoc[Scheduling Mode]. == [[getPoolForName]] Schedulable (Pool) by Name -- getPoolForName Method [source, scala] \u00b6 getPoolForName(pool: String): Option[Schedulable] \u00b6 getPoolForName returns a link:spark-scheduler-Schedulable.adoc[Schedulable] by the pool name, if one exists. NOTE: getPoolForName is part of the Developer's API and may change in the future. Internally, it requests the xref:scheduler:TaskScheduler.adoc#rootPool[TaskScheduler for the root pool] and link:spark-scheduler-Pool.adoc#schedulableNameToSchedulable[looks up the Schedulable by the pool name]. It is exclusively used to link:spark-webui-PoolPage.adoc[show pool details in web UI (for a stage)]. == [[getAllPools]] All Schedulable Pools -- getAllPools Method [source, scala] \u00b6 getAllPools: Seq[Schedulable] \u00b6 getAllPools collects the link:spark-scheduler-Pool.adoc[Pools] in xref:scheduler:TaskScheduler.adoc#contract[TaskScheduler.rootPool]. NOTE: TaskScheduler.rootPool is part of the xref:scheduler:TaskScheduler.adoc#contract[TaskScheduler Contract]. NOTE: getAllPools is part of the Developer's API. CAUTION: FIXME Where is the method used? NOTE: getAllPools is used to calculate pool names for link:spark-webui-AllStagesPage.adoc#pool-names[Stages tab in web UI] with FAIR scheduling mode used. == [[defaultParallelism]] Default Level of Parallelism [source, scala] \u00b6 defaultParallelism: Int \u00b6 defaultParallelism requests < > for the xref:scheduler:TaskScheduler.adoc#defaultParallelism[default level of parallelism]. NOTE: Default level of parallelism specifies the number of link:spark-rdd-partitions.adoc[partitions] in RDDs when created without specifying them explicitly by a user. [NOTE] \u00b6 defaultParallelism is used in < >, SparkContext.range and < > (as well as Spark Streaming's DStream.countByValue and DStream.countByValueAndWindow et al.). defaultParallelism is also used to instantiate xref:rdd:HashPartitioner.adoc[HashPartitioner] and for the minimum number of partitions in xref:rdd:spark-rdd-HadoopRDD.adoc[HadoopRDDs]. \u00b6 == [[taskScheduler]] Current Spark Scheduler (aka TaskScheduler) -- taskScheduler Property [source, scala] \u00b6 taskScheduler: TaskScheduler taskScheduler_=(ts: TaskScheduler): Unit taskScheduler manages (i.e. reads or writes) <<_taskScheduler, _taskScheduler>> internal property. == [[version]] Getting Spark Version -- version Property [source, scala] \u00b6 version: String \u00b6 version returns the Spark version this SparkContext uses. == [[makeRDD]] makeRDD Method CAUTION: FIXME == [[submitJob]] Submitting Jobs Asynchronously -- submitJob Method [source, scala] \u00b6 submitJob T, U, R : SimpleFutureAction[R] submitJob submits a job in an asynchronous, non-blocking way to xref:scheduler:DAGScheduler.adoc#submitJob[DAGScheduler]. It cleans the processPartition input function argument and returns an instance of link:spark-rdd-actions.adoc#FutureAction[SimpleFutureAction] that holds the xref:scheduler:spark-scheduler-JobWaiter.adoc[JobWaiter] instance. CAUTION: FIXME What are resultFunc ? It is used in: link:spark-rdd-actions.adoc#AsyncRDDActions[AsyncRDDActions] methods link:spark-streaming/spark-streaming.adoc[Spark Streaming] for link:spark-streaming/spark-streaming-receivertracker.adoc#ReceiverTrackerEndpoint-startReceiver[ReceiverTrackerEndpoint.startReceiver] == [[spark-configuration]] Spark Configuration CAUTION: FIXME == [[sparkcontext-and-rdd]] SparkContext and RDDs You use a Spark context to create RDDs (see < >). When an RDD is created, it belongs to and is completely owned by the Spark context it originated from. RDDs can't by design be shared between SparkContexts. .A Spark context creates a living space for RDDs. image::diagrams/sparkcontext-rdds.png[align=\"center\"] == [[creating-rdds]][[parallelize]] Creating RDD -- parallelize Method SparkContext allows you to create many different RDDs from input sources like: Scala's collections, i.e. sc.parallelize(0 to 100) local or remote filesystems, i.e. sc.textFile(\"README.md\") Any Hadoop InputSource using sc.newAPIHadoopFile Read xref:rdd:index.adoc#creating-rdds[Creating RDDs] in xref:rdd:index.adoc[RDD - Resilient Distributed Dataset]. == [[unpersist]] Unpersisting RDD (Marking RDD as Non-Persistent) -- unpersist Method CAUTION: FIXME unpersist removes an RDD from the master's xref:storage:BlockManager.adoc[Block Manager] (calls removeRdd(rddId: Int, blocking: Boolean) ) and the internal < > mapping. It finally posts xref:ROOT:SparkListener.adoc#SparkListenerUnpersistRDD[SparkListenerUnpersistRDD] message to listenerBus . == [[setCheckpointDir]] Setting Checkpoint Directory -- setCheckpointDir Method [source, scala] \u00b6 setCheckpointDir(directory: String) \u00b6 setCheckpointDir method is used to set up the checkpoint directory...FIXME CAUTION: FIXME == [[register]] Registering Accumulator -- register Methods [source, scala] \u00b6 register(acc: AccumulatorV2[ , _]): Unit register(acc: AccumulatorV2[ , _], name: String): Unit register registers the acc link:spark-accumulators.adoc[accumulator]. You can optionally give an accumulator a name . TIP: You can create built-in accumulators for longs, doubles, and collection types using < >. Internally, register link:spark-accumulators.adoc#register[registers acc accumulator] (with the current SparkContext). == [[creating-accumulators]][[longAccumulator]][[doubleAccumulator]][[collectionAccumulator]] Creating Built-In Accumulators [source, scala] \u00b6 longAccumulator: LongAccumulator longAccumulator(name: String): LongAccumulator doubleAccumulator: DoubleAccumulator doubleAccumulator(name: String): DoubleAccumulator collectionAccumulator[T]: CollectionAccumulator[T] collectionAccumulator T : CollectionAccumulator[T] You can use longAccumulator , doubleAccumulator or collectionAccumulator to create and register link:spark-accumulators.adoc[accumulators] for simple and collection values. longAccumulator returns link:spark-accumulators.adoc#LongAccumulator[LongAccumulator] with the zero value 0 . doubleAccumulator returns link:spark-accumulators.adoc#DoubleAccumulator[DoubleAccumulator] with the zero value 0.0 . collectionAccumulator returns link:spark-accumulators.adoc#CollectionAccumulator[CollectionAccumulator] with the zero value java.util.List[T] . [source, scala] \u00b6 scala> val acc = sc.longAccumulator acc: org.apache.spark.util.LongAccumulator = LongAccumulator(id: 0, name: None, value: 0) scala> val counter = sc.longAccumulator(\"counter\") counter: org.apache.spark.util.LongAccumulator = LongAccumulator(id: 1, name: Some(counter), value: 0) scala> counter.value res0: Long = 0 scala> sc.parallelize(0 to 9).foreach(n => counter.add(n)) scala> counter.value res3: Long = 45 The name input parameter allows you to give a name to an accumulator and have it displayed in link:spark-webui-StagePage.adoc#accumulators[Spark UI] (under Stages tab for a given stage). .Accumulators in the Spark UI image::spark-webui-accumulators.png[align=\"center\"] TIP: You can register custom accumulators using < > methods. == [[broadcast]] Creating Broadcast Variable -- broadcast Method [source, scala] \u00b6 broadcast T : Broadcast[T] broadcast method creates a xref:ROOT:Broadcast.adoc[]. It is a shared memory with value (as broadcast blocks) on the driver and later on all Spark executors. [source,plaintext] \u00b6 val sc: SparkContext = ??? scala> val hello = sc.broadcast(\"hello\") hello: org.apache.spark.broadcast.Broadcast[String] = Broadcast(0) Spark transfers the value to Spark executors once , and tasks can share it without incurring repetitive network transmissions when the broadcast variable is used multiple times. .Broadcasting a value to executors image::sparkcontext-broadcast-executors.png[align=\"center\"] Internally, broadcast requests BroadcastManager for a xref:core:BroadcastManager.adoc#newBroadcast[new broadcast variable]. NOTE: The current BroadcastManager is available using xref:core:SparkEnv.adoc#broadcastManager[ SparkEnv.broadcastManager ] attribute and is always xref:core:BroadcastManager.adoc[BroadcastManager] (with few internal configuration changes to reflect where it runs, i.e. inside the driver or executors). You should see the following INFO message in the logs: Created broadcast [id] from [callSite] If ContextCleaner is defined, the xref:core:ContextCleaner.adoc#[new broadcast variable is registered for cleanup]. [NOTE] \u00b6 Spark does not support broadcasting RDDs. scala> sc.broadcast(sc.range(0, 10)) java.lang.IllegalArgumentException: requirement failed: Can not directly broadcast RDDs; instead, call collect() and broadcast the result. at scala.Predef$.require(Predef.scala:224) at org.apache.spark.SparkContext.broadcast(SparkContext.scala:1392) ... 48 elided \u00b6 Once created, the broadcast variable (and other blocks) are displayed per executor and the driver in web UI (under link:spark-webui-executors.adoc[Executors tab]). .Broadcast Variables In web UI's Executors Tab image::spark-broadcast-webui-executors-rdd-blocks.png[align=\"center\"] == [[jars]] Distribute JARs to workers The jar you specify with SparkContext.addJar will be copied to all the worker nodes. The configuration setting spark.jars is a comma-separated list of jar paths to be included in all tasks executed from this SparkContext. A path can either be a local file, a file in HDFS (or other Hadoop-supported filesystems), an HTTP, HTTPS or FTP URI, or local:/path for a file on every worker node. scala> sc.addJar(\"build.sbt\") 15/11/11 21:54:54 INFO SparkContext: Added JAR build.sbt at http://192.168.1.4:49427/jars/build.sbt with timestamp 1447275294457 CAUTION: FIXME Why is HttpFileServer used for addJar? === SparkContext as Application-Wide Counter SparkContext keeps track of: [[nextShuffleId]] * shuffle ids using nextShuffleId internal counter for xref:scheduler:ShuffleMapStage.adoc[registering shuffle dependencies] to xref:shuffle:ShuffleManager.adoc[Shuffle Service]. == [[runJob]] Running Job Synchronously xref:rdd:index.adoc#actions[RDD actions] run link:spark-scheduler-ActiveJob.adoc[jobs] using one of runJob methods. [source, scala] \u00b6 runJob T, U : Unit runJob T, U : Array[U] runJob T, U : Array[U] runJob T, U : Array[U] runJob T, U : Array[U] runJob T, U runJob T, U: ClassTag runJob executes a function on one or many partitions of a RDD (in a SparkContext space) to produce a collection of values per partition. NOTE: runJob can only work when a SparkContext is not < >. Internally, runJob first makes sure that the SparkContext is not < >. If it is, you should see the following IllegalStateException exception in the logs: java.lang.IllegalStateException: SparkContext has been shutdown at org.apache.spark.SparkContext.runJob(SparkContext.scala:1893) at org.apache.spark.SparkContext.runJob(SparkContext.scala:1914) at org.apache.spark.SparkContext.runJob(SparkContext.scala:1934) ... 48 elided runJob then < > and < func closure>>. You should see the following INFO message in the logs: INFO SparkContext: Starting job: [callSite] With link:spark-rdd-lineage.adoc#spark_logLineage[spark.logLineage] enabled (which is not by default), you should see the following INFO message with link:spark-rdd-lineage.adoc#toDebugString[toDebugString] (executed on rdd ): INFO SparkContext: RDD's recursive dependencies: [toDebugString] runJob requests xref:scheduler:DAGScheduler.adoc#runJob[ DAGScheduler to run a job]. TIP: runJob just prepares input parameters for xref:scheduler:DAGScheduler.adoc#runJob[ DAGScheduler to run a job]. After DAGScheduler is done and the job has finished, runJob link:spark-sparkcontext-ConsoleProgressBar.adoc#finishAll[stops ConsoleProgressBar ] and xref:ROOT:rdd-checkpointing.adoc#doCheckpoint[performs RDD checkpointing of rdd ]. TIP: For some actions, e.g. first() and lookup() , there is no need to compute all the partitions of the RDD in a job. And Spark knows it. [source,scala] \u00b6 // RDD to work with val lines = sc.parallelize(Seq(\"hello world\", \"nice to see you\")) import org.apache.spark.TaskContext scala> sc.runJob(lines, (t: TaskContext, i: Iterator[String]) => 1) // <1> res0: Array[Int] = Array(1, 1) // <2> <1> Run a job using runJob on lines RDD with a function that returns 1 for every partition (of lines RDD). <2> What can you say about the number of partitions of the lines RDD? Is your result res0 different than mine? Why? TIP: Read link:spark-TaskContext.adoc[TaskContext]. Running a job is essentially executing a func function on all or a subset of partitions in an rdd RDD and returning the result as an array (with elements being the results per partition). .Executing action image::spark-runjob.png[align=\"center\"] == [[stop]][[stopping]] Stopping SparkContext -- stop Method [source, scala] \u00b6 stop(): Unit \u00b6 stop stops the SparkContext. Internally, stop enables stopped internal flag. If already stopped, you should see the following INFO message in the logs: INFO SparkContext: SparkContext already stopped. stop then does the following: Removes _shutdownHookRef from ShutdownHookManager < SparkListenerApplicationEnd >> (to < >) link:spark-webui-SparkUI.adoc#stop[Stops web UI] link:spark-metrics-MetricsSystem.adoc#report[Requests MetricSystem to report metrics] (from all registered sinks) xref:core:ContextCleaner.adoc#stop[Stops ContextCleaner ] link:spark-ExecutorAllocationManager.adoc#stop[Requests ExecutorAllocationManager to stop] If LiveListenerBus was started, xref:scheduler:LiveListenerBus.adoc#stop[requests LiveListenerBus to stop] Requests xref:spark-history-server:EventLoggingListener.adoc#stop[ EventLoggingListener to stop] Requests xref:scheduler:DAGScheduler.adoc#stop[ DAGScheduler to stop] Requests xref:rpc:index.adoc#stop[RpcEnv to stop HeartbeatReceiver endpoint] Requests link:spark-sparkcontext-ConsoleProgressBar.adoc#stop[ ConsoleProgressBar to stop] Clears the reference to TaskScheduler , i.e. _taskScheduler is null Requests xref:core:SparkEnv.adoc#stop[ SparkEnv to stop] and clears SparkEnv Clears link:yarn/spark-yarn-client.adoc#SPARK_YARN_MODE[ SPARK_YARN_MODE flag] < > Ultimately, you should see the following INFO message in the logs: INFO SparkContext: Successfully stopped SparkContext == [[addSparkListener]] Registering SparkListener -- addSparkListener Method [source, scala] \u00b6 addSparkListener(listener: SparkListenerInterface): Unit \u00b6 You can register a custom xref:ROOT:SparkListener.adoc#SparkListenerInterface[SparkListenerInterface] using addSparkListener method NOTE: You can also register custom listeners using xref:ROOT:configuration-properties.adoc#spark.extraListeners[spark.extraListeners] configuration property. == [[custom-schedulers]] Custom SchedulerBackend, TaskScheduler and DAGScheduler By default, SparkContext uses ( private[spark] class) org.apache.spark.scheduler.DAGScheduler , but you can develop your own custom DAGScheduler implementation, and use ( private[spark] ) SparkContext.dagScheduler_=(ds: DAGScheduler) method to assign yours. It is also applicable to SchedulerBackend and TaskScheduler using schedulerBackend_=(sb: SchedulerBackend) and taskScheduler_=(ts: TaskScheduler) methods, respectively. CAUTION: FIXME Make it an advanced exercise. == [[events]] Events When a Spark context starts, it triggers xref:ROOT:SparkListener.adoc#SparkListenerEnvironmentUpdate[SparkListenerEnvironmentUpdate] and xref:ROOT:SparkListener.adoc#SparkListenerApplicationStart[SparkListenerApplicationStart] messages. Refer to the section < >. == [[setLogLevel]][[setting-default-log-level]] Setting Default Logging Level -- setLogLevel Method [source, scala] \u00b6 setLogLevel(logLevel: String) \u00b6 setLogLevel allows you to set the root logging level in a Spark application, e.g. link:spark-shell.adoc[Spark shell]. Internally, setLogLevel calls link:++ http://logging.apache.org/log4j/2.x/log4j-api/apidocs/org/apache/logging/log4j/Level.html#toLevel(java.lang.String)++[org.apache.log4j.Level.toLevel(logLevel )] that it then uses to set using link:++ http://logging.apache.org/log4j/2.x/log4j-api/apidocs/org/apache/logging/log4j/LogManager.html#getRootLogger()++[org.apache.log4j.LogManager.getRootLogger().setLevel(level )]. [TIP] \u00b6 You can directly set the logging level using link:++ http://logging.apache.org/log4j/2.x/log4j-api/apidocs/org/apache/logging/log4j/LogManager.html#getLogger()++[org.apache.log4j.LogManager.getLogger ()]. [source, scala] \u00b6 LogManager.getLogger(\"org\").setLevel(Level.OFF) \u00b6 ==== == [[clean]][[closure-cleaning]] Closure Cleaning -- clean Method [source, scala] \u00b6 clean(f: F, checkSerializable: Boolean = true): F \u00b6 Every time an action is called, Spark cleans up the closure, i.e. the body of the action, before it is serialized and sent over the wire to executors. SparkContext comes with clean(f: F, checkSerializable: Boolean = true) method that does this. It in turn calls ClosureCleaner.clean method. Not only does ClosureCleaner.clean method clean the closure, but also does it transitively, i.e. referenced closures are cleaned transitively. A closure is considered serializable as long as it does not explicitly reference unserializable objects. It does so by traversing the hierarchy of enclosing closures and null out any references that are not actually used by the starting closure. [TIP] \u00b6 Enable DEBUG logging level for org.apache.spark.util.ClosureCleaner logger to see what happens inside the class. Add the following line to conf/log4j.properties : log4j.logger.org.apache.spark.util.ClosureCleaner=DEBUG Refer to link:spark-logging.adoc[Logging]. \u00b6 With DEBUG logging level you should see the following messages in the logs: +++ Cleaning closure [func] ([func.getClass.getName]) +++ + declared fields: [declaredFields.size] [field] ... +++ closure [func] ([func.getClass.getName]) is now cleaned +++ Serialization is verified using a new instance of Serializer (as xref:core:SparkEnv.adoc#closureSerializer[closure Serializer]). Refer to link:spark-serialization.adoc[Serialization]. CAUTION: FIXME an example, please. == [[hadoopConfiguration]] Hadoop Configuration While a < >, so is a Hadoop configuration (as an instance of https://hadoop.apache.org/docs/current/api/org/apache/hadoop/conf/Configuration.html[org.apache.hadoop.conf.Configuration ] that is available as _hadoopConfiguration ). NOTE: link:spark-SparkHadoopUtil.adoc#newConfiguration[SparkHadoopUtil.get.newConfiguration] is used. If a SparkConf is provided it is used to build the configuration as described. Otherwise, the default Configuration object is returned. If AWS_ACCESS_KEY_ID and AWS_SECRET_ACCESS_KEY are both available, the following settings are set for the Hadoop configuration: fs.s3.awsAccessKeyId , fs.s3n.awsAccessKeyId , fs.s3a.access.key are set to the value of AWS_ACCESS_KEY_ID fs.s3.awsSecretAccessKey , fs.s3n.awsSecretAccessKey , and fs.s3a.secret.key are set to the value of AWS_SECRET_ACCESS_KEY Every spark.hadoop. setting becomes a setting of the configuration with the prefix spark.hadoop. removed for the key. The value of spark.buffer.size (default: 65536 ) is used as the value of io.file.buffer.size . == [[listenerBus]] listenerBus -- LiveListenerBus Event Bus listenerBus is a xref:scheduler:LiveListenerBus.adoc[] object that acts as a mechanism to announce events to other services on the link:spark-driver.adoc[driver]. NOTE: It is created and started when link:spark-SparkContext-creating-instance-internals.adoc[SparkContext starts] and, since it is a single-JVM event bus, is exclusively used on the driver. NOTE: listenerBus is a private[spark] value in SparkContext. == [[startTime]] Time when SparkContext was Created -- startTime Property [source, scala] \u00b6 startTime: Long \u00b6 startTime is the time in milliseconds when < >. [source, scala] \u00b6 scala> sc.startTime res0: Long = 1464425605653 == [[sparkUser]] Spark User -- sparkUser Property [source, scala] \u00b6 sparkUser: String \u00b6 sparkUser is the user who started the SparkContext instance. NOTE: It is computed when link:spark-SparkContext-creating-instance-internals.adoc#sparkUser[SparkContext is created] using link:spark-SparkContext-creating-instance-internals.adoc#[Utils.getCurrentUserName]. == [[submitMapStage]] Submitting ShuffleDependency for Execution -- submitMapStage Internal Method [source, scala] \u00b6 submitMapStage K, V, C : SimpleFutureAction[MapOutputStatistics] submitMapStage xref:scheduler:DAGScheduler.adoc#submitMapStage[submits the input ShuffleDependency to DAGScheduler for execution] and returns a SimpleFutureAction . Internally, submitMapStage < > first and submits it with localProperties . NOTE: Interestingly, submitMapStage is used exclusively when Spark SQL's link:spark-sql-SparkPlan-ShuffleExchange.adoc[ShuffleExchange] physical operator is executed. NOTE: submitMapStage seems related to xref:scheduler:DAGScheduler.adoc#adaptive-query-planning[Adaptive Query Planning / Adaptive Scheduling]. == [[getCallSite]] Calculating Call Site -- getCallSite Method CAUTION: FIXME == [[cancelJobGroup]] Cancelling Job Group -- cancelJobGroup Method [source, scala] \u00b6 cancelJobGroup(groupId: String) \u00b6 cancelJobGroup requests DAGScheduler xref:scheduler:DAGScheduler.adoc#cancelJobGroup[to cancel a group of active Spark jobs]. NOTE: cancelJobGroup is used exclusively when SparkExecuteStatementOperation does cancel . == [[cancelAllJobs]] Cancelling All Running and Scheduled Jobs -- cancelAllJobs Method CAUTION: FIXME NOTE: cancelAllJobs is used when link:spark-shell.adoc[spark-shell] is terminated (e.g. using Ctrl+C, so it can in turn terminate all active Spark jobs) or SparkSQLCLIDriver is terminated. == [[setJobGroup]] Setting Local Properties to Group Spark Jobs -- setJobGroup Method [source, scala] \u00b6 setJobGroup( groupId: String, description: String, interruptOnCancel: Boolean = false): Unit setJobGroup link:spark-sparkcontext-local-properties.adoc#setLocalProperty[sets local properties]: spark.jobGroup.id as groupId spark.job.description as description spark.job.interruptOnCancel as interruptOnCancel [NOTE] \u00b6 setJobGroup is used when: Spark Thrift Server's SparkExecuteStatementOperation runs a query Structured Streaming's StreamExecution runs batches \u00b6 == [[cleaner]] ContextCleaner [source, scala] \u00b6 cleaner: Option[ContextCleaner] \u00b6 SparkContext may have a xref:core:ContextCleaner.adoc[ContextCleaner] defined. ContextCleaner is created when xref:ROOT:spark-SparkContext-creating-instance-internals.adoc#_cleaner[SparkContext is created] with xref:ROOT:configuration-properties.adoc#spark.cleaner.referenceTracking[spark.cleaner.referenceTracking] configuration property enabled. == [[getPreferredLocs]] Finding Preferred Locations (Placement Preferences) for RDD Partition [source, scala] \u00b6 getPreferredLocs( rdd: RDD[_], partition: Int): Seq[TaskLocation] getPreferredLocs simply xref:scheduler:DAGScheduler.adoc#getPreferredLocs[requests DAGScheduler for the preferred locations for partition ]. NOTE: Preferred locations of a partition of a RDD are also called placement preferences or locality preferences . getPreferredLocs is used in CoalescedRDDPartition, DefaultPartitionCoalescer and PartitionerAwareUnionRDD. == [[persistRDD]] Registering RDD in persistentRdds Internal Registry -- persistRDD Internal Method [source, scala] \u00b6 persistRDD(rdd: RDD[_]): Unit \u00b6 persistRDD registers rdd in < > internal registry. NOTE: persistRDD is used exclusively when RDD is xref:rdd:index.adoc#persist-internal[persisted or locally checkpointed]. == [[getRDDStorageInfo]] Getting Storage Status of Cached RDDs (as RDDInfos) -- getRDDStorageInfo Methods [source, scala] \u00b6 getRDDStorageInfo: Array[RDDInfo] // <1> getRDDStorageInfo(filter: RDD[_] => Boolean): Array[RDDInfo] // <2> <1> Part of Spark's Developer API that uses <2> filtering no RDDs getRDDStorageInfo takes all the RDDs (from < > registry) that match filter and creates a collection of xref:storage:RDDInfo.adoc[RDDInfo] instances. getRDDStorageInfo then link:spark-webui-StorageListener.adoc#StorageUtils.updateRddInfo[updates the RDDInfos] with the < > (in a Spark application). In the end, getRDDStorageInfo gives only the RDD that are cached (i.e. the sum of memory and disk sizes as well as the number of partitions cached are greater than 0 ). NOTE: getRDDStorageInfo is used when RDD link:spark-rdd-lineage.adoc#toDebugString[is requested for RDD lineage graph]. == [[settings]] Settings === [[spark.driver.allowMultipleContexts]] spark.driver.allowMultipleContexts Quoting the scaladoc of http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.SparkContext[org.apache.spark.SparkContext ]: Only one SparkContext may be active per JVM. You must stop() the active SparkContext before creating a new one. You can however control the behaviour using spark.driver.allowMultipleContexts flag. It is disabled, i.e. false , by default. If enabled (i.e. true ), Spark prints the following WARN message to the logs: WARN Multiple running SparkContexts detected in the same JVM! If disabled (default), it will throw an SparkException exception: Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at: [ctx.creationSite.longForm] When creating an instance of SparkContext, Spark marks the current thread as having it being created (very early in the instantiation process). CAUTION: It's not guaranteed that Spark will work properly with two or more SparkContexts. Consider the feature a work in progress. == [[statusStore]] Accessing AppStatusStore [source, scala] \u00b6 statusStore: AppStatusStore \u00b6 statusStore gives the current xref:core:AppStatusStore.adoc[]. statusStore is used when: SparkContext is requested to < > ConsoleProgressBar is requested to xref:ROOT:spark-sparkcontext-ConsoleProgressBar.adoc#refresh[refresh] SharedState (Spark SQL) is requested for a SQLAppStatusStore == [[uiWebUrl]] Requesting URL of web UI -- uiWebUrl Method [source, scala] \u00b6 uiWebUrl: Option[String] \u00b6 uiWebUrl requests the link:spark-SparkContext-creating-instance-internals.adoc#_ui[SparkUI] for link:spark-webui-WebUI.adoc#webUrl[webUrl]. == [[maxNumConcurrentTasks]] maxNumConcurrentTasks Method [source, scala] \u00b6 maxNumConcurrentTasks(): Int \u00b6 maxNumConcurrentTasks simply requests the < > for the xref:scheduler:SchedulerBackend.adoc#maxNumConcurrentTasks[maximum number of tasks that can be launched concurrently]. NOTE: maxNumConcurrentTasks is used exclusively when DAGScheduler is requested to xref:scheduler:DAGScheduler.adoc#checkBarrierStageWithNumSlots[checkBarrierStageWithNumSlots]. == [[createTaskScheduler]] Creating SchedulerBackend and TaskScheduler -- createTaskScheduler Internal Factory Method [source, scala] \u00b6 createTaskScheduler( sc: SparkContext, master: String, deployMode: String): (SchedulerBackend, TaskScheduler) createTaskScheduler creates the xref:scheduler:SchedulerBackend.adoc[SchedulerBackend] and the xref:scheduler:TaskScheduler.adoc[TaskScheduler] for the given master URL and deployment mode. .SparkContext creates Task Scheduler and Scheduler Backend image::diagrams/sparkcontext-createtaskscheduler.png[align=\"center\"] Internally, createTaskScheduler branches off per the given master URL (link:spark-deployment-environments.adoc#master-urls[master URL]) to select the requested implementations. createTaskScheduler understands the following master URLs: local - local mode with 1 thread only local[n] or local[*] - local mode with n threads local[n, m] or local[*, m] -- local mode with n threads and m number of failures spark://hostname:port for Spark Standalone local-cluster[n, m, z] -- local cluster with n workers, m cores per worker, and z memory per worker any other URL is passed to < getClusterManager to load an external cluster manager>>. CAUTION: FIXME == [[environment-variables]] Environment Variables .Environment Variables [cols=\"1,1,2\",options=\"header\",width=\"100%\"] |=== | Environment Variable | Default Value | Description | [[SPARK_EXECUTOR_MEMORY]] SPARK_EXECUTOR_MEMORY | 1024 | Amount of memory to allocate for a Spark executor in MB. See xref:executor:Executor.adoc#memory[Executor Memory]. [[SPARK_USER]] SPARK_USER The user who is running SparkContext. Available later as < >. === == [[postEnvironmentUpdate]] Posting SparkListenerEnvironmentUpdate Event [source, scala] \u00b6 postEnvironmentUpdate(): Unit \u00b6 postEnvironmentUpdate ...FIXME NOTE: postEnvironmentUpdate is used when SparkContext is < >, and requested to < > and < >. == [[addJar-internals]] addJar Method [source, scala] \u00b6 addJar(path: String): Unit \u00b6 addJar ...FIXME NOTE: addJar is used when...FIXME == [[runApproximateJob]] Running Approximate Job [source, scala] \u00b6 runApproximateJob T, U, R : PartialResult[R] runApproximateJob...FIXME runApproximateJob is used when: DoubleRDDFunctions is requested to meanApprox and sumApprox RDD is requested to countApprox and countByValueApprox == [[killTaskAttempt]] Killing Task [source, scala] \u00b6 killTaskAttempt( taskId: Long, interruptThread: Boolean = true, reason: String = \"killed via SparkContext.killTaskAttempt\"): Boolean killTaskAttempt requests the < > to xref:scheduler:DAGScheduler.adoc#killTaskAttempt[kill a task]. == [[checkpointFile]] checkpointFile Internal Method [source, scala] \u00b6 checkpointFile T: ClassTag : RDD[T] checkpointFile...FIXME == [[logging]] Logging Enable ALL logging level for org.apache.spark.SparkContext logger to see what happens inside. Add the following line to conf/log4j.properties : [source,plaintext] \u00b6 log4j.logger.org.apache.spark.SparkContext=ALL \u00b6 Refer to xref:ROOT:spark-logging.adoc[Logging]. == [[internal-properties]] Internal Properties === [[checkpointDir]] Checkpoint Directory [source,scala] \u00b6 checkpointDir: Option[String] = None \u00b6 checkpointDir is...FIXME === [[persistentRdds]] persistentRdds Lookup Table Lookup table of persistent/cached RDDs per their ids. Used when SparkContext is requested to: < > < > < > < > === [[stopped]] stopped Flag Flag that says whether...FIXME ( true ) or not ( false ) === [[_taskScheduler]] TaskScheduler xref:scheduler:TaskScheduler.adoc[TaskScheduler]","title":"SparkContext"},{"location":"SparkContext/#source-scala","text":"","title":"[source, scala]"},{"location":"SparkContext/#addjarpath-string-unit","text":"| a| More to be added soon |=== Spark context link:spark-SparkContext-creating-instance-internals.adoc[sets up internal services] and establishes a connection to a link:spark-deployment-environments.adoc[Spark execution environment]. Once a < > you can use it to < >, < > and < >, access Spark services and < > (until SparkContext is < >). A Spark context is essentially a client of Spark's execution environment and acts as the master of your Spark application (don't get confused with the other meaning of link:spark-master.adoc[Master] in Spark, though). .Spark context acts as the master of your Spark application image::diagrams/sparkcontext-services.png[align=\"center\"] SparkContext offers the following functions: Getting current status of a Spark application ** < > ** < > ** < > ** < > ** < > ** < > ** < > that specifies the number of link:spark-rdd-partitions.adoc[partitions] in RDDs when they are created without specifying the number explicitly by a user. ** < > ** < > ** < > ** < > ** < > Setting Configuration ** < > ** link:spark-sparkcontext-local-properties.adoc[Local Properties -- Creating Logical Job Groups] ** < > ** < > Creating Distributed Entities ** < > ** < > ** < > Accessing services, e.g. < >, < >, xref:scheduler:LiveListenerBus.adoc[], xref:storage:BlockManager.adoc[BlockManager], xref:scheduler:SchedulerBackend.adoc[SchedulerBackends], xref:shuffle:ShuffleManager.adoc[ShuffleManager] and the < >. < > < > < > < > < > < > < > < > < > < > TIP: Read the scaladoc of http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.SparkContext[org.apache.spark.SparkContext ]. == [[addFile]] addFile Method","title":"addJar(path: String): Unit"},{"location":"SparkContext/#source-scala_1","text":"addFile( path: String): Unit // <1> addFile( path: String, recursive: Boolean): Unit <1> recursive flag is off addFile adds the path file to be downloaded...FIXME","title":"[source, scala]"},{"location":"SparkContext/#note","text":"addFile is used when: SparkContext is link:spark-SparkContext-creating-instance-internals.adoc#files[initialized] (and files were defined) Spark SQL's AddFileCommand is executed","title":"[NOTE]"},{"location":"SparkContext/#spark-sqls-sessionresourceloader-is-requested-to-load-a-file-resource","text":"== [[unpersistRDD]] Removing RDD Blocks from BlockManagerMaster -- unpersistRDD Internal Method","title":"* Spark SQL's SessionResourceLoader is requested to load a file resource"},{"location":"SparkContext/#source-scala_2","text":"","title":"[source, scala]"},{"location":"SparkContext/#unpersistrddrddid-int-blocking-boolean-true-unit","text":"unpersistRDD requests BlockManagerMaster to xref:storage:BlockManagerMaster.adoc#removeRdd[remove the blocks for the RDD] (given rddId ). NOTE: unpersistRDD uses SparkEnv xref:core:SparkEnv.adoc#blockManager[to access the current BlockManager ] that is in turn used to xref:storage:BlockManager.adoc#master[access the current BlockManagerMaster ]. unpersistRDD removes rddId from < > registry. In the end, unpersistRDD posts a xref:ROOT:SparkListener.adoc#SparkListenerUnpersistRDD[SparkListenerUnpersistRDD] (with rddId ) to < >.","title":"unpersistRDD(rddId: Int, blocking: Boolean = true): Unit"},{"location":"SparkContext/#note_1","text":"unpersistRDD is used when: ContextCleaner does xref:core:ContextCleaner.adoc#doCleanupRDD[doCleanupRDD]","title":"[NOTE]"},{"location":"SparkContext/#sparkcontext-ie-marks-an-rdd-as-non-persistent","text":"== [[applicationId]] Unique Identifier of Spark Application -- applicationId Method CAUTION: FIXME == [[postApplicationStart]] postApplicationStart Internal Method","title":"SparkContext &lt;&gt; (i.e. marks an RDD as non-persistent)"},{"location":"SparkContext/#source-scala_3","text":"","title":"[source, scala]"},{"location":"SparkContext/#postapplicationstart-unit","text":"postApplicationStart ...FIXME NOTE: postApplicationStart is used exclusively while SparkContext is being < > == [[postApplicationEnd]] postApplicationEnd Method CAUTION: FIXME == [[clearActiveContext]] clearActiveContext Method CAUTION: FIXME == [[getPersistentRDDs]] Accessing persistent RDDs -- getPersistentRDDs Method","title":"postApplicationStart(): Unit"},{"location":"SparkContext/#source-scala_4","text":"","title":"[source, scala]"},{"location":"SparkContext/#getpersistentrdds-mapint-rdd_","text":"getPersistentRDDs returns the collection of RDDs that have marked themselves as persistent via link:spark-rdd-caching.adoc#cache[cache]. Internally, getPersistentRDDs returns < > internal registry. == [[cancelJob]] Cancelling Job -- cancelJob Method","title":"getPersistentRDDs: Map[Int, RDD[_]]"},{"location":"SparkContext/#source-scala_5","text":"","title":"[source, scala]"},{"location":"SparkContext/#canceljobjobid-int","text":"cancelJob requests DAGScheduler xref:scheduler:DAGScheduler.adoc#cancelJob[to cancel a Spark job]. == [[cancelStage]] Cancelling Stage -- cancelStage Methods","title":"cancelJob(jobId: Int)"},{"location":"SparkContext/#source-scala_6","text":"cancelStage(stageId: Int): Unit cancelStage(stageId: Int, reason: String): Unit cancelStage simply requests DAGScheduler xref:scheduler:DAGScheduler.adoc#cancelJob[to cancel a Spark stage] (with an optional reason ). NOTE: cancelStage is used when StagesTab link:spark-webui-StagesTab.adoc#handleKillRequest[handles a kill request] (from a user in web UI). == [[dynamic-allocation]] Programmable Dynamic Allocation SparkContext offers the following methods as the developer API for xref:ROOT:spark-dynamic-allocation.adoc[]: < > < > < > (private!) < > === [[requestExecutors]] Requesting New Executors -- requestExecutors Method","title":"[source, scala]"},{"location":"SparkContext/#source-scala_7","text":"","title":"[source, scala]"},{"location":"SparkContext/#requestexecutorsnumadditionalexecutors-int-boolean","text":"requestExecutors requests numAdditionalExecutors executors from xref:scheduler:CoarseGrainedSchedulerBackend.adoc[CoarseGrainedSchedulerBackend]. === [[killExecutors]] Requesting to Kill Executors -- killExecutors Method","title":"requestExecutors(numAdditionalExecutors: Int): Boolean"},{"location":"SparkContext/#source-scala_8","text":"","title":"[source, scala]"},{"location":"SparkContext/#killexecutorsexecutorids-seqstring-boolean","text":"CAUTION: FIXME === [[requestTotalExecutors]] Requesting Total Executors -- requestTotalExecutors Method","title":"killExecutors(executorIds: Seq[String]): Boolean"},{"location":"SparkContext/#source-scala_9","text":"requestTotalExecutors( numExecutors: Int, localityAwareTasks: Int, hostToLocalTaskCount: Map[String, Int]): Boolean requestTotalExecutors is a private[spark] method that xref:scheduler:CoarseGrainedSchedulerBackend.adoc#requestTotalExecutors[requests the exact number of executors from a coarse-grained scheduler backend]. NOTE: It works for xref:scheduler:CoarseGrainedSchedulerBackend.adoc[coarse-grained scheduler backends] only. When called for other scheduler backends you should see the following WARN message in the logs: WARN Requesting executors is only supported in coarse-grained mode === [[getExecutorIds]] Getting Executor Ids -- getExecutorIds Method getExecutorIds is a private[spark] method that is part of link:spark-service-ExecutorAllocationClient.adoc[ExecutorAllocationClient contract]. It simply xref:scheduler:CoarseGrainedSchedulerBackend.adoc#getExecutorIds[passes the call on to the current coarse-grained scheduler backend, i.e. calls getExecutorIds ]. NOTE: It works for xref:scheduler:CoarseGrainedSchedulerBackend.adoc[coarse-grained scheduler backends] only. When called for other scheduler backends you should see the following WARN message in the logs: WARN Requesting executors is only supported in coarse-grained mode CAUTION: FIXME Why does SparkContext implement the method for coarse-grained scheduler backends? Why doesn't SparkContext throw an exception when the method is called? Nobody seems to be using it (!) == [[creating-instance]] Creating SparkContext Instance You can create a SparkContext instance with or without creating a xref:ROOT:SparkConf.adoc[SparkConf] object first. NOTE: You may want to read link:spark-SparkContext-creating-instance-internals.adoc[Inside Creating SparkContext] to learn what happens behind the scenes when SparkContext is created. === [[getOrCreate]] Getting Existing or Creating New SparkContext -- getOrCreate Methods","title":"[source, scala]"},{"location":"SparkContext/#source-scala_10","text":"getOrCreate(): SparkContext getOrCreate(conf: SparkConf): SparkContext getOrCreate methods allow you to get the existing SparkContext or create a new one.","title":"[source, scala]"},{"location":"SparkContext/#source-scala_11","text":"import org.apache.spark.SparkContext val sc = SparkContext.getOrCreate() // Using an explicit SparkConf object import org.apache.spark.SparkConf val conf = new SparkConf() .setMaster(\"local[*]\") .setAppName(\"SparkMe App\") val sc = SparkContext.getOrCreate(conf) The no-param getOrCreate method requires that the two mandatory Spark settings - < > and < > - are specified using link:spark-submit.adoc[spark-submit]. === [[constructors]] Constructors","title":"[source, scala]"},{"location":"SparkContext/#source-scala_12","text":"SparkContext() SparkContext(conf: SparkConf) SparkContext(master: String, appName: String, conf: SparkConf) SparkContext( master: String, appName: String, sparkHome: String = null, jars: Seq[String] = Nil, environment: Map[String, String] = Map()) You can create a SparkContext instance using the four constructors.","title":"[source, scala]"},{"location":"SparkContext/#source-scala_13","text":"import org.apache.spark.SparkConf val conf = new SparkConf() .setMaster(\"local[*]\") .setAppName(\"SparkMe App\") import org.apache.spark.SparkContext val sc = new SparkContext(conf) When a Spark context starts up you should see the following INFO in the logs (amongst the other messages that come from the Spark services): INFO SparkContext: Running Spark version 2.0.0-SNAPSHOT NOTE: Only one SparkContext may be running in a single JVM (check out https://issues.apache.org/jira/browse/SPARK-2243[SPARK-2243 Support multiple SparkContexts in the same JVM]). Sharing access to a SparkContext in the JVM is the solution to share data within Spark (without relying on other means of data sharing using external data stores). == [[env]] Accessing Current SparkEnv -- env Method CAUTION: FIXME == [[getConf]] Getting Current SparkConf -- getConf Method","title":"[source, scala]"},{"location":"SparkContext/#source-scala_14","text":"","title":"[source, scala]"},{"location":"SparkContext/#getconf-sparkconf","text":"getConf returns the current xref:ROOT:SparkConf.adoc[SparkConf]. NOTE: Changing the SparkConf object does not change the current configuration (as the method returns a copy). == [[master]][[master-url]] Deployment Environment -- master Method","title":"getConf: SparkConf"},{"location":"SparkContext/#source-scala_15","text":"","title":"[source, scala]"},{"location":"SparkContext/#master-string","text":"master method returns the current value of xref:ROOT:configuration-properties.adoc#spark.master[spark.master] which is the link:spark-deployment-environments.adoc[deployment environment] in use. == [[appName]] Application Name -- appName Method","title":"master: String"},{"location":"SparkContext/#source-scala_16","text":"","title":"[source, scala]"},{"location":"SparkContext/#appname-string","text":"appName gives the value of the mandatory xref:ROOT:SparkConf.adoc#spark.app.name[spark.app.name] setting. NOTE: appName is used when link:spark-standalone.adoc#SparkDeploySchedulerBackend[ SparkDeploySchedulerBackend starts], link:spark-webui-SparkUI.adoc#createLiveUI[ SparkUI creates a web UI], when postApplicationStart is executed, and for Mesos and checkpointing in Spark Streaming. == [[applicationAttemptId]] Unique Identifier of Execution Attempt -- applicationAttemptId Method","title":"appName: String"},{"location":"SparkContext/#source-scala_17","text":"","title":"[source, scala]"},{"location":"SparkContext/#applicationattemptid-optionstring","text":"applicationAttemptId gives the unique identifier of the execution attempt of a Spark application.","title":"applicationAttemptId: Option[String]"},{"location":"SparkContext/#note_2","text":"applicationAttemptId is used when: xref:scheduler:ShuffleMapTask.adoc#creating-instance[ShuffleMapTask] and xref:scheduler:ResultTask.adoc#creating-instance[ResultTask] are created","title":"[NOTE]"},{"location":"SparkContext/#sparkcontext","text":"== [[getExecutorStorageStatus]] Storage Status (of All BlockManagers) -- getExecutorStorageStatus Method","title":"* SparkContext &lt;&gt;"},{"location":"SparkContext/#source-scala_18","text":"","title":"[source, scala]"},{"location":"SparkContext/#getexecutorstoragestatus-arraystoragestatus","text":"getExecutorStorageStatus xref:storage:BlockManagerMaster.adoc#getStorageStatus[requests BlockManagerMaster for storage status] (of all xref:storage:BlockManager.adoc[BlockManagers]). NOTE: getExecutorStorageStatus is a developer API.","title":"getExecutorStorageStatus: Array[StorageStatus]"},{"location":"SparkContext/#note_3","text":"getExecutorStorageStatus is used when: SparkContext < >","title":"[NOTE]"},{"location":"SparkContext/#sparkstatustracker-linkspark-sparkcontext-sparkstatustrackeradocgetexecutorinfosis-requested-for-information-about-all-known-executors","text":"== [[deployMode]] Deploy Mode -- deployMode Method","title":"* SparkStatusTracker link:spark-sparkcontext-SparkStatusTracker.adoc#getExecutorInfos[is requested for information about all known executors]"},{"location":"SparkContext/#sourcescala","text":"","title":"[source,scala]"},{"location":"SparkContext/#deploymode-string","text":"deployMode returns the current value of link:spark-deploy-mode.adoc[spark.submit.deployMode] setting or client if not set. == [[getSchedulingMode]] Scheduling Mode -- getSchedulingMode Method","title":"deployMode: String"},{"location":"SparkContext/#source-scala_19","text":"","title":"[source, scala]"},{"location":"SparkContext/#getschedulingmode-schedulingmodeschedulingmode","text":"getSchedulingMode returns the current link:spark-scheduler-SchedulingMode.adoc[Scheduling Mode]. == [[getPoolForName]] Schedulable (Pool) by Name -- getPoolForName Method","title":"getSchedulingMode: SchedulingMode.SchedulingMode"},{"location":"SparkContext/#source-scala_20","text":"","title":"[source, scala]"},{"location":"SparkContext/#getpoolfornamepool-string-optionschedulable","text":"getPoolForName returns a link:spark-scheduler-Schedulable.adoc[Schedulable] by the pool name, if one exists. NOTE: getPoolForName is part of the Developer's API and may change in the future. Internally, it requests the xref:scheduler:TaskScheduler.adoc#rootPool[TaskScheduler for the root pool] and link:spark-scheduler-Pool.adoc#schedulableNameToSchedulable[looks up the Schedulable by the pool name]. It is exclusively used to link:spark-webui-PoolPage.adoc[show pool details in web UI (for a stage)]. == [[getAllPools]] All Schedulable Pools -- getAllPools Method","title":"getPoolForName(pool: String): Option[Schedulable]"},{"location":"SparkContext/#source-scala_21","text":"","title":"[source, scala]"},{"location":"SparkContext/#getallpools-seqschedulable","text":"getAllPools collects the link:spark-scheduler-Pool.adoc[Pools] in xref:scheduler:TaskScheduler.adoc#contract[TaskScheduler.rootPool]. NOTE: TaskScheduler.rootPool is part of the xref:scheduler:TaskScheduler.adoc#contract[TaskScheduler Contract]. NOTE: getAllPools is part of the Developer's API. CAUTION: FIXME Where is the method used? NOTE: getAllPools is used to calculate pool names for link:spark-webui-AllStagesPage.adoc#pool-names[Stages tab in web UI] with FAIR scheduling mode used. == [[defaultParallelism]] Default Level of Parallelism","title":"getAllPools: Seq[Schedulable]"},{"location":"SparkContext/#source-scala_22","text":"","title":"[source, scala]"},{"location":"SparkContext/#defaultparallelism-int","text":"defaultParallelism requests < > for the xref:scheduler:TaskScheduler.adoc#defaultParallelism[default level of parallelism]. NOTE: Default level of parallelism specifies the number of link:spark-rdd-partitions.adoc[partitions] in RDDs when created without specifying them explicitly by a user.","title":"defaultParallelism: Int"},{"location":"SparkContext/#note_4","text":"defaultParallelism is used in < >, SparkContext.range and < > (as well as Spark Streaming's DStream.countByValue and DStream.countByValueAndWindow et al.).","title":"[NOTE]"},{"location":"SparkContext/#defaultparallelism-is-also-used-to-instantiate-xrefrddhashpartitioneradochashpartitioner-and-for-the-minimum-number-of-partitions-in-xrefrddspark-rdd-hadooprddadochadooprdds","text":"== [[taskScheduler]] Current Spark Scheduler (aka TaskScheduler) -- taskScheduler Property","title":"defaultParallelism is also used to instantiate xref:rdd:HashPartitioner.adoc[HashPartitioner] and for the minimum number of partitions in xref:rdd:spark-rdd-HadoopRDD.adoc[HadoopRDDs]."},{"location":"SparkContext/#source-scala_23","text":"taskScheduler: TaskScheduler taskScheduler_=(ts: TaskScheduler): Unit taskScheduler manages (i.e. reads or writes) <<_taskScheduler, _taskScheduler>> internal property. == [[version]] Getting Spark Version -- version Property","title":"[source, scala]"},{"location":"SparkContext/#source-scala_24","text":"","title":"[source, scala]"},{"location":"SparkContext/#version-string","text":"version returns the Spark version this SparkContext uses. == [[makeRDD]] makeRDD Method CAUTION: FIXME == [[submitJob]] Submitting Jobs Asynchronously -- submitJob Method","title":"version: String"},{"location":"SparkContext/#source-scala_25","text":"submitJob T, U, R : SimpleFutureAction[R] submitJob submits a job in an asynchronous, non-blocking way to xref:scheduler:DAGScheduler.adoc#submitJob[DAGScheduler]. It cleans the processPartition input function argument and returns an instance of link:spark-rdd-actions.adoc#FutureAction[SimpleFutureAction] that holds the xref:scheduler:spark-scheduler-JobWaiter.adoc[JobWaiter] instance. CAUTION: FIXME What are resultFunc ? It is used in: link:spark-rdd-actions.adoc#AsyncRDDActions[AsyncRDDActions] methods link:spark-streaming/spark-streaming.adoc[Spark Streaming] for link:spark-streaming/spark-streaming-receivertracker.adoc#ReceiverTrackerEndpoint-startReceiver[ReceiverTrackerEndpoint.startReceiver] == [[spark-configuration]] Spark Configuration CAUTION: FIXME == [[sparkcontext-and-rdd]] SparkContext and RDDs You use a Spark context to create RDDs (see < >). When an RDD is created, it belongs to and is completely owned by the Spark context it originated from. RDDs can't by design be shared between SparkContexts. .A Spark context creates a living space for RDDs. image::diagrams/sparkcontext-rdds.png[align=\"center\"] == [[creating-rdds]][[parallelize]] Creating RDD -- parallelize Method SparkContext allows you to create many different RDDs from input sources like: Scala's collections, i.e. sc.parallelize(0 to 100) local or remote filesystems, i.e. sc.textFile(\"README.md\") Any Hadoop InputSource using sc.newAPIHadoopFile Read xref:rdd:index.adoc#creating-rdds[Creating RDDs] in xref:rdd:index.adoc[RDD - Resilient Distributed Dataset]. == [[unpersist]] Unpersisting RDD (Marking RDD as Non-Persistent) -- unpersist Method CAUTION: FIXME unpersist removes an RDD from the master's xref:storage:BlockManager.adoc[Block Manager] (calls removeRdd(rddId: Int, blocking: Boolean) ) and the internal < > mapping. It finally posts xref:ROOT:SparkListener.adoc#SparkListenerUnpersistRDD[SparkListenerUnpersistRDD] message to listenerBus . == [[setCheckpointDir]] Setting Checkpoint Directory -- setCheckpointDir Method","title":"[source, scala]"},{"location":"SparkContext/#source-scala_26","text":"","title":"[source, scala]"},{"location":"SparkContext/#setcheckpointdirdirectory-string","text":"setCheckpointDir method is used to set up the checkpoint directory...FIXME CAUTION: FIXME == [[register]] Registering Accumulator -- register Methods","title":"setCheckpointDir(directory: String)"},{"location":"SparkContext/#source-scala_27","text":"register(acc: AccumulatorV2[ , _]): Unit register(acc: AccumulatorV2[ , _], name: String): Unit register registers the acc link:spark-accumulators.adoc[accumulator]. You can optionally give an accumulator a name . TIP: You can create built-in accumulators for longs, doubles, and collection types using < >. Internally, register link:spark-accumulators.adoc#register[registers acc accumulator] (with the current SparkContext). == [[creating-accumulators]][[longAccumulator]][[doubleAccumulator]][[collectionAccumulator]] Creating Built-In Accumulators","title":"[source, scala]"},{"location":"SparkContext/#source-scala_28","text":"longAccumulator: LongAccumulator longAccumulator(name: String): LongAccumulator doubleAccumulator: DoubleAccumulator doubleAccumulator(name: String): DoubleAccumulator collectionAccumulator[T]: CollectionAccumulator[T] collectionAccumulator T : CollectionAccumulator[T] You can use longAccumulator , doubleAccumulator or collectionAccumulator to create and register link:spark-accumulators.adoc[accumulators] for simple and collection values. longAccumulator returns link:spark-accumulators.adoc#LongAccumulator[LongAccumulator] with the zero value 0 . doubleAccumulator returns link:spark-accumulators.adoc#DoubleAccumulator[DoubleAccumulator] with the zero value 0.0 . collectionAccumulator returns link:spark-accumulators.adoc#CollectionAccumulator[CollectionAccumulator] with the zero value java.util.List[T] .","title":"[source, scala]"},{"location":"SparkContext/#source-scala_29","text":"scala> val acc = sc.longAccumulator acc: org.apache.spark.util.LongAccumulator = LongAccumulator(id: 0, name: None, value: 0) scala> val counter = sc.longAccumulator(\"counter\") counter: org.apache.spark.util.LongAccumulator = LongAccumulator(id: 1, name: Some(counter), value: 0) scala> counter.value res0: Long = 0 scala> sc.parallelize(0 to 9).foreach(n => counter.add(n)) scala> counter.value res3: Long = 45 The name input parameter allows you to give a name to an accumulator and have it displayed in link:spark-webui-StagePage.adoc#accumulators[Spark UI] (under Stages tab for a given stage). .Accumulators in the Spark UI image::spark-webui-accumulators.png[align=\"center\"] TIP: You can register custom accumulators using < > methods. == [[broadcast]] Creating Broadcast Variable -- broadcast Method","title":"[source, scala]"},{"location":"SparkContext/#source-scala_30","text":"broadcast T : Broadcast[T] broadcast method creates a xref:ROOT:Broadcast.adoc[]. It is a shared memory with value (as broadcast blocks) on the driver and later on all Spark executors.","title":"[source, scala]"},{"location":"SparkContext/#sourceplaintext","text":"val sc: SparkContext = ??? scala> val hello = sc.broadcast(\"hello\") hello: org.apache.spark.broadcast.Broadcast[String] = Broadcast(0) Spark transfers the value to Spark executors once , and tasks can share it without incurring repetitive network transmissions when the broadcast variable is used multiple times. .Broadcasting a value to executors image::sparkcontext-broadcast-executors.png[align=\"center\"] Internally, broadcast requests BroadcastManager for a xref:core:BroadcastManager.adoc#newBroadcast[new broadcast variable]. NOTE: The current BroadcastManager is available using xref:core:SparkEnv.adoc#broadcastManager[ SparkEnv.broadcastManager ] attribute and is always xref:core:BroadcastManager.adoc[BroadcastManager] (with few internal configuration changes to reflect where it runs, i.e. inside the driver or executors). You should see the following INFO message in the logs: Created broadcast [id] from [callSite] If ContextCleaner is defined, the xref:core:ContextCleaner.adoc#[new broadcast variable is registered for cleanup].","title":"[source,plaintext]"},{"location":"SparkContext/#note_5","text":"Spark does not support broadcasting RDDs.","title":"[NOTE]"},{"location":"SparkContext/#scala-scbroadcastscrange0-10-javalangillegalargumentexception-requirement-failed-can-not-directly-broadcast-rdds-instead-call-collect-and-broadcast-the-result-at-scalapredefrequirepredefscala224-at-orgapachesparksparkcontextbroadcastsparkcontextscala1392-48-elided","text":"Once created, the broadcast variable (and other blocks) are displayed per executor and the driver in web UI (under link:spark-webui-executors.adoc[Executors tab]). .Broadcast Variables In web UI's Executors Tab image::spark-broadcast-webui-executors-rdd-blocks.png[align=\"center\"] == [[jars]] Distribute JARs to workers The jar you specify with SparkContext.addJar will be copied to all the worker nodes. The configuration setting spark.jars is a comma-separated list of jar paths to be included in all tasks executed from this SparkContext. A path can either be a local file, a file in HDFS (or other Hadoop-supported filesystems), an HTTP, HTTPS or FTP URI, or local:/path for a file on every worker node. scala> sc.addJar(\"build.sbt\") 15/11/11 21:54:54 INFO SparkContext: Added JAR build.sbt at http://192.168.1.4:49427/jars/build.sbt with timestamp 1447275294457 CAUTION: FIXME Why is HttpFileServer used for addJar? === SparkContext as Application-Wide Counter SparkContext keeps track of: [[nextShuffleId]] * shuffle ids using nextShuffleId internal counter for xref:scheduler:ShuffleMapStage.adoc[registering shuffle dependencies] to xref:shuffle:ShuffleManager.adoc[Shuffle Service]. == [[runJob]] Running Job Synchronously xref:rdd:index.adoc#actions[RDD actions] run link:spark-scheduler-ActiveJob.adoc[jobs] using one of runJob methods.","title":"scala&gt; sc.broadcast(sc.range(0, 10))\njava.lang.IllegalArgumentException: requirement failed: Can not directly broadcast RDDs; instead, call collect() and broadcast the result.\n  at scala.Predef$.require(Predef.scala:224)\n  at org.apache.spark.SparkContext.broadcast(SparkContext.scala:1392)\n  ... 48 elided\n"},{"location":"SparkContext/#source-scala_31","text":"runJob T, U : Unit runJob T, U : Array[U] runJob T, U : Array[U] runJob T, U : Array[U] runJob T, U : Array[U] runJob T, U runJob T, U: ClassTag runJob executes a function on one or many partitions of a RDD (in a SparkContext space) to produce a collection of values per partition. NOTE: runJob can only work when a SparkContext is not < >. Internally, runJob first makes sure that the SparkContext is not < >. If it is, you should see the following IllegalStateException exception in the logs: java.lang.IllegalStateException: SparkContext has been shutdown at org.apache.spark.SparkContext.runJob(SparkContext.scala:1893) at org.apache.spark.SparkContext.runJob(SparkContext.scala:1914) at org.apache.spark.SparkContext.runJob(SparkContext.scala:1934) ... 48 elided runJob then < > and < func closure>>. You should see the following INFO message in the logs: INFO SparkContext: Starting job: [callSite] With link:spark-rdd-lineage.adoc#spark_logLineage[spark.logLineage] enabled (which is not by default), you should see the following INFO message with link:spark-rdd-lineage.adoc#toDebugString[toDebugString] (executed on rdd ): INFO SparkContext: RDD's recursive dependencies: [toDebugString] runJob requests xref:scheduler:DAGScheduler.adoc#runJob[ DAGScheduler to run a job]. TIP: runJob just prepares input parameters for xref:scheduler:DAGScheduler.adoc#runJob[ DAGScheduler to run a job]. After DAGScheduler is done and the job has finished, runJob link:spark-sparkcontext-ConsoleProgressBar.adoc#finishAll[stops ConsoleProgressBar ] and xref:ROOT:rdd-checkpointing.adoc#doCheckpoint[performs RDD checkpointing of rdd ]. TIP: For some actions, e.g. first() and lookup() , there is no need to compute all the partitions of the RDD in a job. And Spark knows it.","title":"[source, scala]"},{"location":"SparkContext/#sourcescala_1","text":"// RDD to work with val lines = sc.parallelize(Seq(\"hello world\", \"nice to see you\")) import org.apache.spark.TaskContext scala> sc.runJob(lines, (t: TaskContext, i: Iterator[String]) => 1) // <1> res0: Array[Int] = Array(1, 1) // <2> <1> Run a job using runJob on lines RDD with a function that returns 1 for every partition (of lines RDD). <2> What can you say about the number of partitions of the lines RDD? Is your result res0 different than mine? Why? TIP: Read link:spark-TaskContext.adoc[TaskContext]. Running a job is essentially executing a func function on all or a subset of partitions in an rdd RDD and returning the result as an array (with elements being the results per partition). .Executing action image::spark-runjob.png[align=\"center\"] == [[stop]][[stopping]] Stopping SparkContext -- stop Method","title":"[source,scala]"},{"location":"SparkContext/#source-scala_32","text":"","title":"[source, scala]"},{"location":"SparkContext/#stop-unit","text":"stop stops the SparkContext. Internally, stop enables stopped internal flag. If already stopped, you should see the following INFO message in the logs: INFO SparkContext: SparkContext already stopped. stop then does the following: Removes _shutdownHookRef from ShutdownHookManager < SparkListenerApplicationEnd >> (to < >) link:spark-webui-SparkUI.adoc#stop[Stops web UI] link:spark-metrics-MetricsSystem.adoc#report[Requests MetricSystem to report metrics] (from all registered sinks) xref:core:ContextCleaner.adoc#stop[Stops ContextCleaner ] link:spark-ExecutorAllocationManager.adoc#stop[Requests ExecutorAllocationManager to stop] If LiveListenerBus was started, xref:scheduler:LiveListenerBus.adoc#stop[requests LiveListenerBus to stop] Requests xref:spark-history-server:EventLoggingListener.adoc#stop[ EventLoggingListener to stop] Requests xref:scheduler:DAGScheduler.adoc#stop[ DAGScheduler to stop] Requests xref:rpc:index.adoc#stop[RpcEnv to stop HeartbeatReceiver endpoint] Requests link:spark-sparkcontext-ConsoleProgressBar.adoc#stop[ ConsoleProgressBar to stop] Clears the reference to TaskScheduler , i.e. _taskScheduler is null Requests xref:core:SparkEnv.adoc#stop[ SparkEnv to stop] and clears SparkEnv Clears link:yarn/spark-yarn-client.adoc#SPARK_YARN_MODE[ SPARK_YARN_MODE flag] < > Ultimately, you should see the following INFO message in the logs: INFO SparkContext: Successfully stopped SparkContext == [[addSparkListener]] Registering SparkListener -- addSparkListener Method","title":"stop(): Unit"},{"location":"SparkContext/#source-scala_33","text":"","title":"[source, scala]"},{"location":"SparkContext/#addsparklistenerlistener-sparklistenerinterface-unit","text":"You can register a custom xref:ROOT:SparkListener.adoc#SparkListenerInterface[SparkListenerInterface] using addSparkListener method NOTE: You can also register custom listeners using xref:ROOT:configuration-properties.adoc#spark.extraListeners[spark.extraListeners] configuration property. == [[custom-schedulers]] Custom SchedulerBackend, TaskScheduler and DAGScheduler By default, SparkContext uses ( private[spark] class) org.apache.spark.scheduler.DAGScheduler , but you can develop your own custom DAGScheduler implementation, and use ( private[spark] ) SparkContext.dagScheduler_=(ds: DAGScheduler) method to assign yours. It is also applicable to SchedulerBackend and TaskScheduler using schedulerBackend_=(sb: SchedulerBackend) and taskScheduler_=(ts: TaskScheduler) methods, respectively. CAUTION: FIXME Make it an advanced exercise. == [[events]] Events When a Spark context starts, it triggers xref:ROOT:SparkListener.adoc#SparkListenerEnvironmentUpdate[SparkListenerEnvironmentUpdate] and xref:ROOT:SparkListener.adoc#SparkListenerApplicationStart[SparkListenerApplicationStart] messages. Refer to the section < >. == [[setLogLevel]][[setting-default-log-level]] Setting Default Logging Level -- setLogLevel Method","title":"addSparkListener(listener: SparkListenerInterface): Unit"},{"location":"SparkContext/#source-scala_34","text":"","title":"[source, scala]"},{"location":"SparkContext/#setloglevelloglevel-string","text":"setLogLevel allows you to set the root logging level in a Spark application, e.g. link:spark-shell.adoc[Spark shell]. Internally, setLogLevel calls link:++ http://logging.apache.org/log4j/2.x/log4j-api/apidocs/org/apache/logging/log4j/Level.html#toLevel(java.lang.String)++[org.apache.log4j.Level.toLevel(logLevel )] that it then uses to set using link:++ http://logging.apache.org/log4j/2.x/log4j-api/apidocs/org/apache/logging/log4j/LogManager.html#getRootLogger()++[org.apache.log4j.LogManager.getRootLogger().setLevel(level )].","title":"setLogLevel(logLevel: String)"},{"location":"SparkContext/#tip","text":"You can directly set the logging level using link:++ http://logging.apache.org/log4j/2.x/log4j-api/apidocs/org/apache/logging/log4j/LogManager.html#getLogger()++[org.apache.log4j.LogManager.getLogger ()].","title":"[TIP]"},{"location":"SparkContext/#source-scala_35","text":"","title":"[source, scala]"},{"location":"SparkContext/#logmanagergetloggerorgsetlevelleveloff","text":"==== == [[clean]][[closure-cleaning]] Closure Cleaning -- clean Method","title":"LogManager.getLogger(\"org\").setLevel(Level.OFF)"},{"location":"SparkContext/#source-scala_36","text":"","title":"[source, scala]"},{"location":"SparkContext/#cleanf-f-checkserializable-boolean-true-f","text":"Every time an action is called, Spark cleans up the closure, i.e. the body of the action, before it is serialized and sent over the wire to executors. SparkContext comes with clean(f: F, checkSerializable: Boolean = true) method that does this. It in turn calls ClosureCleaner.clean method. Not only does ClosureCleaner.clean method clean the closure, but also does it transitively, i.e. referenced closures are cleaned transitively. A closure is considered serializable as long as it does not explicitly reference unserializable objects. It does so by traversing the hierarchy of enclosing closures and null out any references that are not actually used by the starting closure.","title":"clean(f: F, checkSerializable: Boolean = true): F"},{"location":"SparkContext/#tip_1","text":"Enable DEBUG logging level for org.apache.spark.util.ClosureCleaner logger to see what happens inside the class. Add the following line to conf/log4j.properties : log4j.logger.org.apache.spark.util.ClosureCleaner=DEBUG","title":"[TIP]"},{"location":"SparkContext/#refer-to-linkspark-loggingadoclogging","text":"With DEBUG logging level you should see the following messages in the logs: +++ Cleaning closure [func] ([func.getClass.getName]) +++ + declared fields: [declaredFields.size] [field] ... +++ closure [func] ([func.getClass.getName]) is now cleaned +++ Serialization is verified using a new instance of Serializer (as xref:core:SparkEnv.adoc#closureSerializer[closure Serializer]). Refer to link:spark-serialization.adoc[Serialization]. CAUTION: FIXME an example, please. == [[hadoopConfiguration]] Hadoop Configuration While a < >, so is a Hadoop configuration (as an instance of https://hadoop.apache.org/docs/current/api/org/apache/hadoop/conf/Configuration.html[org.apache.hadoop.conf.Configuration ] that is available as _hadoopConfiguration ). NOTE: link:spark-SparkHadoopUtil.adoc#newConfiguration[SparkHadoopUtil.get.newConfiguration] is used. If a SparkConf is provided it is used to build the configuration as described. Otherwise, the default Configuration object is returned. If AWS_ACCESS_KEY_ID and AWS_SECRET_ACCESS_KEY are both available, the following settings are set for the Hadoop configuration: fs.s3.awsAccessKeyId , fs.s3n.awsAccessKeyId , fs.s3a.access.key are set to the value of AWS_ACCESS_KEY_ID fs.s3.awsSecretAccessKey , fs.s3n.awsSecretAccessKey , and fs.s3a.secret.key are set to the value of AWS_SECRET_ACCESS_KEY Every spark.hadoop. setting becomes a setting of the configuration with the prefix spark.hadoop. removed for the key. The value of spark.buffer.size (default: 65536 ) is used as the value of io.file.buffer.size . == [[listenerBus]] listenerBus -- LiveListenerBus Event Bus listenerBus is a xref:scheduler:LiveListenerBus.adoc[] object that acts as a mechanism to announce events to other services on the link:spark-driver.adoc[driver]. NOTE: It is created and started when link:spark-SparkContext-creating-instance-internals.adoc[SparkContext starts] and, since it is a single-JVM event bus, is exclusively used on the driver. NOTE: listenerBus is a private[spark] value in SparkContext. == [[startTime]] Time when SparkContext was Created -- startTime Property","title":"Refer to link:spark-logging.adoc[Logging]."},{"location":"SparkContext/#source-scala_37","text":"","title":"[source, scala]"},{"location":"SparkContext/#starttime-long","text":"startTime is the time in milliseconds when < >.","title":"startTime: Long"},{"location":"SparkContext/#source-scala_38","text":"scala> sc.startTime res0: Long = 1464425605653 == [[sparkUser]] Spark User -- sparkUser Property","title":"[source, scala]"},{"location":"SparkContext/#source-scala_39","text":"","title":"[source, scala]"},{"location":"SparkContext/#sparkuser-string","text":"sparkUser is the user who started the SparkContext instance. NOTE: It is computed when link:spark-SparkContext-creating-instance-internals.adoc#sparkUser[SparkContext is created] using link:spark-SparkContext-creating-instance-internals.adoc#[Utils.getCurrentUserName]. == [[submitMapStage]] Submitting ShuffleDependency for Execution -- submitMapStage Internal Method","title":"sparkUser: String"},{"location":"SparkContext/#source-scala_40","text":"submitMapStage K, V, C : SimpleFutureAction[MapOutputStatistics] submitMapStage xref:scheduler:DAGScheduler.adoc#submitMapStage[submits the input ShuffleDependency to DAGScheduler for execution] and returns a SimpleFutureAction . Internally, submitMapStage < > first and submits it with localProperties . NOTE: Interestingly, submitMapStage is used exclusively when Spark SQL's link:spark-sql-SparkPlan-ShuffleExchange.adoc[ShuffleExchange] physical operator is executed. NOTE: submitMapStage seems related to xref:scheduler:DAGScheduler.adoc#adaptive-query-planning[Adaptive Query Planning / Adaptive Scheduling]. == [[getCallSite]] Calculating Call Site -- getCallSite Method CAUTION: FIXME == [[cancelJobGroup]] Cancelling Job Group -- cancelJobGroup Method","title":"[source, scala]"},{"location":"SparkContext/#source-scala_41","text":"","title":"[source, scala]"},{"location":"SparkContext/#canceljobgroupgroupid-string","text":"cancelJobGroup requests DAGScheduler xref:scheduler:DAGScheduler.adoc#cancelJobGroup[to cancel a group of active Spark jobs]. NOTE: cancelJobGroup is used exclusively when SparkExecuteStatementOperation does cancel . == [[cancelAllJobs]] Cancelling All Running and Scheduled Jobs -- cancelAllJobs Method CAUTION: FIXME NOTE: cancelAllJobs is used when link:spark-shell.adoc[spark-shell] is terminated (e.g. using Ctrl+C, so it can in turn terminate all active Spark jobs) or SparkSQLCLIDriver is terminated. == [[setJobGroup]] Setting Local Properties to Group Spark Jobs -- setJobGroup Method","title":"cancelJobGroup(groupId: String)"},{"location":"SparkContext/#source-scala_42","text":"setJobGroup( groupId: String, description: String, interruptOnCancel: Boolean = false): Unit setJobGroup link:spark-sparkcontext-local-properties.adoc#setLocalProperty[sets local properties]: spark.jobGroup.id as groupId spark.job.description as description spark.job.interruptOnCancel as interruptOnCancel","title":"[source, scala]"},{"location":"SparkContext/#note_6","text":"setJobGroup is used when: Spark Thrift Server's SparkExecuteStatementOperation runs a query","title":"[NOTE]"},{"location":"SparkContext/#structured-streamings-streamexecution-runs-batches","text":"== [[cleaner]] ContextCleaner","title":"Structured Streaming's StreamExecution runs batches"},{"location":"SparkContext/#source-scala_43","text":"","title":"[source, scala]"},{"location":"SparkContext/#cleaner-optioncontextcleaner","text":"SparkContext may have a xref:core:ContextCleaner.adoc[ContextCleaner] defined. ContextCleaner is created when xref:ROOT:spark-SparkContext-creating-instance-internals.adoc#_cleaner[SparkContext is created] with xref:ROOT:configuration-properties.adoc#spark.cleaner.referenceTracking[spark.cleaner.referenceTracking] configuration property enabled. == [[getPreferredLocs]] Finding Preferred Locations (Placement Preferences) for RDD Partition","title":"cleaner: Option[ContextCleaner]"},{"location":"SparkContext/#source-scala_44","text":"getPreferredLocs( rdd: RDD[_], partition: Int): Seq[TaskLocation] getPreferredLocs simply xref:scheduler:DAGScheduler.adoc#getPreferredLocs[requests DAGScheduler for the preferred locations for partition ]. NOTE: Preferred locations of a partition of a RDD are also called placement preferences or locality preferences . getPreferredLocs is used in CoalescedRDDPartition, DefaultPartitionCoalescer and PartitionerAwareUnionRDD. == [[persistRDD]] Registering RDD in persistentRdds Internal Registry -- persistRDD Internal Method","title":"[source, scala]"},{"location":"SparkContext/#source-scala_45","text":"","title":"[source, scala]"},{"location":"SparkContext/#persistrddrdd-rdd_-unit","text":"persistRDD registers rdd in < > internal registry. NOTE: persistRDD is used exclusively when RDD is xref:rdd:index.adoc#persist-internal[persisted or locally checkpointed]. == [[getRDDStorageInfo]] Getting Storage Status of Cached RDDs (as RDDInfos) -- getRDDStorageInfo Methods","title":"persistRDD(rdd: RDD[_]): Unit"},{"location":"SparkContext/#source-scala_46","text":"getRDDStorageInfo: Array[RDDInfo] // <1> getRDDStorageInfo(filter: RDD[_] => Boolean): Array[RDDInfo] // <2> <1> Part of Spark's Developer API that uses <2> filtering no RDDs getRDDStorageInfo takes all the RDDs (from < > registry) that match filter and creates a collection of xref:storage:RDDInfo.adoc[RDDInfo] instances. getRDDStorageInfo then link:spark-webui-StorageListener.adoc#StorageUtils.updateRddInfo[updates the RDDInfos] with the < > (in a Spark application). In the end, getRDDStorageInfo gives only the RDD that are cached (i.e. the sum of memory and disk sizes as well as the number of partitions cached are greater than 0 ). NOTE: getRDDStorageInfo is used when RDD link:spark-rdd-lineage.adoc#toDebugString[is requested for RDD lineage graph]. == [[settings]] Settings === [[spark.driver.allowMultipleContexts]] spark.driver.allowMultipleContexts Quoting the scaladoc of http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.SparkContext[org.apache.spark.SparkContext ]: Only one SparkContext may be active per JVM. You must stop() the active SparkContext before creating a new one. You can however control the behaviour using spark.driver.allowMultipleContexts flag. It is disabled, i.e. false , by default. If enabled (i.e. true ), Spark prints the following WARN message to the logs: WARN Multiple running SparkContexts detected in the same JVM! If disabled (default), it will throw an SparkException exception: Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at: [ctx.creationSite.longForm] When creating an instance of SparkContext, Spark marks the current thread as having it being created (very early in the instantiation process). CAUTION: It's not guaranteed that Spark will work properly with two or more SparkContexts. Consider the feature a work in progress. == [[statusStore]] Accessing AppStatusStore","title":"[source, scala]"},{"location":"SparkContext/#source-scala_47","text":"","title":"[source, scala]"},{"location":"SparkContext/#statusstore-appstatusstore","text":"statusStore gives the current xref:core:AppStatusStore.adoc[]. statusStore is used when: SparkContext is requested to < > ConsoleProgressBar is requested to xref:ROOT:spark-sparkcontext-ConsoleProgressBar.adoc#refresh[refresh] SharedState (Spark SQL) is requested for a SQLAppStatusStore == [[uiWebUrl]] Requesting URL of web UI -- uiWebUrl Method","title":"statusStore: AppStatusStore"},{"location":"SparkContext/#source-scala_48","text":"","title":"[source, scala]"},{"location":"SparkContext/#uiweburl-optionstring","text":"uiWebUrl requests the link:spark-SparkContext-creating-instance-internals.adoc#_ui[SparkUI] for link:spark-webui-WebUI.adoc#webUrl[webUrl]. == [[maxNumConcurrentTasks]] maxNumConcurrentTasks Method","title":"uiWebUrl: Option[String]"},{"location":"SparkContext/#source-scala_49","text":"","title":"[source, scala]"},{"location":"SparkContext/#maxnumconcurrenttasks-int","text":"maxNumConcurrentTasks simply requests the < > for the xref:scheduler:SchedulerBackend.adoc#maxNumConcurrentTasks[maximum number of tasks that can be launched concurrently]. NOTE: maxNumConcurrentTasks is used exclusively when DAGScheduler is requested to xref:scheduler:DAGScheduler.adoc#checkBarrierStageWithNumSlots[checkBarrierStageWithNumSlots]. == [[createTaskScheduler]] Creating SchedulerBackend and TaskScheduler -- createTaskScheduler Internal Factory Method","title":"maxNumConcurrentTasks(): Int"},{"location":"SparkContext/#source-scala_50","text":"createTaskScheduler( sc: SparkContext, master: String, deployMode: String): (SchedulerBackend, TaskScheduler) createTaskScheduler creates the xref:scheduler:SchedulerBackend.adoc[SchedulerBackend] and the xref:scheduler:TaskScheduler.adoc[TaskScheduler] for the given master URL and deployment mode. .SparkContext creates Task Scheduler and Scheduler Backend image::diagrams/sparkcontext-createtaskscheduler.png[align=\"center\"] Internally, createTaskScheduler branches off per the given master URL (link:spark-deployment-environments.adoc#master-urls[master URL]) to select the requested implementations. createTaskScheduler understands the following master URLs: local - local mode with 1 thread only local[n] or local[*] - local mode with n threads local[n, m] or local[*, m] -- local mode with n threads and m number of failures spark://hostname:port for Spark Standalone local-cluster[n, m, z] -- local cluster with n workers, m cores per worker, and z memory per worker any other URL is passed to < getClusterManager to load an external cluster manager>>. CAUTION: FIXME == [[environment-variables]] Environment Variables .Environment Variables [cols=\"1,1,2\",options=\"header\",width=\"100%\"] |=== | Environment Variable | Default Value | Description | [[SPARK_EXECUTOR_MEMORY]] SPARK_EXECUTOR_MEMORY | 1024 | Amount of memory to allocate for a Spark executor in MB. See xref:executor:Executor.adoc#memory[Executor Memory]. [[SPARK_USER]] SPARK_USER The user who is running SparkContext. Available later as < >. === == [[postEnvironmentUpdate]] Posting SparkListenerEnvironmentUpdate Event","title":"[source, scala]"},{"location":"SparkContext/#source-scala_51","text":"","title":"[source, scala]"},{"location":"SparkContext/#postenvironmentupdate-unit","text":"postEnvironmentUpdate ...FIXME NOTE: postEnvironmentUpdate is used when SparkContext is < >, and requested to < > and < >. == [[addJar-internals]] addJar Method","title":"postEnvironmentUpdate(): Unit"},{"location":"SparkContext/#source-scala_52","text":"","title":"[source, scala]"},{"location":"SparkContext/#addjarpath-string-unit_1","text":"addJar ...FIXME NOTE: addJar is used when...FIXME == [[runApproximateJob]] Running Approximate Job","title":"addJar(path: String): Unit"},{"location":"SparkContext/#source-scala_53","text":"runApproximateJob T, U, R : PartialResult[R] runApproximateJob...FIXME runApproximateJob is used when: DoubleRDDFunctions is requested to meanApprox and sumApprox RDD is requested to countApprox and countByValueApprox == [[killTaskAttempt]] Killing Task","title":"[source, scala]"},{"location":"SparkContext/#source-scala_54","text":"killTaskAttempt( taskId: Long, interruptThread: Boolean = true, reason: String = \"killed via SparkContext.killTaskAttempt\"): Boolean killTaskAttempt requests the < > to xref:scheduler:DAGScheduler.adoc#killTaskAttempt[kill a task]. == [[checkpointFile]] checkpointFile Internal Method","title":"[source, scala]"},{"location":"SparkContext/#source-scala_55","text":"checkpointFile T: ClassTag : RDD[T] checkpointFile...FIXME == [[logging]] Logging Enable ALL logging level for org.apache.spark.SparkContext logger to see what happens inside. Add the following line to conf/log4j.properties :","title":"[source, scala]"},{"location":"SparkContext/#sourceplaintext_1","text":"","title":"[source,plaintext]"},{"location":"SparkContext/#log4jloggerorgapachesparksparkcontextall","text":"Refer to xref:ROOT:spark-logging.adoc[Logging]. == [[internal-properties]] Internal Properties === [[checkpointDir]] Checkpoint Directory","title":"log4j.logger.org.apache.spark.SparkContext=ALL"},{"location":"SparkContext/#sourcescala_2","text":"","title":"[source,scala]"},{"location":"SparkContext/#checkpointdir-optionstring-none","text":"checkpointDir is...FIXME === [[persistentRdds]] persistentRdds Lookup Table Lookup table of persistent/cached RDDs per their ids. Used when SparkContext is requested to: < > < > < > < > === [[stopped]] stopped Flag Flag that says whether...FIXME ( true ) or not ( false ) === [[_taskScheduler]] TaskScheduler xref:scheduler:TaskScheduler.adoc[TaskScheduler]","title":"checkpointDir: Option[String] = None"},{"location":"SparkEnv/","text":"SparkEnv \u00b6 SparkEnv is a handle to Spark Execution Environment with the core services of Apache Spark (that interact with each other to establish a distributed computing platform for a Spark application). There are two separate SparkEnv s of the driver and executors . Core Services \u00b6 Property Service blockManager BlockManager broadcastManager BroadcastManager closureSerializer Serializer conf SparkConf mapOutputTracker MapOutputTracker memoryManager MemoryManager metricsSystem MetricsSystem outputCommitCoordinator OutputCommitCoordinator rpcEnv RpcEnv securityManager SecurityManager serializer Serializer serializerManager SerializerManager shuffleManager ShuffleManager Creating Instance \u00b6 SparkEnv takes the following to be created: Executor ID RpcEnv Serializer Serializer SerializerManager MapOutputTracker ShuffleManager BroadcastManager BlockManager SecurityManager MetricsSystem MemoryManager OutputCommitCoordinator SparkConf SparkEnv is created using create utility. Creating SparkEnv for Driver \u00b6 createDriverEnv ( conf : SparkConf , isLocal : Boolean , listenerBus : LiveListenerBus , numCores : Int , mockOutputCommitCoordinator : Option [ OutputCommitCoordinator ] = None ) : SparkEnv createDriverEnv creates a SparkEnv execution environment for the driver. createDriverEnv accepts an instance of xref:ROOT:SparkConf.md[SparkConf], link:spark-deployment-environments.md[whether it runs in local mode or not], xref:scheduler:LiveListenerBus.md[], the number of cores to use for execution in local mode or 0 otherwise, and a xref:scheduler:OutputCommitCoordinator.md[OutputCommitCoordinator] (default: none). createDriverEnv ensures that link:spark-driver.md#spark_driver_host[spark.driver.host] and link:spark-driver.md#spark_driver_port[spark.driver.port] settings are defined. It then passes the call straight on to the < > (with driver executor id, isDriver enabled, and the input parameters). createDriverEnv is used when SparkContext is created . Creating SparkEnv for Executor \u00b6 createExecutorEnv ( conf : SparkConf , executorId : String , hostname : String , numCores : Int , ioEncryptionKey : Option [ Array [ Byte ]], isLocal : Boolean ) : SparkEnv createExecutorEnv ( conf : SparkConf , executorId : String , bindAddress : String , hostname : String , numCores : Int , ioEncryptionKey : Option [ Array [ Byte ]], isLocal : Boolean ) : SparkEnv createExecutorEnv creates an executor's (execution) environment that is the Spark execution environment for an executor. createExecutorEnv simply < > (passing in all the input parameters) and < >. NOTE: The number of cores numCores is configured using --cores command-line option of CoarseGrainedExecutorBackend and is specific to a cluster manager. createExecutorEnv is used when CoarseGrainedExecutorBackend utility is requested to run . Creating \"Base\" SparkEnv (for Driver and Executors) \u00b6 create ( conf : SparkConf , executorId : String , bindAddress : String , advertiseAddress : String , port : Option [ Int ], isLocal : Boolean , numUsableCores : Int , ioEncryptionKey : Option [ Array [ Byte ]], listenerBus : LiveListenerBus = null , mockOutputCommitCoordinator : Option [ OutputCommitCoordinator ] = None ) : SparkEnv create is an utility to create the \"base\" SparkEnv (that is \"enhanced\" for the driver and executors later on). .create's Input Arguments and Their Usage [cols=\"1,2\",options=\"header\",width=\"100%\"] |=== | Input Argument | Usage | bindAddress | Used to create xref:rpc:index.md[RpcEnv] and xref:storage:NettyBlockTransferService.md#creating-instance[NettyBlockTransferService]. | advertiseAddress | Used to create xref:rpc:index.md[RpcEnv] and xref:storage:NettyBlockTransferService.md#creating-instance[NettyBlockTransferService]. | numUsableCores | Used to create xref:memory:MemoryManager.md[MemoryManager], xref:storage:NettyBlockTransferService.md#creating-instance[NettyBlockTransferService] and xref:storage:BlockManager.md#creating-instance[BlockManager]. |=== [[create-Serializer]] create creates a Serializer (based on < > setting). You should see the following DEBUG message in the logs: Using serializer: [serializer] [[create-closure-Serializer]] create creates a closure Serializer (based on < >). [[ShuffleManager]][[create-ShuffleManager]] create creates a xref:shuffle:ShuffleManager.md[ShuffleManager] given the value of xref:ROOT:configuration-properties.md#spark.shuffle.manager[spark.shuffle.manager] configuration property. [[MemoryManager]][[create-MemoryManager]] create creates a xref:memory:MemoryManager.md[MemoryManager] based on xref:ROOT:configuration-properties.md#spark.memory.useLegacyMode[spark.memory.useLegacyMode] setting (with xref:memory:UnifiedMemoryManager.md[UnifiedMemoryManager] being the default and numCores the input numUsableCores ). [[NettyBlockTransferService]][[create-NettyBlockTransferService]] create creates a xref:storage:NettyBlockTransferService.md#creating-instance[NettyBlockTransferService] with the following ports: link:spark-driver.md#spark_driver_blockManager_port[spark.driver.blockManager.port] for the driver (default: 0 ) xref:storage:BlockManager.md#spark_blockManager_port[spark.blockManager.port] for an executor (default: 0 ) NOTE: create uses the NettyBlockTransferService to < >. CAUTION: FIXME A picture with SparkEnv, NettyBlockTransferService and the ports \"armed\". [[BlockManagerMaster]][[create-BlockManagerMaster]] create creates a xref:storage:BlockManagerMaster.md#creating-instance[BlockManagerMaster] object with the BlockManagerMaster RPC endpoint reference (by < > and xref:storage:BlockManagerMasterEndpoint.md[]), the input xref:ROOT:SparkConf.md[SparkConf], and the input isDriver flag. .Creating BlockManager for the Driver image::sparkenv-driver-blockmanager.png[align=\"center\"] NOTE: create registers the BlockManagerMaster RPC endpoint for the driver and looks it up for executors. .Creating BlockManager for Executor image::sparkenv-executor-blockmanager.png[align=\"center\"] [[BlockManager]][[create-BlockManager]] create creates a xref:storage:BlockManager.md#creating-instance[BlockManager] (using the above < >, < > and other services). create creates a xref:core:BroadcastManager.md[]. [[MapOutputTracker]][[create-MapOutputTracker]] create creates a xref:scheduler:MapOutputTrackerMaster.md[MapOutputTrackerMaster] or xref:scheduler:MapOutputTrackerWorker.md[MapOutputTrackerWorker] for the driver and executors, respectively. NOTE: The choice of the real implementation of xref:scheduler:MapOutputTracker.md[MapOutputTracker] is based on whether the input executorId is driver or not. [[MapOutputTrackerMasterEndpoint]][[create-MapOutputTrackerMasterEndpoint]] create < RpcEndpoint >> as MapOutputTracker . It registers xref:scheduler:MapOutputTrackerMasterEndpoint.md[MapOutputTrackerMasterEndpoint] on the driver and creates a RPC endpoint reference on executors. The RPC endpoint reference gets assigned as the xref:scheduler:MapOutputTracker.md#trackerEndpoint[MapOutputTracker RPC endpoint]. CAUTION: FIXME [[create-CacheManager]] It creates a CacheManager. [[create-MetricsSystem]] It creates a MetricsSystem for a driver and a worker separately. It initializes userFiles temporary directory used for downloading dependencies for a driver while this is the executor's current working directory for an executor. [[create-OutputCommitCoordinator]] An OutputCommitCoordinator is created. Usage \u00b6 create is used when SparkEnv utility is used to create a SparkEnv for the driver and executors . == [[get]] Accessing SparkEnv [source, scala] \u00b6 get: SparkEnv \u00b6 get returns the SparkEnv on the driver and executors. [source, scala] \u00b6 import org.apache.spark.SparkEnv assert(SparkEnv.get.isInstanceOf[SparkEnv]) == [[registerOrLookupEndpoint]] Registering or Looking up RPC Endpoint by Name [source, scala] \u00b6 registerOrLookupEndpoint( name: String, endpointCreator: => RpcEndpoint) registerOrLookupEndpoint registers or looks up a RPC endpoint by name . If called from the driver, you should see the following INFO message in the logs: Registering [name] And the RPC endpoint is registered in the RPC environment. Otherwise, it obtains a RPC endpoint reference by name . == [[stop]] Stopping SparkEnv [source, scala] \u00b6 stop(): Unit \u00b6 stop checks < > internal flag and does nothing when enabled already. Otherwise, stop turns isStopped flag on, stops all pythonWorkers and requests the following services to stop: xref:scheduler:MapOutputTracker.md#stop[MapOutputTracker] xref:shuffle:ShuffleManager.md#stop[ShuffleManager] xref:core:BroadcastManager.md#stop[BroadcastManager] xref:storage:BlockManager.md#stop[BlockManager] xref:storage:BlockManagerMaster.md#stop[BlockManagerMaster] link:spark-metrics-MetricsSystem.md#stop[MetricsSystem] xref:scheduler:OutputCommitCoordinator.md#stop[OutputCommitCoordinator] stop xref:rpc:index.md#shutdown[requests RpcEnv to shut down] and xref:rpc:index.md#awaitTermination[waits till it terminates]. Only on the driver, stop deletes the < >. You can see the following WARN message in the logs if the deletion fails. Exception while deleting Spark temp dir: [path] NOTE: stop is used when xref:ROOT:SparkContext.md#stop[ SparkContext stops] (on the driver) and xref:executor:Executor.md#stop[ Executor stops]. == [[set]] set Method [source, scala] \u00b6 set(e: SparkEnv): Unit \u00b6 set saves the input SparkEnv to < > internal registry (as the default SparkEnv). NOTE: set is used when...FIXME == [[environmentDetails]] environmentDetails Utility [source, scala] \u00b6 environmentDetails( conf: SparkConf, schedulingMode: String, addedJars: Seq[String], addedFiles: Seq[String]): Map[String, Seq[(String, String)]] environmentDetails...FIXME environmentDetails is used when SparkContext is requested to xref:ROOT:SparkContext.md#postEnvironmentUpdate[post a SparkListenerEnvironmentUpdate event]. == [[logging]] Logging Enable ALL logging level for org.apache.spark.SparkEnv logger to see what happens inside. Add the following line to conf/log4j.properties : [source] \u00b6 log4j.logger.org.apache.spark.SparkEnv=ALL \u00b6 Refer to xref:ROOT:spark-logging.md[Logging]. == [[internal-properties]] Internal Properties [cols=\"30m,70\",options=\"header\",width=\"100%\"] |=== | Name | Description | isStopped | [[isStopped]] Used to mark SparkEnv stopped Default: false | driverTmpDir | [[driverTmpDir]] |===","title":"SparkEnv"},{"location":"SparkEnv/#sparkenv","text":"SparkEnv is a handle to Spark Execution Environment with the core services of Apache Spark (that interact with each other to establish a distributed computing platform for a Spark application). There are two separate SparkEnv s of the driver and executors .","title":"SparkEnv"},{"location":"SparkEnv/#core-services","text":"Property Service blockManager BlockManager broadcastManager BroadcastManager closureSerializer Serializer conf SparkConf mapOutputTracker MapOutputTracker memoryManager MemoryManager metricsSystem MetricsSystem outputCommitCoordinator OutputCommitCoordinator rpcEnv RpcEnv securityManager SecurityManager serializer Serializer serializerManager SerializerManager shuffleManager ShuffleManager","title":" Core Services"},{"location":"SparkEnv/#creating-instance","text":"SparkEnv takes the following to be created: Executor ID RpcEnv Serializer Serializer SerializerManager MapOutputTracker ShuffleManager BroadcastManager BlockManager SecurityManager MetricsSystem MemoryManager OutputCommitCoordinator SparkConf SparkEnv is created using create utility.","title":"Creating Instance"},{"location":"SparkEnv/#creating-sparkenv-for-driver","text":"createDriverEnv ( conf : SparkConf , isLocal : Boolean , listenerBus : LiveListenerBus , numCores : Int , mockOutputCommitCoordinator : Option [ OutputCommitCoordinator ] = None ) : SparkEnv createDriverEnv creates a SparkEnv execution environment for the driver. createDriverEnv accepts an instance of xref:ROOT:SparkConf.md[SparkConf], link:spark-deployment-environments.md[whether it runs in local mode or not], xref:scheduler:LiveListenerBus.md[], the number of cores to use for execution in local mode or 0 otherwise, and a xref:scheduler:OutputCommitCoordinator.md[OutputCommitCoordinator] (default: none). createDriverEnv ensures that link:spark-driver.md#spark_driver_host[spark.driver.host] and link:spark-driver.md#spark_driver_port[spark.driver.port] settings are defined. It then passes the call straight on to the < > (with driver executor id, isDriver enabled, and the input parameters). createDriverEnv is used when SparkContext is created .","title":" Creating SparkEnv for Driver"},{"location":"SparkEnv/#creating-sparkenv-for-executor","text":"createExecutorEnv ( conf : SparkConf , executorId : String , hostname : String , numCores : Int , ioEncryptionKey : Option [ Array [ Byte ]], isLocal : Boolean ) : SparkEnv createExecutorEnv ( conf : SparkConf , executorId : String , bindAddress : String , hostname : String , numCores : Int , ioEncryptionKey : Option [ Array [ Byte ]], isLocal : Boolean ) : SparkEnv createExecutorEnv creates an executor's (execution) environment that is the Spark execution environment for an executor. createExecutorEnv simply < > (passing in all the input parameters) and < >. NOTE: The number of cores numCores is configured using --cores command-line option of CoarseGrainedExecutorBackend and is specific to a cluster manager. createExecutorEnv is used when CoarseGrainedExecutorBackend utility is requested to run .","title":" Creating SparkEnv for Executor"},{"location":"SparkEnv/#creating-base-sparkenv-for-driver-and-executors","text":"create ( conf : SparkConf , executorId : String , bindAddress : String , advertiseAddress : String , port : Option [ Int ], isLocal : Boolean , numUsableCores : Int , ioEncryptionKey : Option [ Array [ Byte ]], listenerBus : LiveListenerBus = null , mockOutputCommitCoordinator : Option [ OutputCommitCoordinator ] = None ) : SparkEnv create is an utility to create the \"base\" SparkEnv (that is \"enhanced\" for the driver and executors later on). .create's Input Arguments and Their Usage [cols=\"1,2\",options=\"header\",width=\"100%\"] |=== | Input Argument | Usage | bindAddress | Used to create xref:rpc:index.md[RpcEnv] and xref:storage:NettyBlockTransferService.md#creating-instance[NettyBlockTransferService]. | advertiseAddress | Used to create xref:rpc:index.md[RpcEnv] and xref:storage:NettyBlockTransferService.md#creating-instance[NettyBlockTransferService]. | numUsableCores | Used to create xref:memory:MemoryManager.md[MemoryManager], xref:storage:NettyBlockTransferService.md#creating-instance[NettyBlockTransferService] and xref:storage:BlockManager.md#creating-instance[BlockManager]. |=== [[create-Serializer]] create creates a Serializer (based on < > setting). You should see the following DEBUG message in the logs: Using serializer: [serializer] [[create-closure-Serializer]] create creates a closure Serializer (based on < >). [[ShuffleManager]][[create-ShuffleManager]] create creates a xref:shuffle:ShuffleManager.md[ShuffleManager] given the value of xref:ROOT:configuration-properties.md#spark.shuffle.manager[spark.shuffle.manager] configuration property. [[MemoryManager]][[create-MemoryManager]] create creates a xref:memory:MemoryManager.md[MemoryManager] based on xref:ROOT:configuration-properties.md#spark.memory.useLegacyMode[spark.memory.useLegacyMode] setting (with xref:memory:UnifiedMemoryManager.md[UnifiedMemoryManager] being the default and numCores the input numUsableCores ). [[NettyBlockTransferService]][[create-NettyBlockTransferService]] create creates a xref:storage:NettyBlockTransferService.md#creating-instance[NettyBlockTransferService] with the following ports: link:spark-driver.md#spark_driver_blockManager_port[spark.driver.blockManager.port] for the driver (default: 0 ) xref:storage:BlockManager.md#spark_blockManager_port[spark.blockManager.port] for an executor (default: 0 ) NOTE: create uses the NettyBlockTransferService to < >. CAUTION: FIXME A picture with SparkEnv, NettyBlockTransferService and the ports \"armed\". [[BlockManagerMaster]][[create-BlockManagerMaster]] create creates a xref:storage:BlockManagerMaster.md#creating-instance[BlockManagerMaster] object with the BlockManagerMaster RPC endpoint reference (by < > and xref:storage:BlockManagerMasterEndpoint.md[]), the input xref:ROOT:SparkConf.md[SparkConf], and the input isDriver flag. .Creating BlockManager for the Driver image::sparkenv-driver-blockmanager.png[align=\"center\"] NOTE: create registers the BlockManagerMaster RPC endpoint for the driver and looks it up for executors. .Creating BlockManager for Executor image::sparkenv-executor-blockmanager.png[align=\"center\"] [[BlockManager]][[create-BlockManager]] create creates a xref:storage:BlockManager.md#creating-instance[BlockManager] (using the above < >, < > and other services). create creates a xref:core:BroadcastManager.md[]. [[MapOutputTracker]][[create-MapOutputTracker]] create creates a xref:scheduler:MapOutputTrackerMaster.md[MapOutputTrackerMaster] or xref:scheduler:MapOutputTrackerWorker.md[MapOutputTrackerWorker] for the driver and executors, respectively. NOTE: The choice of the real implementation of xref:scheduler:MapOutputTracker.md[MapOutputTracker] is based on whether the input executorId is driver or not. [[MapOutputTrackerMasterEndpoint]][[create-MapOutputTrackerMasterEndpoint]] create < RpcEndpoint >> as MapOutputTracker . It registers xref:scheduler:MapOutputTrackerMasterEndpoint.md[MapOutputTrackerMasterEndpoint] on the driver and creates a RPC endpoint reference on executors. The RPC endpoint reference gets assigned as the xref:scheduler:MapOutputTracker.md#trackerEndpoint[MapOutputTracker RPC endpoint]. CAUTION: FIXME [[create-CacheManager]] It creates a CacheManager. [[create-MetricsSystem]] It creates a MetricsSystem for a driver and a worker separately. It initializes userFiles temporary directory used for downloading dependencies for a driver while this is the executor's current working directory for an executor. [[create-OutputCommitCoordinator]] An OutputCommitCoordinator is created.","title":" Creating \"Base\" SparkEnv (for Driver and Executors)"},{"location":"SparkEnv/#usage","text":"create is used when SparkEnv utility is used to create a SparkEnv for the driver and executors . == [[get]] Accessing SparkEnv","title":"Usage"},{"location":"SparkEnv/#source-scala","text":"","title":"[source, scala]"},{"location":"SparkEnv/#get-sparkenv","text":"get returns the SparkEnv on the driver and executors.","title":"get: SparkEnv"},{"location":"SparkEnv/#source-scala_1","text":"import org.apache.spark.SparkEnv assert(SparkEnv.get.isInstanceOf[SparkEnv]) == [[registerOrLookupEndpoint]] Registering or Looking up RPC Endpoint by Name","title":"[source, scala]"},{"location":"SparkEnv/#source-scala_2","text":"registerOrLookupEndpoint( name: String, endpointCreator: => RpcEndpoint) registerOrLookupEndpoint registers or looks up a RPC endpoint by name . If called from the driver, you should see the following INFO message in the logs: Registering [name] And the RPC endpoint is registered in the RPC environment. Otherwise, it obtains a RPC endpoint reference by name . == [[stop]] Stopping SparkEnv","title":"[source, scala]"},{"location":"SparkEnv/#source-scala_3","text":"","title":"[source, scala]"},{"location":"SparkEnv/#stop-unit","text":"stop checks < > internal flag and does nothing when enabled already. Otherwise, stop turns isStopped flag on, stops all pythonWorkers and requests the following services to stop: xref:scheduler:MapOutputTracker.md#stop[MapOutputTracker] xref:shuffle:ShuffleManager.md#stop[ShuffleManager] xref:core:BroadcastManager.md#stop[BroadcastManager] xref:storage:BlockManager.md#stop[BlockManager] xref:storage:BlockManagerMaster.md#stop[BlockManagerMaster] link:spark-metrics-MetricsSystem.md#stop[MetricsSystem] xref:scheduler:OutputCommitCoordinator.md#stop[OutputCommitCoordinator] stop xref:rpc:index.md#shutdown[requests RpcEnv to shut down] and xref:rpc:index.md#awaitTermination[waits till it terminates]. Only on the driver, stop deletes the < >. You can see the following WARN message in the logs if the deletion fails. Exception while deleting Spark temp dir: [path] NOTE: stop is used when xref:ROOT:SparkContext.md#stop[ SparkContext stops] (on the driver) and xref:executor:Executor.md#stop[ Executor stops]. == [[set]] set Method","title":"stop(): Unit"},{"location":"SparkEnv/#source-scala_4","text":"","title":"[source, scala]"},{"location":"SparkEnv/#sete-sparkenv-unit","text":"set saves the input SparkEnv to < > internal registry (as the default SparkEnv). NOTE: set is used when...FIXME == [[environmentDetails]] environmentDetails Utility","title":"set(e: SparkEnv): Unit"},{"location":"SparkEnv/#source-scala_5","text":"environmentDetails( conf: SparkConf, schedulingMode: String, addedJars: Seq[String], addedFiles: Seq[String]): Map[String, Seq[(String, String)]] environmentDetails...FIXME environmentDetails is used when SparkContext is requested to xref:ROOT:SparkContext.md#postEnvironmentUpdate[post a SparkListenerEnvironmentUpdate event]. == [[logging]] Logging Enable ALL logging level for org.apache.spark.SparkEnv logger to see what happens inside. Add the following line to conf/log4j.properties :","title":"[source, scala]"},{"location":"SparkEnv/#source","text":"","title":"[source]"},{"location":"SparkEnv/#log4jloggerorgapachesparksparkenvall","text":"Refer to xref:ROOT:spark-logging.md[Logging]. == [[internal-properties]] Internal Properties [cols=\"30m,70\",options=\"header\",width=\"100%\"] |=== | Name | Description | isStopped | [[isStopped]] Used to mark SparkEnv stopped Default: false | driverTmpDir | [[driverTmpDir]] |===","title":"log4j.logger.org.apache.spark.SparkEnv=ALL"},{"location":"overview/","text":"Apache Spark \u00b6 Apache Spark is an open-source distributed general-purpose cluster computing framework with (mostly) in-memory data processing engine that can do ETL, analytics, machine learning and graph processing on large volumes of data at rest (batch processing) or in motion (streaming processing) with rich concise high-level APIs for the programming languages: Scala, Python, Java, R, and SQL. You could also describe Spark as a distributed, data processing engine for batch and streaming modes featuring SQL queries, graph processing, and machine learning. In contrast to Hadoop\u2019s two-stage disk-based MapReduce computation engine, Spark's multi-stage (mostly) in-memory computing engine allows for running most computations in memory, and hence most of the time provides better performance for certain applications, e.g. iterative algorithms or interactive data mining (read Spark officially sets a new record in large-scale sorting ). Spark aims at speed, ease of use, extensibility and interactive analytics. Spark is a distributed platform for executing complex multi-stage applications , like machine learning algorithms , and interactive ad hoc queries . Spark provides an efficient abstraction for in-memory cluster computing called Resilient Distributed Dataset . Using Spark Application Frameworks, Spark simplifies access to machine learning and predictive analytics at scale. Spark is mainly written in http://scala-lang.org/[Scala ], but provides developer API for languages like Java, Python, and R. If you have large amounts of data that requires low latency processing that a typical MapReduce program cannot provide, Spark is a viable alternative. Access any data type across any data source. Huge demand for storage and data processing. The Apache Spark project is an umbrella for https://jaceklaskowski.gitbooks.io/mastering-spark-sql/[SQL ] (with Datasets), https://jaceklaskowski.gitbooks.io/spark-structured-streaming/[streaming ], http://spark.apache.org/mllib/[machine learning] (pipelines) and http://spark.apache.org/graphx/[graph ] processing engines built on top of the Spark Core. You can run them all in a single application using a consistent API. Spark runs locally as well as in clusters, on-premises or in cloud. It runs on top of Hadoop YARN, Apache Mesos, standalone or in the cloud (Amazon EC2 or IBM Bluemix). Apache Spark's https://jaceklaskowski.gitbooks.io/spark-structured-streaming/[Structured Streaming] and https://jaceklaskowski.gitbooks.io/mastering-spark-sql/[SQL ] programming models with MLlib and GraphX make it easier for developers and data scientists to build applications that exploit machine learning and graph analytics. At a high level, any Spark application creates RDDs out of some input, run xref:rdd:index.adoc[(lazy) transformations] of these RDDs to some other form (shape), and finally perform xref:rdd:index.adoc[actions] to collect or store data. Not much, huh? You can look at Spark from programmer's, data engineer's and administrator's point of view. And to be honest, all three types of people will spend quite a lot of their time with Spark to finally reach the point where they exploit all the available features. Programmers use language-specific APIs (and work at the level of RDDs using transformations and actions), data engineers use higher-level abstractions like DataFrames or Pipelines APIs or external tools (that connect to Spark), and finally it all can only be possible to run because administrators set up Spark clusters to deploy Spark applications to. It is Spark's goal to be a general-purpose computing platform with various specialized applications frameworks on top of a single unified engine. NOTE: When you hear \"Apache Spark\" it can be two things -- the Spark engine aka Spark Core or the Apache Spark open source project which is an \"umbrella\" term for Spark Core and the accompanying Spark Application Frameworks, i.e. Spark SQL, link:spark-streaming/spark-streaming.adoc[Spark Streaming], link:spark-mllib/spark-mllib.adoc[Spark MLlib] and link:spark-graphx.adoc[Spark GraphX] that sit on top of Spark Core and the main data abstraction in Spark called xref:rdd:index.adoc[RDD - Resilient Distributed Dataset]. == [[why-spark]] Why Spark Let's list a few of the many reasons for Spark. We are doing it first, and then comes the overview that lends a more technical helping hand. === Easy to Get Started Spark offers link:spark-shell.adoc[spark-shell] that makes for a very easy head start to writing and running Spark applications on the command line on your laptop. You could then use link:spark-standalone.adoc[Spark Standalone] built-in cluster manager to deploy your Spark applications to a production-grade cluster to run on a full dataset. === Unified Engine for Diverse Workloads As said by Matei Zaharia - the author of Apache Spark - in https://youtu.be/49Hr5xZyTEA[Introduction to AmpLab Spark Internals video] (quoting with few changes): One of the Spark project goals was to deliver a platform that supports a very wide array of diverse workflows - not only MapReduce batch jobs (there were available in Hadoop already at that time), but also iterative computations like graph algorithms or Machine Learning. And also different scales of workloads from sub-second interactive jobs to jobs that run for many hours. Spark combines batch, interactive, and streaming workloads under one rich concise API. Spark supports near real-time streaming workloads via link:spark-streaming/spark-streaming.adoc[Spark Streaming] application framework. ETL workloads and Analytics workloads are different, however Spark attempts to offer a unified platform for a wide variety of workloads. Graph and Machine Learning algorithms are iterative by nature and less saves to disk or transfers over network means better performance. There is also support for interactive workloads using Spark shell. You should watch the video https://youtu.be/SxAxAhn-BDU[What is Apache Spark?] by Mike Olson, Chief Strategy Officer and Co-Founder at Cloudera, who provides a very exceptional overview of Apache Spark, its rise in popularity in the open source community, and how Spark is primed to replace MapReduce as the general processing engine in Hadoop. === Leverages the Best in distributed batch data processing When you think about distributed batch data processing , link:varia/spark-hadoop.adoc[Hadoop] naturally comes to mind as a viable solution. Spark draws many ideas out of Hadoop MapReduce. They work together well - Spark on YARN and HDFS - while improving on the performance and simplicity of the distributed computing engine. For many, Spark is Hadoop++, i.e. MapReduce done in a better way. And it should not come as a surprise, without Hadoop MapReduce (its advances and deficiencies), Spark would not have been born at all. === RDD - Distributed Parallel Scala Collections As a Scala developer, you may find Spark's RDD API very similar (if not identical) to http://www.scala-lang.org/docu/files/collections-api/collections.html[Scala's Collections API]. It is also exposed in Java, Python and R (as well as SQL, i.e. SparkSQL, in a sense). So, when you have a need for distributed Collections API in Scala, Spark with RDD API should be a serious contender. === [[rich-standard-library]] Rich Standard Library Not only can you use map and reduce (as in Hadoop MapReduce jobs) in Spark, but also a vast array of other higher-level operators to ease your Spark queries and application development. It expanded on the available computation styles beyond the only map-and-reduce available in Hadoop MapReduce. === Unified development and deployment environment for all Regardless of the Spark tools you use - the Spark API for the many programming languages supported - Scala, Java, Python, R, or link:spark-shell.adoc[the Spark shell], or the many Spark Application Frameworks leveraging the concept of xref:rdd:index.adoc[RDD], i.e. Spark SQL, link:spark-streaming/spark-streaming.adoc[Spark Streaming], link:spark-mllib/spark-mllib.adoc[Spark MLlib] and link:spark-graphx.adoc[Spark GraphX], you still use the same development and deployment environment to for large data sets to yield a result, be it a prediction (link:spark-mllib/spark-mllib.adoc[Spark MLlib]), a structured data queries (Spark SQL) or just a large distributed batch (Spark Core) or streaming (Spark Streaming) computation. It's also very productive of Spark that teams can exploit the different skills the team members have acquired so far. Data analysts, data scientists, Python programmers, or Java, or Scala, or R, can all use the same Spark platform using tailor-made API. It makes for bringing skilled people with their expertise in different programming languages together to a Spark project. === Interactive Exploration / Exploratory Analytics It is also called ad hoc queries . Using link:spark-shell.adoc[the Spark shell] you can execute computations to process large amount of data ( The Big Data ). It's all interactive and very useful to explore the data before final production release. Also, using the Spark shell you can access any link:spark-cluster.adoc[Spark cluster] as if it was your local machine. Just point the Spark shell to a 20-node of 10TB RAM memory in total (using --master ) and use all the components (and their abstractions) like Spark SQL, Spark MLlib, link:spark-streaming/spark-streaming.adoc[Spark Streaming], and Spark GraphX. Depending on your needs and skills, you may see a better fit for SQL vs programming APIs or apply machine learning algorithms (Spark MLlib) from data in graph data structures (Spark GraphX). === Single Environment Regardless of which programming language you are good at, be it Scala, Java, Python, R or SQL, you can use the same single clustered runtime environment for prototyping, ad hoc queries, and deploying your applications leveraging the many ingestion data points offered by the Spark platform. You can be as low-level as using RDD API directly or leverage higher-level APIs of Spark SQL (Datasets), Spark MLlib (ML Pipelines), Spark GraphX (Graphs) or link:spark-streaming/spark-streaming.adoc[Spark Streaming] (DStreams). Or use them all in a single application. The single programming model and execution engine for different kinds of workloads simplify development and deployment architectures. === Data Integration Toolkit with Rich Set of Supported Data Sources Spark can read from many types of data sources -- relational, NoSQL, file systems, etc. -- using many types of data formats - Parquet, Avro, CSV, JSON. Both, input and output data sources, allow programmers and data engineers use Spark as the platform with the large amount of data that is read from or saved to for processing, interactively (using Spark shell) or in applications. === Tools unavailable then, at your fingertips now As much and often as it's recommended http://c2.com/cgi/wiki?PickTheRightToolForTheJob[to pick the right tool for the job], it's not always feasible. Time, personal preference, operating system you work on are all factors to decide what is right at a time (and using a hammer can be a reasonable choice). Spark embraces many concepts in a single unified development and runtime environment. Machine learning that is so tool- and feature-rich in Python, e.g. SciKit library, can now be used by Scala developers (as Pipeline API in Spark MLlib or calling pipe() ). DataFrames from R are available in Scala, Java, Python, R APIs. Single node computations in machine learning algorithms are migrated to their distributed versions in Spark MLlib. This single platform gives plenty of opportunities for Python, Scala, Java, and R programmers as well as data engineers (SparkR) and scientists (using proprietary enterprise data warehouses with link:spark-sql-thrift-server.adoc[Thrift JDBC/ODBC Server] in Spark SQL). Mind the proverb https://en.wiktionary.org/wiki/if_all_you_have_is_a_hammer,_everything_looks_like_a_nail[if all you have is a hammer, everything looks like a nail], too. === Low-level Optimizations Apache Spark uses a xref:scheduler:DAGScheduler.adoc[directed acyclic graph (DAG) of computation stages] (aka execution DAG ). It postpones any processing until really required for actions. Spark's lazy evaluation gives plenty of opportunities to induce low-level optimizations (so users have to know less to do more). Mind the proverb https://en.wiktionary.org/wiki/less_is_more[less is more]. === Excels at low-latency iterative workloads Spark supports diverse workloads, but successfully targets low-latency iterative ones. They are often used in Machine Learning and graph algorithms. Many Machine Learning algorithms require plenty of iterations before the result models get optimal, like logistic regression. The same applies to graph algorithms to traverse all the nodes and edges when needed. Such computations can increase their performance when the interim partial results are stored in memory or at very fast solid state drives. Spark can link:spark-rdd-caching.adoc[cache intermediate data in memory for faster model building and training]. Once the data is loaded to memory (as an initial step), reusing it multiple times incurs no performance slowdowns. Also, graph algorithms can traverse graphs one connection per iteration with the partial result in memory. Less disk access and network can make a huge difference when you need to process lots of data, esp. when it is a BIG Data. === ETL done easier Spark gives Extract, Transform and Load (ETL) a new look with the many programming languages supported - Scala, Java, Python (less likely R). You can use them all or pick the best for a problem. Scala in Spark, especially, makes for a much less boiler-plate code (comparing to other languages and approaches like MapReduce in Java). === [[unified-api]] Unified Concise High-Level API Spark offers a unified, concise, high-level APIs for batch analytics (RDD API), SQL queries (Dataset API), real-time analysis (DStream API), machine learning (ML Pipeline API) and graph processing (Graph API). Developers no longer have to learn many different processing engines and platforms, and let the time be spent on mastering framework APIs per use case (atop a single computation engine Spark). === Different kinds of data processing using unified API Spark offers three kinds of data processing using batch , interactive , and stream processing with the unified API and data structures. === Little to no disk use for better performance In the no-so-long-ago times, when the most prevalent distributed computing framework was link:varia/spark-hadoop.adoc[Hadoop MapReduce], you could reuse a data between computation (even partial ones!) only after you've written it to an external storage like link:varia/spark-hadoop.adoc[Hadoop Distributed Filesystem (HDFS)]. It can cost you a lot of time to compute even very basic multi-stage computations. It simply suffers from IO (and perhaps network) overhead. One of the many motivations to build Spark was to have a framework that is good at data reuse. Spark cuts it out in a way to keep as much data as possible in memory and keep it there until a job is finished. It doesn't matter how many stages belong to a job. What does matter is the available memory and how effective you are in using Spark API (so xref:rdd:index.adoc[no shuffle occur]). The less network and disk IO, the better performance, and Spark tries hard to find ways to minimize both. === Fault Tolerance included Faults are not considered a special case in Spark, but obvious consequence of being a parallel and distributed system. Spark handles and recovers from faults by default without particularly complex logic to deal with them. === Small Codebase Invites Contributors Spark's design is fairly simple and the code that comes out of it is not huge comparing to the features it offers. The reasonably small codebase of Spark invites project contributors - programmers who extend the platform and fix bugs in a more steady pace. == [[i-want-more]] Further reading or watching (video) https://youtu.be/L029ZNBG7bk[Keynote : Spark 2.0 - Matei Zaharia, Apache Spark Creator and CTO of Databricks]","title":"Overview"},{"location":"overview/#apache-spark","text":"Apache Spark is an open-source distributed general-purpose cluster computing framework with (mostly) in-memory data processing engine that can do ETL, analytics, machine learning and graph processing on large volumes of data at rest (batch processing) or in motion (streaming processing) with rich concise high-level APIs for the programming languages: Scala, Python, Java, R, and SQL. You could also describe Spark as a distributed, data processing engine for batch and streaming modes featuring SQL queries, graph processing, and machine learning. In contrast to Hadoop\u2019s two-stage disk-based MapReduce computation engine, Spark's multi-stage (mostly) in-memory computing engine allows for running most computations in memory, and hence most of the time provides better performance for certain applications, e.g. iterative algorithms or interactive data mining (read Spark officially sets a new record in large-scale sorting ). Spark aims at speed, ease of use, extensibility and interactive analytics. Spark is a distributed platform for executing complex multi-stage applications , like machine learning algorithms , and interactive ad hoc queries . Spark provides an efficient abstraction for in-memory cluster computing called Resilient Distributed Dataset . Using Spark Application Frameworks, Spark simplifies access to machine learning and predictive analytics at scale. Spark is mainly written in http://scala-lang.org/[Scala ], but provides developer API for languages like Java, Python, and R. If you have large amounts of data that requires low latency processing that a typical MapReduce program cannot provide, Spark is a viable alternative. Access any data type across any data source. Huge demand for storage and data processing. The Apache Spark project is an umbrella for https://jaceklaskowski.gitbooks.io/mastering-spark-sql/[SQL ] (with Datasets), https://jaceklaskowski.gitbooks.io/spark-structured-streaming/[streaming ], http://spark.apache.org/mllib/[machine learning] (pipelines) and http://spark.apache.org/graphx/[graph ] processing engines built on top of the Spark Core. You can run them all in a single application using a consistent API. Spark runs locally as well as in clusters, on-premises or in cloud. It runs on top of Hadoop YARN, Apache Mesos, standalone or in the cloud (Amazon EC2 or IBM Bluemix). Apache Spark's https://jaceklaskowski.gitbooks.io/spark-structured-streaming/[Structured Streaming] and https://jaceklaskowski.gitbooks.io/mastering-spark-sql/[SQL ] programming models with MLlib and GraphX make it easier for developers and data scientists to build applications that exploit machine learning and graph analytics. At a high level, any Spark application creates RDDs out of some input, run xref:rdd:index.adoc[(lazy) transformations] of these RDDs to some other form (shape), and finally perform xref:rdd:index.adoc[actions] to collect or store data. Not much, huh? You can look at Spark from programmer's, data engineer's and administrator's point of view. And to be honest, all three types of people will spend quite a lot of their time with Spark to finally reach the point where they exploit all the available features. Programmers use language-specific APIs (and work at the level of RDDs using transformations and actions), data engineers use higher-level abstractions like DataFrames or Pipelines APIs or external tools (that connect to Spark), and finally it all can only be possible to run because administrators set up Spark clusters to deploy Spark applications to. It is Spark's goal to be a general-purpose computing platform with various specialized applications frameworks on top of a single unified engine. NOTE: When you hear \"Apache Spark\" it can be two things -- the Spark engine aka Spark Core or the Apache Spark open source project which is an \"umbrella\" term for Spark Core and the accompanying Spark Application Frameworks, i.e. Spark SQL, link:spark-streaming/spark-streaming.adoc[Spark Streaming], link:spark-mllib/spark-mllib.adoc[Spark MLlib] and link:spark-graphx.adoc[Spark GraphX] that sit on top of Spark Core and the main data abstraction in Spark called xref:rdd:index.adoc[RDD - Resilient Distributed Dataset]. == [[why-spark]] Why Spark Let's list a few of the many reasons for Spark. We are doing it first, and then comes the overview that lends a more technical helping hand. === Easy to Get Started Spark offers link:spark-shell.adoc[spark-shell] that makes for a very easy head start to writing and running Spark applications on the command line on your laptop. You could then use link:spark-standalone.adoc[Spark Standalone] built-in cluster manager to deploy your Spark applications to a production-grade cluster to run on a full dataset. === Unified Engine for Diverse Workloads As said by Matei Zaharia - the author of Apache Spark - in https://youtu.be/49Hr5xZyTEA[Introduction to AmpLab Spark Internals video] (quoting with few changes): One of the Spark project goals was to deliver a platform that supports a very wide array of diverse workflows - not only MapReduce batch jobs (there were available in Hadoop already at that time), but also iterative computations like graph algorithms or Machine Learning. And also different scales of workloads from sub-second interactive jobs to jobs that run for many hours. Spark combines batch, interactive, and streaming workloads under one rich concise API. Spark supports near real-time streaming workloads via link:spark-streaming/spark-streaming.adoc[Spark Streaming] application framework. ETL workloads and Analytics workloads are different, however Spark attempts to offer a unified platform for a wide variety of workloads. Graph and Machine Learning algorithms are iterative by nature and less saves to disk or transfers over network means better performance. There is also support for interactive workloads using Spark shell. You should watch the video https://youtu.be/SxAxAhn-BDU[What is Apache Spark?] by Mike Olson, Chief Strategy Officer and Co-Founder at Cloudera, who provides a very exceptional overview of Apache Spark, its rise in popularity in the open source community, and how Spark is primed to replace MapReduce as the general processing engine in Hadoop. === Leverages the Best in distributed batch data processing When you think about distributed batch data processing , link:varia/spark-hadoop.adoc[Hadoop] naturally comes to mind as a viable solution. Spark draws many ideas out of Hadoop MapReduce. They work together well - Spark on YARN and HDFS - while improving on the performance and simplicity of the distributed computing engine. For many, Spark is Hadoop++, i.e. MapReduce done in a better way. And it should not come as a surprise, without Hadoop MapReduce (its advances and deficiencies), Spark would not have been born at all. === RDD - Distributed Parallel Scala Collections As a Scala developer, you may find Spark's RDD API very similar (if not identical) to http://www.scala-lang.org/docu/files/collections-api/collections.html[Scala's Collections API]. It is also exposed in Java, Python and R (as well as SQL, i.e. SparkSQL, in a sense). So, when you have a need for distributed Collections API in Scala, Spark with RDD API should be a serious contender. === [[rich-standard-library]] Rich Standard Library Not only can you use map and reduce (as in Hadoop MapReduce jobs) in Spark, but also a vast array of other higher-level operators to ease your Spark queries and application development. It expanded on the available computation styles beyond the only map-and-reduce available in Hadoop MapReduce. === Unified development and deployment environment for all Regardless of the Spark tools you use - the Spark API for the many programming languages supported - Scala, Java, Python, R, or link:spark-shell.adoc[the Spark shell], or the many Spark Application Frameworks leveraging the concept of xref:rdd:index.adoc[RDD], i.e. Spark SQL, link:spark-streaming/spark-streaming.adoc[Spark Streaming], link:spark-mllib/spark-mllib.adoc[Spark MLlib] and link:spark-graphx.adoc[Spark GraphX], you still use the same development and deployment environment to for large data sets to yield a result, be it a prediction (link:spark-mllib/spark-mllib.adoc[Spark MLlib]), a structured data queries (Spark SQL) or just a large distributed batch (Spark Core) or streaming (Spark Streaming) computation. It's also very productive of Spark that teams can exploit the different skills the team members have acquired so far. Data analysts, data scientists, Python programmers, or Java, or Scala, or R, can all use the same Spark platform using tailor-made API. It makes for bringing skilled people with their expertise in different programming languages together to a Spark project. === Interactive Exploration / Exploratory Analytics It is also called ad hoc queries . Using link:spark-shell.adoc[the Spark shell] you can execute computations to process large amount of data ( The Big Data ). It's all interactive and very useful to explore the data before final production release. Also, using the Spark shell you can access any link:spark-cluster.adoc[Spark cluster] as if it was your local machine. Just point the Spark shell to a 20-node of 10TB RAM memory in total (using --master ) and use all the components (and their abstractions) like Spark SQL, Spark MLlib, link:spark-streaming/spark-streaming.adoc[Spark Streaming], and Spark GraphX. Depending on your needs and skills, you may see a better fit for SQL vs programming APIs or apply machine learning algorithms (Spark MLlib) from data in graph data structures (Spark GraphX). === Single Environment Regardless of which programming language you are good at, be it Scala, Java, Python, R or SQL, you can use the same single clustered runtime environment for prototyping, ad hoc queries, and deploying your applications leveraging the many ingestion data points offered by the Spark platform. You can be as low-level as using RDD API directly or leverage higher-level APIs of Spark SQL (Datasets), Spark MLlib (ML Pipelines), Spark GraphX (Graphs) or link:spark-streaming/spark-streaming.adoc[Spark Streaming] (DStreams). Or use them all in a single application. The single programming model and execution engine for different kinds of workloads simplify development and deployment architectures. === Data Integration Toolkit with Rich Set of Supported Data Sources Spark can read from many types of data sources -- relational, NoSQL, file systems, etc. -- using many types of data formats - Parquet, Avro, CSV, JSON. Both, input and output data sources, allow programmers and data engineers use Spark as the platform with the large amount of data that is read from or saved to for processing, interactively (using Spark shell) or in applications. === Tools unavailable then, at your fingertips now As much and often as it's recommended http://c2.com/cgi/wiki?PickTheRightToolForTheJob[to pick the right tool for the job], it's not always feasible. Time, personal preference, operating system you work on are all factors to decide what is right at a time (and using a hammer can be a reasonable choice). Spark embraces many concepts in a single unified development and runtime environment. Machine learning that is so tool- and feature-rich in Python, e.g. SciKit library, can now be used by Scala developers (as Pipeline API in Spark MLlib or calling pipe() ). DataFrames from R are available in Scala, Java, Python, R APIs. Single node computations in machine learning algorithms are migrated to their distributed versions in Spark MLlib. This single platform gives plenty of opportunities for Python, Scala, Java, and R programmers as well as data engineers (SparkR) and scientists (using proprietary enterprise data warehouses with link:spark-sql-thrift-server.adoc[Thrift JDBC/ODBC Server] in Spark SQL). Mind the proverb https://en.wiktionary.org/wiki/if_all_you_have_is_a_hammer,_everything_looks_like_a_nail[if all you have is a hammer, everything looks like a nail], too. === Low-level Optimizations Apache Spark uses a xref:scheduler:DAGScheduler.adoc[directed acyclic graph (DAG) of computation stages] (aka execution DAG ). It postpones any processing until really required for actions. Spark's lazy evaluation gives plenty of opportunities to induce low-level optimizations (so users have to know less to do more). Mind the proverb https://en.wiktionary.org/wiki/less_is_more[less is more]. === Excels at low-latency iterative workloads Spark supports diverse workloads, but successfully targets low-latency iterative ones. They are often used in Machine Learning and graph algorithms. Many Machine Learning algorithms require plenty of iterations before the result models get optimal, like logistic regression. The same applies to graph algorithms to traverse all the nodes and edges when needed. Such computations can increase their performance when the interim partial results are stored in memory or at very fast solid state drives. Spark can link:spark-rdd-caching.adoc[cache intermediate data in memory for faster model building and training]. Once the data is loaded to memory (as an initial step), reusing it multiple times incurs no performance slowdowns. Also, graph algorithms can traverse graphs one connection per iteration with the partial result in memory. Less disk access and network can make a huge difference when you need to process lots of data, esp. when it is a BIG Data. === ETL done easier Spark gives Extract, Transform and Load (ETL) a new look with the many programming languages supported - Scala, Java, Python (less likely R). You can use them all or pick the best for a problem. Scala in Spark, especially, makes for a much less boiler-plate code (comparing to other languages and approaches like MapReduce in Java). === [[unified-api]] Unified Concise High-Level API Spark offers a unified, concise, high-level APIs for batch analytics (RDD API), SQL queries (Dataset API), real-time analysis (DStream API), machine learning (ML Pipeline API) and graph processing (Graph API). Developers no longer have to learn many different processing engines and platforms, and let the time be spent on mastering framework APIs per use case (atop a single computation engine Spark). === Different kinds of data processing using unified API Spark offers three kinds of data processing using batch , interactive , and stream processing with the unified API and data structures. === Little to no disk use for better performance In the no-so-long-ago times, when the most prevalent distributed computing framework was link:varia/spark-hadoop.adoc[Hadoop MapReduce], you could reuse a data between computation (even partial ones!) only after you've written it to an external storage like link:varia/spark-hadoop.adoc[Hadoop Distributed Filesystem (HDFS)]. It can cost you a lot of time to compute even very basic multi-stage computations. It simply suffers from IO (and perhaps network) overhead. One of the many motivations to build Spark was to have a framework that is good at data reuse. Spark cuts it out in a way to keep as much data as possible in memory and keep it there until a job is finished. It doesn't matter how many stages belong to a job. What does matter is the available memory and how effective you are in using Spark API (so xref:rdd:index.adoc[no shuffle occur]). The less network and disk IO, the better performance, and Spark tries hard to find ways to minimize both. === Fault Tolerance included Faults are not considered a special case in Spark, but obvious consequence of being a parallel and distributed system. Spark handles and recovers from faults by default without particularly complex logic to deal with them. === Small Codebase Invites Contributors Spark's design is fairly simple and the code that comes out of it is not huge comparing to the features it offers. The reasonably small codebase of Spark invites project contributors - programmers who extend the platform and fix bugs in a more steady pace. == [[i-want-more]] Further reading or watching (video) https://youtu.be/L029ZNBG7bk[Keynote : Spark 2.0 - Matei Zaharia, Apache Spark Creator and CTO of Databricks]","title":"Apache Spark"},{"location":"spark-SparkContext-creating-instance-internals/","text":"== Inside Creating SparkContext This document describes what happens when you xref:ROOT:SparkContext.adoc#creating-instance[create a new SparkContext]. [source, scala] \u00b6 import org.apache.spark.{SparkConf, SparkContext} // 1. Create Spark configuration val conf = new SparkConf() .setAppName(\"SparkMe Application\") .setMaster(\"local[*]\") // local mode // 2. Create Spark context val sc = new SparkContext(conf) NOTE: The example uses Spark in link:local/spark-local.adoc[local mode], but the initialization with link:spark-cluster.adoc[the other cluster modes] would follow similar steps. Creating SparkContext instance starts by setting the internal allowMultipleContexts field with the value of xref:ROOT:SparkContext.adoc#spark.driver.allowMultipleContexts[spark.driver.allowMultipleContexts] and marking this SparkContext instance as partially constructed. It makes sure that no other thread is creating a SparkContext instance in this JVM. It does so by synchronizing on SPARK_CONTEXT_CONSTRUCTOR_LOCK and using the internal atomic reference activeContext (that eventually has a fully-created SparkContext instance). [NOTE] \u00b6 The entire code of SparkContext that creates a fully-working SparkContext instance is between two statements: [source, scala] \u00b6 SparkContext.markPartiallyConstructed(this, allowMultipleContexts) // the SparkContext code goes here SparkContext.setActiveContext(this, allowMultipleContexts) \u00b6 ==== xref:ROOT:SparkContext.adoc#startTime[startTime] is set to the current time in milliseconds. < > internal flag is set to false . The very first information printed out is the version of Spark as an INFO message: INFO SparkContext: Running Spark version 2.0.0-SNAPSHOT TIP: You can use xref:ROOT:SparkContext.adoc#version[version] method to learn about the current Spark version or org.apache.spark.SPARK_VERSION value. A xref:scheduler:LiveListenerBus.adoc#creating-instance[LiveListenerBus instance is created] (as listenerBus ). [[sparkUser]] The xref:ROOT:SparkContext.adoc#sparkUser[current user name] is computed. CAUTION: FIXME Where is sparkUser used? It saves the input SparkConf (as _conf ). CAUTION: FIXME Review _conf.validateSettings() It ensures that the first mandatory setting - spark.master is defined. SparkException is thrown if not. A master URL must be set in your configuration It ensures that the other mandatory setting - spark.app.name is defined. SparkException is thrown if not. An application name must be set in your configuration For link:yarn/spark-yarn-cluster-yarnclusterschedulerbackend.adoc[Spark on YARN in cluster deploy mode], it checks existence of spark.yarn.app.id . SparkException is thrown if it does not exist. Detected yarn cluster mode, but isn't running on a cluster. Deployment to YARN is not supported directly by SparkContext. Please use spark-submit. CAUTION: FIXME How to \"trigger\" the exception? What are the steps? When spark.logConf is enabled xref:ROOT:SparkConf.adoc[SparkConf.toDebugString] is called. NOTE: SparkConf.toDebugString is called very early in the initialization process and other settings configured afterwards are not included. Use sc.getConf.toDebugString once SparkContext is initialized. The driver's host and port are set if missing. link:spark-driver.adoc#spark_driver_host[spark.driver.host] becomes the value of < > (or an exception is thrown) while link:spark-driver.adoc#spark_driver_port[spark.driver.port] is set to 0 . NOTE: link:spark-driver.adoc#spark_driver_host[spark.driver.host] and link:spark-driver.adoc#spark_driver_port[spark.driver.port] are expected to be set on the driver. It is later asserted by xref:core:SparkEnv.adoc#createDriverEnv[SparkEnv]. xref:executor:Executor.adoc#spark.executor.id[spark.executor.id] setting is set to driver . TIP: Use sc.getConf.get(\"spark.executor.id\") to know where the code is executed -- xref:core:SparkEnv.adoc[driver or executors]. It sets the jars and files based on spark.jars and spark.files , respectively. These are files that are required for proper task execution on executors. If xref:spark-history-server:EventLoggingListener.adoc[event logging] is enabled, i.e. link:EventLoggingListener.adoc#spark_eventLog_enabled[spark.eventLog.enabled] flag is true , the internal field _eventLogDir is set to the value of link:EventLoggingListener.adoc#spark_eventLog_dir[spark.eventLog.dir] setting or the default value /tmp/spark-events . [[_eventLogCodec]] Also, if xref:spark-history-server:EventLoggingListener.adoc#spark_eventLog_compress[spark.eventLog.compress] is enabled (it is not by default), the short name of the xref CompressionCodec.adoc[CompressionCodec] is assigned to _eventLogCodec . The config key is xref:core:BroadcastManager.adoc#spark_io_compression_codec[spark.io.compression.codec] (default: lz4 ). TIP: Read about compression codecs in xref:core:BroadcastManager.adoc#compression[Compression]. === [[_listenerBus]] Creating LiveListenerBus SparkContext creates a xref:scheduler:LiveListenerBus.adoc#creating-instance[LiveListenerBus]. === [[_statusStore]] Creating Live AppStatusStore SparkContext requests AppStatusStore to create a xref:core:AppStatusStore.adoc#createLiveStore[live store] (i.e. the AppStatusStore for a live Spark application) and requests < > to add the xref:core:AppStatusStore.adoc#listener[AppStatusListener] to the xref:scheduler:LiveListenerBus.adoc#addToStatusQueue[status queue]. NOTE: The current AppStatusStore is available as xref:ROOT:SparkContext.adoc#statusStore[statusStore] property of the SparkContext . === [[_env]] Creating SparkEnv SparkContext creates a < > and requests SparkEnv to xref:core:SparkEnv.adoc#set[use the instance as the default SparkEnv]. CAUTION: FIXME Describe the following steps. MetadataCleaner is created. CAUTION: FIXME What's MetadataCleaner? === [[_statusTracker]] Creating SparkStatusTracker SparkContext creates a link:spark-sparkcontext-SparkStatusTracker.adoc#creating-instance[SparkStatusTracker] (with itself and the <<_statusStore, AppStatusStore>>). === [[_progressBar]] Creating ConsoleProgressBar SparkContext creates the optional link:spark-sparkcontext-ConsoleProgressBar.adoc#creating-instance[ConsoleProgressBar] when link:spark-webui-properties.adoc#spark.ui.showConsoleProgress[spark.ui.showConsoleProgress] property is enabled and the INFO logging level for SparkContext is disabled. === [[_ui]][[ui]] Creating SparkUI SparkContext creates a link:spark-webui-SparkUI.adoc#create[SparkUI] when link:spark-webui-properties.adoc#spark.ui.enabled[spark.ui.enabled] configuration property is enabled (i.e. true ) with the following: <<_statusStore, AppStatusStore>> Name of the Spark application that is exactly the value of xref:ROOT:SparkConf.adoc#spark.app.name[spark.app.name] configuration property Empty base path NOTE: link:spark-webui-properties.adoc#spark.ui.enabled[spark.ui.enabled] Spark property is assumed enabled when undefined. CAUTION: FIXME Where's _ui used? A Hadoop configuration is created. See xref:ROOT:SparkContext.adoc#hadoopConfiguration[Hadoop Configuration]. [[jars]] If there are jars given through the SparkContext constructor, they are added using addJar . [[files]] If there were files specified, they are added using xref:ROOT:SparkContext.adoc#addFile[addFile]. At this point in time, the amount of memory to allocate to each executor (as _executorMemory ) is calculated. It is the value of xref:executor:Executor.adoc#spark.executor.memory[spark.executor.memory] setting, or xref:ROOT:SparkContext.adoc#environment-variables[SPARK_EXECUTOR_MEMORY] environment variable (or currently-deprecated SPARK_MEM ), or defaults to 1024 . _executorMemory is later available as sc.executorMemory and used for LOCAL_CLUSTER_REGEX, link:spark-standalone.adoc#SparkDeploySchedulerBackend[Spark Standalone's SparkDeploySchedulerBackend], to set executorEnvs(\"SPARK_EXECUTOR_MEMORY\") , MesosSchedulerBackend, CoarseMesosSchedulerBackend. The value of SPARK_PREPEND_CLASSES environment variable is included in executorEnvs . [CAUTION] \u00b6 FIXME What's _executorMemory ? What's the unit of the value of _executorMemory exactly? What are \"SPARK_TESTING\", \"spark.testing\"? How do they contribute to executorEnvs ? What's executorEnvs ? \u00b6 The Mesos scheduler backend's configuration is included in executorEnvs , i.e. xref:ROOT:SparkContext.adoc#environment-variables[SPARK_EXECUTOR_MEMORY], _conf.getExecutorEnv , and SPARK_USER . [[_heartbeatReceiver]] SparkContext registers link:spark-HeartbeatReceiver.adoc[HeartbeatReceiver RPC endpoint]. SparkContext object is requested to xref:ROOT:SparkContext.adoc#createTaskScheduler[create the SchedulerBackend with the TaskScheduler] (for the given master URL) and the result becomes the internal _schedulerBackend and _taskScheduler . NOTE: The internal _schedulerBackend and _taskScheduler are used by schedulerBackend and taskScheduler methods, respectively. xref:scheduler:DAGScheduler.adoc#creating-instance[DAGScheduler is created] (as _dagScheduler ). [[TaskSchedulerIsSet]] SparkContext sends a blocking link:spark-HeartbeatReceiver.adoc#TaskSchedulerIsSet[ TaskSchedulerIsSet message to HeartbeatReceiver RPC endpoint] (to inform that the TaskScheduler is now available). === [[taskScheduler-start]] Starting TaskScheduler SparkContext xref:scheduler:TaskScheduler.adoc#start[starts TaskScheduler ]. === [[_applicationId]][[_applicationAttemptId]] Setting Spark Application's and Execution Attempt's IDs -- _applicationId and _applicationAttemptId SparkContext sets the internal fields -- _applicationId and _applicationAttemptId -- (using applicationId and applicationAttemptId methods from the xref:scheduler:TaskScheduler.adoc#contract[TaskScheduler Contract]). NOTE: SparkContext requests TaskScheduler for the xref:scheduler:TaskScheduler.adoc#applicationId[unique identifier of a Spark application] (that is currently only implemented by xref:scheduler:TaskSchedulerImpl.adoc#applicationId[TaskSchedulerImpl] that uses SchedulerBackend to xref:scheduler:SchedulerBackend.adoc#applicationId[request the identifier]). NOTE: The unique identifier of a Spark application is used to initialize link:spark-webui-SparkUI.adoc#setAppId[SparkUI] and xref:storage:BlockManager.adoc#initialize[BlockManager]. NOTE: _applicationAttemptId is used when SparkContext is requested for the xref:ROOT:SparkContext.adoc#applicationAttemptId[unique identifier of execution attempt of a Spark application] and when EventLoggingListener xref:spark-history-server:EventLoggingListener.adoc#creating-instance[is created]. === [[spark.app.id]] Setting spark.app.id Spark Property in SparkConf SparkContext sets xref:ROOT:SparkConf.adoc#spark.app.id[spark.app.id] property to be the <<_applicationId, unique identifier of a Spark application>> and, if enabled, link:spark-webui-SparkUI.adoc#setAppId[passes it on to SparkUI ]. === [[BlockManager-initialization]] Initializing BlockManager The xref:storage:BlockManager.adoc#initialize[BlockManager (for the driver) is initialized] (with _applicationId ). === [[MetricsSystem-start]] Starting MetricsSystem SparkContext requests the MetricsSystem to link:spark-metrics-MetricsSystem.adoc#start[start]. NOTE: SparkContext starts MetricsSystem after < > as MetricsSystem uses it to link:spark-metrics-MetricsSystem.adoc#buildRegistryName[build unique identifiers fo metrics sources]. === [[MetricsSystem-getServletHandlers]] Requesting JSON Servlet Handler SparkContext requests the MetricsSystem for a link:spark-metrics-MetricsSystem.adoc#getServletHandlers[JSON servlet handler] and requests the <<_ui, SparkUI>> to link:spark-webui-WebUI.adoc#attachHandler[attach it]. [[_eventLogger]] _eventLogger is created and started if isEventLogEnabled . It uses xref:spark-history-server:EventLoggingListener.adoc[EventLoggingListener] that gets registered to xref:scheduler:LiveListenerBus.adoc[]. CAUTION: FIXME Why is _eventLogger required to be the internal field of SparkContext? Where is this used? [[ExecutorAllocationManager]] For xref:ROOT:spark-dynamic-allocation.adoc[], link:spark-ExecutorAllocationManager.adoc#creating-instance[ ExecutorAllocationManager is created] (as _executorAllocationManager ) and immediately link:spark-ExecutorAllocationManager.adoc#start[started]. NOTE: _executorAllocationManager is exposed (as a method) to link:yarn/spark-yarn-yarnschedulerbackend.adoc#reset[YARN scheduler backends to reset their state to the initial state]. [[_cleaner]][[ContextCleaner]] With xref:ROOT:configuration-properties.adoc#spark.cleaner.referenceTracking[spark.cleaner.referenceTracking] configuration property enabled, SparkContext xref:core:ContextCleaner.adoc#creating-instance[creates ContextCleaner ] (as _cleaner ) and xref:core:ContextCleaner.adoc#start[started] immediately. Otherwise, _cleaner is empty. CAUTION: FIXME It'd be quite useful to have all the properties with their default values in sc.getConf.toDebugString , so when a configuration is not included but does change Spark runtime configuration, it should be added to _conf . [[registering_SparkListeners]] It < SparkListenerEvent event delivery to the listeners>>. [[postEnvironmentUpdate]] postEnvironmentUpdate is called that posts xref:ROOT:SparkListener.adoc#SparkListenerEnvironmentUpdate[SparkListenerEnvironmentUpdate] message on xref:scheduler:LiveListenerBus.adoc[] with information about Task Scheduler's scheduling mode, added jar and file paths, and other environmental details. They are displayed in web UI's link:spark-webui-environment.adoc[Environment tab]. [[postApplicationStart]] xref:ROOT:SparkListener.adoc#SparkListenerApplicationStart[SparkListenerApplicationStart] message is posted to xref:scheduler:LiveListenerBus.adoc[] (using the internal postApplicationStart method). [[postStartHook]] TaskScheduler xref:scheduler:TaskScheduler.adoc#postStartHook[is notified that SparkContext is almost fully initialized]. NOTE: xref:scheduler:TaskScheduler.adoc#postStartHook[TaskScheduler.postStartHook] does nothing by default, but custom implementations offer more advanced features, i.e. TaskSchedulerImpl xref:scheduler:TaskSchedulerImpl.adoc#postStartHook[blocks the current thread until SchedulerBackend is ready]. There is also YarnClusterScheduler for Spark on YARN in cluster deploy mode. === [[registerSource]] Registering Metrics Sources SparkContext requests MetricsSystem to link:spark-metrics-MetricsSystem.adoc#registerSource[register metrics sources] for the following services: . xref:scheduler:DAGScheduler.adoc#metricsSource[DAGScheduler] . link:spark-BlockManager-BlockManagerSource.adoc[BlockManager] . link:spark-ExecutorAllocationManager.adoc#executorAllocationManagerSource[ExecutorAllocationManager] (for xref:ROOT:spark-dynamic-allocation.adoc[]) === [[addShutdownHook]] Adding Shutdown Hook SparkContext adds a shutdown hook (using ShutdownHookManager.addShutdownHook() ). You should see the following DEBUG message in the logs: DEBUG Adding shutdown hook CAUTION: FIXME ShutdownHookManager.addShutdownHook() Any non-fatal Exception leads to termination of the Spark context instance. CAUTION: FIXME What does NonFatal represent in Scala? CAUTION: FIXME Finish me === [[nextShuffleId]][[nextRddId]] Initializing nextShuffleId and nextRddId Internal Counters nextShuffleId and nextRddId start with 0 . CAUTION: FIXME Where are nextShuffleId and nextRddId used? A new instance of Spark context is created and ready for operation. === [[getClusterManager]] Loading External Cluster Manager for URL (getClusterManager method) [source, scala] \u00b6 getClusterManager(url: String): Option[ExternalClusterManager] \u00b6 getClusterManager loads xref:scheduler:ExternalClusterManager.adoc[] that xref:scheduler:ExternalClusterManager.adoc#canCreate[can handle the input url ]. If there are two or more external cluster managers that could handle url , a SparkException is thrown: Multiple Cluster Managers ([serviceLoaders]) registered for the url [url]. NOTE: getClusterManager uses Java's link:++ https://docs.oracle.com/javase/8/docs/api/java/util/ServiceLoader.html#load-java.lang.Class-java.lang.ClassLoader-++[ServiceLoader.load ] method. NOTE: getClusterManager is used to find a cluster manager for a master URL when xref:ROOT:SparkContext.adoc#createTaskScheduler[creating a SchedulerBackend and a TaskScheduler for the driver]. === [[setupAndStartListenerBus]] setupAndStartListenerBus [source, scala] \u00b6 setupAndStartListenerBus(): Unit \u00b6 setupAndStartListenerBus is an internal method that reads xref:ROOT:configuration-properties.adoc#spark.extraListeners[spark.extraListeners] configuration property from the current xref:ROOT:SparkConf.adoc[SparkConf] to create and register xref:ROOT:SparkListener.adoc#SparkListenerInterface[SparkListenerInterface] listeners. It expects that the class name represents a SparkListenerInterface listener with one of the following constructors (in this order): a single-argument constructor that accepts xref:ROOT:SparkConf.adoc[SparkConf] a zero-argument constructor setupAndStartListenerBus xref:scheduler:LiveListenerBus.adoc#ListenerBus-addListener[registers every listener class]. You should see the following INFO message in the logs: INFO Registered listener [className] It xref:scheduler:LiveListenerBus.adoc#start[starts LiveListenerBus] and records it in the internal _listenerBusStarted . When no single- SparkConf or zero-argument constructor could be found for a class name in xref:ROOT:configuration-properties.adoc#spark.extraListeners[spark.extraListeners] configuration property, a SparkException is thrown with the message: [className] did not have a zero-argument constructor or a single-argument constructor that accepts SparkConf. Note: if the class is defined inside of another Scala class, then its constructors may accept an implicit parameter that references the enclosing class; in this case, you must define the listener as a top-level class in order to prevent this extra parameter from breaking Spark's ability to find a valid constructor. Any exception while registering a xref:ROOT:SparkListener.adoc#SparkListenerInterface[SparkListenerInterface] listener xref:ROOT:SparkContext.adoc#stop[stops the SparkContext] and a SparkException is thrown and the source exception's message. Exception when registering SparkListener [TIP] \u00b6 Set INFO on org.apache.spark.SparkContext logger to see the extra listeners being registered. INFO SparkContext: Registered listener pl.japila.spark.CustomSparkListener \u00b6 === [[createSparkEnv]] Creating SparkEnv for Driver -- createSparkEnv Method [source, scala] \u00b6 createSparkEnv( conf: SparkConf, isLocal: Boolean, listenerBus: LiveListenerBus): SparkEnv createSparkEnv simply delegates the call to xref:core:SparkEnv.adoc#createDriverEnv[SparkEnv to create a SparkEnv for the driver]. It calculates the number of cores to 1 for local master URL, the number of processors available for JVM for * or the exact number in the master URL, or 0 for the cluster master URLs. === [[getCurrentUserName]] Utils.getCurrentUserName Method [source, scala] \u00b6 getCurrentUserName(): String \u00b6 getCurrentUserName computes the user name who has started the xref:ROOT:SparkContext.adoc[SparkContext] instance. NOTE: It is later available as xref:ROOT:SparkContext.adoc#sparkUser[SparkContext.sparkUser]. Internally, it reads xref:ROOT:SparkContext.adoc#SPARK_USER[SPARK_USER] environment variable and, if not set, reverts to Hadoop Security API's UserGroupInformation.getCurrentUser().getShortUserName() . NOTE: It is another place where Spark relies on Hadoop API for its operation. === [[localHostName]] Utils.localHostName Method localHostName computes the local host name. It starts by checking SPARK_LOCAL_HOSTNAME environment variable for the value. If it is not defined, it uses SPARK_LOCAL_IP to find the name (using InetAddress.getByName ). If it is not defined either, it calls InetAddress.getLocalHost for the name. NOTE: Utils.localHostName is executed while xref:ROOT:SparkContext.adoc#creating-instance[ SparkContext is created] and also to compute the default value of link:spark-driver.adoc#spark_driver_host[spark.driver.host Spark property]. CAUTION: FIXME Review the rest. === [[stopped]] stopped Flag CAUTION: FIXME Where is this used?","title":"Creating SparkContext"},{"location":"spark-SparkContext-creating-instance-internals/#source-scala","text":"import org.apache.spark.{SparkConf, SparkContext} // 1. Create Spark configuration val conf = new SparkConf() .setAppName(\"SparkMe Application\") .setMaster(\"local[*]\") // local mode // 2. Create Spark context val sc = new SparkContext(conf) NOTE: The example uses Spark in link:local/spark-local.adoc[local mode], but the initialization with link:spark-cluster.adoc[the other cluster modes] would follow similar steps. Creating SparkContext instance starts by setting the internal allowMultipleContexts field with the value of xref:ROOT:SparkContext.adoc#spark.driver.allowMultipleContexts[spark.driver.allowMultipleContexts] and marking this SparkContext instance as partially constructed. It makes sure that no other thread is creating a SparkContext instance in this JVM. It does so by synchronizing on SPARK_CONTEXT_CONSTRUCTOR_LOCK and using the internal atomic reference activeContext (that eventually has a fully-created SparkContext instance).","title":"[source, scala]"},{"location":"spark-SparkContext-creating-instance-internals/#note","text":"The entire code of SparkContext that creates a fully-working SparkContext instance is between two statements:","title":"[NOTE]"},{"location":"spark-SparkContext-creating-instance-internals/#source-scala_1","text":"SparkContext.markPartiallyConstructed(this, allowMultipleContexts) // the SparkContext code goes here","title":"[source, scala]"},{"location":"spark-SparkContext-creating-instance-internals/#sparkcontextsetactivecontextthis-allowmultiplecontexts","text":"==== xref:ROOT:SparkContext.adoc#startTime[startTime] is set to the current time in milliseconds. < > internal flag is set to false . The very first information printed out is the version of Spark as an INFO message: INFO SparkContext: Running Spark version 2.0.0-SNAPSHOT TIP: You can use xref:ROOT:SparkContext.adoc#version[version] method to learn about the current Spark version or org.apache.spark.SPARK_VERSION value. A xref:scheduler:LiveListenerBus.adoc#creating-instance[LiveListenerBus instance is created] (as listenerBus ). [[sparkUser]] The xref:ROOT:SparkContext.adoc#sparkUser[current user name] is computed. CAUTION: FIXME Where is sparkUser used? It saves the input SparkConf (as _conf ). CAUTION: FIXME Review _conf.validateSettings() It ensures that the first mandatory setting - spark.master is defined. SparkException is thrown if not. A master URL must be set in your configuration It ensures that the other mandatory setting - spark.app.name is defined. SparkException is thrown if not. An application name must be set in your configuration For link:yarn/spark-yarn-cluster-yarnclusterschedulerbackend.adoc[Spark on YARN in cluster deploy mode], it checks existence of spark.yarn.app.id . SparkException is thrown if it does not exist. Detected yarn cluster mode, but isn't running on a cluster. Deployment to YARN is not supported directly by SparkContext. Please use spark-submit. CAUTION: FIXME How to \"trigger\" the exception? What are the steps? When spark.logConf is enabled xref:ROOT:SparkConf.adoc[SparkConf.toDebugString] is called. NOTE: SparkConf.toDebugString is called very early in the initialization process and other settings configured afterwards are not included. Use sc.getConf.toDebugString once SparkContext is initialized. The driver's host and port are set if missing. link:spark-driver.adoc#spark_driver_host[spark.driver.host] becomes the value of < > (or an exception is thrown) while link:spark-driver.adoc#spark_driver_port[spark.driver.port] is set to 0 . NOTE: link:spark-driver.adoc#spark_driver_host[spark.driver.host] and link:spark-driver.adoc#spark_driver_port[spark.driver.port] are expected to be set on the driver. It is later asserted by xref:core:SparkEnv.adoc#createDriverEnv[SparkEnv]. xref:executor:Executor.adoc#spark.executor.id[spark.executor.id] setting is set to driver . TIP: Use sc.getConf.get(\"spark.executor.id\") to know where the code is executed -- xref:core:SparkEnv.adoc[driver or executors]. It sets the jars and files based on spark.jars and spark.files , respectively. These are files that are required for proper task execution on executors. If xref:spark-history-server:EventLoggingListener.adoc[event logging] is enabled, i.e. link:EventLoggingListener.adoc#spark_eventLog_enabled[spark.eventLog.enabled] flag is true , the internal field _eventLogDir is set to the value of link:EventLoggingListener.adoc#spark_eventLog_dir[spark.eventLog.dir] setting or the default value /tmp/spark-events . [[_eventLogCodec]] Also, if xref:spark-history-server:EventLoggingListener.adoc#spark_eventLog_compress[spark.eventLog.compress] is enabled (it is not by default), the short name of the xref CompressionCodec.adoc[CompressionCodec] is assigned to _eventLogCodec . The config key is xref:core:BroadcastManager.adoc#spark_io_compression_codec[spark.io.compression.codec] (default: lz4 ). TIP: Read about compression codecs in xref:core:BroadcastManager.adoc#compression[Compression]. === [[_listenerBus]] Creating LiveListenerBus SparkContext creates a xref:scheduler:LiveListenerBus.adoc#creating-instance[LiveListenerBus]. === [[_statusStore]] Creating Live AppStatusStore SparkContext requests AppStatusStore to create a xref:core:AppStatusStore.adoc#createLiveStore[live store] (i.e. the AppStatusStore for a live Spark application) and requests < > to add the xref:core:AppStatusStore.adoc#listener[AppStatusListener] to the xref:scheduler:LiveListenerBus.adoc#addToStatusQueue[status queue]. NOTE: The current AppStatusStore is available as xref:ROOT:SparkContext.adoc#statusStore[statusStore] property of the SparkContext . === [[_env]] Creating SparkEnv SparkContext creates a < > and requests SparkEnv to xref:core:SparkEnv.adoc#set[use the instance as the default SparkEnv]. CAUTION: FIXME Describe the following steps. MetadataCleaner is created. CAUTION: FIXME What's MetadataCleaner? === [[_statusTracker]] Creating SparkStatusTracker SparkContext creates a link:spark-sparkcontext-SparkStatusTracker.adoc#creating-instance[SparkStatusTracker] (with itself and the <<_statusStore, AppStatusStore>>). === [[_progressBar]] Creating ConsoleProgressBar SparkContext creates the optional link:spark-sparkcontext-ConsoleProgressBar.adoc#creating-instance[ConsoleProgressBar] when link:spark-webui-properties.adoc#spark.ui.showConsoleProgress[spark.ui.showConsoleProgress] property is enabled and the INFO logging level for SparkContext is disabled. === [[_ui]][[ui]] Creating SparkUI SparkContext creates a link:spark-webui-SparkUI.adoc#create[SparkUI] when link:spark-webui-properties.adoc#spark.ui.enabled[spark.ui.enabled] configuration property is enabled (i.e. true ) with the following: <<_statusStore, AppStatusStore>> Name of the Spark application that is exactly the value of xref:ROOT:SparkConf.adoc#spark.app.name[spark.app.name] configuration property Empty base path NOTE: link:spark-webui-properties.adoc#spark.ui.enabled[spark.ui.enabled] Spark property is assumed enabled when undefined. CAUTION: FIXME Where's _ui used? A Hadoop configuration is created. See xref:ROOT:SparkContext.adoc#hadoopConfiguration[Hadoop Configuration]. [[jars]] If there are jars given through the SparkContext constructor, they are added using addJar . [[files]] If there were files specified, they are added using xref:ROOT:SparkContext.adoc#addFile[addFile]. At this point in time, the amount of memory to allocate to each executor (as _executorMemory ) is calculated. It is the value of xref:executor:Executor.adoc#spark.executor.memory[spark.executor.memory] setting, or xref:ROOT:SparkContext.adoc#environment-variables[SPARK_EXECUTOR_MEMORY] environment variable (or currently-deprecated SPARK_MEM ), or defaults to 1024 . _executorMemory is later available as sc.executorMemory and used for LOCAL_CLUSTER_REGEX, link:spark-standalone.adoc#SparkDeploySchedulerBackend[Spark Standalone's SparkDeploySchedulerBackend], to set executorEnvs(\"SPARK_EXECUTOR_MEMORY\") , MesosSchedulerBackend, CoarseMesosSchedulerBackend. The value of SPARK_PREPEND_CLASSES environment variable is included in executorEnvs .","title":"SparkContext.setActiveContext(this, allowMultipleContexts)"},{"location":"spark-SparkContext-creating-instance-internals/#caution","text":"FIXME What's _executorMemory ? What's the unit of the value of _executorMemory exactly? What are \"SPARK_TESTING\", \"spark.testing\"? How do they contribute to executorEnvs ?","title":"[CAUTION]"},{"location":"spark-SparkContext-creating-instance-internals/#whats-executorenvs","text":"The Mesos scheduler backend's configuration is included in executorEnvs , i.e. xref:ROOT:SparkContext.adoc#environment-variables[SPARK_EXECUTOR_MEMORY], _conf.getExecutorEnv , and SPARK_USER . [[_heartbeatReceiver]] SparkContext registers link:spark-HeartbeatReceiver.adoc[HeartbeatReceiver RPC endpoint]. SparkContext object is requested to xref:ROOT:SparkContext.adoc#createTaskScheduler[create the SchedulerBackend with the TaskScheduler] (for the given master URL) and the result becomes the internal _schedulerBackend and _taskScheduler . NOTE: The internal _schedulerBackend and _taskScheduler are used by schedulerBackend and taskScheduler methods, respectively. xref:scheduler:DAGScheduler.adoc#creating-instance[DAGScheduler is created] (as _dagScheduler ). [[TaskSchedulerIsSet]] SparkContext sends a blocking link:spark-HeartbeatReceiver.adoc#TaskSchedulerIsSet[ TaskSchedulerIsSet message to HeartbeatReceiver RPC endpoint] (to inform that the TaskScheduler is now available). === [[taskScheduler-start]] Starting TaskScheduler SparkContext xref:scheduler:TaskScheduler.adoc#start[starts TaskScheduler ]. === [[_applicationId]][[_applicationAttemptId]] Setting Spark Application's and Execution Attempt's IDs -- _applicationId and _applicationAttemptId SparkContext sets the internal fields -- _applicationId and _applicationAttemptId -- (using applicationId and applicationAttemptId methods from the xref:scheduler:TaskScheduler.adoc#contract[TaskScheduler Contract]). NOTE: SparkContext requests TaskScheduler for the xref:scheduler:TaskScheduler.adoc#applicationId[unique identifier of a Spark application] (that is currently only implemented by xref:scheduler:TaskSchedulerImpl.adoc#applicationId[TaskSchedulerImpl] that uses SchedulerBackend to xref:scheduler:SchedulerBackend.adoc#applicationId[request the identifier]). NOTE: The unique identifier of a Spark application is used to initialize link:spark-webui-SparkUI.adoc#setAppId[SparkUI] and xref:storage:BlockManager.adoc#initialize[BlockManager]. NOTE: _applicationAttemptId is used when SparkContext is requested for the xref:ROOT:SparkContext.adoc#applicationAttemptId[unique identifier of execution attempt of a Spark application] and when EventLoggingListener xref:spark-history-server:EventLoggingListener.adoc#creating-instance[is created]. === [[spark.app.id]] Setting spark.app.id Spark Property in SparkConf SparkContext sets xref:ROOT:SparkConf.adoc#spark.app.id[spark.app.id] property to be the <<_applicationId, unique identifier of a Spark application>> and, if enabled, link:spark-webui-SparkUI.adoc#setAppId[passes it on to SparkUI ]. === [[BlockManager-initialization]] Initializing BlockManager The xref:storage:BlockManager.adoc#initialize[BlockManager (for the driver) is initialized] (with _applicationId ). === [[MetricsSystem-start]] Starting MetricsSystem SparkContext requests the MetricsSystem to link:spark-metrics-MetricsSystem.adoc#start[start]. NOTE: SparkContext starts MetricsSystem after < > as MetricsSystem uses it to link:spark-metrics-MetricsSystem.adoc#buildRegistryName[build unique identifiers fo metrics sources]. === [[MetricsSystem-getServletHandlers]] Requesting JSON Servlet Handler SparkContext requests the MetricsSystem for a link:spark-metrics-MetricsSystem.adoc#getServletHandlers[JSON servlet handler] and requests the <<_ui, SparkUI>> to link:spark-webui-WebUI.adoc#attachHandler[attach it]. [[_eventLogger]] _eventLogger is created and started if isEventLogEnabled . It uses xref:spark-history-server:EventLoggingListener.adoc[EventLoggingListener] that gets registered to xref:scheduler:LiveListenerBus.adoc[]. CAUTION: FIXME Why is _eventLogger required to be the internal field of SparkContext? Where is this used? [[ExecutorAllocationManager]] For xref:ROOT:spark-dynamic-allocation.adoc[], link:spark-ExecutorAllocationManager.adoc#creating-instance[ ExecutorAllocationManager is created] (as _executorAllocationManager ) and immediately link:spark-ExecutorAllocationManager.adoc#start[started]. NOTE: _executorAllocationManager is exposed (as a method) to link:yarn/spark-yarn-yarnschedulerbackend.adoc#reset[YARN scheduler backends to reset their state to the initial state]. [[_cleaner]][[ContextCleaner]] With xref:ROOT:configuration-properties.adoc#spark.cleaner.referenceTracking[spark.cleaner.referenceTracking] configuration property enabled, SparkContext xref:core:ContextCleaner.adoc#creating-instance[creates ContextCleaner ] (as _cleaner ) and xref:core:ContextCleaner.adoc#start[started] immediately. Otherwise, _cleaner is empty. CAUTION: FIXME It'd be quite useful to have all the properties with their default values in sc.getConf.toDebugString , so when a configuration is not included but does change Spark runtime configuration, it should be added to _conf . [[registering_SparkListeners]] It < SparkListenerEvent event delivery to the listeners>>. [[postEnvironmentUpdate]] postEnvironmentUpdate is called that posts xref:ROOT:SparkListener.adoc#SparkListenerEnvironmentUpdate[SparkListenerEnvironmentUpdate] message on xref:scheduler:LiveListenerBus.adoc[] with information about Task Scheduler's scheduling mode, added jar and file paths, and other environmental details. They are displayed in web UI's link:spark-webui-environment.adoc[Environment tab]. [[postApplicationStart]] xref:ROOT:SparkListener.adoc#SparkListenerApplicationStart[SparkListenerApplicationStart] message is posted to xref:scheduler:LiveListenerBus.adoc[] (using the internal postApplicationStart method). [[postStartHook]] TaskScheduler xref:scheduler:TaskScheduler.adoc#postStartHook[is notified that SparkContext is almost fully initialized]. NOTE: xref:scheduler:TaskScheduler.adoc#postStartHook[TaskScheduler.postStartHook] does nothing by default, but custom implementations offer more advanced features, i.e. TaskSchedulerImpl xref:scheduler:TaskSchedulerImpl.adoc#postStartHook[blocks the current thread until SchedulerBackend is ready]. There is also YarnClusterScheduler for Spark on YARN in cluster deploy mode. === [[registerSource]] Registering Metrics Sources SparkContext requests MetricsSystem to link:spark-metrics-MetricsSystem.adoc#registerSource[register metrics sources] for the following services: . xref:scheduler:DAGScheduler.adoc#metricsSource[DAGScheduler] . link:spark-BlockManager-BlockManagerSource.adoc[BlockManager] . link:spark-ExecutorAllocationManager.adoc#executorAllocationManagerSource[ExecutorAllocationManager] (for xref:ROOT:spark-dynamic-allocation.adoc[]) === [[addShutdownHook]] Adding Shutdown Hook SparkContext adds a shutdown hook (using ShutdownHookManager.addShutdownHook() ). You should see the following DEBUG message in the logs: DEBUG Adding shutdown hook CAUTION: FIXME ShutdownHookManager.addShutdownHook() Any non-fatal Exception leads to termination of the Spark context instance. CAUTION: FIXME What does NonFatal represent in Scala? CAUTION: FIXME Finish me === [[nextShuffleId]][[nextRddId]] Initializing nextShuffleId and nextRddId Internal Counters nextShuffleId and nextRddId start with 0 . CAUTION: FIXME Where are nextShuffleId and nextRddId used? A new instance of Spark context is created and ready for operation. === [[getClusterManager]] Loading External Cluster Manager for URL (getClusterManager method)","title":"What's executorEnvs?"},{"location":"spark-SparkContext-creating-instance-internals/#source-scala_2","text":"","title":"[source, scala]"},{"location":"spark-SparkContext-creating-instance-internals/#getclustermanagerurl-string-optionexternalclustermanager","text":"getClusterManager loads xref:scheduler:ExternalClusterManager.adoc[] that xref:scheduler:ExternalClusterManager.adoc#canCreate[can handle the input url ]. If there are two or more external cluster managers that could handle url , a SparkException is thrown: Multiple Cluster Managers ([serviceLoaders]) registered for the url [url]. NOTE: getClusterManager uses Java's link:++ https://docs.oracle.com/javase/8/docs/api/java/util/ServiceLoader.html#load-java.lang.Class-java.lang.ClassLoader-++[ServiceLoader.load ] method. NOTE: getClusterManager is used to find a cluster manager for a master URL when xref:ROOT:SparkContext.adoc#createTaskScheduler[creating a SchedulerBackend and a TaskScheduler for the driver]. === [[setupAndStartListenerBus]] setupAndStartListenerBus","title":"getClusterManager(url: String): Option[ExternalClusterManager]"},{"location":"spark-SparkContext-creating-instance-internals/#source-scala_3","text":"","title":"[source, scala]"},{"location":"spark-SparkContext-creating-instance-internals/#setupandstartlistenerbus-unit","text":"setupAndStartListenerBus is an internal method that reads xref:ROOT:configuration-properties.adoc#spark.extraListeners[spark.extraListeners] configuration property from the current xref:ROOT:SparkConf.adoc[SparkConf] to create and register xref:ROOT:SparkListener.adoc#SparkListenerInterface[SparkListenerInterface] listeners. It expects that the class name represents a SparkListenerInterface listener with one of the following constructors (in this order): a single-argument constructor that accepts xref:ROOT:SparkConf.adoc[SparkConf] a zero-argument constructor setupAndStartListenerBus xref:scheduler:LiveListenerBus.adoc#ListenerBus-addListener[registers every listener class]. You should see the following INFO message in the logs: INFO Registered listener [className] It xref:scheduler:LiveListenerBus.adoc#start[starts LiveListenerBus] and records it in the internal _listenerBusStarted . When no single- SparkConf or zero-argument constructor could be found for a class name in xref:ROOT:configuration-properties.adoc#spark.extraListeners[spark.extraListeners] configuration property, a SparkException is thrown with the message: [className] did not have a zero-argument constructor or a single-argument constructor that accepts SparkConf. Note: if the class is defined inside of another Scala class, then its constructors may accept an implicit parameter that references the enclosing class; in this case, you must define the listener as a top-level class in order to prevent this extra parameter from breaking Spark's ability to find a valid constructor. Any exception while registering a xref:ROOT:SparkListener.adoc#SparkListenerInterface[SparkListenerInterface] listener xref:ROOT:SparkContext.adoc#stop[stops the SparkContext] and a SparkException is thrown and the source exception's message. Exception when registering SparkListener","title":"setupAndStartListenerBus(): Unit"},{"location":"spark-SparkContext-creating-instance-internals/#tip","text":"Set INFO on org.apache.spark.SparkContext logger to see the extra listeners being registered.","title":"[TIP]"},{"location":"spark-SparkContext-creating-instance-internals/#info-sparkcontext-registered-listener-pljapilasparkcustomsparklistener","text":"=== [[createSparkEnv]] Creating SparkEnv for Driver -- createSparkEnv Method","title":"INFO SparkContext: Registered listener pl.japila.spark.CustomSparkListener\n"},{"location":"spark-SparkContext-creating-instance-internals/#source-scala_4","text":"createSparkEnv( conf: SparkConf, isLocal: Boolean, listenerBus: LiveListenerBus): SparkEnv createSparkEnv simply delegates the call to xref:core:SparkEnv.adoc#createDriverEnv[SparkEnv to create a SparkEnv for the driver]. It calculates the number of cores to 1 for local master URL, the number of processors available for JVM for * or the exact number in the master URL, or 0 for the cluster master URLs. === [[getCurrentUserName]] Utils.getCurrentUserName Method","title":"[source, scala]"},{"location":"spark-SparkContext-creating-instance-internals/#source-scala_5","text":"","title":"[source, scala]"},{"location":"spark-SparkContext-creating-instance-internals/#getcurrentusername-string","text":"getCurrentUserName computes the user name who has started the xref:ROOT:SparkContext.adoc[SparkContext] instance. NOTE: It is later available as xref:ROOT:SparkContext.adoc#sparkUser[SparkContext.sparkUser]. Internally, it reads xref:ROOT:SparkContext.adoc#SPARK_USER[SPARK_USER] environment variable and, if not set, reverts to Hadoop Security API's UserGroupInformation.getCurrentUser().getShortUserName() . NOTE: It is another place where Spark relies on Hadoop API for its operation. === [[localHostName]] Utils.localHostName Method localHostName computes the local host name. It starts by checking SPARK_LOCAL_HOSTNAME environment variable for the value. If it is not defined, it uses SPARK_LOCAL_IP to find the name (using InetAddress.getByName ). If it is not defined either, it calls InetAddress.getLocalHost for the name. NOTE: Utils.localHostName is executed while xref:ROOT:SparkContext.adoc#creating-instance[ SparkContext is created] and also to compute the default value of link:spark-driver.adoc#spark_driver_host[spark.driver.host Spark property]. CAUTION: FIXME Review the rest. === [[stopped]] stopped Flag CAUTION: FIXME Where is this used?","title":"getCurrentUserName(): String"},{"location":"spark-logging/","text":"Logging \u00b6 Spark uses log4j for logging. Logging Levels \u00b6 The valid logging levels are log4j's Levels (from most specific to least): OFF (most specific, no logging) FATAL (most specific, little data) ERROR WARN INFO DEBUG TRACE (least specific, a lot of data) ALL (least specific, all data) conf/log4j.properties \u00b6 You can set up the default logging for Spark shell in conf/log4j.properties . Use conf/log4j.properties.template as a starting point. Setting Default Log Level Programatically \u00b6 Refer to Setting Default Log Level Programatically in SparkContext -- Entry Point to Spark Core . Setting Log Levels in Spark Applications \u00b6 In standalone Spark applications or while in Spark Shell session, use the following: import org.apache.log4j.{Level, Logger} Logger.getLogger(classOf[RackResolver]).getLevel Logger.getLogger(\"org\").setLevel(Level.OFF) Logger.getLogger(\"akka\").setLevel(Level.OFF) sbt \u00b6 When running a Spark application from within sbt using run task, you can use the following build.sbt to configure logging levels: fork in run := true javaOptions in run ++= Seq( \"-Dlog4j.debug=true\", \"-Dlog4j.configuration=log4j.properties\") outputStrategy := Some(StdoutOutput) With the above configuration log4j.properties file should be on CLASSPATH which can be in src/main/resources directory (that is included in CLASSPATH by default). When run starts, you should see the following output in sbt: [spark-activator]> run [info] Running StreamingApp log4j: Trying to find [log4j.properties] using context classloader sun.misc.Launcher$AppClassLoader@1b6d3586. log4j: Using URL [file:/Users/jacek/dev/oss/spark-activator/target/scala-2.11/classes/log4j.properties] for automatic log4j configuration. log4j: Reading configuration from URL file:/Users/jacek/dev/oss/spark-activator/target/scala-2.11/classes/log4j.properties Disabling Logging \u00b6 Use the following conf/log4j.properties to disable logging completely: log4j.logger.org=OFF","title":"Logging"},{"location":"spark-logging/#logging","text":"Spark uses log4j for logging.","title":"Logging"},{"location":"spark-logging/#logging-levels","text":"The valid logging levels are log4j's Levels (from most specific to least): OFF (most specific, no logging) FATAL (most specific, little data) ERROR WARN INFO DEBUG TRACE (least specific, a lot of data) ALL (least specific, all data)","title":" Logging Levels"},{"location":"spark-logging/#conflog4jproperties","text":"You can set up the default logging for Spark shell in conf/log4j.properties . Use conf/log4j.properties.template as a starting point.","title":"conf/log4j.properties"},{"location":"spark-logging/#setting-default-log-level-programatically","text":"Refer to Setting Default Log Level Programatically in SparkContext -- Entry Point to Spark Core .","title":" Setting Default Log Level Programatically"},{"location":"spark-logging/#setting-log-levels-in-spark-applications","text":"In standalone Spark applications or while in Spark Shell session, use the following: import org.apache.log4j.{Level, Logger} Logger.getLogger(classOf[RackResolver]).getLevel Logger.getLogger(\"org\").setLevel(Level.OFF) Logger.getLogger(\"akka\").setLevel(Level.OFF)","title":" Setting Log Levels in Spark Applications"},{"location":"spark-logging/#sbt","text":"When running a Spark application from within sbt using run task, you can use the following build.sbt to configure logging levels: fork in run := true javaOptions in run ++= Seq( \"-Dlog4j.debug=true\", \"-Dlog4j.configuration=log4j.properties\") outputStrategy := Some(StdoutOutput) With the above configuration log4j.properties file should be on CLASSPATH which can be in src/main/resources directory (that is included in CLASSPATH by default). When run starts, you should see the following output in sbt: [spark-activator]> run [info] Running StreamingApp log4j: Trying to find [log4j.properties] using context classloader sun.misc.Launcher$AppClassLoader@1b6d3586. log4j: Using URL [file:/Users/jacek/dev/oss/spark-activator/target/scala-2.11/classes/log4j.properties] for automatic log4j configuration. log4j: Reading configuration from URL file:/Users/jacek/dev/oss/spark-activator/target/scala-2.11/classes/log4j.properties","title":"sbt"},{"location":"spark-logging/#disabling-logging","text":"Use the following conf/log4j.properties to disable logging completely: log4j.logger.org=OFF","title":"Disabling Logging"},{"location":"demo/diskblockmanager-and-block-data/","text":"= Demo: DiskBlockManager and Block Data The demo shows how Spark stores data blocks on local disk (using xref:storage:DiskBlockManager.adoc[DiskBlockManager] and xref:storage:DiskStore.adoc[DiskStore] among the services). == Configure Local Directories Spark uses spark.local.dir configuration property for one or more local directories to store data blocks. Start spark-shell with the property set to a directory of your choice (say local-dirs ). Use one directory for easier monitoring. $SPARK_HOME/bin/spark-shell --conf spark.local.dir=local-dirs When started, Spark will create a proper directory layout. You are interested in blockmgr-[uuid] directory. == \"Create\" Data Blocks Execute the following Spark application that forces persisting ( caching ) data to disk. import org.apache.spark.storage.StorageLevel spark.range(2).persist(StorageLevel.DISK_ONLY).count == Observe Block Files Go to the blockmgr-[uuid] directory and observe the block files. There should be a few. Do you know how many and why? $ tree local-dirs/blockmgr-b7167b5a-ae8d-404b-8de2-1a0fb101fe00/ local-dirs/blockmgr-b7167b5a-ae8d-404b-8de2-1a0fb101fe00/ \u251c\u2500\u2500 00 \u251c\u2500\u2500 04 \u2502 \u2514\u2500\u2500 shuffle_0_8_0.data \u251c\u2500\u2500 06 \u251c\u2500\u2500 08 \u2502 \u2514\u2500\u2500 shuffle_0_8_0.index ... \u251c\u2500\u2500 37 \u2502 \u2514\u2500\u2500 shuffle_0_7_0.index \u251c\u2500\u2500 38 \u2502 \u2514\u2500\u2500 shuffle_0_4_0.data \u251c\u2500\u2500 39 \u2502 \u2514\u2500\u2500 shuffle_0_9_0.index \u2514\u2500\u2500 3a \u2514\u2500\u2500 shuffle_0_6_0.data 47 directories, 48 files == Use web UI Open http://localhost:4040 and switch to Storage tab (at http://localhost:4040/storage/ ). You should see one RDD cached. .Storage tab in web UI image::demo-DiskBlockManager-and-Block-Data-webui-storage.png[align=\"center\"] Click the link in RDD Name column and review the information. == Enable Logging Enable ALL logging level for xref:storage:DiskStore.adoc#logging[org.apache.spark.storage.DiskStore] and xref:storage:DiskBlockManager.adoc#logging[org.apache.spark.storage.DiskBlockManager] loggers to have an even deeper insight on the block storage internals. log4j.logger.org.apache.spark.storage.DiskBlockManager=ALL log4j.logger.org.apache.spark.storage.DiskStore=ALL","title":"DiskBlockManager and Block Data"},{"location":"metrics/","text":"Spark Metrics \u00b6 Spark Metrics gives you execution metrics of Spark subsystems ( metrics instances , e.g. the driver of a Spark application or the master of a Spark Standalone cluster). Spark Metrics uses Dropwizard Metrics 3.1.0 Java library for the metrics infrastructure. Metrics is a Java library which gives you unparalleled insight into what your code does in production. Metrics provides a powerful toolkit of ways to measure the behavior of critical components in your production environment . Metrics Systems \u00b6 applicationMaster \u00b6 Registered when ApplicationMaster (Hadoop YARN) is requested to createAllocator applications \u00b6 Registered when Master (Spark Standalone) is created driver \u00b6 Registered when SparkEnv is created for the driver executor \u00b6 Registered when SparkEnv is created for an executor master \u00b6 Registered when Master (Spark Standalone) is created mesos_cluster \u00b6 Registered when MesosClusterScheduler (Apache Mesos) is created shuffleService \u00b6 Registered when ExternalShuffleService is created worker \u00b6 Registered when Worker (Spark Standalone) is created MetricsSystem \u00b6 Spark Metrics uses MetricsSystem . MetricsSystem uses Dropwizard Metrics' link:spark-metrics-MetricsSystem.md#registry[MetricRegistry] that acts as the integration point between Spark and the metrics library. A Spark subsystem can access the MetricsSystem through the SparkEnv.metricsSystem property. val metricsSystem = SparkEnv.get.metricsSystem MetricsConfig \u00b6 MetricsConfig is the configuration of the link:spark-metrics-MetricsSystem.md[MetricsSystem] (i.e. metrics link:spark-metrics-Source.md[sources] and link:spark-metrics-Sink.md[sinks]). metrics.properties is the default metrics configuration file. It is configured using link:spark-metrics-properties.md#spark.metrics.conf[spark.metrics.conf] configuration property. The file is first loaded from the path directly before using Spark's CLASSPATH. MetricsConfig also accepts a metrics configuration using spark.metrics.conf. -prefixed configuration properties. Spark comes with conf/metrics.properties.template file that is a template of metrics configuration. MetricsServlet Metrics Sink \u00b6 Among the metrics sinks is link:spark-metrics-MetricsServlet.md[MetricsServlet] that is used when sink.servlet metrics sink is configured in link:spark-metrics-MetricsConfig.md[metrics configuration]. CAUTION: FIXME Describe configuration files and properties JmxSink Metrics Sink \u00b6 Enable org.apache.spark.metrics.sink.JmxSink in link:spark-metrics-MetricsConfig.md[metrics configuration]. You can then use jconsole to access Spark metrics through JMX. *.sink.jmx.class=org.apache.spark.metrics.sink.JmxSink JSON URI Path \u00b6 Metrics System is available at http://localhost:4040/metrics/json (for the default setup of a Spark application). $ http --follow http://localhost:4040/metrics/json HTTP/1.1 200 OK Cache-Control: no-cache, no-store, must-revalidate Content-Length: 2200 Content-Type: text/json;charset=utf-8 Date: Sat, 25 Feb 2017 14:14:16 GMT Server: Jetty(9.2.z-SNAPSHOT) X-Frame-Options: SAMEORIGIN { \"counters\": { \"app-20170225151406-0000.driver.HiveExternalCatalog.fileCacheHits\": { \"count\": 0 }, \"app-20170225151406-0000.driver.HiveExternalCatalog.filesDiscovered\": { \"count\": 0 }, \"app-20170225151406-0000.driver.HiveExternalCatalog.hiveClientCalls\": { \"count\": 2 }, \"app-20170225151406-0000.driver.HiveExternalCatalog.parallelListingJobCount\": { \"count\": 0 }, \"app-20170225151406-0000.driver.HiveExternalCatalog.partitionsFetched\": { \"count\": 0 } }, \"gauges\": { ... \"timers\": { \"app-20170225151406-0000.driver.DAGScheduler.messageProcessingTime\": { \"count\": 0, \"duration_units\": \"milliseconds\", \"m15_rate\": 0.0, \"m1_rate\": 0.0, \"m5_rate\": 0.0, \"max\": 0.0, \"mean\": 0.0, \"mean_rate\": 0.0, \"min\": 0.0, \"p50\": 0.0, \"p75\": 0.0, \"p95\": 0.0, \"p98\": 0.0, \"p99\": 0.0, \"p999\": 0.0, \"rate_units\": \"calls/second\", \"stddev\": 0.0 } }, \"version\": \"3.0.0\" } NOTE: You can access a Spark subsystem's MetricsSystem using its corresponding \"leading\" port, e.g. 4040 for the driver , 8080 for Spark Standalone's master and applications . NOTE: You have to use the trailing slash ( / ) to have the output. Spark Standalone Master \u00b6 $ http http://192.168.1.4:8080/metrics/master/json/path HTTP/1.1 200 OK Cache-Control: no-cache, no-store, must-revalidate Content-Length: 207 Content-Type: text/json;charset=UTF-8 Server: Jetty(8.y.z-SNAPSHOT) X-Frame-Options: SAMEORIGIN { \"counters\": {}, \"gauges\": { \"master.aliveWorkers\": { \"value\": 0 }, \"master.apps\": { \"value\": 0 }, \"master.waitingApps\": { \"value\": 0 }, \"master.workers\": { \"value\": 0 } }, \"histograms\": {}, \"meters\": {}, \"timers\": {}, \"version\": \"3.0.0\" }","title":"Spark Metrics"},{"location":"metrics/#spark-metrics","text":"Spark Metrics gives you execution metrics of Spark subsystems ( metrics instances , e.g. the driver of a Spark application or the master of a Spark Standalone cluster). Spark Metrics uses Dropwizard Metrics 3.1.0 Java library for the metrics infrastructure. Metrics is a Java library which gives you unparalleled insight into what your code does in production. Metrics provides a powerful toolkit of ways to measure the behavior of critical components in your production environment .","title":"Spark Metrics"},{"location":"metrics/#metrics-systems","text":"","title":"Metrics Systems"},{"location":"metrics/#applicationmaster","text":"Registered when ApplicationMaster (Hadoop YARN) is requested to createAllocator","title":"applicationMaster"},{"location":"metrics/#applications","text":"Registered when Master (Spark Standalone) is created","title":"applications"},{"location":"metrics/#driver","text":"Registered when SparkEnv is created for the driver","title":"driver"},{"location":"metrics/#executor","text":"Registered when SparkEnv is created for an executor","title":"executor"},{"location":"metrics/#master","text":"Registered when Master (Spark Standalone) is created","title":"master"},{"location":"metrics/#mesos_cluster","text":"Registered when MesosClusterScheduler (Apache Mesos) is created","title":"mesos_cluster"},{"location":"metrics/#shuffleservice","text":"Registered when ExternalShuffleService is created","title":"shuffleService"},{"location":"metrics/#worker","text":"Registered when Worker (Spark Standalone) is created","title":"worker"},{"location":"metrics/#metricssystem","text":"Spark Metrics uses MetricsSystem . MetricsSystem uses Dropwizard Metrics' link:spark-metrics-MetricsSystem.md#registry[MetricRegistry] that acts as the integration point between Spark and the metrics library. A Spark subsystem can access the MetricsSystem through the SparkEnv.metricsSystem property. val metricsSystem = SparkEnv.get.metricsSystem","title":" MetricsSystem"},{"location":"metrics/#metricsconfig","text":"MetricsConfig is the configuration of the link:spark-metrics-MetricsSystem.md[MetricsSystem] (i.e. metrics link:spark-metrics-Source.md[sources] and link:spark-metrics-Sink.md[sinks]). metrics.properties is the default metrics configuration file. It is configured using link:spark-metrics-properties.md#spark.metrics.conf[spark.metrics.conf] configuration property. The file is first loaded from the path directly before using Spark's CLASSPATH. MetricsConfig also accepts a metrics configuration using spark.metrics.conf. -prefixed configuration properties. Spark comes with conf/metrics.properties.template file that is a template of metrics configuration.","title":" MetricsConfig"},{"location":"metrics/#metricsservlet-metrics-sink","text":"Among the metrics sinks is link:spark-metrics-MetricsServlet.md[MetricsServlet] that is used when sink.servlet metrics sink is configured in link:spark-metrics-MetricsConfig.md[metrics configuration]. CAUTION: FIXME Describe configuration files and properties","title":" MetricsServlet Metrics Sink"},{"location":"metrics/#jmxsink-metrics-sink","text":"Enable org.apache.spark.metrics.sink.JmxSink in link:spark-metrics-MetricsConfig.md[metrics configuration]. You can then use jconsole to access Spark metrics through JMX. *.sink.jmx.class=org.apache.spark.metrics.sink.JmxSink","title":" JmxSink Metrics Sink"},{"location":"metrics/#json-uri-path","text":"Metrics System is available at http://localhost:4040/metrics/json (for the default setup of a Spark application). $ http --follow http://localhost:4040/metrics/json HTTP/1.1 200 OK Cache-Control: no-cache, no-store, must-revalidate Content-Length: 2200 Content-Type: text/json;charset=utf-8 Date: Sat, 25 Feb 2017 14:14:16 GMT Server: Jetty(9.2.z-SNAPSHOT) X-Frame-Options: SAMEORIGIN { \"counters\": { \"app-20170225151406-0000.driver.HiveExternalCatalog.fileCacheHits\": { \"count\": 0 }, \"app-20170225151406-0000.driver.HiveExternalCatalog.filesDiscovered\": { \"count\": 0 }, \"app-20170225151406-0000.driver.HiveExternalCatalog.hiveClientCalls\": { \"count\": 2 }, \"app-20170225151406-0000.driver.HiveExternalCatalog.parallelListingJobCount\": { \"count\": 0 }, \"app-20170225151406-0000.driver.HiveExternalCatalog.partitionsFetched\": { \"count\": 0 } }, \"gauges\": { ... \"timers\": { \"app-20170225151406-0000.driver.DAGScheduler.messageProcessingTime\": { \"count\": 0, \"duration_units\": \"milliseconds\", \"m15_rate\": 0.0, \"m1_rate\": 0.0, \"m5_rate\": 0.0, \"max\": 0.0, \"mean\": 0.0, \"mean_rate\": 0.0, \"min\": 0.0, \"p50\": 0.0, \"p75\": 0.0, \"p95\": 0.0, \"p98\": 0.0, \"p99\": 0.0, \"p999\": 0.0, \"rate_units\": \"calls/second\", \"stddev\": 0.0 } }, \"version\": \"3.0.0\" } NOTE: You can access a Spark subsystem's MetricsSystem using its corresponding \"leading\" port, e.g. 4040 for the driver , 8080 for Spark Standalone's master and applications . NOTE: You have to use the trailing slash ( / ) to have the output.","title":"JSON URI Path"},{"location":"metrics/#spark-standalone-master","text":"$ http http://192.168.1.4:8080/metrics/master/json/path HTTP/1.1 200 OK Cache-Control: no-cache, no-store, must-revalidate Content-Length: 207 Content-Type: text/json;charset=UTF-8 Server: Jetty(8.y.z-SNAPSHOT) X-Frame-Options: SAMEORIGIN { \"counters\": {}, \"gauges\": { \"master.aliveWorkers\": { \"value\": 0 }, \"master.apps\": { \"value\": 0 }, \"master.waitingApps\": { \"value\": 0 }, \"master.workers\": { \"value\": 0 } }, \"histograms\": {}, \"meters\": {}, \"timers\": {}, \"version\": \"3.0.0\" }","title":"Spark Standalone Master"},{"location":"metrics/DAGSchedulerSource/","text":"DAGSchedulerSource \u00b6 DAGSchedulerSource is the metrics source of DAGScheduler . DAGScheduler uses Spark Metrics System to report metrics about internal status. The name of the source is DAGScheduler . DAGSchedulerSource emits the following metrics: stage.failedStages - the number of failed stages stage.runningStages - the number of running stages stage.waitingStages - the number of waiting stages job.allJobs - the number of all jobs job.activeJobs - the number of active jobs","title":"DAGSchedulerSource"},{"location":"metrics/DAGSchedulerSource/#dagschedulersource","text":"DAGSchedulerSource is the metrics source of DAGScheduler . DAGScheduler uses Spark Metrics System to report metrics about internal status. The name of the source is DAGScheduler . DAGSchedulerSource emits the following metrics: stage.failedStages - the number of failed stages stage.runningStages - the number of running stages stage.waitingStages - the number of waiting stages job.allJobs - the number of all jobs job.activeJobs - the number of active jobs","title":"DAGSchedulerSource"},{"location":"metrics/JvmSource/","text":"JvmSource \u00b6 JvmSource is a metrics source . The name of the source is jvm . JvmSource registers the build-in Codehale metrics: GarbageCollectorMetricSet MemoryUsageGaugeSet BufferPoolMetricSet Among the metrics is total.committed (from MemoryUsageGaugeSet ) that describes the current usage of the heap and non-heap memories.","title":"JvmSource"},{"location":"metrics/JvmSource/#jvmsource","text":"JvmSource is a metrics source . The name of the source is jvm . JvmSource registers the build-in Codehale metrics: GarbageCollectorMetricSet MemoryUsageGaugeSet BufferPoolMetricSet Among the metrics is total.committed (from MemoryUsageGaugeSet ) that describes the current usage of the heap and non-heap memories.","title":"JvmSource"},{"location":"metrics/MetricsConfig/","text":"MetricsConfig \u00b6 MetricsConfig is the configuration of the MetricsSystem (i.e. metrics sources and sinks ). MetricsConfig is < > when link:spark-metrics-MetricsSystem.adoc#creating-instance[MetricsSystem] is. MetricsConfig uses metrics.properties as the default metrics configuration file. It is configured using link:spark-metrics-properties.adoc#spark.metrics.conf[spark.metrics.conf] configuration property. The file is first loaded from the path directly before using Spark's CLASSPATH. MetricsConfig accepts a metrics configuration using spark.metrics.conf. -prefixed configuration properties. Spark comes with conf/metrics.properties.template file that is a template of metrics configuration. MetricsConfig < > that the < > are always defined. [[default-properties]] .MetricsConfig's Default Metrics Properties [cols=\"1,2\",options=\"header\",width=\"100%\"] |=== | Name | Description | *.sink.servlet.class | org.apache.spark.metrics.sink.MetricsServlet | *.sink.servlet.path | /metrics/json | master.sink.servlet.path | /metrics/master/json | applications.sink.servlet.path | /metrics/applications/json |=== [NOTE] \u00b6 The order of precedence of metrics configuration settings is as follows: . < > . link:spark-metrics-properties.adoc#spark.metrics.conf[spark.metrics.conf] configuration property or metrics.properties configuration file . spark.metrics.conf. -prefixed Spark properties ==== [[creating-instance]] [[conf]] MetricsConfig takes a xref:ROOT:SparkConf.adoc[SparkConf] when created. [[internal-registries]] .MetricsConfig's Internal Registries and Counters [cols=\"1,2\",options=\"header\",width=\"100%\"] |=== | Name | Description | [[properties]] properties | https://docs.oracle.com/javase/8/docs/api/java/util/Properties.html[java.util.Properties ] with metrics properties Used to < > per-subsystem's < >. | [[perInstanceSubProperties]] perInstanceSubProperties | Lookup table of metrics properties per subsystem |=== === [[initialize]] Initializing MetricsConfig -- initialize Method [source, scala] \u00b6 initialize(): Unit \u00b6 initialize < > and < > (that is defined using link:spark-metrics-properties.adoc#spark.metrics.conf[spark.metrics.conf] configuration property). initialize takes all Spark properties that start with spark.metrics.conf. prefix from < > and adds them to < > (without the prefix). In the end, initialize splits < > with the default configuration (denoted as * ) assigned to all subsystems afterwards. NOTE: initialize accepts * (star) for the default configuration or any combination of lower- and upper-case letters for Spark subsystem names. NOTE: initialize is used exclusively when MetricsSystem is link:spark-metrics-MetricsSystem.adoc#creating-instance[created]. === [[setDefaultProperties]] setDefaultProperties Internal Method [source, scala] \u00b6 setDefaultProperties(prop: Properties): Unit \u00b6 setDefaultProperties sets the < > (in the input prop ). NOTE: setDefaultProperties is used exclusively when MetricsConfig < >. === [[loadPropertiesFromFile]] Loading Custom Metrics Configuration File or metrics.properties -- loadPropertiesFromFile Method [source, scala] \u00b6 loadPropertiesFromFile(path: Option[String]): Unit \u00b6 loadPropertiesFromFile tries to open the input path file (if defined) or the default metrics configuration file metrics.properties (on CLASSPATH). If either file is available, loadPropertiesFromFile loads the properties (to < > registry). In case of exceptions, you should see the following ERROR message in the logs followed by the exception. ERROR Error loading configuration file [file] NOTE: loadPropertiesFromFile is used exclusively when MetricsConfig < >. === [[subProperties]] Grouping Properties Per Subsystem -- subProperties Method [source, scala] \u00b6 subProperties(prop: Properties, regex: Regex): mutable.HashMap[String, Properties] \u00b6 subProperties takes prop properties and destructures keys given regex . subProperties takes the matching prefix (of a key per regex ) and uses it as a new key with the value(s) being the matching suffix(es). [source, scala] \u00b6 driver.hello.world => (driver, (hello.world)) \u00b6 NOTE: subProperties is used when MetricsConfig < > (to apply the default metrics configuration) and when MetricsSystem link:spark-metrics-MetricsSystem.adoc#registerSources[registers metrics sources] and link:spark-metrics-MetricsSystem.adoc#registerSinks[sinks]. === [[getInstance]] getInstance Method [source, scala] \u00b6 getInstance(inst: String): Properties \u00b6 getInstance ...FIXME NOTE: getInstance is used when...FIXME","title":"MetricsConfig"},{"location":"metrics/MetricsConfig/#metricsconfig","text":"MetricsConfig is the configuration of the MetricsSystem (i.e. metrics sources and sinks ). MetricsConfig is < > when link:spark-metrics-MetricsSystem.adoc#creating-instance[MetricsSystem] is. MetricsConfig uses metrics.properties as the default metrics configuration file. It is configured using link:spark-metrics-properties.adoc#spark.metrics.conf[spark.metrics.conf] configuration property. The file is first loaded from the path directly before using Spark's CLASSPATH. MetricsConfig accepts a metrics configuration using spark.metrics.conf. -prefixed configuration properties. Spark comes with conf/metrics.properties.template file that is a template of metrics configuration. MetricsConfig < > that the < > are always defined. [[default-properties]] .MetricsConfig's Default Metrics Properties [cols=\"1,2\",options=\"header\",width=\"100%\"] |=== | Name | Description | *.sink.servlet.class | org.apache.spark.metrics.sink.MetricsServlet | *.sink.servlet.path | /metrics/json | master.sink.servlet.path | /metrics/master/json | applications.sink.servlet.path | /metrics/applications/json |===","title":"MetricsConfig"},{"location":"metrics/MetricsConfig/#note","text":"The order of precedence of metrics configuration settings is as follows: . < > . link:spark-metrics-properties.adoc#spark.metrics.conf[spark.metrics.conf] configuration property or metrics.properties configuration file . spark.metrics.conf. -prefixed Spark properties ==== [[creating-instance]] [[conf]] MetricsConfig takes a xref:ROOT:SparkConf.adoc[SparkConf] when created. [[internal-registries]] .MetricsConfig's Internal Registries and Counters [cols=\"1,2\",options=\"header\",width=\"100%\"] |=== | Name | Description | [[properties]] properties | https://docs.oracle.com/javase/8/docs/api/java/util/Properties.html[java.util.Properties ] with metrics properties Used to < > per-subsystem's < >. | [[perInstanceSubProperties]] perInstanceSubProperties | Lookup table of metrics properties per subsystem |=== === [[initialize]] Initializing MetricsConfig -- initialize Method","title":"[NOTE]"},{"location":"metrics/MetricsConfig/#source-scala","text":"","title":"[source, scala]"},{"location":"metrics/MetricsConfig/#initialize-unit","text":"initialize < > and < > (that is defined using link:spark-metrics-properties.adoc#spark.metrics.conf[spark.metrics.conf] configuration property). initialize takes all Spark properties that start with spark.metrics.conf. prefix from < > and adds them to < > (without the prefix). In the end, initialize splits < > with the default configuration (denoted as * ) assigned to all subsystems afterwards. NOTE: initialize accepts * (star) for the default configuration or any combination of lower- and upper-case letters for Spark subsystem names. NOTE: initialize is used exclusively when MetricsSystem is link:spark-metrics-MetricsSystem.adoc#creating-instance[created]. === [[setDefaultProperties]] setDefaultProperties Internal Method","title":"initialize(): Unit"},{"location":"metrics/MetricsConfig/#source-scala_1","text":"","title":"[source, scala]"},{"location":"metrics/MetricsConfig/#setdefaultpropertiesprop-properties-unit","text":"setDefaultProperties sets the < > (in the input prop ). NOTE: setDefaultProperties is used exclusively when MetricsConfig < >. === [[loadPropertiesFromFile]] Loading Custom Metrics Configuration File or metrics.properties -- loadPropertiesFromFile Method","title":"setDefaultProperties(prop: Properties): Unit"},{"location":"metrics/MetricsConfig/#source-scala_2","text":"","title":"[source, scala]"},{"location":"metrics/MetricsConfig/#loadpropertiesfromfilepath-optionstring-unit","text":"loadPropertiesFromFile tries to open the input path file (if defined) or the default metrics configuration file metrics.properties (on CLASSPATH). If either file is available, loadPropertiesFromFile loads the properties (to < > registry). In case of exceptions, you should see the following ERROR message in the logs followed by the exception. ERROR Error loading configuration file [file] NOTE: loadPropertiesFromFile is used exclusively when MetricsConfig < >. === [[subProperties]] Grouping Properties Per Subsystem -- subProperties Method","title":"loadPropertiesFromFile(path: Option[String]): Unit"},{"location":"metrics/MetricsConfig/#source-scala_3","text":"","title":"[source, scala]"},{"location":"metrics/MetricsConfig/#subpropertiesprop-properties-regex-regex-mutablehashmapstring-properties","text":"subProperties takes prop properties and destructures keys given regex . subProperties takes the matching prefix (of a key per regex ) and uses it as a new key with the value(s) being the matching suffix(es).","title":"subProperties(prop: Properties, regex: Regex): mutable.HashMap[String, Properties]"},{"location":"metrics/MetricsConfig/#source-scala_4","text":"","title":"[source, scala]"},{"location":"metrics/MetricsConfig/#driverhelloworld-driver-helloworld","text":"NOTE: subProperties is used when MetricsConfig < > (to apply the default metrics configuration) and when MetricsSystem link:spark-metrics-MetricsSystem.adoc#registerSources[registers metrics sources] and link:spark-metrics-MetricsSystem.adoc#registerSinks[sinks]. === [[getInstance]] getInstance Method","title":"driver.hello.world =&gt; (driver, (hello.world))"},{"location":"metrics/MetricsConfig/#source-scala_5","text":"","title":"[source, scala]"},{"location":"metrics/MetricsConfig/#getinstanceinst-string-properties","text":"getInstance ...FIXME NOTE: getInstance is used when...FIXME","title":"getInstance(inst: String): Properties"},{"location":"metrics/MetricsServlet/","text":"MetricsServlet JSON Metrics Sink \u00b6 MetricsServlet is a metrics sink that gives metrics snapshots in JSON format. MetricsServlet is a \"special\" sink as it is only available to the metrics instances with a web UI: Driver of a Spark application Spark Standalone's Master and Worker You can access the metrics from MetricsServlet at /metrics/json URI by default. The entire URL depends on a metrics instance, e.g. http://localhost:4040/metrics/json/ for a running Spark application. $ http http://localhost:4040/metrics/json/ HTTP/1.1 200 OK Cache-Control: no-cache, no-store, must-revalidate Content-Length: 5005 Content-Type: text/json;charset=utf-8 Date: Mon, 11 Jun 2018 06:29:03 GMT Server: Jetty(9.3.z-SNAPSHOT) X-Content-Type-Options: nosniff X-Frame-Options: SAMEORIGIN X-XSS-Protection: 1; mode=block { \"counters\": { \"local-1528698499919.driver.HiveExternalCatalog.fileCacheHits\": { \"count\": 0 }, \"local-1528698499919.driver.HiveExternalCatalog.filesDiscovered\": { \"count\": 0 }, \"local-1528698499919.driver.HiveExternalCatalog.hiveClientCalls\": { \"count\": 0 }, \"local-1528698499919.driver.HiveExternalCatalog.parallelListingJobCount\": { \"count\": 0 }, \"local-1528698499919.driver.HiveExternalCatalog.partitionsFetched\": { \"count\": 0 }, \"local-1528698499919.driver.LiveListenerBus.numEventsPosted\": { \"count\": 7 }, \"local-1528698499919.driver.LiveListenerBus.queue.appStatus.numDroppedEvents\": { \"count\": 0 }, \"local-1528698499919.driver.LiveListenerBus.queue.executorManagement.numDroppedEvents\": { \"count\": 0 } }, ... MetricsServlet is < > exclusively when MetricsSystem is started (and requested to register metrics sinks ). MetricsServlet can be configured using configuration properties with sink.servlet prefix (in link:spark-metrics-MetricsConfig.adoc[metrics configuration]). That is not required since MetricsConfig link:spark-metrics-MetricsConfig.adoc#setDefaultProperties[makes sure] that MetricsServlet is always configured. MetricsServlet uses https://fasterxml.github.io/jackson-databind/[jackson-databind ], the general data-binding package for Jackson (as < >) with https://metrics.dropwizard.io/3.1.0/[Dropwizard Metrics] library (i.e. registering a Coda Hale MetricsModule ). [[properties]] .MetricsServlet's Configuration Properties [cols=\"1,1,2\",options=\"header\",width=\"100%\"] |=== | Name | Default | Description | path | /metrics/json/ | [[path]] Path URI prefix to bind to | sample | false | [[sample]] Whether to show entire set of samples for histograms |=== [[internal-registries]] .MetricsServlet's Internal Properties (e.g. Registries, Counters and Flags) [cols=\"1,2\",options=\"header\",width=\"100%\"] |=== | Name | Description | mapper | [[mapper]] Jaxson's https://fasterxml.github.io/jackson-databind/javadoc/2.6/com/fasterxml/jackson/databind/ObjectMapper.html[com.fasterxml.jackson.databind.ObjectMapper ] that \"provides functionality for reading and writing JSON, either to and from basic POJOs (Plain Old Java Objects), or to and from a general-purpose JSON Tree Model (JsonNode), as well as related functionality for performing conversions.\" When created, mapper is requested to register a Coda Hale https://metrics.dropwizard.io/3.1.0/apidocs/com/codahale/metrics/json/MetricsModule.html[com.codahale.metrics.json.MetricsModule ]. Used exclusively when MetricsServlet is requested to < >. | servletPath | [[servletPath]] Value of < > configuration property | servletShowSample | [[servletShowSample]] Flag to control whether to show samples ( true ) or not ( false ). servletShowSample is the value of < > configuration property (if defined) or false . Used when < > is requested to register a Coda Hale https://metrics.dropwizard.io/3.1.0/apidocs/com/codahale/metrics/json/MetricsModule.html[com.codahale.metrics.json.MetricsModule ]. |=== === [[creating-instance]] Creating MetricsServlet Instance MetricsServlet takes the following when created: [[property]] Configuration Properties (as Java Properties ) [[registry]] Dropwizard Metrics' https://metrics.dropwizard.io/3.1.0/apidocs/com/codahale/metrics/MetricRegistry.html[MetricRegistry ] [[securityMgr]] SecurityManager MetricsServlet initializes the < >. === [[getMetricsSnapshot]] Requesting Metrics Snapshot -- getMetricsSnapshot Method [source, scala] \u00b6 getMetricsSnapshot(request: HttpServletRequest): String \u00b6 getMetricsSnapshot simply requests the < > to serialize the < > to a JSON string (using link:++ https://fasterxml.github.io/jackson-databind/javadoc/2.6/com/fasterxml/jackson/databind/ObjectMapper.html#writeValueAsString-java.lang.Object-++[ObjectMapper.writeValueAsString ]). NOTE: getMetricsSnapshot is used exclusively when MetricsServlet is requested to < >. === [[getHandlers]] Requesting JSON Servlet Handler -- getHandlers Method [source, scala] \u00b6 getHandlers(conf: SparkConf): Array[ServletContextHandler] \u00b6 getHandlers returns just a single ServletContextHandler (in a collection) that gives < > in JSON format at every request at < > URI path. NOTE: getHandlers is used exclusively when MetricsSystem is requested for link:MetricsSystem.md#getServletHandlers[metrics ServletContextHandlers].","title":"MetricsServlet"},{"location":"metrics/MetricsServlet/#metricsservlet-json-metrics-sink","text":"MetricsServlet is a metrics sink that gives metrics snapshots in JSON format. MetricsServlet is a \"special\" sink as it is only available to the metrics instances with a web UI: Driver of a Spark application Spark Standalone's Master and Worker You can access the metrics from MetricsServlet at /metrics/json URI by default. The entire URL depends on a metrics instance, e.g. http://localhost:4040/metrics/json/ for a running Spark application. $ http http://localhost:4040/metrics/json/ HTTP/1.1 200 OK Cache-Control: no-cache, no-store, must-revalidate Content-Length: 5005 Content-Type: text/json;charset=utf-8 Date: Mon, 11 Jun 2018 06:29:03 GMT Server: Jetty(9.3.z-SNAPSHOT) X-Content-Type-Options: nosniff X-Frame-Options: SAMEORIGIN X-XSS-Protection: 1; mode=block { \"counters\": { \"local-1528698499919.driver.HiveExternalCatalog.fileCacheHits\": { \"count\": 0 }, \"local-1528698499919.driver.HiveExternalCatalog.filesDiscovered\": { \"count\": 0 }, \"local-1528698499919.driver.HiveExternalCatalog.hiveClientCalls\": { \"count\": 0 }, \"local-1528698499919.driver.HiveExternalCatalog.parallelListingJobCount\": { \"count\": 0 }, \"local-1528698499919.driver.HiveExternalCatalog.partitionsFetched\": { \"count\": 0 }, \"local-1528698499919.driver.LiveListenerBus.numEventsPosted\": { \"count\": 7 }, \"local-1528698499919.driver.LiveListenerBus.queue.appStatus.numDroppedEvents\": { \"count\": 0 }, \"local-1528698499919.driver.LiveListenerBus.queue.executorManagement.numDroppedEvents\": { \"count\": 0 } }, ... MetricsServlet is < > exclusively when MetricsSystem is started (and requested to register metrics sinks ). MetricsServlet can be configured using configuration properties with sink.servlet prefix (in link:spark-metrics-MetricsConfig.adoc[metrics configuration]). That is not required since MetricsConfig link:spark-metrics-MetricsConfig.adoc#setDefaultProperties[makes sure] that MetricsServlet is always configured. MetricsServlet uses https://fasterxml.github.io/jackson-databind/[jackson-databind ], the general data-binding package for Jackson (as < >) with https://metrics.dropwizard.io/3.1.0/[Dropwizard Metrics] library (i.e. registering a Coda Hale MetricsModule ). [[properties]] .MetricsServlet's Configuration Properties [cols=\"1,1,2\",options=\"header\",width=\"100%\"] |=== | Name | Default | Description | path | /metrics/json/ | [[path]] Path URI prefix to bind to | sample | false | [[sample]] Whether to show entire set of samples for histograms |=== [[internal-registries]] .MetricsServlet's Internal Properties (e.g. Registries, Counters and Flags) [cols=\"1,2\",options=\"header\",width=\"100%\"] |=== | Name | Description | mapper | [[mapper]] Jaxson's https://fasterxml.github.io/jackson-databind/javadoc/2.6/com/fasterxml/jackson/databind/ObjectMapper.html[com.fasterxml.jackson.databind.ObjectMapper ] that \"provides functionality for reading and writing JSON, either to and from basic POJOs (Plain Old Java Objects), or to and from a general-purpose JSON Tree Model (JsonNode), as well as related functionality for performing conversions.\" When created, mapper is requested to register a Coda Hale https://metrics.dropwizard.io/3.1.0/apidocs/com/codahale/metrics/json/MetricsModule.html[com.codahale.metrics.json.MetricsModule ]. Used exclusively when MetricsServlet is requested to < >. | servletPath | [[servletPath]] Value of < > configuration property | servletShowSample | [[servletShowSample]] Flag to control whether to show samples ( true ) or not ( false ). servletShowSample is the value of < > configuration property (if defined) or false . Used when < > is requested to register a Coda Hale https://metrics.dropwizard.io/3.1.0/apidocs/com/codahale/metrics/json/MetricsModule.html[com.codahale.metrics.json.MetricsModule ]. |=== === [[creating-instance]] Creating MetricsServlet Instance MetricsServlet takes the following when created: [[property]] Configuration Properties (as Java Properties ) [[registry]] Dropwizard Metrics' https://metrics.dropwizard.io/3.1.0/apidocs/com/codahale/metrics/MetricRegistry.html[MetricRegistry ] [[securityMgr]] SecurityManager MetricsServlet initializes the < >. === [[getMetricsSnapshot]] Requesting Metrics Snapshot -- getMetricsSnapshot Method","title":"MetricsServlet JSON Metrics Sink"},{"location":"metrics/MetricsServlet/#source-scala","text":"","title":"[source, scala]"},{"location":"metrics/MetricsServlet/#getmetricssnapshotrequest-httpservletrequest-string","text":"getMetricsSnapshot simply requests the < > to serialize the < > to a JSON string (using link:++ https://fasterxml.github.io/jackson-databind/javadoc/2.6/com/fasterxml/jackson/databind/ObjectMapper.html#writeValueAsString-java.lang.Object-++[ObjectMapper.writeValueAsString ]). NOTE: getMetricsSnapshot is used exclusively when MetricsServlet is requested to < >. === [[getHandlers]] Requesting JSON Servlet Handler -- getHandlers Method","title":"getMetricsSnapshot(request: HttpServletRequest): String"},{"location":"metrics/MetricsServlet/#source-scala_1","text":"","title":"[source, scala]"},{"location":"metrics/MetricsServlet/#gethandlersconf-sparkconf-arrayservletcontexthandler","text":"getHandlers returns just a single ServletContextHandler (in a collection) that gives < > in JSON format at every request at < > URI path. NOTE: getHandlers is used exclusively when MetricsSystem is requested for link:MetricsSystem.md#getServletHandlers[metrics ServletContextHandlers].","title":"getHandlers(conf: SparkConf): Array[ServletContextHandler]"},{"location":"metrics/MetricsSystem/","text":"MetricsSystem \u00b6 MetricsSystem is a registry of metrics sources and sinks of a Spark subsystem . Creating Instance \u00b6 MetricsSystem takes the following to be created: Instance Name SparkConf SecurityManager While being created, MetricsSystem requests the MetricsConfig to initialize . MetricsSystem is created (using createMetricsSystem utility) for the Metrics Systems . Creating MetricsSystem \u00b6 createMetricsSystem ( instance : String conf : SparkConf securityMgr : SecurityManager ) : MetricsSystem createMetricsSystem creates a new MetricsSystem (for the given parameters). createMetricsSystem is used to create metrics systems . Metrics Sources for Spark SQL \u00b6 CodegenMetrics HiveCatalogMetrics Registering Metrics Source \u00b6 registerSource ( source : Source ) : Unit registerSource adds source to the sources internal registry. registerSource creates an identifier for the metrics source and registers it with the MetricRegistry . registerSource uses Metrics' MetricRegistry.register to register a metrics source under a given name. registerSource prints out the following INFO message to the logs when registering a name more than once: Metrics already registered Building Metrics Source Identifier \u00b6 buildRegistryName ( source : Source ) : String buildRegistryName uses spark-metrics-properties.md#spark.metrics.namespace[spark.metrics.namespace] and xref:executor:Executor.md#spark.executor.id[spark.executor.id] Spark properties to differentiate between a Spark application's driver and executors, and the other Spark framework's components. (only when < > is driver or executor ) buildRegistryName builds metrics source name that is made up of link:spark-metrics-properties.md#spark.metrics.namespace[spark.metrics.namespace], xref:executor:Executor.md#spark.executor.id[spark.executor.id] and the name of the source . Note buildRegistryName uses Dropwizard Metrics' MetricRegistry to build metrics source identifiers. FIXME Finish for the other components. buildRegistryName is used when MetricsSystem is requested to register or remove a metrics source. Registering Metrics Sources for Spark Instance \u00b6 registerSources () : Unit registerSources finds < > configuration for the < >. NOTE: instance is defined when MetricsSystem < >. registerSources finds the configuration of all the link:spark-metrics-Source.md[metrics sources] for the subsystem (as described with source. prefix). For every metrics source, registerSources finds class property, creates an instance, and in the end < >. When registerSources fails, you should see the following ERROR message in the logs followed by the exception. Source class [classPath] cannot be instantiated registerSources is used when MetricsSystem is requested to start . Requesting JSON Servlet Handler \u00b6 getServletHandlers : Array [ ServletContextHandler ] If the MetricsSystem is < > and the < > is defined for the metrics system, getServletHandlers simply requests the < > for the link:spark-metrics-MetricsServlet.md#getHandlers[JSON servlet handler]. When MetricsSystem is not < > getServletHandlers throws an IllegalArgumentException . Can only call getServletHandlers on a running MetricsSystem getServletHandlers is used when: SparkContext is created (Spark Standalone) Master and Worker are requested to start Registering Metrics Sinks \u00b6 registerSinks () : Unit registerSinks requests the < > for the link:spark-metrics-MetricsConfig.md#getInstance[configuration] of the < >. registerSinks requests the < > for the link:spark-metrics-MetricsConfig.md#subProperties[configuration] of all metrics sinks (i.e. configuration entries that match ^sink\\\\.(.+)\\\\.(.+) regular expression). For every metrics sink configuration, registerSinks takes class property and (if defined) creates an instance of the metric sink using an constructor that takes the configuration, < > and < >. For a single servlet metrics sink, registerSinks converts the sink to a link:spark-metrics-MetricsServlet.md[MetricsServlet] and sets the < > internal registry. For all other metrics sinks, registerSinks adds the sink to the < > internal registry. In case of an Exception , registerSinks prints out the following ERROR message to the logs: Sink class [classPath] cannot be instantiated registerSinks is used when MetricsSystem is requested to start . Stopping \u00b6 stop () : Unit stop ...FIXME Reporting Metrics \u00b6 report () : Unit report simply requests the registered metrics sinks to report metrics . Starting \u00b6 start () : Unit start turns < > flag on. NOTE: start can only be called once and < > an IllegalArgumentException when called multiple times. start < > the < > for Spark SQL, i.e. CodegenMetrics and HiveCatalogMetrics . start then registers the configured metrics < > and < > for the < >. In the end, start requests the registered < > to link:spark-metrics-Sink.md#start[start]. [[start-IllegalArgumentException]] start throws an IllegalArgumentException when < > flag is on. requirement failed: Attempting to start a MetricsSystem that is already running Logging \u00b6 Enable ALL logging level for org.apache.spark.metrics.MetricsSystem logger to see what happens inside. Add the following line to conf/log4j.properties : log4j.logger.org.apache.spark.metrics.MetricsSystem=ALL Refer to Logging . Internal Registries \u00b6 MetricRegistry \u00b6 Integration point to Dropwizard Metrics' MetricRegistry Used when MetricsSystem is requested to: Register or remove a metrics source Start (that in turn registers metrics sinks ) MetricsConfig \u00b6 MetricsConfig Initialized when MetricsSystem is < >. Used when MetricsSystem registers < > and < >. MetricsServlet \u00b6 MetricsServlet JSON metrics sink that is only available for the < > with a web UI (i.e. the driver of a Spark application and Spark Standalone's Master ). MetricsSystem may have at most one MetricsServlet JSON metrics sink (which is registered by default ). Initialized when MetricsSystem registers < > (and finds a configuration entry with servlet sink name). Used when MetricsSystem is requested for a < >. running Flag \u00b6 Indicates whether MetricsSystem has been started ( true ) or not ( false ) Default: false sinks \u00b6 Metrics sinks Used when MetricsSystem < > and < >. sources \u00b6 Metrics sources Used when MetricsSystem < >.","title":"MetricsSystem"},{"location":"metrics/MetricsSystem/#metricssystem","text":"MetricsSystem is a registry of metrics sources and sinks of a Spark subsystem .","title":"MetricsSystem"},{"location":"metrics/MetricsSystem/#creating-instance","text":"MetricsSystem takes the following to be created: Instance Name SparkConf SecurityManager While being created, MetricsSystem requests the MetricsConfig to initialize . MetricsSystem is created (using createMetricsSystem utility) for the Metrics Systems .","title":"Creating Instance"},{"location":"metrics/MetricsSystem/#creating-metricssystem","text":"createMetricsSystem ( instance : String conf : SparkConf securityMgr : SecurityManager ) : MetricsSystem createMetricsSystem creates a new MetricsSystem (for the given parameters). createMetricsSystem is used to create metrics systems .","title":" Creating MetricsSystem"},{"location":"metrics/MetricsSystem/#metrics-sources-for-spark-sql","text":"CodegenMetrics HiveCatalogMetrics","title":" Metrics Sources for Spark SQL"},{"location":"metrics/MetricsSystem/#registering-metrics-source","text":"registerSource ( source : Source ) : Unit registerSource adds source to the sources internal registry. registerSource creates an identifier for the metrics source and registers it with the MetricRegistry . registerSource uses Metrics' MetricRegistry.register to register a metrics source under a given name. registerSource prints out the following INFO message to the logs when registering a name more than once: Metrics already registered","title":" Registering Metrics Source"},{"location":"metrics/MetricsSystem/#building-metrics-source-identifier","text":"buildRegistryName ( source : Source ) : String buildRegistryName uses spark-metrics-properties.md#spark.metrics.namespace[spark.metrics.namespace] and xref:executor:Executor.md#spark.executor.id[spark.executor.id] Spark properties to differentiate between a Spark application's driver and executors, and the other Spark framework's components. (only when < > is driver or executor ) buildRegistryName builds metrics source name that is made up of link:spark-metrics-properties.md#spark.metrics.namespace[spark.metrics.namespace], xref:executor:Executor.md#spark.executor.id[spark.executor.id] and the name of the source . Note buildRegistryName uses Dropwizard Metrics' MetricRegistry to build metrics source identifiers. FIXME Finish for the other components. buildRegistryName is used when MetricsSystem is requested to register or remove a metrics source.","title":" Building Metrics Source Identifier"},{"location":"metrics/MetricsSystem/#registering-metrics-sources-for-spark-instance","text":"registerSources () : Unit registerSources finds < > configuration for the < >. NOTE: instance is defined when MetricsSystem < >. registerSources finds the configuration of all the link:spark-metrics-Source.md[metrics sources] for the subsystem (as described with source. prefix). For every metrics source, registerSources finds class property, creates an instance, and in the end < >. When registerSources fails, you should see the following ERROR message in the logs followed by the exception. Source class [classPath] cannot be instantiated registerSources is used when MetricsSystem is requested to start .","title":" Registering Metrics Sources for Spark Instance"},{"location":"metrics/MetricsSystem/#requesting-json-servlet-handler","text":"getServletHandlers : Array [ ServletContextHandler ] If the MetricsSystem is < > and the < > is defined for the metrics system, getServletHandlers simply requests the < > for the link:spark-metrics-MetricsServlet.md#getHandlers[JSON servlet handler]. When MetricsSystem is not < > getServletHandlers throws an IllegalArgumentException . Can only call getServletHandlers on a running MetricsSystem getServletHandlers is used when: SparkContext is created (Spark Standalone) Master and Worker are requested to start","title":" Requesting JSON Servlet Handler"},{"location":"metrics/MetricsSystem/#registering-metrics-sinks","text":"registerSinks () : Unit registerSinks requests the < > for the link:spark-metrics-MetricsConfig.md#getInstance[configuration] of the < >. registerSinks requests the < > for the link:spark-metrics-MetricsConfig.md#subProperties[configuration] of all metrics sinks (i.e. configuration entries that match ^sink\\\\.(.+)\\\\.(.+) regular expression). For every metrics sink configuration, registerSinks takes class property and (if defined) creates an instance of the metric sink using an constructor that takes the configuration, < > and < >. For a single servlet metrics sink, registerSinks converts the sink to a link:spark-metrics-MetricsServlet.md[MetricsServlet] and sets the < > internal registry. For all other metrics sinks, registerSinks adds the sink to the < > internal registry. In case of an Exception , registerSinks prints out the following ERROR message to the logs: Sink class [classPath] cannot be instantiated registerSinks is used when MetricsSystem is requested to start .","title":" Registering Metrics Sinks"},{"location":"metrics/MetricsSystem/#stopping","text":"stop () : Unit stop ...FIXME","title":" Stopping"},{"location":"metrics/MetricsSystem/#reporting-metrics","text":"report () : Unit report simply requests the registered metrics sinks to report metrics .","title":" Reporting Metrics"},{"location":"metrics/MetricsSystem/#starting","text":"start () : Unit start turns < > flag on. NOTE: start can only be called once and < > an IllegalArgumentException when called multiple times. start < > the < > for Spark SQL, i.e. CodegenMetrics and HiveCatalogMetrics . start then registers the configured metrics < > and < > for the < >. In the end, start requests the registered < > to link:spark-metrics-Sink.md#start[start]. [[start-IllegalArgumentException]] start throws an IllegalArgumentException when < > flag is on. requirement failed: Attempting to start a MetricsSystem that is already running","title":" Starting"},{"location":"metrics/MetricsSystem/#logging","text":"Enable ALL logging level for org.apache.spark.metrics.MetricsSystem logger to see what happens inside. Add the following line to conf/log4j.properties : log4j.logger.org.apache.spark.metrics.MetricsSystem=ALL Refer to Logging .","title":"Logging"},{"location":"metrics/MetricsSystem/#internal-registries","text":"","title":"Internal Registries"},{"location":"metrics/MetricsSystem/#metricregistry","text":"Integration point to Dropwizard Metrics' MetricRegistry Used when MetricsSystem is requested to: Register or remove a metrics source Start (that in turn registers metrics sinks )","title":" MetricRegistry"},{"location":"metrics/MetricsSystem/#metricsconfig","text":"MetricsConfig Initialized when MetricsSystem is < >. Used when MetricsSystem registers < > and < >.","title":" MetricsConfig"},{"location":"metrics/MetricsSystem/#metricsservlet","text":"MetricsServlet JSON metrics sink that is only available for the < > with a web UI (i.e. the driver of a Spark application and Spark Standalone's Master ). MetricsSystem may have at most one MetricsServlet JSON metrics sink (which is registered by default ). Initialized when MetricsSystem registers < > (and finds a configuration entry with servlet sink name). Used when MetricsSystem is requested for a < >.","title":" MetricsServlet"},{"location":"metrics/MetricsSystem/#running-flag","text":"Indicates whether MetricsSystem has been started ( true ) or not ( false ) Default: false","title":" running Flag"},{"location":"metrics/MetricsSystem/#sinks","text":"Metrics sinks Used when MetricsSystem < > and < >.","title":" sinks"},{"location":"metrics/MetricsSystem/#sources","text":"Metrics sources Used when MetricsSystem < >.","title":" sources"},{"location":"metrics/Sink/","text":"Sink \u00b6 Sink is a < > of metrics sinks . [[contract]] [source, scala] package org.apache.spark.metrics.sink trait Sink { def start(): Unit def stop(): Unit def report(): Unit } NOTE: Sink is a private[spark] contract. .Sink Contract [cols=\"1,2\",options=\"header\",width=\"100%\"] |=== | Method | Description | start | [[start]] Used when...FIXME | stop | [[stop]] Used when...FIXME | report | [[report]] Used when...FIXME |=== [[implementations]] .Sinks [cols=\"1,2\",options=\"header\",width=\"100%\"] |=== | Sink | Description | ConsoleSink | [[ConsoleSink]] | CsvSink | [[CsvSink]] | GraphiteSink | [[GraphiteSink]] | JmxSink | [[JmxSink]] | link:spark-metrics-MetricsServlet.adoc[MetricsServlet] | [[MetricsServlet]] | Slf4jSink | [[Slf4jSink]] | StatsdSink | [[StatsdSink]] |=== NOTE: All known < > in Spark 2.3 are in org.apache.spark.metrics.sink Scala package.","title":"Sink"},{"location":"metrics/Sink/#sink","text":"Sink is a < > of metrics sinks . [[contract]] [source, scala] package org.apache.spark.metrics.sink trait Sink { def start(): Unit def stop(): Unit def report(): Unit } NOTE: Sink is a private[spark] contract. .Sink Contract [cols=\"1,2\",options=\"header\",width=\"100%\"] |=== | Method | Description | start | [[start]] Used when...FIXME | stop | [[stop]] Used when...FIXME | report | [[report]] Used when...FIXME |=== [[implementations]] .Sinks [cols=\"1,2\",options=\"header\",width=\"100%\"] |=== | Sink | Description | ConsoleSink | [[ConsoleSink]] | CsvSink | [[CsvSink]] | GraphiteSink | [[GraphiteSink]] | JmxSink | [[JmxSink]] | link:spark-metrics-MetricsServlet.adoc[MetricsServlet] | [[MetricsServlet]] | Slf4jSink | [[Slf4jSink]] | StatsdSink | [[StatsdSink]] |=== NOTE: All known < > in Spark 2.3 are in org.apache.spark.metrics.sink Scala package.","title":"Sink"},{"location":"metrics/Source/","text":"== [[Source]] Source -- Contract of Metrics Sources Source is a < > of metrics sources . [[contract]] [source, scala] package org.apache.spark.metrics.source trait Source { def sourceName: String def metricRegistry: MetricRegistry } NOTE: Source is a private[spark] contract. .Source Contract [cols=\"1,2\",options=\"header\",width=\"100%\"] |=== | Method | Description | sourceName | [[sourceName]] Used when...FIXME | metricRegistry | [[metricRegistry]] Dropwizard Metrics' https://metrics.dropwizard.io/3.1.0/apidocs/com/codahale/metrics/MetricRegistry.html[MetricRegistry ] Used when...FIXME |=== [[implementations]] .Sources [cols=\"1,2\",options=\"header\",width=\"100%\"] |=== | Source | Description | ApplicationSource | [[ApplicationSource]] | xref:storage:spark-BlockManager-BlockManagerSource.adoc[BlockManagerSource] | [[BlockManagerSource]] | CacheMetrics | [[CacheMetrics]] | CodegenMetrics | [[CodegenMetrics]] | xref:metrics:spark-scheduler-DAGSchedulerSource.adoc[DAGSchedulerSource] | [[DAGSchedulerSource]] | xref:ROOT:spark-service-ExecutorAllocationManagerSource.adoc[ExecutorAllocationManagerSource] | [[ExecutorAllocationManagerSource]] | xref:executor:ExecutorSource.adoc[] | [[ExecutorSource]] | ExternalShuffleServiceSource | [[ExternalShuffleServiceSource]] | HiveCatalogMetrics | [[HiveCatalogMetrics]] | xref:metrics:JvmSource.adoc[JvmSource] | [[JvmSource]] | LiveListenerBusMetrics | [[LiveListenerBusMetrics]] | MasterSource | [[MasterSource]] | MesosClusterSchedulerSource | [[MesosClusterSchedulerSource]] | xref:storage:ShuffleMetricsSource.adoc[] | [[ShuffleMetricsSource]] | StreamingSource | [[StreamingSource]] | WorkerSource | [[WorkerSource]] |===","title":"Source"},{"location":"metrics/configuration-properties/","text":"Configuration Properties \u00b6 spark.metrics.conf \u00b6 The metrics configuration file Default: metrics.properties spark.metrics.namespace \u00b6 Root namespace for metrics reporting Default: Spark Application ID (i.e. spark.app.id configuration property) Since a Spark application's ID changes with every execution of a Spark application, a custom namespace can be specified for an easier metrics reporting. Used when MetricsSystem is requested for a metrics source identifier ( metrics namespace )","title":"Configuration Properties"},{"location":"metrics/configuration-properties/#configuration-properties","text":"","title":"Configuration Properties"},{"location":"metrics/configuration-properties/#sparkmetricsconf","text":"The metrics configuration file Default: metrics.properties","title":" spark.metrics.conf"},{"location":"metrics/configuration-properties/#sparkmetricsnamespace","text":"Root namespace for metrics reporting Default: Spark Application ID (i.e. spark.app.id configuration property) Since a Spark application's ID changes with every execution of a Spark application, a custom namespace can be specified for an easier metrics reporting. Used when MetricsSystem is requested for a metrics source identifier ( metrics namespace )","title":" spark.metrics.namespace"},{"location":"rdd/","text":"Resilient Distributed Dataset (RDD) \u00b6 Resilient Distributed Dataset (aka RDD ) is the primary data abstraction in Apache Spark and the core of Spark (that I often refer to as \"Spark Core\"). .The origins of RDD The original paper that gave birth to the concept of RDD is https://cs.stanford.edu/~matei/papers/2012/nsdi_spark.pdf[Resilient Distributed Datasets: A Fault-Tolerant Abstraction for In-Memory Cluster Computing] by Matei Zaharia, et al. An RDD is a description of a fault-tolerant and resilient computation over a distributed collection of records (spread over < >). NOTE: One could compare RDDs to collections in Scala, i.e. a RDD is computed on many JVMs while a Scala collection lives on a single JVM. Using RDD Spark hides data partitioning and so distribution that in turn allowed them to design parallel computational framework with a higher-level programming interface (API) for four mainstream programming languages. The features of RDDs (decomposing the name): Resilient , i.e. fault-tolerant with the help of < > and so able to recompute missing or damaged partitions due to node failures. Distributed with data residing on multiple nodes in a link:spark-cluster.adoc[cluster]. Dataset is a collection of link:spark-rdd-partitions.adoc[partitioned data] with primitive values or values of values, e.g. tuples or other objects (that represent records of the data you work with). .RDDs image::spark-rdds.png[align=\"center\"] From the scaladoc of http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.rdd.RDD[org.apache.spark.rdd.RDD ]: A Resilient Distributed Dataset (RDD), the basic abstraction in Spark. Represents an immutable, partitioned collection of elements that can be operated on in parallel. From the original paper about RDD - https://cs.stanford.edu/~matei/papers/2012/nsdi_spark.pdf[Resilient Distributed Datasets: A Fault-Tolerant Abstraction for In-Memory Cluster Computing]: Resilient Distributed Datasets (RDDs) are a distributed memory abstraction that lets programmers perform in-memory computations on large clusters in a fault-tolerant manner. Beside the above traits (that are directly embedded in the name of the data abstraction - RDD) it has the following additional traits: In-Memory , i.e. data inside RDD is stored in memory as much (size) and long (time) as possible. Immutable or Read-Only , i.e. it does not change once created and can only be transformed using transformations to new RDDs. Lazy evaluated , i.e. the data inside RDD is not available or transformed until an action is executed that triggers the execution. Cacheable , i.e. you can hold all the data in a persistent \"storage\" like memory (default and the most preferred) or disk (the least preferred due to access speed). Parallel , i.e. process data in parallel. Typed -- RDD records have types, e.g. Long in RDD[Long] or (Int, String) in RDD[(Int, String)] . Partitioned -- records are partitioned (split into logical partitions) and distributed across nodes in a cluster. Location-Stickiness -- RDD can define < > to compute partitions (as close to the records as possible). NOTE: Preferred location (aka locality preferences or placement preferences or locality info ) is information about the locations of RDD records (that Spark's xref:scheduler:DAGScheduler.adoc#preferred-locations[DAGScheduler] uses to place computing partitions on to have the tasks as close to the data as possible). Computing partitions in a RDD is a distributed process by design and to achieve even data distribution as well as leverage link:spark-data-locality.adoc[data locality] (in distributed systems like HDFS or Cassandra in which data is partitioned by default), they are partitioned to a fixed number of link:spark-rdd-partitions.adoc[partitions] - logical chunks (parts) of data. The logical division is for processing only and internally it is not divided whatsoever. Each partition comprises of records . .RDDs image::spark-rdd-partitioned-distributed.png[align=\"center\"] link:spark-rdd-partitions.adoc[Partitions are the units of parallelism]. You can control the number of partitions of a RDD using link:spark-rdd-partitions.adoc#repartition[repartition] or link:spark-rdd-partitions.adoc#coalesce[coalesce] transformations. Spark tries to be as close to data as possible without wasting time to send data across network by means of link:spark-rdd-shuffle.adoc[RDD shuffling], and creates as many partitions as required to follow the storage layout and thus optimize data access. It leads to a one-to-one mapping between (physical) data in distributed data storage, e.g. HDFS or Cassandra, and partitions. RDDs support two kinds of operations: < > - lazy operations that return another RDD. < > - operations that trigger computation and return values. The motivation to create RDD were ( https://www.cs.berkeley.edu/~matei/papers/2012/nsdi_spark.pdf[after the authors]) two types of applications that current computing frameworks handle inefficiently: iterative algorithms in machine learning and graph computations. interactive data mining tools as ad-hoc queries on the same dataset. The goal is to reuse intermediate in-memory results across multiple data-intensive workloads with no need for copying large amounts of data over the network. Technically, RDDs follow the < > defined by the five main intrinsic properties: [[dependencies]] Parent RDDs (aka xref:rdd:RDD.adoc#dependencies[RDD dependencies]) An array of link:spark-rdd-partitions.adoc[partitions] that a dataset is divided to. A xref:rdd:RDD.adoc#compute[compute] function to do a computation on partitions. An optional xref:rdd:Partitioner.adoc[Partitioner] that defines how keys are hashed, and the pairs partitioned (for key-value RDDs) Optional < > (aka locality info ), i.e. hosts for a partition where the records live or are the closest to read from. This RDD abstraction supports an expressive set of operations without having to modify scheduler for each one. [[context]] An RDD is a named (by name ) and uniquely identified (by id ) entity in a xref:ROOT:SparkContext.adoc[] (available as context property). RDDs live in one and only one xref:ROOT:SparkContext.adoc[] that creates a logical boundary. NOTE: RDDs cannot be shared between SparkContexts (see xref:ROOT:SparkContext.adoc#sparkcontext-and-rdd[SparkContext and RDDs]). An RDD can optionally have a friendly name accessible using name that can be changed using = : scala> val ns = sc.parallelize(0 to 10) ns: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[2] at parallelize at <console>:24 scala> ns.id res0: Int = 2 scala> ns.name res1: String = null scala> ns.name = \"Friendly name\" ns.name: String = Friendly name scala> ns.name res2: String = Friendly name scala> ns.toDebugString res3: String = (8) Friendly name ParallelCollectionRDD[2] at parallelize at <console>:24 [] RDDs are a container of instructions on how to materialize big (arrays of) distributed data, and how to split it into partitions so Spark (using xref:executor:Executor.adoc[executors]) can hold some of them. In general data distribution can help executing processing in parallel so a task processes a chunk of data that it could eventually keep in memory. Spark does jobs in parallel, and RDDs are split into partitions to be processed and written in parallel. Inside a partition, data is processed sequentially. Saving partitions results in part-files instead of one single file (unless there is a single partition). == [[transformations]] Transformations A transformation is a lazy operation on a RDD that returns another RDD, e.g. map , flatMap , filter , reduceByKey , join , cogroup , etc. Find out more in xref:rdd:spark-rdd-transformations.adoc[Transformations]. == [[actions]] Actions An action is an operation that triggers execution of < > and returns a value (to a Spark driver - the user program). TIP: Go in-depth in the section link:spark-rdd-actions.adoc[Actions]. == [[creating-rdds]] Creating RDDs === SparkContext.parallelize One way to create a RDD is with SparkContext.parallelize method. It accepts a collection of elements as shown below ( sc is a SparkContext instance): scala> val rdd = sc.parallelize(1 to 1000) rdd: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[0] at parallelize at <console>:25 You may also want to randomize the sample data: scala> val data = Seq.fill(10)(util.Random.nextInt) data: Seq[Int] = List(-964985204, 1662791, -1820544313, -383666422, -111039198, 310967683, 1114081267, 1244509086, 1797452433, 124035586) scala> val rdd = sc.parallelize(data) rdd: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[0] at parallelize at <console>:29 Given the reason to use Spark to process more data than your own laptop could handle, SparkContext.parallelize is mainly used to learn Spark in the Spark shell. SparkContext.parallelize requires all the data to be available on a single machine - the Spark driver - that eventually hits the limits of your laptop. === SparkContext.makeRDD CAUTION: FIXME What's the use case for makeRDD ? scala> sc.makeRDD(0 to 1000) res0: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[1] at makeRDD at <console>:25 === SparkContext.textFile One of the easiest ways to create an RDD is to use SparkContext.textFile to read files. You can use the local README.md file (and then flatMap over the lines inside to have an RDD of words): scala> val words = sc.textFile(\"README.md\").flatMap(_.split(\"\\\\W+\")).cache words: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[27] at flatMap at <console>:24 NOTE: You link:spark-rdd-caching.adoc[cache] it so the computation is not performed every time you work with words . == [[creating-rdds-from-input]] Creating RDDs from Input Refer to link:spark-io.adoc[Using Input and Output (I/O)] to learn about the IO API to create RDDs. === Transformations RDD transformations by definition transform an RDD into another RDD and hence are the way to create new ones. Refer to < > section to learn more. == RDDs in Web UI It is quite informative to look at RDDs in the Web UI that is at http://localhost:4040 for link:spark-shell.adoc[Spark shell]. Execute the following Spark application (type all the lines in spark-shell ): [source,scala] \u00b6 val ints = sc.parallelize(1 to 100) // <1> ints.setName(\"Hundred ints\") // <2> ints.cache // <3> ints.count // <4> <1> Creates an RDD with hundred of numbers (with as many partitions as possible) <2> Sets the name of the RDD <3> Caches the RDD for performance reasons that also makes it visible in Storage tab in the web UI <4> Executes action (and materializes the RDD) With the above executed, you should see the following in the Web UI: .RDD with custom name image::spark-ui-rdd-name.png[align=\"center\"] Click the name of the RDD (under RDD Name ) and you will get the details of how the RDD is cached. .RDD Storage Info image::spark-ui-storage-hundred-ints.png[align=\"center\"] Execute the following Spark job and you will see how the number of partitions decreases. ints.repartition(2).count .Number of tasks after repartition image::spark-ui-repartition-2.png[align=\"center\"]","title":"Resilient Distributed Dataset"},{"location":"rdd/#resilient-distributed-dataset-rdd","text":"Resilient Distributed Dataset (aka RDD ) is the primary data abstraction in Apache Spark and the core of Spark (that I often refer to as \"Spark Core\"). .The origins of RDD The original paper that gave birth to the concept of RDD is https://cs.stanford.edu/~matei/papers/2012/nsdi_spark.pdf[Resilient Distributed Datasets: A Fault-Tolerant Abstraction for In-Memory Cluster Computing] by Matei Zaharia, et al. An RDD is a description of a fault-tolerant and resilient computation over a distributed collection of records (spread over < >). NOTE: One could compare RDDs to collections in Scala, i.e. a RDD is computed on many JVMs while a Scala collection lives on a single JVM. Using RDD Spark hides data partitioning and so distribution that in turn allowed them to design parallel computational framework with a higher-level programming interface (API) for four mainstream programming languages. The features of RDDs (decomposing the name): Resilient , i.e. fault-tolerant with the help of < > and so able to recompute missing or damaged partitions due to node failures. Distributed with data residing on multiple nodes in a link:spark-cluster.adoc[cluster]. Dataset is a collection of link:spark-rdd-partitions.adoc[partitioned data] with primitive values or values of values, e.g. tuples or other objects (that represent records of the data you work with). .RDDs image::spark-rdds.png[align=\"center\"] From the scaladoc of http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.rdd.RDD[org.apache.spark.rdd.RDD ]: A Resilient Distributed Dataset (RDD), the basic abstraction in Spark. Represents an immutable, partitioned collection of elements that can be operated on in parallel. From the original paper about RDD - https://cs.stanford.edu/~matei/papers/2012/nsdi_spark.pdf[Resilient Distributed Datasets: A Fault-Tolerant Abstraction for In-Memory Cluster Computing]: Resilient Distributed Datasets (RDDs) are a distributed memory abstraction that lets programmers perform in-memory computations on large clusters in a fault-tolerant manner. Beside the above traits (that are directly embedded in the name of the data abstraction - RDD) it has the following additional traits: In-Memory , i.e. data inside RDD is stored in memory as much (size) and long (time) as possible. Immutable or Read-Only , i.e. it does not change once created and can only be transformed using transformations to new RDDs. Lazy evaluated , i.e. the data inside RDD is not available or transformed until an action is executed that triggers the execution. Cacheable , i.e. you can hold all the data in a persistent \"storage\" like memory (default and the most preferred) or disk (the least preferred due to access speed). Parallel , i.e. process data in parallel. Typed -- RDD records have types, e.g. Long in RDD[Long] or (Int, String) in RDD[(Int, String)] . Partitioned -- records are partitioned (split into logical partitions) and distributed across nodes in a cluster. Location-Stickiness -- RDD can define < > to compute partitions (as close to the records as possible). NOTE: Preferred location (aka locality preferences or placement preferences or locality info ) is information about the locations of RDD records (that Spark's xref:scheduler:DAGScheduler.adoc#preferred-locations[DAGScheduler] uses to place computing partitions on to have the tasks as close to the data as possible). Computing partitions in a RDD is a distributed process by design and to achieve even data distribution as well as leverage link:spark-data-locality.adoc[data locality] (in distributed systems like HDFS or Cassandra in which data is partitioned by default), they are partitioned to a fixed number of link:spark-rdd-partitions.adoc[partitions] - logical chunks (parts) of data. The logical division is for processing only and internally it is not divided whatsoever. Each partition comprises of records . .RDDs image::spark-rdd-partitioned-distributed.png[align=\"center\"] link:spark-rdd-partitions.adoc[Partitions are the units of parallelism]. You can control the number of partitions of a RDD using link:spark-rdd-partitions.adoc#repartition[repartition] or link:spark-rdd-partitions.adoc#coalesce[coalesce] transformations. Spark tries to be as close to data as possible without wasting time to send data across network by means of link:spark-rdd-shuffle.adoc[RDD shuffling], and creates as many partitions as required to follow the storage layout and thus optimize data access. It leads to a one-to-one mapping between (physical) data in distributed data storage, e.g. HDFS or Cassandra, and partitions. RDDs support two kinds of operations: < > - lazy operations that return another RDD. < > - operations that trigger computation and return values. The motivation to create RDD were ( https://www.cs.berkeley.edu/~matei/papers/2012/nsdi_spark.pdf[after the authors]) two types of applications that current computing frameworks handle inefficiently: iterative algorithms in machine learning and graph computations. interactive data mining tools as ad-hoc queries on the same dataset. The goal is to reuse intermediate in-memory results across multiple data-intensive workloads with no need for copying large amounts of data over the network. Technically, RDDs follow the < > defined by the five main intrinsic properties: [[dependencies]] Parent RDDs (aka xref:rdd:RDD.adoc#dependencies[RDD dependencies]) An array of link:spark-rdd-partitions.adoc[partitions] that a dataset is divided to. A xref:rdd:RDD.adoc#compute[compute] function to do a computation on partitions. An optional xref:rdd:Partitioner.adoc[Partitioner] that defines how keys are hashed, and the pairs partitioned (for key-value RDDs) Optional < > (aka locality info ), i.e. hosts for a partition where the records live or are the closest to read from. This RDD abstraction supports an expressive set of operations without having to modify scheduler for each one. [[context]] An RDD is a named (by name ) and uniquely identified (by id ) entity in a xref:ROOT:SparkContext.adoc[] (available as context property). RDDs live in one and only one xref:ROOT:SparkContext.adoc[] that creates a logical boundary. NOTE: RDDs cannot be shared between SparkContexts (see xref:ROOT:SparkContext.adoc#sparkcontext-and-rdd[SparkContext and RDDs]). An RDD can optionally have a friendly name accessible using name that can be changed using = : scala> val ns = sc.parallelize(0 to 10) ns: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[2] at parallelize at <console>:24 scala> ns.id res0: Int = 2 scala> ns.name res1: String = null scala> ns.name = \"Friendly name\" ns.name: String = Friendly name scala> ns.name res2: String = Friendly name scala> ns.toDebugString res3: String = (8) Friendly name ParallelCollectionRDD[2] at parallelize at <console>:24 [] RDDs are a container of instructions on how to materialize big (arrays of) distributed data, and how to split it into partitions so Spark (using xref:executor:Executor.adoc[executors]) can hold some of them. In general data distribution can help executing processing in parallel so a task processes a chunk of data that it could eventually keep in memory. Spark does jobs in parallel, and RDDs are split into partitions to be processed and written in parallel. Inside a partition, data is processed sequentially. Saving partitions results in part-files instead of one single file (unless there is a single partition). == [[transformations]] Transformations A transformation is a lazy operation on a RDD that returns another RDD, e.g. map , flatMap , filter , reduceByKey , join , cogroup , etc. Find out more in xref:rdd:spark-rdd-transformations.adoc[Transformations]. == [[actions]] Actions An action is an operation that triggers execution of < > and returns a value (to a Spark driver - the user program). TIP: Go in-depth in the section link:spark-rdd-actions.adoc[Actions]. == [[creating-rdds]] Creating RDDs === SparkContext.parallelize One way to create a RDD is with SparkContext.parallelize method. It accepts a collection of elements as shown below ( sc is a SparkContext instance): scala> val rdd = sc.parallelize(1 to 1000) rdd: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[0] at parallelize at <console>:25 You may also want to randomize the sample data: scala> val data = Seq.fill(10)(util.Random.nextInt) data: Seq[Int] = List(-964985204, 1662791, -1820544313, -383666422, -111039198, 310967683, 1114081267, 1244509086, 1797452433, 124035586) scala> val rdd = sc.parallelize(data) rdd: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[0] at parallelize at <console>:29 Given the reason to use Spark to process more data than your own laptop could handle, SparkContext.parallelize is mainly used to learn Spark in the Spark shell. SparkContext.parallelize requires all the data to be available on a single machine - the Spark driver - that eventually hits the limits of your laptop. === SparkContext.makeRDD CAUTION: FIXME What's the use case for makeRDD ? scala> sc.makeRDD(0 to 1000) res0: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[1] at makeRDD at <console>:25 === SparkContext.textFile One of the easiest ways to create an RDD is to use SparkContext.textFile to read files. You can use the local README.md file (and then flatMap over the lines inside to have an RDD of words): scala> val words = sc.textFile(\"README.md\").flatMap(_.split(\"\\\\W+\")).cache words: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[27] at flatMap at <console>:24 NOTE: You link:spark-rdd-caching.adoc[cache] it so the computation is not performed every time you work with words . == [[creating-rdds-from-input]] Creating RDDs from Input Refer to link:spark-io.adoc[Using Input and Output (I/O)] to learn about the IO API to create RDDs. === Transformations RDD transformations by definition transform an RDD into another RDD and hence are the way to create new ones. Refer to < > section to learn more. == RDDs in Web UI It is quite informative to look at RDDs in the Web UI that is at http://localhost:4040 for link:spark-shell.adoc[Spark shell]. Execute the following Spark application (type all the lines in spark-shell ):","title":"Resilient Distributed Dataset (RDD)"},{"location":"rdd/#sourcescala","text":"val ints = sc.parallelize(1 to 100) // <1> ints.setName(\"Hundred ints\") // <2> ints.cache // <3> ints.count // <4> <1> Creates an RDD with hundred of numbers (with as many partitions as possible) <2> Sets the name of the RDD <3> Caches the RDD for performance reasons that also makes it visible in Storage tab in the web UI <4> Executes action (and materializes the RDD) With the above executed, you should see the following in the Web UI: .RDD with custom name image::spark-ui-rdd-name.png[align=\"center\"] Click the name of the RDD (under RDD Name ) and you will get the details of how the RDD is cached. .RDD Storage Info image::spark-ui-storage-hundred-ints.png[align=\"center\"] Execute the following Spark job and you will see how the number of partitions decreases. ints.repartition(2).count .Number of tasks after repartition image::spark-ui-repartition-2.png[align=\"center\"]","title":"[source,scala]"},{"location":"rdd/Aggregator/","text":"= [[Aggregator]] Aggregator Aggregator is a set of < > used to aggregate data using xref:rdd:PairRDDFunctions.adoc#combineByKeyWithClassTag[PairRDDFunctions.combineByKeyWithClassTag] transformation. Aggregator[K, V, C] is a parameterized type of K keys, V values, and C combiner (partial) values. [[creating-instance]][[aggregation-functions]] Aggregator transforms an RDD[(K, V)] into an RDD[(K, C)] (for a \"combined type\" C) using the functions: [[createCombiner]] createCombiner: V => C [[mergeValue]] mergeValue: (C, V) => C [[mergeCombiners]] mergeCombiners: (C, C) => C Aggregator is used to create a xref:rdd:ShuffleDependency.adoc[ShuffleDependency] and xref:shuffle:ExternalSorter.adoc[ExternalSorter]. == [[combineValuesByKey]] combineValuesByKey Method [source, scala] \u00b6 combineValuesByKey( iter: Iterator[_ <: Product2[K, V]], context: TaskContext): Iterator[(K, C)] combineValuesByKey creates a new xref:shuffle:ExternalAppendOnlyMap.adoc[ExternalAppendOnlyMap] (with the < >). combineValuesByKey requests the ExternalAppendOnlyMap to xref:shuffle:ExternalAppendOnlyMap.adoc#insertAll[insert all key-value pairs] from the given iterator (that is the values of a partition). combineValuesByKey < >. In the end, combineValuesByKey requests the ExternalAppendOnlyMap for an xref:shuffle:ExternalAppendOnlyMap.adoc#iterator[iterator of \"combined\" pairs]. combineValuesByKey is used when: xref:rdd:PairRDDFunctions.adoc#combineByKeyWithClassTag[PairRDDFunctions.combineByKeyWithClassTag] transformation is used (with the same Partitioner as the RDD's) BlockStoreShuffleReader is requested to xref:shuffle:BlockStoreShuffleReader.adoc#read[read combined records for a reduce task] (with the xref:rdd:ShuffleDependency.adoc#mapSideCombine[Map-Size Partial Aggregation Flag] off) == [[combineCombinersByKey]] combineCombinersByKey Method [source, scala] \u00b6 combineCombinersByKey( iter: Iterator[_ <: Product2[K, C]], context: TaskContext): Iterator[(K, C)] combineCombinersByKey...FIXME combineCombinersByKey is used when BlockStoreShuffleReader is requested to xref:shuffle:BlockStoreShuffleReader.adoc#read[read combined records for a reduce task] (with the xref:rdd:ShuffleDependency.adoc#mapSideCombine[Map-Size Partial Aggregation Flag] on). == [[updateMetrics]] Updating Task Metrics [source, scala] \u00b6 updateMetrics( context: TaskContext, map: ExternalAppendOnlyMap[_, _, _]): Unit updateMetrics requests the input xref:scheduler:spark-TaskContext.adoc[TaskContext] for the xref:scheduler:spark-TaskContext.adoc#taskMetrics[TaskMetrics] to update the metrics based on the metrics of the input xref:shuffle:ExternalAppendOnlyMap.adoc[ExternalAppendOnlyMap]: xref:executor:TaskMetrics.adoc#incMemoryBytesSpilled[Increment memory bytes spilled] xref:executor:TaskMetrics.adoc#incDiskBytesSpilled[Increment disk bytes spilled] xref:executor:TaskMetrics.adoc#incPeakExecutionMemory[Increment peak execution memory] updateMetrics is used when Aggregator is requested to < > and < >.","title":"Aggregator"},{"location":"rdd/Aggregator/#source-scala","text":"combineValuesByKey( iter: Iterator[_ <: Product2[K, V]], context: TaskContext): Iterator[(K, C)] combineValuesByKey creates a new xref:shuffle:ExternalAppendOnlyMap.adoc[ExternalAppendOnlyMap] (with the < >). combineValuesByKey requests the ExternalAppendOnlyMap to xref:shuffle:ExternalAppendOnlyMap.adoc#insertAll[insert all key-value pairs] from the given iterator (that is the values of a partition). combineValuesByKey < >. In the end, combineValuesByKey requests the ExternalAppendOnlyMap for an xref:shuffle:ExternalAppendOnlyMap.adoc#iterator[iterator of \"combined\" pairs]. combineValuesByKey is used when: xref:rdd:PairRDDFunctions.adoc#combineByKeyWithClassTag[PairRDDFunctions.combineByKeyWithClassTag] transformation is used (with the same Partitioner as the RDD's) BlockStoreShuffleReader is requested to xref:shuffle:BlockStoreShuffleReader.adoc#read[read combined records for a reduce task] (with the xref:rdd:ShuffleDependency.adoc#mapSideCombine[Map-Size Partial Aggregation Flag] off) == [[combineCombinersByKey]] combineCombinersByKey Method","title":"[source, scala]"},{"location":"rdd/Aggregator/#source-scala_1","text":"combineCombinersByKey( iter: Iterator[_ <: Product2[K, C]], context: TaskContext): Iterator[(K, C)] combineCombinersByKey...FIXME combineCombinersByKey is used when BlockStoreShuffleReader is requested to xref:shuffle:BlockStoreShuffleReader.adoc#read[read combined records for a reduce task] (with the xref:rdd:ShuffleDependency.adoc#mapSideCombine[Map-Size Partial Aggregation Flag] on). == [[updateMetrics]] Updating Task Metrics","title":"[source, scala]"},{"location":"rdd/Aggregator/#source-scala_2","text":"updateMetrics( context: TaskContext, map: ExternalAppendOnlyMap[_, _, _]): Unit updateMetrics requests the input xref:scheduler:spark-TaskContext.adoc[TaskContext] for the xref:scheduler:spark-TaskContext.adoc#taskMetrics[TaskMetrics] to update the metrics based on the metrics of the input xref:shuffle:ExternalAppendOnlyMap.adoc[ExternalAppendOnlyMap]: xref:executor:TaskMetrics.adoc#incMemoryBytesSpilled[Increment memory bytes spilled] xref:executor:TaskMetrics.adoc#incDiskBytesSpilled[Increment disk bytes spilled] xref:executor:TaskMetrics.adoc#incPeakExecutionMemory[Increment peak execution memory] updateMetrics is used when Aggregator is requested to < > and < >.","title":"[source, scala]"},{"location":"rdd/CheckpointRDD/","text":"= CheckpointRDD CheckpointRDD is...FIXME","title":"CheckpointRDD"},{"location":"rdd/HashPartitioner/","text":"= HashPartitioner HashPartitioner is a xref:rdd:Partitioner.adoc[Partitioner] for hash-based partitioning. HashPartitioner is used as the default Partitioner. == [[partitions]][[numPartitions]] Number of Partitions HashPartitioner takes a number of partitions to be created. HashPartitioner uses the number of partitions to find the < > (of a key-value record). == [[getPartition]] Finding Partition ID for Key [source, scala] \u00b6 getPartition(key: Any): Int \u00b6 getPartition returns 0 as the partition ID for null keys. For non- null keys, getPartition uses the key's {java-javadoc-url}/java/lang/Object.html#++hashCode--++[Object.hashCode] modulo the configured < >. For a negative result, getPartition adds the < > (used for the modulo operator) to make it positive. getPartition is part of the xref:rdd:Partitioner.adoc#getPartition[Partitioner] abstraction. == [[equals]] equals Method [source, scala] \u00b6 equals(other: Any): Boolean \u00b6 Two HashPartitioners are considered equal when the < > are the same. == [[hashCode]] hashCode Method [source, scala] \u00b6 hashCode: Int \u00b6 hashCode is the < >.","title":"HashPartitioner"},{"location":"rdd/HashPartitioner/#source-scala","text":"","title":"[source, scala]"},{"location":"rdd/HashPartitioner/#getpartitionkey-any-int","text":"getPartition returns 0 as the partition ID for null keys. For non- null keys, getPartition uses the key's {java-javadoc-url}/java/lang/Object.html#++hashCode--++[Object.hashCode] modulo the configured < >. For a negative result, getPartition adds the < > (used for the modulo operator) to make it positive. getPartition is part of the xref:rdd:Partitioner.adoc#getPartition[Partitioner] abstraction. == [[equals]] equals Method","title":"getPartition(key: Any): Int"},{"location":"rdd/HashPartitioner/#source-scala_1","text":"","title":"[source, scala]"},{"location":"rdd/HashPartitioner/#equalsother-any-boolean","text":"Two HashPartitioners are considered equal when the < > are the same. == [[hashCode]] hashCode Method","title":"equals(other: Any): Boolean"},{"location":"rdd/HashPartitioner/#source-scala_2","text":"","title":"[source, scala]"},{"location":"rdd/HashPartitioner/#hashcode-int","text":"hashCode is the < >.","title":"hashCode: Int"},{"location":"rdd/LocalRDDCheckpointData/","text":"= LocalRDDCheckpointData LocalRDDCheckpointData is...FIXME","title":"LocalRDDCheckpointData"},{"location":"rdd/PairRDDFunctions/","text":"= [[PairRDDFunctions]] PairRDDFunctions :page-toctitle: Transformations PairRDDFunctions is an extension of RDD API to provide additional < > for RDDs of key-value pairs ( RDD[(K, V)] ). PairRDDFunctions is available in RDDs of key-value pairs via Scala implicit conversion. [[transformations]] .PairRDDFunctions' Transformations [cols=\"30m,70\",options=\"header\",width=\"100%\"] |=== | Method | Description | aggregateByKey a| [[aggregateByKey]] [source, scala] \u00b6 aggregateByKey U: ClassTag ( seqOp: (U, V) => U, combOp: (U, U) => U): RDD[(K, U)] aggregateByKey U: ClassTag ( seqOp: (U, V) => U, combOp: (U, U) => U): RDD[(K, U)] aggregateByKey U: ClassTag ( seqOp: (U, V) => U, combOp: (U, U) => U): RDD[(K, U)] | combineByKey a| [[combineByKey]] [source, scala] \u00b6 combineByKey C : RDD[(K, C)] combineByKey C : RDD[(K, C)] combineByKey C : RDD[(K, C)] | countApproxDistinctByKey a| [[countApproxDistinctByKey]] [source, scala] \u00b6 countApproxDistinctByKey( relativeSD: Double = 0.05): RDD[(K, Long)] countApproxDistinctByKey( relativeSD: Double, numPartitions: Int): RDD[(K, Long)] countApproxDistinctByKey( relativeSD: Double, partitioner: Partitioner): RDD[(K, Long)] countApproxDistinctByKey( p: Int, sp: Int, partitioner: Partitioner): RDD[(K, Long)] | flatMapValues a| [[flatMapValues]] [source, scala] \u00b6 flatMapValues U : RDD[(K, U)] | foldByKey a| [[foldByKey]] [source, scala] \u00b6 foldByKey( zeroValue: V)( func: (V, V) => V): RDD[(K, V)] foldByKey( zeroValue: V, numPartitions: Int)( func: (V, V) => V): RDD[(K, V)] foldByKey( zeroValue: V, partitioner: Partitioner)( func: (V, V) => V): RDD[(K, V)] | mapValues a| [[mapValues]] [source, scala] \u00b6 mapValues U : RDD[(K, U)] | partitionBy a| [[partitionBy]] [source, scala] \u00b6 partitionBy( partitioner: Partitioner): RDD[(K, V)] | saveAsHadoopDataset a| [[saveAsHadoopDataset]] [source, scala] \u00b6 saveAsHadoopDataset( conf: JobConf): Unit saveAsHadoopDataset uses the SparkHadoopWriter utility to < > with a < > (for the given Hadoop https://hadoop.apache.org/docs/r2.7.3/api/org/apache/hadoop/mapred/JobConf.html[JobConf ]) | saveAsHadoopFile a| [[saveAsHadoopFile]] [source, scala] \u00b6 saveAsHadoopFile( path: String, keyClass: Class[ ], valueClass: Class[ ], outputFormatClass: Class[_ <: OutputFormat[ , _]], codec: Class[ <: CompressionCodec]): Unit saveAsHadoopFile( path: String, keyClass: Class[ ], valueClass: Class[ ], outputFormatClass: Class[_ <: OutputFormat[ , _]], conf: JobConf = new JobConf(self.context.hadoopConfiguration), codec: Option[Class[ <: CompressionCodec]] = None): Unit saveAsHadoopFile F <: OutputFormat[K, V] (implicit fm: ClassTag[F]): Unit saveAsHadoopFile F <: OutputFormat[K, V] (implicit fm: ClassTag[F]): Unit | saveAsNewAPIHadoopDataset a| [[saveAsNewAPIHadoopDataset]] [source, scala] \u00b6 saveAsNewAPIHadoopDataset( conf: Configuration): Unit Saves this RDD of key-value pairs ( RDD[K,V] ) to any Hadoop-supported storage system with new Hadoop API (using a Hadoop https://hadoop.apache.org/docs/r2.7.3/api/org/apache/hadoop/conf/Configuration.html[Configuration ] object for that storage system). The configuration should set relevant output params (an https://hadoop.apache.org/docs/r2.7.3/api/org/apache/hadoop/mapreduce/OutputFormat.html[output format], output paths, e.g. a table name to write to) in the same way as it would be configured for a Hadoop MapReduce job. saveAsNewAPIHadoopDataset uses the SparkHadoopWriter utility to < > with a < > (for the given Hadoop https://hadoop.apache.org/docs/r2.7.3/api/org/apache/hadoop/conf/Configuration.html[Configuration ]) | saveAsNewAPIHadoopFile a| [[saveAsNewAPIHadoopFile]] [source, scala] \u00b6 saveAsNewAPIHadoopFile( path: String, keyClass: Class[ ], valueClass: Class[ ], outputFormatClass: Class[_ <: NewOutputFormat[_, _]], conf: Configuration = self.context.hadoopConfiguration): Unit saveAsNewAPIHadoopFile F <: NewOutputFormat[K, V] (implicit fm: ClassTag[F]): Unit |=== == [[reduceByKey]][[groupByKey]] groupByKey and reduceByKey reduceByKey is sort of a particular case of < >. You may want to look at the number of partitions from another angle. It may often not be important to have a given number of partitions upfront (at RDD creation time upon link:spark-data-sources.adoc[loading data from data sources]), so only \"regrouping\" the data by key after it is an RDD might be...the key ( pun not intended ). You can use groupByKey or another PairRDDFunctions method to have a key in one processing flow. You could use partitionBy that is available for RDDs to be RDDs of tuples, i.e. PairRDD : rdd.keyBy(_.kind) .partitionBy(new HashPartitioner(PARTITIONS)) .foreachPartition(...) Think of situations where kind has low cardinality or highly skewed distribution and using the technique for partitioning might be not an optimal solution. You could do as follows: rdd.keyBy(_.kind).reduceByKey(....) or mapValues or plenty of other solutions. FIXME, man . == [[combineByKeyWithClassTag]] combineByKeyWithClassTag [source, scala] \u00b6 combineByKeyWithClassTag C (implicit ct: ClassTag[C]): RDD[(K, C)] // <1> combineByKeyWithClassTag C (implicit ct: ClassTag[C]): RDD[(K, C)] // <2> combineByKeyWithClassTag C (implicit ct: ClassTag[C]): RDD[(K, C)] <1> Uses the xref:rdd:Partitioner.adoc#defaultPartitioner[default partitioner] <2> Uses a xref:rdd:HashPartitioner.adoc[HashPartitioner] with the given number of partitions combineByKeyWithClassTag creates an xref:rdd:Aggregator.adoc[Aggregator] for the given aggregation functions. combineByKeyWithClassTag branches off per the given xref:rdd:Partitioner.adoc[Partitioner]. If the input partitioner and the RDD's are the same, combineByKeyWithClassTag simply xref:rdd:spark-rdd-transformations.adoc#mapPartitions[mapPartitions] on the RDD with the following arguments: Iterator of the xref:rdd:Aggregator.adoc#combineValuesByKey[Aggregator] preservesPartitioning flag turned on If the input partitioner is different than the RDD's, combineByKeyWithClassTag creates a xref:rdd:ShuffledRDD.adoc[ShuffledRDD] (with the Serializer, the Aggregator, and the mapSideCombine flag). === [[combineByKeyWithClassTag-usage]] Usage combineByKeyWithClassTag lays the foundation for the following transformations: < > < > < > < > < > < > === [[combineByKeyWithClassTag-requirements]] Requirements combineByKeyWithClassTag requires that the mergeCombiners is defined (not- null ) or throws an IllegalArgumentException: [source,plaintext] \u00b6 mergeCombiners must be defined \u00b6 combineByKeyWithClassTag throws a SparkException for the keys being of type array with the mapSideCombine flag enabled: [source,plaintext] \u00b6 Cannot use map-side combining with array keys. \u00b6 combineByKeyWithClassTag throws a SparkException for the keys being of type array with the partitioner being a xref:rdd:HashPartitioner.adoc[HashPartitioner]: [source,plaintext] \u00b6 HashPartitioner cannot partition array keys. \u00b6 === [[combineByKeyWithClassTag-example]] Example [source,scala] \u00b6 val nums = sc.parallelize(0 to 9, numSlices = 4) val groups = nums.keyBy(_ % 2) def createCombiner(n: Int) = { println(s\"createCombiner( n)\") n } def mergeValue(n1: Int, n2: Int) = { println(s\"mergeValue( n)\") n } def mergeValue(n1: Int, n2: Int) = { println(s\"mergeValue( n1, n2)\") n1 + n2 } def mergeCombiners(c1: Int, c2: Int) = { println(s\"mergeCombiners( n2)\") n1 + n2 } def mergeCombiners(c1: Int, c2: Int) = { println(s\"mergeCombiners( c1, $c2)\") c1 + c2 } val countByGroup = groups.combineByKeyWithClassTag( createCombiner, mergeValue, mergeCombiners) println(countByGroup.toDebugString) /* (4) ShuffledRDD[3] at combineByKeyWithClassTag at :31 [] +-(4) MapPartitionsRDD[1] at keyBy at :25 [] | ParallelCollectionRDD[0] at parallelize at :24 [] */","title":"PairRDDFunctions"},{"location":"rdd/PairRDDFunctions/#source-scala","text":"aggregateByKey U: ClassTag ( seqOp: (U, V) => U, combOp: (U, U) => U): RDD[(K, U)] aggregateByKey U: ClassTag ( seqOp: (U, V) => U, combOp: (U, U) => U): RDD[(K, U)] aggregateByKey U: ClassTag ( seqOp: (U, V) => U, combOp: (U, U) => U): RDD[(K, U)] | combineByKey a| [[combineByKey]]","title":"[source, scala]"},{"location":"rdd/PairRDDFunctions/#source-scala_1","text":"combineByKey C : RDD[(K, C)] combineByKey C : RDD[(K, C)] combineByKey C : RDD[(K, C)] | countApproxDistinctByKey a| [[countApproxDistinctByKey]]","title":"[source, scala]"},{"location":"rdd/PairRDDFunctions/#source-scala_2","text":"countApproxDistinctByKey( relativeSD: Double = 0.05): RDD[(K, Long)] countApproxDistinctByKey( relativeSD: Double, numPartitions: Int): RDD[(K, Long)] countApproxDistinctByKey( relativeSD: Double, partitioner: Partitioner): RDD[(K, Long)] countApproxDistinctByKey( p: Int, sp: Int, partitioner: Partitioner): RDD[(K, Long)] | flatMapValues a| [[flatMapValues]]","title":"[source, scala]"},{"location":"rdd/PairRDDFunctions/#source-scala_3","text":"flatMapValues U : RDD[(K, U)] | foldByKey a| [[foldByKey]]","title":"[source, scala]"},{"location":"rdd/PairRDDFunctions/#source-scala_4","text":"foldByKey( zeroValue: V)( func: (V, V) => V): RDD[(K, V)] foldByKey( zeroValue: V, numPartitions: Int)( func: (V, V) => V): RDD[(K, V)] foldByKey( zeroValue: V, partitioner: Partitioner)( func: (V, V) => V): RDD[(K, V)] | mapValues a| [[mapValues]]","title":"[source, scala]"},{"location":"rdd/PairRDDFunctions/#source-scala_5","text":"mapValues U : RDD[(K, U)] | partitionBy a| [[partitionBy]]","title":"[source, scala]"},{"location":"rdd/PairRDDFunctions/#source-scala_6","text":"partitionBy( partitioner: Partitioner): RDD[(K, V)] | saveAsHadoopDataset a| [[saveAsHadoopDataset]]","title":"[source, scala]"},{"location":"rdd/PairRDDFunctions/#source-scala_7","text":"saveAsHadoopDataset( conf: JobConf): Unit saveAsHadoopDataset uses the SparkHadoopWriter utility to < > with a < > (for the given Hadoop https://hadoop.apache.org/docs/r2.7.3/api/org/apache/hadoop/mapred/JobConf.html[JobConf ]) | saveAsHadoopFile a| [[saveAsHadoopFile]]","title":"[source, scala]"},{"location":"rdd/PairRDDFunctions/#source-scala_8","text":"saveAsHadoopFile( path: String, keyClass: Class[ ], valueClass: Class[ ], outputFormatClass: Class[_ <: OutputFormat[ , _]], codec: Class[ <: CompressionCodec]): Unit saveAsHadoopFile( path: String, keyClass: Class[ ], valueClass: Class[ ], outputFormatClass: Class[_ <: OutputFormat[ , _]], conf: JobConf = new JobConf(self.context.hadoopConfiguration), codec: Option[Class[ <: CompressionCodec]] = None): Unit saveAsHadoopFile F <: OutputFormat[K, V] (implicit fm: ClassTag[F]): Unit saveAsHadoopFile F <: OutputFormat[K, V] (implicit fm: ClassTag[F]): Unit | saveAsNewAPIHadoopDataset a| [[saveAsNewAPIHadoopDataset]]","title":"[source, scala]"},{"location":"rdd/PairRDDFunctions/#source-scala_9","text":"saveAsNewAPIHadoopDataset( conf: Configuration): Unit Saves this RDD of key-value pairs ( RDD[K,V] ) to any Hadoop-supported storage system with new Hadoop API (using a Hadoop https://hadoop.apache.org/docs/r2.7.3/api/org/apache/hadoop/conf/Configuration.html[Configuration ] object for that storage system). The configuration should set relevant output params (an https://hadoop.apache.org/docs/r2.7.3/api/org/apache/hadoop/mapreduce/OutputFormat.html[output format], output paths, e.g. a table name to write to) in the same way as it would be configured for a Hadoop MapReduce job. saveAsNewAPIHadoopDataset uses the SparkHadoopWriter utility to < > with a < > (for the given Hadoop https://hadoop.apache.org/docs/r2.7.3/api/org/apache/hadoop/conf/Configuration.html[Configuration ]) | saveAsNewAPIHadoopFile a| [[saveAsNewAPIHadoopFile]]","title":"[source, scala]"},{"location":"rdd/PairRDDFunctions/#source-scala_10","text":"saveAsNewAPIHadoopFile( path: String, keyClass: Class[ ], valueClass: Class[ ], outputFormatClass: Class[_ <: NewOutputFormat[_, _]], conf: Configuration = self.context.hadoopConfiguration): Unit saveAsNewAPIHadoopFile F <: NewOutputFormat[K, V] (implicit fm: ClassTag[F]): Unit |=== == [[reduceByKey]][[groupByKey]] groupByKey and reduceByKey reduceByKey is sort of a particular case of < >. You may want to look at the number of partitions from another angle. It may often not be important to have a given number of partitions upfront (at RDD creation time upon link:spark-data-sources.adoc[loading data from data sources]), so only \"regrouping\" the data by key after it is an RDD might be...the key ( pun not intended ). You can use groupByKey or another PairRDDFunctions method to have a key in one processing flow. You could use partitionBy that is available for RDDs to be RDDs of tuples, i.e. PairRDD : rdd.keyBy(_.kind) .partitionBy(new HashPartitioner(PARTITIONS)) .foreachPartition(...) Think of situations where kind has low cardinality or highly skewed distribution and using the technique for partitioning might be not an optimal solution. You could do as follows: rdd.keyBy(_.kind).reduceByKey(....) or mapValues or plenty of other solutions. FIXME, man . == [[combineByKeyWithClassTag]] combineByKeyWithClassTag","title":"[source, scala]"},{"location":"rdd/PairRDDFunctions/#source-scala_11","text":"combineByKeyWithClassTag C (implicit ct: ClassTag[C]): RDD[(K, C)] // <1> combineByKeyWithClassTag C (implicit ct: ClassTag[C]): RDD[(K, C)] // <2> combineByKeyWithClassTag C (implicit ct: ClassTag[C]): RDD[(K, C)] <1> Uses the xref:rdd:Partitioner.adoc#defaultPartitioner[default partitioner] <2> Uses a xref:rdd:HashPartitioner.adoc[HashPartitioner] with the given number of partitions combineByKeyWithClassTag creates an xref:rdd:Aggregator.adoc[Aggregator] for the given aggregation functions. combineByKeyWithClassTag branches off per the given xref:rdd:Partitioner.adoc[Partitioner]. If the input partitioner and the RDD's are the same, combineByKeyWithClassTag simply xref:rdd:spark-rdd-transformations.adoc#mapPartitions[mapPartitions] on the RDD with the following arguments: Iterator of the xref:rdd:Aggregator.adoc#combineValuesByKey[Aggregator] preservesPartitioning flag turned on If the input partitioner is different than the RDD's, combineByKeyWithClassTag creates a xref:rdd:ShuffledRDD.adoc[ShuffledRDD] (with the Serializer, the Aggregator, and the mapSideCombine flag). === [[combineByKeyWithClassTag-usage]] Usage combineByKeyWithClassTag lays the foundation for the following transformations: < > < > < > < > < > < > === [[combineByKeyWithClassTag-requirements]] Requirements combineByKeyWithClassTag requires that the mergeCombiners is defined (not- null ) or throws an IllegalArgumentException:","title":"[source, scala]"},{"location":"rdd/PairRDDFunctions/#sourceplaintext","text":"","title":"[source,plaintext]"},{"location":"rdd/PairRDDFunctions/#mergecombiners-must-be-defined","text":"combineByKeyWithClassTag throws a SparkException for the keys being of type array with the mapSideCombine flag enabled:","title":"mergeCombiners must be defined"},{"location":"rdd/PairRDDFunctions/#sourceplaintext_1","text":"","title":"[source,plaintext]"},{"location":"rdd/PairRDDFunctions/#cannot-use-map-side-combining-with-array-keys","text":"combineByKeyWithClassTag throws a SparkException for the keys being of type array with the partitioner being a xref:rdd:HashPartitioner.adoc[HashPartitioner]:","title":"Cannot use map-side combining with array keys."},{"location":"rdd/PairRDDFunctions/#sourceplaintext_2","text":"","title":"[source,plaintext]"},{"location":"rdd/PairRDDFunctions/#hashpartitioner-cannot-partition-array-keys","text":"=== [[combineByKeyWithClassTag-example]] Example","title":"HashPartitioner cannot partition array keys."},{"location":"rdd/PairRDDFunctions/#sourcescala","text":"val nums = sc.parallelize(0 to 9, numSlices = 4) val groups = nums.keyBy(_ % 2) def createCombiner(n: Int) = { println(s\"createCombiner( n)\") n } def mergeValue(n1: Int, n2: Int) = { println(s\"mergeValue( n)\") n } def mergeValue(n1: Int, n2: Int) = { println(s\"mergeValue( n1, n2)\") n1 + n2 } def mergeCombiners(c1: Int, c2: Int) = { println(s\"mergeCombiners( n2)\") n1 + n2 } def mergeCombiners(c1: Int, c2: Int) = { println(s\"mergeCombiners( c1, $c2)\") c1 + c2 } val countByGroup = groups.combineByKeyWithClassTag( createCombiner, mergeValue, mergeCombiners) println(countByGroup.toDebugString) /* (4) ShuffledRDD[3] at combineByKeyWithClassTag at :31 [] +-(4) MapPartitionsRDD[1] at keyBy at :25 [] | ParallelCollectionRDD[0] at parallelize at :24 [] */","title":"[source,scala]"},{"location":"rdd/Partitioner/","text":"= Partitioner Partitioner is an abstraction to define how the elements in a key-value pair RDD are partitioned by key. Partitioner < > (from 0 to < > - 1). Partitioner is used to ensure that records for a given key have to reside on a single partition. == [[implementations]] Available Partitioners [cols=\"30,70\",options=\"header\",width=\"100%\"] |=== | Partitioner | Description | xref:rdd:HashPartitioner.adoc[HashPartitioner] | [[HashPartitioner]] Hash-based partitioning | xref:rdd:RangePartitioner.adoc[RangePartitioner] | [[RangePartitioner]] |=== == [[numPartitions]] numPartitions Method [source, scala] \u00b6 numPartitions: Int \u00b6 numPartitions is the number of partition to use for < >. numPartitions is used when...FIXME == [[getPartition]] getPartition Method [source, scala] \u00b6 getPartition(key: Any): Int \u00b6 getPartition maps a given key to a partition ID (from 0 to < > - 1) getPartition is used when...FIXME == [[defaultPartitioner]] defaultPartitioner Method [source, scala] \u00b6 defaultPartitioner( rdd: RDD[ ], others: RDD[ ]*): Partitioner defaultPartitioner...FIXME defaultPartitioner is used when...FIXME","title":"Partitioner"},{"location":"rdd/Partitioner/#source-scala","text":"","title":"[source, scala]"},{"location":"rdd/Partitioner/#numpartitions-int","text":"numPartitions is the number of partition to use for < >. numPartitions is used when...FIXME == [[getPartition]] getPartition Method","title":"numPartitions: Int"},{"location":"rdd/Partitioner/#source-scala_1","text":"","title":"[source, scala]"},{"location":"rdd/Partitioner/#getpartitionkey-any-int","text":"getPartition maps a given key to a partition ID (from 0 to < > - 1) getPartition is used when...FIXME == [[defaultPartitioner]] defaultPartitioner Method","title":"getPartition(key: Any): Int"},{"location":"rdd/Partitioner/#source-scala_2","text":"defaultPartitioner( rdd: RDD[ ], others: RDD[ ]*): Partitioner defaultPartitioner...FIXME defaultPartitioner is used when...FIXME","title":"[source, scala]"},{"location":"rdd/RDD/","text":"= [[RDD]] RDD -- Description of Distributed Computation :navtitle: RDD [[T]] RDD is a description of a fault-tolerant and resilient computation over a possibly distributed collection of records (of type T ). == [[contract]] RDD Contract === [[compute]] Computing Partition (in TaskContext) [source, scala] \u00b6 compute( split: Partition, context: TaskContext): Iterator[T] compute computes the input split xref:rdd:spark-rdd-partitions.adoc[partition] in the xref:scheduler:spark-TaskContext.adoc[TaskContext] to produce a collection of values (of type T ). compute is implemented by any type of RDD in Spark and is called every time the records are requested unless RDD is xref:rdd:spark-rdd-caching.adoc[cached] or xref:ROOT:rdd-checkpointing.adoc[checkpointed] (and the records can be read from an external storage, but this time closer to the compute node). When an RDD is xref:rdd:spark-rdd-caching.adoc[cached], for specified xref:storage:StorageLevel.adoc[storage levels] (i.e. all but NONE )...FIXME compute runs on the xref:ROOT:spark-driver.adoc[driver]. compute is used when RDD is requested to < >. === [[getPartitions]] Partitions [source, scala] \u00b6 getPartitions: Array[Partition] \u00b6 getPartitions is used when RDD is requested for the < > (called only once as the value is cached afterwards). === [[getDependencies]] Dependencies [source, scala] \u00b6 getDependencies: Seq[Dependency[_]] \u00b6 getDependencies is used when RDD is requested for the < > (called only once as the value is cached afterwards). === [[getPreferredLocations]] Preferred Locations (Placement Preferences) [source, scala] \u00b6 getPreferredLocations( split: Partition): Seq[String] = Nil getPreferredLocations is used when RDD is requested for the < > of a given xref:rdd:spark-rdd-Partition.adoc[partition]. === [[partitioner]] Partitioner [source, scala] \u00b6 partitioner: Option[Partitioner] = None \u00b6 RDD can have a xref:rdd:Partitioner.adoc[Partitioner] defined. == [[extensions]][[implementations]] (Subset of) Available RDDs [cols=\"30,70\",options=\"header\",width=\"100%\"] |=== | RDD | Description | xref:rdd:spark-rdd-CoGroupedRDD.adoc[CoGroupedRDD] | [[CoGroupedRDD]] | CoalescedRDD | [[CoalescedRDD]] Result of xref:rdd:spark-rdd-partitions.adoc#repartition[repartition] or xref:rdd:spark-rdd-partitions.adoc#coalesce[coalesce] transformations | xref:rdd:spark-rdd-HadoopRDD.adoc[HadoopRDD] | [[HadoopRDD]] Allows for reading data stored in HDFS using the older MapReduce API. The most notable use case is the return RDD of SparkContext.textFile . | xref:rdd:spark-rdd-MapPartitionsRDD.adoc[MapPartitionsRDD] | [[MapPartitionsRDD]] Result of calling map-like operations (e.g. map , flatMap , filter , xref:rdd:spark-rdd-transformations.adoc#mapPartitions[mapPartitions]) | xref:rdd:spark-rdd-ParallelCollectionRDD.adoc[ParallelCollectionRDD] | [[ParallelCollectionRDD]] | xref:rdd:ShuffledRDD.adoc[ShuffledRDD] | [[ShuffledRDD]] Result of \"shuffle\" operators (e.g. xref:rdd:spark-rdd-partitions.adoc#repartition[repartition] or xref:rdd:spark-rdd-partitions.adoc#coalesce[coalesce]) |=== == [[creating-instance]] Creating Instance RDD takes the following to be created: [[_sc]] xref:ROOT:SparkContext.adoc[] [[deps]] Parent RDDs , i.e. xref:rdd:spark-rdd-Dependency.adoc[Dependencies] (that have to be all computed successfully before this RDD) RDD is an abstract class and cannot be created directly. It is created indirectly for the < >. == [[storageLevel]][[getStorageLevel]] StorageLevel RDD can have a xref:storage:StorageLevel.adoc[StorageLevel] specified. The default StorageLevel is xref:storage:StorageLevel.adoc#NONE[NONE]. storageLevel can be specified using < > method. storageLevel becomes NONE again after < >. The current StorageLevel is available using getStorageLevel method. [source, scala] \u00b6 getStorageLevel: StorageLevel \u00b6 == [[id]] Unique Identifier [source, scala] \u00b6 id: Int \u00b6 id is an unique identifier (aka RDD ID ) in the given <<_sc, SparkContext>>. id requests the < > for xref:ROOT:SparkContext.adoc#newRddId[newRddId] right when RDD is created. == [[isBarrier_]][[isBarrier]] Barrier Stage An RDD can be part of a xref:ROOT:spark-barrier-execution-mode.adoc#barrier-stage[barrier stage]. By default, isBarrier flag is enabled ( true ) when: . There are no xref:rdd:ShuffleDependency.adoc[ShuffleDependencies] among the < > . There is at least one xref:rdd:spark-rdd-Dependency.adoc#rdd[parent RDD] that has the flag enabled xref:rdd:ShuffledRDD.adoc[ShuffledRDD] has the flag always disabled. xref:rdd:spark-rdd-MapPartitionsRDD.adoc[MapPartitionsRDD] is the only one RDD that can have the flag enabled. == [[getOrCompute]] Getting Or Computing RDD Partition [source, scala] \u00b6 getOrCompute( partition: Partition, context: TaskContext): Iterator[T] getOrCompute creates a xref:storage:BlockId.adoc#RDDBlockId[RDDBlockId] for the < > and the link:spark-rdd-Partition.adoc#index[partition index]. getOrCompute requests the BlockManager to xref:storage:BlockManager.adoc#getOrElseUpdate[getOrElseUpdate] for the block ID (with the < > and the makeIterator function). NOTE: getOrCompute uses xref:core:SparkEnv.adoc#get[SparkEnv] to access the current xref:core:SparkEnv.adoc#blockManager[BlockManager]. [[getOrCompute-readCachedBlock]] getOrCompute records whether...FIXME (readCachedBlock) getOrCompute branches off per the response from the xref:storage:BlockManager.adoc#getOrElseUpdate[BlockManager] and whether the internal readCachedBlock flag is now on or still off. In either case, getOrCompute creates an link:spark-InterruptibleIterator.adoc[InterruptibleIterator]. NOTE: link:spark-InterruptibleIterator.adoc[InterruptibleIterator] simply delegates to a wrapped internal Iterator , but allows for link:spark-TaskContext.adoc#isInterrupted[task killing functionality]. For a BlockResult available and readCachedBlock flag on, getOrCompute ...FIXME For a BlockResult available and readCachedBlock flag off, getOrCompute ...FIXME NOTE: The BlockResult could be found in a local block manager or fetched from a remote block manager. It may also have been stored (persisted) just now. In either case, the BlockResult is available (and xref:storage:BlockManager.adoc#getOrElseUpdate[BlockManager.getOrElseUpdate] gives a Left value with the BlockResult ). For Right(iter) (regardless of the value of readCachedBlock flag since...FIXME), getOrCompute ...FIXME NOTE: xref:storage:BlockManager.adoc#getOrElseUpdate[BlockManager.getOrElseUpdate] gives a Right(iter) value to indicate an error with a block. NOTE: getOrCompute is used on Spark executors. NOTE: getOrCompute is used exclusively when RDD is requested for the < >. == [[dependencies]] RDD Dependencies [source, scala] \u00b6 dependencies: Seq[Dependency[_]] \u00b6 dependencies returns the link:spark-rdd-Dependency.adoc[dependencies of a RDD]. NOTE: dependencies is a final method that no class in Spark can ever override. Internally, dependencies checks out whether the RDD is xref:ROOT:rdd-checkpointing.adoc[checkpointed] and acts accordingly. For a RDD being checkpointed, dependencies returns a single-element collection with a link:spark-rdd-NarrowDependency.adoc#OneToOneDependency[OneToOneDependency]. For a non-checkpointed RDD, dependencies collection is computed using < getDependencies method>>. NOTE: getDependencies method is an abstract method that custom RDDs are required to provide. == [[iterator]] Accessing Records For Partition Lazily [source, scala] \u00b6 iterator( split: Partition, context: TaskContext): Iterator[T] iterator < split partition>> when xref:rdd:spark-rdd-caching.adoc[cached] or < >. == [[checkpointRDD]] Getting CheckpointRDD [source, scala] \u00b6 checkpointRDD: Option[CheckpointRDD[T]] \u00b6 checkpointRDD gives the CheckpointRDD from the < > internal registry if available (if the RDD was checkpointed). checkpointRDD is used when RDD is requested for the < >, < > and < >. == [[isCheckpointedAndMaterialized]] isCheckpointedAndMaterialized Method [source, scala] \u00b6 isCheckpointedAndMaterialized: Boolean \u00b6 isCheckpointedAndMaterialized...FIXME isCheckpointedAndMaterialized is used when RDD is requested to < >, < > and < >. == [[getNarrowAncestors]] getNarrowAncestors Method [source, scala] \u00b6 getNarrowAncestors: Seq[RDD[_]] \u00b6 getNarrowAncestors...FIXME getNarrowAncestors is used when StageInfo is requested to xref:scheduler:spark-scheduler-StageInfo.adoc#fromStage[fromStage]. == [[toLocalIterator]] toLocalIterator Method [source, scala] \u00b6 toLocalIterator: Iterator[T] \u00b6 toLocalIterator...FIXME == [[persist]] Persisting RDD [source, scala] \u00b6 persist(): this.type persist( newLevel: StorageLevel): this.type Refer to xref:rdd:spark-rdd-caching.adoc#persist[Persisting RDD]. == [[persist-internal]] persist Internal Method [source, scala] \u00b6 persist( newLevel: StorageLevel, allowOverride: Boolean): this.type persist...FIXME persist (private) is used when RDD is requested to < > and < >. == [[unpersist]] unpersist Method [source, scala] \u00b6 unpersist(blocking: Boolean = true): this.type \u00b6 unpersist...FIXME == [[localCheckpoint]] localCheckpoint Method [source, scala] \u00b6 localCheckpoint(): this.type \u00b6 localCheckpoint marks this RDD for xref:ROOT:rdd-checkpointing.adoc[local checkpointing] using Spark's caching layer. == [[computeOrReadCheckpoint]] Computing Partition or Reading From Checkpoint [source, scala] \u00b6 computeOrReadCheckpoint( split: Partition, context: TaskContext): Iterator[T] computeOrReadCheckpoint reads split partition from a checkpoint (< >) or < > yourself. computeOrReadCheckpoint is used when RDD is requested to < > or < >. == [[getNumPartitions]] Getting Number of Partitions [source, scala] \u00b6 getNumPartitions: Int \u00b6 getNumPartitions gives the number of partitions of a RDD. [source, scala] \u00b6 scala> sc.textFile(\"README.md\").getNumPartitions res0: Int = 2 scala> sc.textFile(\"README.md\", 5).getNumPartitions res1: Int = 5 == [[preferredLocations]] Defining Placement Preferences of RDD Partition [source, scala] \u00b6 preferredLocations( split: Partition): Seq[String] preferredLocations requests the CheckpointRDD for < > (if the RDD is checkpointed) or < >. preferredLocations is a template method that uses < > that custom RDDs can override to specify placement preferences for a partition. getPreferredLocations defines no placement preferences by default. preferredLocations is mainly used when DAGScheduler is requested to xref:scheduler:DAGScheduler.adoc#getPreferredLocs[compute the preferred locations for missing partitions]. == [[partitions]] Accessing RDD Partitions [source, scala] \u00b6 partitions: Array[Partition] \u00b6 partitions returns the xref:rdd:spark-rdd-partitions.adoc[Partitions] of a RDD . partitions requests CheckpointRDD for the < > (if the RDD is checkpointed) or < > and cache (in < > internal registry that is used next time). Partitions have the property that their internal index should be equal to their position in the owning RDD. == [[markCheckpointed]] markCheckpointed Method [source, scala] \u00b6 markCheckpointed(): Unit \u00b6 markCheckpointed...FIXME markCheckpointed is used when...FIXME == [[doCheckpoint]] doCheckpoint Method [source, scala] \u00b6 doCheckpoint(): Unit \u00b6 doCheckpoint...FIXME doCheckpoint is used when SparkContext is requested to xref:ROOT:SparkContext.adoc#runJob[run a job (synchronously)]. == [[checkpoint]] Reliable Checkpointing -- checkpoint Method [source, scala] \u00b6 checkpoint(): Unit \u00b6 checkpoint...FIXME checkpoint is used when...FIXME == [[isReliablyCheckpointed]] isReliablyCheckpointed Method [source, scala] \u00b6 isReliablyCheckpointed: Boolean \u00b6 isReliablyCheckpointed...FIXME isReliablyCheckpointed is used when...FIXME == [[getCheckpointFile]] getCheckpointFile Method [source, scala] \u00b6 getCheckpointFile: Option[String] \u00b6 getCheckpointFile...FIXME getCheckpointFile is used when...FIXME","title":"RDD"},{"location":"rdd/RDD/#source-scala","text":"compute( split: Partition, context: TaskContext): Iterator[T] compute computes the input split xref:rdd:spark-rdd-partitions.adoc[partition] in the xref:scheduler:spark-TaskContext.adoc[TaskContext] to produce a collection of values (of type T ). compute is implemented by any type of RDD in Spark and is called every time the records are requested unless RDD is xref:rdd:spark-rdd-caching.adoc[cached] or xref:ROOT:rdd-checkpointing.adoc[checkpointed] (and the records can be read from an external storage, but this time closer to the compute node). When an RDD is xref:rdd:spark-rdd-caching.adoc[cached], for specified xref:storage:StorageLevel.adoc[storage levels] (i.e. all but NONE )...FIXME compute runs on the xref:ROOT:spark-driver.adoc[driver]. compute is used when RDD is requested to < >. === [[getPartitions]] Partitions","title":"[source, scala]"},{"location":"rdd/RDD/#source-scala_1","text":"","title":"[source, scala]"},{"location":"rdd/RDD/#getpartitions-arraypartition","text":"getPartitions is used when RDD is requested for the < > (called only once as the value is cached afterwards). === [[getDependencies]] Dependencies","title":"getPartitions: Array[Partition]"},{"location":"rdd/RDD/#source-scala_2","text":"","title":"[source, scala]"},{"location":"rdd/RDD/#getdependencies-seqdependency_","text":"getDependencies is used when RDD is requested for the < > (called only once as the value is cached afterwards). === [[getPreferredLocations]] Preferred Locations (Placement Preferences)","title":"getDependencies: Seq[Dependency[_]]"},{"location":"rdd/RDD/#source-scala_3","text":"getPreferredLocations( split: Partition): Seq[String] = Nil getPreferredLocations is used when RDD is requested for the < > of a given xref:rdd:spark-rdd-Partition.adoc[partition]. === [[partitioner]] Partitioner","title":"[source, scala]"},{"location":"rdd/RDD/#source-scala_4","text":"","title":"[source, scala]"},{"location":"rdd/RDD/#partitioner-optionpartitioner-none","text":"RDD can have a xref:rdd:Partitioner.adoc[Partitioner] defined. == [[extensions]][[implementations]] (Subset of) Available RDDs [cols=\"30,70\",options=\"header\",width=\"100%\"] |=== | RDD | Description | xref:rdd:spark-rdd-CoGroupedRDD.adoc[CoGroupedRDD] | [[CoGroupedRDD]] | CoalescedRDD | [[CoalescedRDD]] Result of xref:rdd:spark-rdd-partitions.adoc#repartition[repartition] or xref:rdd:spark-rdd-partitions.adoc#coalesce[coalesce] transformations | xref:rdd:spark-rdd-HadoopRDD.adoc[HadoopRDD] | [[HadoopRDD]] Allows for reading data stored in HDFS using the older MapReduce API. The most notable use case is the return RDD of SparkContext.textFile . | xref:rdd:spark-rdd-MapPartitionsRDD.adoc[MapPartitionsRDD] | [[MapPartitionsRDD]] Result of calling map-like operations (e.g. map , flatMap , filter , xref:rdd:spark-rdd-transformations.adoc#mapPartitions[mapPartitions]) | xref:rdd:spark-rdd-ParallelCollectionRDD.adoc[ParallelCollectionRDD] | [[ParallelCollectionRDD]] | xref:rdd:ShuffledRDD.adoc[ShuffledRDD] | [[ShuffledRDD]] Result of \"shuffle\" operators (e.g. xref:rdd:spark-rdd-partitions.adoc#repartition[repartition] or xref:rdd:spark-rdd-partitions.adoc#coalesce[coalesce]) |=== == [[creating-instance]] Creating Instance RDD takes the following to be created: [[_sc]] xref:ROOT:SparkContext.adoc[] [[deps]] Parent RDDs , i.e. xref:rdd:spark-rdd-Dependency.adoc[Dependencies] (that have to be all computed successfully before this RDD) RDD is an abstract class and cannot be created directly. It is created indirectly for the < >. == [[storageLevel]][[getStorageLevel]] StorageLevel RDD can have a xref:storage:StorageLevel.adoc[StorageLevel] specified. The default StorageLevel is xref:storage:StorageLevel.adoc#NONE[NONE]. storageLevel can be specified using < > method. storageLevel becomes NONE again after < >. The current StorageLevel is available using getStorageLevel method.","title":"partitioner: Option[Partitioner] = None"},{"location":"rdd/RDD/#source-scala_5","text":"","title":"[source, scala]"},{"location":"rdd/RDD/#getstoragelevel-storagelevel","text":"== [[id]] Unique Identifier","title":"getStorageLevel: StorageLevel"},{"location":"rdd/RDD/#source-scala_6","text":"","title":"[source, scala]"},{"location":"rdd/RDD/#id-int","text":"id is an unique identifier (aka RDD ID ) in the given <<_sc, SparkContext>>. id requests the < > for xref:ROOT:SparkContext.adoc#newRddId[newRddId] right when RDD is created. == [[isBarrier_]][[isBarrier]] Barrier Stage An RDD can be part of a xref:ROOT:spark-barrier-execution-mode.adoc#barrier-stage[barrier stage]. By default, isBarrier flag is enabled ( true ) when: . There are no xref:rdd:ShuffleDependency.adoc[ShuffleDependencies] among the < > . There is at least one xref:rdd:spark-rdd-Dependency.adoc#rdd[parent RDD] that has the flag enabled xref:rdd:ShuffledRDD.adoc[ShuffledRDD] has the flag always disabled. xref:rdd:spark-rdd-MapPartitionsRDD.adoc[MapPartitionsRDD] is the only one RDD that can have the flag enabled. == [[getOrCompute]] Getting Or Computing RDD Partition","title":"id: Int"},{"location":"rdd/RDD/#source-scala_7","text":"getOrCompute( partition: Partition, context: TaskContext): Iterator[T] getOrCompute creates a xref:storage:BlockId.adoc#RDDBlockId[RDDBlockId] for the < > and the link:spark-rdd-Partition.adoc#index[partition index]. getOrCompute requests the BlockManager to xref:storage:BlockManager.adoc#getOrElseUpdate[getOrElseUpdate] for the block ID (with the < > and the makeIterator function). NOTE: getOrCompute uses xref:core:SparkEnv.adoc#get[SparkEnv] to access the current xref:core:SparkEnv.adoc#blockManager[BlockManager]. [[getOrCompute-readCachedBlock]] getOrCompute records whether...FIXME (readCachedBlock) getOrCompute branches off per the response from the xref:storage:BlockManager.adoc#getOrElseUpdate[BlockManager] and whether the internal readCachedBlock flag is now on or still off. In either case, getOrCompute creates an link:spark-InterruptibleIterator.adoc[InterruptibleIterator]. NOTE: link:spark-InterruptibleIterator.adoc[InterruptibleIterator] simply delegates to a wrapped internal Iterator , but allows for link:spark-TaskContext.adoc#isInterrupted[task killing functionality]. For a BlockResult available and readCachedBlock flag on, getOrCompute ...FIXME For a BlockResult available and readCachedBlock flag off, getOrCompute ...FIXME NOTE: The BlockResult could be found in a local block manager or fetched from a remote block manager. It may also have been stored (persisted) just now. In either case, the BlockResult is available (and xref:storage:BlockManager.adoc#getOrElseUpdate[BlockManager.getOrElseUpdate] gives a Left value with the BlockResult ). For Right(iter) (regardless of the value of readCachedBlock flag since...FIXME), getOrCompute ...FIXME NOTE: xref:storage:BlockManager.adoc#getOrElseUpdate[BlockManager.getOrElseUpdate] gives a Right(iter) value to indicate an error with a block. NOTE: getOrCompute is used on Spark executors. NOTE: getOrCompute is used exclusively when RDD is requested for the < >. == [[dependencies]] RDD Dependencies","title":"[source, scala]"},{"location":"rdd/RDD/#source-scala_8","text":"","title":"[source, scala]"},{"location":"rdd/RDD/#dependencies-seqdependency_","text":"dependencies returns the link:spark-rdd-Dependency.adoc[dependencies of a RDD]. NOTE: dependencies is a final method that no class in Spark can ever override. Internally, dependencies checks out whether the RDD is xref:ROOT:rdd-checkpointing.adoc[checkpointed] and acts accordingly. For a RDD being checkpointed, dependencies returns a single-element collection with a link:spark-rdd-NarrowDependency.adoc#OneToOneDependency[OneToOneDependency]. For a non-checkpointed RDD, dependencies collection is computed using < getDependencies method>>. NOTE: getDependencies method is an abstract method that custom RDDs are required to provide. == [[iterator]] Accessing Records For Partition Lazily","title":"dependencies: Seq[Dependency[_]]"},{"location":"rdd/RDD/#source-scala_9","text":"iterator( split: Partition, context: TaskContext): Iterator[T] iterator < split partition>> when xref:rdd:spark-rdd-caching.adoc[cached] or < >. == [[checkpointRDD]] Getting CheckpointRDD","title":"[source, scala]"},{"location":"rdd/RDD/#source-scala_10","text":"","title":"[source, scala]"},{"location":"rdd/RDD/#checkpointrdd-optioncheckpointrddt","text":"checkpointRDD gives the CheckpointRDD from the < > internal registry if available (if the RDD was checkpointed). checkpointRDD is used when RDD is requested for the < >, < > and < >. == [[isCheckpointedAndMaterialized]] isCheckpointedAndMaterialized Method","title":"checkpointRDD: Option[CheckpointRDD[T]]"},{"location":"rdd/RDD/#source-scala_11","text":"","title":"[source, scala]"},{"location":"rdd/RDD/#ischeckpointedandmaterialized-boolean","text":"isCheckpointedAndMaterialized...FIXME isCheckpointedAndMaterialized is used when RDD is requested to < >, < > and < >. == [[getNarrowAncestors]] getNarrowAncestors Method","title":"isCheckpointedAndMaterialized: Boolean"},{"location":"rdd/RDD/#source-scala_12","text":"","title":"[source, scala]"},{"location":"rdd/RDD/#getnarrowancestors-seqrdd_","text":"getNarrowAncestors...FIXME getNarrowAncestors is used when StageInfo is requested to xref:scheduler:spark-scheduler-StageInfo.adoc#fromStage[fromStage]. == [[toLocalIterator]] toLocalIterator Method","title":"getNarrowAncestors: Seq[RDD[_]]"},{"location":"rdd/RDD/#source-scala_13","text":"","title":"[source, scala]"},{"location":"rdd/RDD/#tolocaliterator-iteratort","text":"toLocalIterator...FIXME == [[persist]] Persisting RDD","title":"toLocalIterator: Iterator[T]"},{"location":"rdd/RDD/#source-scala_14","text":"persist(): this.type persist( newLevel: StorageLevel): this.type Refer to xref:rdd:spark-rdd-caching.adoc#persist[Persisting RDD]. == [[persist-internal]] persist Internal Method","title":"[source, scala]"},{"location":"rdd/RDD/#source-scala_15","text":"persist( newLevel: StorageLevel, allowOverride: Boolean): this.type persist...FIXME persist (private) is used when RDD is requested to < > and < >. == [[unpersist]] unpersist Method","title":"[source, scala]"},{"location":"rdd/RDD/#source-scala_16","text":"","title":"[source, scala]"},{"location":"rdd/RDD/#unpersistblocking-boolean-true-thistype","text":"unpersist...FIXME == [[localCheckpoint]] localCheckpoint Method","title":"unpersist(blocking: Boolean = true): this.type"},{"location":"rdd/RDD/#source-scala_17","text":"","title":"[source, scala]"},{"location":"rdd/RDD/#localcheckpoint-thistype","text":"localCheckpoint marks this RDD for xref:ROOT:rdd-checkpointing.adoc[local checkpointing] using Spark's caching layer. == [[computeOrReadCheckpoint]] Computing Partition or Reading From Checkpoint","title":"localCheckpoint(): this.type"},{"location":"rdd/RDD/#source-scala_18","text":"computeOrReadCheckpoint( split: Partition, context: TaskContext): Iterator[T] computeOrReadCheckpoint reads split partition from a checkpoint (< >) or < > yourself. computeOrReadCheckpoint is used when RDD is requested to < > or < >. == [[getNumPartitions]] Getting Number of Partitions","title":"[source, scala]"},{"location":"rdd/RDD/#source-scala_19","text":"","title":"[source, scala]"},{"location":"rdd/RDD/#getnumpartitions-int","text":"getNumPartitions gives the number of partitions of a RDD.","title":"getNumPartitions: Int"},{"location":"rdd/RDD/#source-scala_20","text":"scala> sc.textFile(\"README.md\").getNumPartitions res0: Int = 2 scala> sc.textFile(\"README.md\", 5).getNumPartitions res1: Int = 5 == [[preferredLocations]] Defining Placement Preferences of RDD Partition","title":"[source, scala]"},{"location":"rdd/RDD/#source-scala_21","text":"preferredLocations( split: Partition): Seq[String] preferredLocations requests the CheckpointRDD for < > (if the RDD is checkpointed) or < >. preferredLocations is a template method that uses < > that custom RDDs can override to specify placement preferences for a partition. getPreferredLocations defines no placement preferences by default. preferredLocations is mainly used when DAGScheduler is requested to xref:scheduler:DAGScheduler.adoc#getPreferredLocs[compute the preferred locations for missing partitions]. == [[partitions]] Accessing RDD Partitions","title":"[source, scala]"},{"location":"rdd/RDD/#source-scala_22","text":"","title":"[source, scala]"},{"location":"rdd/RDD/#partitions-arraypartition","text":"partitions returns the xref:rdd:spark-rdd-partitions.adoc[Partitions] of a RDD . partitions requests CheckpointRDD for the < > (if the RDD is checkpointed) or < > and cache (in < > internal registry that is used next time). Partitions have the property that their internal index should be equal to their position in the owning RDD. == [[markCheckpointed]] markCheckpointed Method","title":"partitions: Array[Partition]"},{"location":"rdd/RDD/#source-scala_23","text":"","title":"[source, scala]"},{"location":"rdd/RDD/#markcheckpointed-unit","text":"markCheckpointed...FIXME markCheckpointed is used when...FIXME == [[doCheckpoint]] doCheckpoint Method","title":"markCheckpointed(): Unit"},{"location":"rdd/RDD/#source-scala_24","text":"","title":"[source, scala]"},{"location":"rdd/RDD/#docheckpoint-unit","text":"doCheckpoint...FIXME doCheckpoint is used when SparkContext is requested to xref:ROOT:SparkContext.adoc#runJob[run a job (synchronously)]. == [[checkpoint]] Reliable Checkpointing -- checkpoint Method","title":"doCheckpoint(): Unit"},{"location":"rdd/RDD/#source-scala_25","text":"","title":"[source, scala]"},{"location":"rdd/RDD/#checkpoint-unit","text":"checkpoint...FIXME checkpoint is used when...FIXME == [[isReliablyCheckpointed]] isReliablyCheckpointed Method","title":"checkpoint(): Unit"},{"location":"rdd/RDD/#source-scala_26","text":"","title":"[source, scala]"},{"location":"rdd/RDD/#isreliablycheckpointed-boolean","text":"isReliablyCheckpointed...FIXME isReliablyCheckpointed is used when...FIXME == [[getCheckpointFile]] getCheckpointFile Method","title":"isReliablyCheckpointed: Boolean"},{"location":"rdd/RDD/#source-scala_27","text":"","title":"[source, scala]"},{"location":"rdd/RDD/#getcheckpointfile-optionstring","text":"getCheckpointFile...FIXME getCheckpointFile is used when...FIXME","title":"getCheckpointFile: Option[String]"},{"location":"rdd/RDDCheckpointData/","text":"= RDDCheckpointData RDDCheckpointData is an abstraction of information related to RDD checkpointing. == [[implementations]] Available RDDCheckpointDatas [cols=\"30,70\",options=\"header\",width=\"100%\"] |=== | RDDCheckpointData | Description | xref:rdd:LocalRDDCheckpointData.adoc[LocalRDDCheckpointData] | [[LocalRDDCheckpointData]] | xref:rdd:ReliableRDDCheckpointData.adoc[ReliableRDDCheckpointData] | [[ReliableRDDCheckpointData]] xref:ROOT:rdd-checkpointing.adoc#reliable-checkpointing[Reliable Checkpointing] |=== == [[creating-instance]] Creating Instance RDDCheckpointData takes the following to be created: [[rdd]] xref:rdd:RDD.adoc[RDD] == [[Serializable]] RDDCheckpointData as Serializable RDDCheckpointData is java.io.Serializable. == [[cpState]] States [[Initialized]] Initialized [[CheckpointingInProgress]] CheckpointingInProgress [[Checkpointed]] Checkpointed == [[checkpoint]] Checkpointing RDD [source, scala] \u00b6 checkpoint(): CheckpointRDD[T] \u00b6 checkpoint changes the < > to < > only when in < > state. Otherwise, checkpoint does nothing and returns. checkpoint < > that gives an CheckpointRDD (that is the < > internal registry). checkpoint changes the < > to < >. In the end, checkpoint requests the given < > to xref:rdd:RDD.adoc#markCheckpointed[markCheckpointed]. checkpoint is used when RDD is requested to xref:rdd:RDD.adoc#doCheckpoint[doCheckpoint]. == [[doCheckpoint]] doCheckpoint Method [source, scala] \u00b6 doCheckpoint(): CheckpointRDD[T] \u00b6 doCheckpoint is used when RDDCheckpointData is requested to < >.","title":"RDDCheckpointData"},{"location":"rdd/RDDCheckpointData/#source-scala","text":"","title":"[source, scala]"},{"location":"rdd/RDDCheckpointData/#checkpoint-checkpointrddt","text":"checkpoint changes the < > to < > only when in < > state. Otherwise, checkpoint does nothing and returns. checkpoint < > that gives an CheckpointRDD (that is the < > internal registry). checkpoint changes the < > to < >. In the end, checkpoint requests the given < > to xref:rdd:RDD.adoc#markCheckpointed[markCheckpointed]. checkpoint is used when RDD is requested to xref:rdd:RDD.adoc#doCheckpoint[doCheckpoint]. == [[doCheckpoint]] doCheckpoint Method","title":"checkpoint(): CheckpointRDD[T]"},{"location":"rdd/RDDCheckpointData/#source-scala_1","text":"","title":"[source, scala]"},{"location":"rdd/RDDCheckpointData/#docheckpoint-checkpointrddt","text":"doCheckpoint is used when RDDCheckpointData is requested to < >.","title":"doCheckpoint(): CheckpointRDD[T]"},{"location":"rdd/RangePartitioner/","text":"= RangePartitioner RangePartitioner is a xref:rdd:Partitioner.adoc[Partitioner] for...FIXME [[ordering]] RangePartitioner[K : Ordering : ClassTag, V] is a parameterized type of K keys that can be sorted ( ordered ) and V values. RangePartitioner is used for xref:rdd:spark-rdd-OrderedRDDFunctions.adoc#sortByKey[sortByKey] operator. == [[creating-instance]] Creating Instance RangePartitioner takes the following to be created: [[partitions]] Number of partitions [[rdd]] xref:rdd:RDD.adoc[RDD] ( RDD[_ <: Product2[K, V]] ) [[ascending]] ascending flag (default: true ) [[samplePointsPerPartitionHint]] samplePointsPerPartitionHint (default: 20) == [[rangeBounds]] rangeBounds Array RangePartitioner uses rangeBounds registry (of type Array[K] ) when requested for < > and < >, < >. == [[numPartitions]] Number of Partitions [source,scala] \u00b6 numPartitions: Int \u00b6 numPartitions is simply one more than the length of the < > array. numPartitions is part of the xref:rdd:Partitioner.adoc#numPartitions[Partitioner] abstraction. == [[getPartition]] Finding Partition ID for Key [source, scala] \u00b6 getPartition(key: Any): Int \u00b6 getPartition...FIXME getPartition is part of the xref:rdd:Partitioner.adoc#getPartition[Partitioner] abstraction.","title":"RangePartitioner"},{"location":"rdd/RangePartitioner/#sourcescala","text":"","title":"[source,scala]"},{"location":"rdd/RangePartitioner/#numpartitions-int","text":"numPartitions is simply one more than the length of the < > array. numPartitions is part of the xref:rdd:Partitioner.adoc#numPartitions[Partitioner] abstraction. == [[getPartition]] Finding Partition ID for Key","title":"numPartitions: Int"},{"location":"rdd/RangePartitioner/#source-scala","text":"","title":"[source, scala]"},{"location":"rdd/RangePartitioner/#getpartitionkey-any-int","text":"getPartition...FIXME getPartition is part of the xref:rdd:Partitioner.adoc#getPartition[Partitioner] abstraction.","title":"getPartition(key: Any): Int"},{"location":"rdd/ReliableCheckpointRDD/","text":"= ReliableCheckpointRDD ReliableCheckpointRDD is an xref:rdd:CheckpointRDD.adoc[CheckpointRDD]...FIXME == [[creating-instance]] Creating Instance ReliableCheckpointRDD takes the following to be created: [[sc]] xref:ROOT:SparkContext.adoc[] [[checkpointPath]] Checkpoint Directory (on a Hadoop DFS-compatible file system) <<_partitioner, Partitioner>> ReliableCheckpointRDD is created when: ReliableCheckpointRDD utility is used to < >. SparkContext is requested to xref:ROOT:SparkContext.adoc#checkpointFile[checkpointFile] == [[checkpointPartitionerFileName]] Checkpointed Partitioner File ReliableCheckpointRDD uses _partitioner as the name of the file in the < > with the < > serialized to. == [[partitioner]] Partitioner ReliableCheckpointRDD can be given a xref:rdd:Partitioner.adoc[Partitioner] to be created. When xref:rdd:RDD.adoc#partitioner[requested for the Partitioner] (as an RDD), ReliableCheckpointRDD returns the one it was created with or < >. == [[writeRDDToCheckpointDirectory]] Writing RDD to Checkpoint Directory [source, scala] \u00b6 writeRDDToCheckpointDirectory T: ClassTag : ReliableCheckpointRDD[T] writeRDDToCheckpointDirectory...FIXME writeRDDToCheckpointDirectory is used when ReliableRDDCheckpointData is requested to xref:rdd:ReliableRDDCheckpointData.adoc#doCheckpoint[doCheckpoint]. == [[writePartitionerToCheckpointDir]] Writing Partitioner to Checkpoint Directory [source,scala] \u00b6 writePartitionerToCheckpointDir( sc: SparkContext, partitioner: Partitioner, checkpointDirPath: Path): Unit writePartitionerToCheckpointDir creates the < > with the buffer size based on xref:ROOT:configuration-properties.adoc#spark.buffer.size[spark.buffer.size] configuration property. writePartitionerToCheckpointDir requests the xref:core:SparkEnv.adoc#serializer[default Serializer] for a new xref:serializer:Serializer.adoc#newInstance[SerializerInstance]. writePartitionerToCheckpointDir requests the SerializerInstance to xref:serializer:SerializerInstance.adoc#serializeStream[serialize the output stream] and xref:serializer:DeserializationStream.adoc#writeObject[writes] the given Partitioner. In the end, writePartitionerToCheckpointDir prints out the following DEBUG message to the logs: [source,plaintext] \u00b6 Written partitioner to [partitionerFilePath] \u00b6 In case of any non-fatal exception, writePartitionerToCheckpointDir prints out the following DEBUG message to the logs: [source,plaintext] \u00b6 Error writing partitioner [partitioner] to [checkpointDirPath] \u00b6 writePartitionerToCheckpointDir is used when ReliableCheckpointRDD is requested to < >. == [[readCheckpointedPartitionerFile]] Reading Partitioner from Checkpointed Directory [source,scala] \u00b6 readCheckpointedPartitionerFile( sc: SparkContext, checkpointDirPath: String): Option[Partitioner] readCheckpointedPartitionerFile opens the < > with the buffer size based on xref:ROOT:configuration-properties.adoc#spark.buffer.size[spark.buffer.size] configuration property. readCheckpointedPartitionerFile requests the xref:core:SparkEnv.adoc#serializer[default Serializer] for a new xref:serializer:Serializer.adoc#newInstance[SerializerInstance]. readCheckpointedPartitionerFile requests the SerializerInstance to xref:serializer:SerializerInstance.adoc#deserializeStream[deserialize the input stream] and xref:serializer:DeserializationStream.adoc#readObject[read the Partitioner] from the partitioner file. readCheckpointedPartitionerFile prints out the following DEBUG message to the logs and returns the partitioner. [source,plaintext] \u00b6 Read partitioner from [partitionerFilePath] \u00b6 In case of FileNotFoundException or any non-fatal exceptions, readCheckpointedPartitionerFile prints out a corresponding message to the logs and returns None. readCheckpointedPartitionerFile is used when ReliableCheckpointRDD is requested for the < >. == [[logging]] Logging Enable ALL logging level for org.apache.spark.rdd.ReliableCheckpointRDD$ logger to see what happens inside. Add the following line to conf/log4j.properties : [source,plaintext] \u00b6 log4j.logger.org.apache.spark.rdd.ReliableCheckpointRDD$=ALL \u00b6 Refer to xref:ROOT:spark-logging.adoc[Logging].","title":"ReliableCheckpointRDD"},{"location":"rdd/ReliableCheckpointRDD/#source-scala","text":"writeRDDToCheckpointDirectory T: ClassTag : ReliableCheckpointRDD[T] writeRDDToCheckpointDirectory...FIXME writeRDDToCheckpointDirectory is used when ReliableRDDCheckpointData is requested to xref:rdd:ReliableRDDCheckpointData.adoc#doCheckpoint[doCheckpoint]. == [[writePartitionerToCheckpointDir]] Writing Partitioner to Checkpoint Directory","title":"[source, scala]"},{"location":"rdd/ReliableCheckpointRDD/#sourcescala","text":"writePartitionerToCheckpointDir( sc: SparkContext, partitioner: Partitioner, checkpointDirPath: Path): Unit writePartitionerToCheckpointDir creates the < > with the buffer size based on xref:ROOT:configuration-properties.adoc#spark.buffer.size[spark.buffer.size] configuration property. writePartitionerToCheckpointDir requests the xref:core:SparkEnv.adoc#serializer[default Serializer] for a new xref:serializer:Serializer.adoc#newInstance[SerializerInstance]. writePartitionerToCheckpointDir requests the SerializerInstance to xref:serializer:SerializerInstance.adoc#serializeStream[serialize the output stream] and xref:serializer:DeserializationStream.adoc#writeObject[writes] the given Partitioner. In the end, writePartitionerToCheckpointDir prints out the following DEBUG message to the logs:","title":"[source,scala]"},{"location":"rdd/ReliableCheckpointRDD/#sourceplaintext","text":"","title":"[source,plaintext]"},{"location":"rdd/ReliableCheckpointRDD/#written-partitioner-to-partitionerfilepath","text":"In case of any non-fatal exception, writePartitionerToCheckpointDir prints out the following DEBUG message to the logs:","title":"Written partitioner to [partitionerFilePath]"},{"location":"rdd/ReliableCheckpointRDD/#sourceplaintext_1","text":"","title":"[source,plaintext]"},{"location":"rdd/ReliableCheckpointRDD/#error-writing-partitioner-partitioner-to-checkpointdirpath","text":"writePartitionerToCheckpointDir is used when ReliableCheckpointRDD is requested to < >. == [[readCheckpointedPartitionerFile]] Reading Partitioner from Checkpointed Directory","title":"Error writing partitioner [partitioner] to [checkpointDirPath]"},{"location":"rdd/ReliableCheckpointRDD/#sourcescala_1","text":"readCheckpointedPartitionerFile( sc: SparkContext, checkpointDirPath: String): Option[Partitioner] readCheckpointedPartitionerFile opens the < > with the buffer size based on xref:ROOT:configuration-properties.adoc#spark.buffer.size[spark.buffer.size] configuration property. readCheckpointedPartitionerFile requests the xref:core:SparkEnv.adoc#serializer[default Serializer] for a new xref:serializer:Serializer.adoc#newInstance[SerializerInstance]. readCheckpointedPartitionerFile requests the SerializerInstance to xref:serializer:SerializerInstance.adoc#deserializeStream[deserialize the input stream] and xref:serializer:DeserializationStream.adoc#readObject[read the Partitioner] from the partitioner file. readCheckpointedPartitionerFile prints out the following DEBUG message to the logs and returns the partitioner.","title":"[source,scala]"},{"location":"rdd/ReliableCheckpointRDD/#sourceplaintext_2","text":"","title":"[source,plaintext]"},{"location":"rdd/ReliableCheckpointRDD/#read-partitioner-from-partitionerfilepath","text":"In case of FileNotFoundException or any non-fatal exceptions, readCheckpointedPartitionerFile prints out a corresponding message to the logs and returns None. readCheckpointedPartitionerFile is used when ReliableCheckpointRDD is requested for the < >. == [[logging]] Logging Enable ALL logging level for org.apache.spark.rdd.ReliableCheckpointRDD$ logger to see what happens inside. Add the following line to conf/log4j.properties :","title":"Read partitioner from [partitionerFilePath]"},{"location":"rdd/ReliableCheckpointRDD/#sourceplaintext_3","text":"","title":"[source,plaintext]"},{"location":"rdd/ReliableCheckpointRDD/#log4jloggerorgapachesparkrddreliablecheckpointrddall","text":"Refer to xref:ROOT:spark-logging.adoc[Logging].","title":"log4j.logger.org.apache.spark.rdd.ReliableCheckpointRDD$=ALL"},{"location":"rdd/ReliableRDDCheckpointData/","text":"= ReliableRDDCheckpointData ReliableRDDCheckpointData is a xref:rdd:RDDCheckpointData.adoc[RDDCheckpointData] for xref:ROOT:rdd-checkpointing.adoc#reliable-checkpointing[Reliable Checkpointing]. == [[creating-instance]] Creating Instance ReliableRDDCheckpointData takes the following to be created: [[rdd]] xref:rdd:RDD.adoc[++RDD[T]++] ReliableRDDCheckpointData is created for xref:rdd:RDD.adoc#checkpoint[RDD.checkpoint] operator. == [[cpDir]][[checkpointPath]] Checkpoint Directory ReliableRDDCheckpointData creates a subdirectory of the xref:ROOT:SparkContext.adoc#checkpointDir[application-wide checkpoint directory] for < > the given < >. The name of the subdirectory uses the xref:rdd:RDD.adoc#id[unique identifier] of the < >: [source,plaintext] \u00b6 rdd-[id] \u00b6 == [[doCheckpoint]] Checkpointing RDD [source, scala] \u00b6 doCheckpoint(): CheckpointRDD[T] \u00b6 doCheckpoint xref:rdd:ReliableCheckpointRDD.adoc#writeRDDToCheckpointDirectory[writes] the < > to the < > (that creates a new RDD). With xref:ROOT:configuration-properties.adoc#spark.cleaner.referenceTracking.cleanCheckpoints[spark.cleaner.referenceTracking.cleanCheckpoints] configuration property enabled, doCheckpoint requests the xref:ROOT:SparkContext.adoc#cleaner[ContextCleaner] to xref:core:ContextCleaner.adoc#registerRDDCheckpointDataForCleanup[registerRDDCheckpointDataForCleanup] for the new RDD. In the end, doCheckpoint prints out the following INFO message to the logs and returns the new RDD. [source,plaintext] \u00b6 Done checkpointing RDD [id] to [cpDir], new parent is RDD [id] \u00b6 doCheckpoint is part of the xref:rdd:RDDCheckpointData.adoc#doCheckpoint[RDDCheckpointData] abstraction.","title":"ReliableRDDCheckpointData"},{"location":"rdd/ReliableRDDCheckpointData/#sourceplaintext","text":"","title":"[source,plaintext]"},{"location":"rdd/ReliableRDDCheckpointData/#rdd-id","text":"== [[doCheckpoint]] Checkpointing RDD","title":"rdd-[id]"},{"location":"rdd/ReliableRDDCheckpointData/#source-scala","text":"","title":"[source, scala]"},{"location":"rdd/ReliableRDDCheckpointData/#docheckpoint-checkpointrddt","text":"doCheckpoint xref:rdd:ReliableCheckpointRDD.adoc#writeRDDToCheckpointDirectory[writes] the < > to the < > (that creates a new RDD). With xref:ROOT:configuration-properties.adoc#spark.cleaner.referenceTracking.cleanCheckpoints[spark.cleaner.referenceTracking.cleanCheckpoints] configuration property enabled, doCheckpoint requests the xref:ROOT:SparkContext.adoc#cleaner[ContextCleaner] to xref:core:ContextCleaner.adoc#registerRDDCheckpointDataForCleanup[registerRDDCheckpointDataForCleanup] for the new RDD. In the end, doCheckpoint prints out the following INFO message to the logs and returns the new RDD.","title":"doCheckpoint(): CheckpointRDD[T]"},{"location":"rdd/ReliableRDDCheckpointData/#sourceplaintext_1","text":"","title":"[source,plaintext]"},{"location":"rdd/ReliableRDDCheckpointData/#done-checkpointing-rdd-id-to-cpdir-new-parent-is-rdd-id","text":"doCheckpoint is part of the xref:rdd:RDDCheckpointData.adoc#doCheckpoint[RDDCheckpointData] abstraction.","title":"Done checkpointing RDD [id] to [cpDir], new parent is RDD [id]"},{"location":"rdd/ShuffleDependency/","text":"= [[ShuffleDependency]] ShuffleDependency ShuffleDependency is a xref:rdd:spark-rdd-Dependency.adoc[Dependency] on the output of a xref:scheduler:ShuffleMapStage.adoc[ShuffleMapStage] for a < >. ShuffleDependency uses the < > to know the number of (map-side/pre-shuffle) partitions and the < > for the number of (reduce-size/post-shuffle) partitions. ShuffleDependency is created as a dependency of xref:rdd:ShuffledRDD.adoc[ShuffledRDD]. ShuffleDependency can also be created as a dependency of xref:rdd:spark-rdd-CoGroupedRDD.adoc[CoGroupedRDD] and xref:rdd:spark-rdd-SubtractedRDD.adoc[SubtractedRDD]. == [[creating-instance]] Creating Instance ShuffleDependency takes the following to be created: < > of key-value pairs ( RDD[_ <: Product2[K, V]] ) < > [[serializer]] xref:serializer:Serializer.adoc[Serializer] [[keyOrdering]] Ordering for K keys ( Option[Ordering[K]] ) < > ( Option[Aggregator[K, V, C]] ) < > flag (default: false ) When created, ShuffleDependency gets xref:ROOT:SparkContext.adoc#nextShuffleId[shuffle id] (as shuffleId ). NOTE: ShuffleDependency uses the xref:rdd:index.adoc#context[input RDD to access SparkContext ] and so the shuffleId . ShuffleDependency xref:shuffle:ShuffleManager.adoc#registerShuffle[registers itself with ShuffleManager ] and gets a ShuffleHandle (available as < > property). NOTE: ShuffleDependency accesses xref:core:SparkEnv.adoc#shuffleManager[ ShuffleManager using SparkEnv ]. In the end, ShuffleDependency xref:core:ContextCleaner.adoc#registerShuffleForCleanup[registers itself for cleanup with ContextCleaner ]. NOTE: ShuffleDependency accesses the xref:ROOT:SparkContext.adoc#cleaner[optional ContextCleaner through SparkContext ]. NOTE: ShuffleDependency is created when xref:ShuffledRDD.adoc#getDependencies[ShuffledRDD], link:spark-rdd-CoGroupedRDD.adoc#getDependencies[CoGroupedRDD], and link:spark-rdd-SubtractedRDD.adoc#getDependencies[SubtractedRDD] return their RDD dependencies. == [[shuffleId]] Shuffle ID Every ShuffleDependency has a unique application-wide shuffle ID that is assigned when < > (and is used throughout Spark's code to reference a ShuffleDependency). Shuffle IDs are tracked by xref:ROOT:SparkContext.adoc#nextShuffleId[SparkContext]. == [[rdd]] Parent RDD ShuffleDependency is given the parent xref:rdd:RDD.adoc[RDD] of key-value pairs ( RDD[_ <: Product2[K, V]] ). The parent RDD is available as rdd property that is part of the xref:rdd:spark-rdd-Dependency.adoc#rdd[Dependency] abstraction. [source,scala] \u00b6 rdd: RDD[Product2[K, V]] \u00b6 == [[partitioner]] Partitioner ShuffleDependency is given a xref:rdd:Partitioner.adoc[Partitioner] that is used to partition the shuffle output (when xref:shuffle:SortShuffleWriter.adoc[SortShuffleWriter], xref:shuffle:BypassMergeSortShuffleWriter.adoc[BypassMergeSortShuffleWriter] and xref:shuffle:UnsafeShuffleWriter.adoc[UnsafeShuffleWriter] are requested to write). == [[shuffleHandle]] ShuffleHandle [source, scala] \u00b6 shuffleHandle: ShuffleHandle \u00b6 shuffleHandle is the ShuffleHandle of a ShuffleDependency as assigned eagerly when < >. shuffleHandle is used to compute link:spark-rdd-CoGroupedRDD.adoc#compute[CoGroupedRDDs], xref:ShuffledRDD.adoc#compute[ShuffledRDD], link:spark-rdd-SubtractedRDD.adoc#compute[SubtractedRDD], and link:spark-sql-ShuffledRowRDD.adoc[ShuffledRowRDD] (to get a link:spark-shuffle-ShuffleReader.adoc[ShuffleReader] for a ShuffleDependency) and when a xref:scheduler:ShuffleMapTask.adoc#runTask[ ShuffleMapTask runs] (to get a ShuffleWriter for a ShuffleDependency). == [[mapSideCombine]] Map-Size Partial Aggregation Flag ShuffleDependency uses a mapSideCombine flag that controls whether to perform map-side partial aggregation ( map-side combine ) using an < >. mapSideCombine is disabled ( false ) by default and can be enabled ( true ) for some use cases of xref:rdd:ShuffledRDD.adoc#mapSideCombine[ShuffledRDD]. ShuffleDependency requires that the optional < > is defined when the flag is enabled. mapSideCombine is used when: BlockStoreShuffleReader is requested to xref:shuffle:BlockStoreShuffleReader.adoc#read[read combined records for a reduce task] SortShuffleManager is requested to xref:shuffle:SortShuffleManager.adoc#registerShuffle[register a shuffle] SortShuffleWriter is requested to xref:shuffle:SortShuffleWriter.adoc#write[write records] == [[aggregator]] Optional Aggregator [source, scala] \u00b6 aggregator: Option[Aggregator[K, V, C]] = None \u00b6 aggregator is a xref:rdd:Aggregator.adoc[map/reduce-side Aggregator] (for a RDD's shuffle). aggregator is by default undefined (i.e. None ) when < >. NOTE: aggregator is used when xref:shuffle:SortShuffleWriter.adoc#write[ SortShuffleWriter writes records] and xref:shuffle:BlockStoreShuffleReader.adoc#read[ BlockStoreShuffleReader reads combined key-values for a reduce task].","title":"ShuffleDependency"},{"location":"rdd/ShuffleDependency/#sourcescala","text":"","title":"[source,scala]"},{"location":"rdd/ShuffleDependency/#rdd-rddproduct2k-v","text":"== [[partitioner]] Partitioner ShuffleDependency is given a xref:rdd:Partitioner.adoc[Partitioner] that is used to partition the shuffle output (when xref:shuffle:SortShuffleWriter.adoc[SortShuffleWriter], xref:shuffle:BypassMergeSortShuffleWriter.adoc[BypassMergeSortShuffleWriter] and xref:shuffle:UnsafeShuffleWriter.adoc[UnsafeShuffleWriter] are requested to write). == [[shuffleHandle]] ShuffleHandle","title":"rdd: RDD[Product2[K, V]]"},{"location":"rdd/ShuffleDependency/#source-scala","text":"","title":"[source, scala]"},{"location":"rdd/ShuffleDependency/#shufflehandle-shufflehandle","text":"shuffleHandle is the ShuffleHandle of a ShuffleDependency as assigned eagerly when < >. shuffleHandle is used to compute link:spark-rdd-CoGroupedRDD.adoc#compute[CoGroupedRDDs], xref:ShuffledRDD.adoc#compute[ShuffledRDD], link:spark-rdd-SubtractedRDD.adoc#compute[SubtractedRDD], and link:spark-sql-ShuffledRowRDD.adoc[ShuffledRowRDD] (to get a link:spark-shuffle-ShuffleReader.adoc[ShuffleReader] for a ShuffleDependency) and when a xref:scheduler:ShuffleMapTask.adoc#runTask[ ShuffleMapTask runs] (to get a ShuffleWriter for a ShuffleDependency). == [[mapSideCombine]] Map-Size Partial Aggregation Flag ShuffleDependency uses a mapSideCombine flag that controls whether to perform map-side partial aggregation ( map-side combine ) using an < >. mapSideCombine is disabled ( false ) by default and can be enabled ( true ) for some use cases of xref:rdd:ShuffledRDD.adoc#mapSideCombine[ShuffledRDD]. ShuffleDependency requires that the optional < > is defined when the flag is enabled. mapSideCombine is used when: BlockStoreShuffleReader is requested to xref:shuffle:BlockStoreShuffleReader.adoc#read[read combined records for a reduce task] SortShuffleManager is requested to xref:shuffle:SortShuffleManager.adoc#registerShuffle[register a shuffle] SortShuffleWriter is requested to xref:shuffle:SortShuffleWriter.adoc#write[write records] == [[aggregator]] Optional Aggregator","title":"shuffleHandle: ShuffleHandle"},{"location":"rdd/ShuffleDependency/#source-scala_1","text":"","title":"[source, scala]"},{"location":"rdd/ShuffleDependency/#aggregator-optionaggregatork-v-c-none","text":"aggregator is a xref:rdd:Aggregator.adoc[map/reduce-side Aggregator] (for a RDD's shuffle). aggregator is by default undefined (i.e. None ) when < >. NOTE: aggregator is used when xref:shuffle:SortShuffleWriter.adoc#write[ SortShuffleWriter writes records] and xref:shuffle:BlockStoreShuffleReader.adoc#read[ BlockStoreShuffleReader reads combined key-values for a reduce task].","title":"aggregator: Option[Aggregator[K, V, C]] = None"},{"location":"rdd/ShuffledRDD/","text":"= [[ShuffledRDD]] ShuffledRDD ShuffledRDD is an xref:rdd:RDD.adoc[RDD] of key-value pairs that represents a shuffle step in a xref:spark-rdd-lineage.adoc[RDD lineage]. ShuffledRDD is given an < > of key-value pairs of K and V types, respectively, when < > and < > key-value pairs of K and C types, respectively. ShuffledRDD is < > for the following RDD transformations: xref:spark-rdd-OrderedRDDFunctions.adoc#sortByKey[OrderedRDDFunctions.sortByKey] and xref:spark-rdd-OrderedRDDFunctions.adoc#repartitionAndSortWithinPartitions[OrderedRDDFunctions.repartitionAndSortWithinPartitions] xref:rdd:PairRDDFunctions.adoc#combineByKeyWithClassTag[PairRDDFunctions.combineByKeyWithClassTag] and xref:rdd:PairRDDFunctions.adoc#partitionBy[PairRDDFunctions.partitionBy] xref:spark-rdd-transformations.adoc#coalesce[RDD.coalesce] (with shuffle flag enabled) ShuffledRDD uses custom < > partitions. [[isBarrier]] ShuffledRDD has xref:rdd:RDD.adoc#isBarrier[isBarrier] flag always disabled ( false ). == [[creating-instance]] Creating Instance ShuffledRDD takes the following to be created: [[prev]] Previous xref:rdd:RDD.adoc[RDD] of key-value pairs ( RDD[_ <: Product2[K, V]] ) [[part]] xref:rdd:Partitioner.adoc[Partitioner] == [[mapSideCombine]][[setMapSideCombine]] Map-Side Combine Flag ShuffledRDD uses a map-side combine flag to create a xref:rdd:ShuffleDependency.adoc[ShuffleDependency] when requested for the < > (there is always only one). The flag is disabled ( false ) by default and can be changed using setMapSideCombine method. [source,scala] \u00b6 setMapSideCombine( mapSideCombine: Boolean): ShuffledRDD[K, V, C] setMapSideCombine is used for xref:rdd:PairRDDFunctions.adoc#combineByKeyWithClassTag[PairRDDFunctions.combineByKeyWithClassTag] transformation (which defaults to the flag enabled). == [[compute]] Computing Partition [source, scala] \u00b6 compute( split: Partition, context: TaskContext): Iterator[(K, C)] compute requests the only xref:rdd:RDD.adoc#dependencies[dependency] (that is assumed a xref:rdd:ShuffleDependency.adoc[ShuffleDependency]) for the xref:rdd:ShuffleDependency.adoc#shuffleHandle[ShuffleHandle]. compute uses the xref:core:SparkEnv.adoc[SparkEnv] to access the xref:core:SparkEnv.adoc#shuffleManager[ShuffleManager]. compute requests the xref:shuffle:ShuffleManager.adoc#shuffleManager[ShuffleManager] for the xref:shuffle:ShuffleManager.adoc#getReader[ShuffleReader] (for the ShuffleHandle, the xref:rdd:spark-rdd-Partition.adoc[partition]). In the end, compute requests the ShuffleReader to xref:shuffle:spark-shuffle-ShuffleReader.adoc#read[read] the combined key-value pairs (of type (K, C) ). compute is part of the xref:rdd:RDD.adoc#compute[RDD] abstraction. == [[getPreferredLocations]] Placement Preferences of Partition [source, scala] \u00b6 getPreferredLocations( partition: Partition): Seq[String] getPreferredLocations requests MapOutputTrackerMaster for the xref:scheduler:MapOutputTrackerMaster.adoc#getPreferredLocationsForShuffle[preferred locations] of the given xref:rdd:spark-rdd-Partition.adoc[partition] (xref:storage:BlockManager.adoc[BlockManagers] with the most map outputs). getPreferredLocations uses SparkEnv to access the current xref:core:SparkEnv.adoc#mapOutputTracker[MapOutputTrackerMaster]. getPreferredLocations is part of the xref:rdd:RDD.adoc#compute[RDD] abstraction. == [[getDependencies]] Dependencies [source, scala] \u00b6 getDependencies: Seq[Dependency[_]] \u00b6 getDependencies uses the < > if defined or requests the current xref:serializer:SerializerManager.adoc[SerializerManager] for xref:serializer:SerializerManager.adoc#getSerializer[one]. getDependencies uses the < > internal flag for the types of the keys and values (i.e. K and C or K and V when the flag is enabled or not, respectively). In the end, getDependencies returns a single xref:rdd:ShuffleDependency.adoc[ShuffleDependency] (with the < >, the < >, and the Serializer). getDependencies is part of the xref:rdd:RDD.adoc#getDependencies[RDD] abstraction. == [[ShuffledRDDPartition]] ShuffledRDDPartition ShuffledRDDPartition gets an index to be created (that in turn is the index of partitions as calculated by the xref:rdd:Partitioner.adoc[Partitioner] of a < >). == Demos === Demo: ShuffledRDD and coalesce [source,plaintext] \u00b6 val data = sc.parallelize(0 to 9) val coalesced = data.coalesce(numPartitions = 4, shuffle = true) scala> println(coalesced.toDebugString) (4) MapPartitionsRDD[9] at coalesce at :75 [] | CoalescedRDD[8] at coalesce at :75 [] | ShuffledRDD[7] at coalesce at :75 [] +-(16) MapPartitionsRDD[6] at coalesce at :75 [] | ParallelCollectionRDD[5] at parallelize at :74 [] === Demo: ShuffledRDD and sortByKey [source,plaintext] \u00b6 val data = sc.parallelize(0 to 9) val grouped = rdd.groupBy(_ % 2) val sorted = grouped.sortByKey(numPartitions = 2) scala> println(sorted.toDebugString) (2) ShuffledRDD[15] at sortByKey at :74 [] +-(4) ShuffledRDD[12] at groupBy at :74 [] +-(4) MapPartitionsRDD[11] at groupBy at :74 [] | MapPartitionsRDD[9] at coalesce at :75 [] | CoalescedRDD[8] at coalesce at :75 [] | ShuffledRDD[7] at coalesce at :75 [] +-(16) MapPartitionsRDD[6] at coalesce at :75 [] | ParallelCollectionRDD[5] at parallelize at :74 [] == [[internal-properties]] Internal Properties [cols=\"30m,70\",options=\"header\",width=\"100%\"] |=== | Name | Description | userSpecifiedSerializer a| [[userSpecifiedSerializer]] User-specified xref:serializer:Serializer.adoc[Serializer] for the single xref:rdd:ShuffleDependency.adoc[ShuffleDependency] dependency [source, scala] \u00b6 userSpecifiedSerializer: Option[Serializer] = None \u00b6 userSpecifiedSerializer is undefined ( None ) by default and can be changed using setSerializer method (that is used for xref:rdd:PairRDDFunctions.adoc#combineByKeyWithClassTag[PairRDDFunctions.combineByKeyWithClassTag] transformation). |===","title":"ShuffledRDD"},{"location":"rdd/ShuffledRDD/#sourcescala","text":"setMapSideCombine( mapSideCombine: Boolean): ShuffledRDD[K, V, C] setMapSideCombine is used for xref:rdd:PairRDDFunctions.adoc#combineByKeyWithClassTag[PairRDDFunctions.combineByKeyWithClassTag] transformation (which defaults to the flag enabled). == [[compute]] Computing Partition","title":"[source,scala]"},{"location":"rdd/ShuffledRDD/#source-scala","text":"compute( split: Partition, context: TaskContext): Iterator[(K, C)] compute requests the only xref:rdd:RDD.adoc#dependencies[dependency] (that is assumed a xref:rdd:ShuffleDependency.adoc[ShuffleDependency]) for the xref:rdd:ShuffleDependency.adoc#shuffleHandle[ShuffleHandle]. compute uses the xref:core:SparkEnv.adoc[SparkEnv] to access the xref:core:SparkEnv.adoc#shuffleManager[ShuffleManager]. compute requests the xref:shuffle:ShuffleManager.adoc#shuffleManager[ShuffleManager] for the xref:shuffle:ShuffleManager.adoc#getReader[ShuffleReader] (for the ShuffleHandle, the xref:rdd:spark-rdd-Partition.adoc[partition]). In the end, compute requests the ShuffleReader to xref:shuffle:spark-shuffle-ShuffleReader.adoc#read[read] the combined key-value pairs (of type (K, C) ). compute is part of the xref:rdd:RDD.adoc#compute[RDD] abstraction. == [[getPreferredLocations]] Placement Preferences of Partition","title":"[source, scala]"},{"location":"rdd/ShuffledRDD/#source-scala_1","text":"getPreferredLocations( partition: Partition): Seq[String] getPreferredLocations requests MapOutputTrackerMaster for the xref:scheduler:MapOutputTrackerMaster.adoc#getPreferredLocationsForShuffle[preferred locations] of the given xref:rdd:spark-rdd-Partition.adoc[partition] (xref:storage:BlockManager.adoc[BlockManagers] with the most map outputs). getPreferredLocations uses SparkEnv to access the current xref:core:SparkEnv.adoc#mapOutputTracker[MapOutputTrackerMaster]. getPreferredLocations is part of the xref:rdd:RDD.adoc#compute[RDD] abstraction. == [[getDependencies]] Dependencies","title":"[source, scala]"},{"location":"rdd/ShuffledRDD/#source-scala_2","text":"","title":"[source, scala]"},{"location":"rdd/ShuffledRDD/#getdependencies-seqdependency_","text":"getDependencies uses the < > if defined or requests the current xref:serializer:SerializerManager.adoc[SerializerManager] for xref:serializer:SerializerManager.adoc#getSerializer[one]. getDependencies uses the < > internal flag for the types of the keys and values (i.e. K and C or K and V when the flag is enabled or not, respectively). In the end, getDependencies returns a single xref:rdd:ShuffleDependency.adoc[ShuffleDependency] (with the < >, the < >, and the Serializer). getDependencies is part of the xref:rdd:RDD.adoc#getDependencies[RDD] abstraction. == [[ShuffledRDDPartition]] ShuffledRDDPartition ShuffledRDDPartition gets an index to be created (that in turn is the index of partitions as calculated by the xref:rdd:Partitioner.adoc[Partitioner] of a < >). == Demos === Demo: ShuffledRDD and coalesce","title":"getDependencies: Seq[Dependency[_]]"},{"location":"rdd/ShuffledRDD/#sourceplaintext","text":"val data = sc.parallelize(0 to 9) val coalesced = data.coalesce(numPartitions = 4, shuffle = true) scala> println(coalesced.toDebugString) (4) MapPartitionsRDD[9] at coalesce at :75 [] | CoalescedRDD[8] at coalesce at :75 [] | ShuffledRDD[7] at coalesce at :75 [] +-(16) MapPartitionsRDD[6] at coalesce at :75 [] | ParallelCollectionRDD[5] at parallelize at :74 [] === Demo: ShuffledRDD and sortByKey","title":"[source,plaintext]"},{"location":"rdd/ShuffledRDD/#sourceplaintext_1","text":"val data = sc.parallelize(0 to 9) val grouped = rdd.groupBy(_ % 2) val sorted = grouped.sortByKey(numPartitions = 2) scala> println(sorted.toDebugString) (2) ShuffledRDD[15] at sortByKey at :74 [] +-(4) ShuffledRDD[12] at groupBy at :74 [] +-(4) MapPartitionsRDD[11] at groupBy at :74 [] | MapPartitionsRDD[9] at coalesce at :75 [] | CoalescedRDD[8] at coalesce at :75 [] | ShuffledRDD[7] at coalesce at :75 [] +-(16) MapPartitionsRDD[6] at coalesce at :75 [] | ParallelCollectionRDD[5] at parallelize at :74 [] == [[internal-properties]] Internal Properties [cols=\"30m,70\",options=\"header\",width=\"100%\"] |=== | Name | Description | userSpecifiedSerializer a| [[userSpecifiedSerializer]] User-specified xref:serializer:Serializer.adoc[Serializer] for the single xref:rdd:ShuffleDependency.adoc[ShuffleDependency] dependency","title":"[source,plaintext]"},{"location":"rdd/ShuffledRDD/#source-scala_3","text":"","title":"[source, scala]"},{"location":"rdd/ShuffledRDD/#userspecifiedserializer-optionserializer-none","text":"userSpecifiedSerializer is undefined ( None ) by default and can be changed using setSerializer method (that is used for xref:rdd:PairRDDFunctions.adoc#combineByKeyWithClassTag[PairRDDFunctions.combineByKeyWithClassTag] transformation). |===","title":"userSpecifiedSerializer: Option[Serializer] = None"},{"location":"rdd/spark-rdd-CoGroupedRDD/","text":"= CoGroupedRDD A RDD that cogroups its pair RDD parents. For each key k in parent RDDs, the resulting RDD contains a tuple with the list of values for that key. Use RDD.cogroup(...) to create one. == [[getDependencies]] getDependencies Method CAUTION: FIXME == [[compute]] Computing Partition (in TaskContext) [source, scala] \u00b6 compute(s: Partition, context: TaskContext): Iterator[(K, Array[Iterable[_]])] \u00b6 compute...FIXME compute is part of xref:rdd:RDD.adoc#compute[RDD] abstraction.","title":"CoGroupedRDD"},{"location":"rdd/spark-rdd-CoGroupedRDD/#source-scala","text":"","title":"[source, scala]"},{"location":"rdd/spark-rdd-CoGroupedRDD/#computes-partition-context-taskcontext-iteratork-arrayiterable_","text":"compute...FIXME compute is part of xref:rdd:RDD.adoc#compute[RDD] abstraction.","title":"compute(s: Partition, context: TaskContext): Iterator[(K, Array[Iterable[_]])]"},{"location":"rdd/spark-rdd-Dependency/","text":"== [[Dependency]] RDD Dependencies Dependency class is the base (abstract) class to model a dependency relationship between two or more RDDs. [[rdd]] Dependency has a single method rdd to access the RDD that is behind a dependency. [source, scala] \u00b6 def rdd: RDD[T] \u00b6 Whenever you apply a link:spark-rdd-transformations.adoc[transformation] (e.g. map , flatMap ) to a RDD you build the so-called link:spark-rdd-lineage.adoc[RDD lineage graph]. Dependency -ies represent the edges in a lineage graph. NOTE: link:spark-rdd-NarrowDependency.adoc[NarrowDependency] and xref:rdd:ShuffleDependency.adoc[ShuffleDependency] are the two top-level subclasses of Dependency abstract class. .Kinds of Dependencies [cols=\"1,2\",options=\"header\",width=\"100%\"] |=== | Name | Description | link:spark-rdd-NarrowDependency.adoc[NarrowDependency] | | xref:rdd:ShuffleDependency.adoc[ShuffleDependency] | | link:spark-rdd-NarrowDependency.adoc#OneToOneDependency[OneToOneDependency] | | link:spark-rdd-NarrowDependency.adoc#PruneDependency[PruneDependency] | | link:spark-rdd-NarrowDependency.adoc#RangeDependency[RangeDependency] | |=== [NOTE] \u00b6 The dependencies of a RDD are available using xref:rdd:index.adoc#dependencies[dependencies] method. // A demo RDD scala> val myRdd = sc.parallelize(0 to 9).groupBy(_ % 2) myRdd: org.apache.spark.rdd.RDD[(Int, Iterable[Int])] = ShuffledRDD[8] at groupBy at <console>:24 scala> myRdd.foreach(println) (0,CompactBuffer(0, 2, 4, 6, 8)) (1,CompactBuffer(1, 3, 5, 7, 9)) scala> myRdd.dependencies res5: Seq[org.apache.spark.Dependency[_]] = List(org.apache.spark.ShuffleDependency@27ace619) // Access all RDDs in the demo RDD lineage scala> myRdd.dependencies.map(_.rdd).foreach(println) MapPartitionsRDD[7] at groupBy at <console>:24 You use link:spark-rdd-lineage.adoc#toDebugString[toDebugString] method to print out the RDD lineage in a user-friendly way. scala> myRdd.toDebugString res6: String = (8) ShuffledRDD[8] at groupBy at <console>:24 [] +-(8) MapPartitionsRDD[7] at groupBy at <console>:24 [] | ParallelCollectionRDD[6] at parallelize at <console>:24 [] \u00b6","title":"Dependencies"},{"location":"rdd/spark-rdd-Dependency/#source-scala","text":"","title":"[source, scala]"},{"location":"rdd/spark-rdd-Dependency/#def-rdd-rddt","text":"Whenever you apply a link:spark-rdd-transformations.adoc[transformation] (e.g. map , flatMap ) to a RDD you build the so-called link:spark-rdd-lineage.adoc[RDD lineage graph]. Dependency -ies represent the edges in a lineage graph. NOTE: link:spark-rdd-NarrowDependency.adoc[NarrowDependency] and xref:rdd:ShuffleDependency.adoc[ShuffleDependency] are the two top-level subclasses of Dependency abstract class. .Kinds of Dependencies [cols=\"1,2\",options=\"header\",width=\"100%\"] |=== | Name | Description | link:spark-rdd-NarrowDependency.adoc[NarrowDependency] | | xref:rdd:ShuffleDependency.adoc[ShuffleDependency] | | link:spark-rdd-NarrowDependency.adoc#OneToOneDependency[OneToOneDependency] | | link:spark-rdd-NarrowDependency.adoc#PruneDependency[PruneDependency] | | link:spark-rdd-NarrowDependency.adoc#RangeDependency[RangeDependency] | |===","title":"def rdd: RDD[T]"},{"location":"rdd/spark-rdd-Dependency/#note","text":"The dependencies of a RDD are available using xref:rdd:index.adoc#dependencies[dependencies] method. // A demo RDD scala> val myRdd = sc.parallelize(0 to 9).groupBy(_ % 2) myRdd: org.apache.spark.rdd.RDD[(Int, Iterable[Int])] = ShuffledRDD[8] at groupBy at <console>:24 scala> myRdd.foreach(println) (0,CompactBuffer(0, 2, 4, 6, 8)) (1,CompactBuffer(1, 3, 5, 7, 9)) scala> myRdd.dependencies res5: Seq[org.apache.spark.Dependency[_]] = List(org.apache.spark.ShuffleDependency@27ace619) // Access all RDDs in the demo RDD lineage scala> myRdd.dependencies.map(_.rdd).foreach(println) MapPartitionsRDD[7] at groupBy at <console>:24 You use link:spark-rdd-lineage.adoc#toDebugString[toDebugString] method to print out the RDD lineage in a user-friendly way.","title":"[NOTE]"},{"location":"rdd/spark-rdd-Dependency/#scala-myrddtodebugstring-res6-string-8-shuffledrdd8-at-groupby-at-console24-8-mappartitionsrdd7-at-groupby-at-console24-parallelcollectionrdd6-at-parallelize-at-console24","text":"","title":"scala&gt; myRdd.toDebugString\nres6: String =\n(8) ShuffledRDD[8] at groupBy at &lt;console&gt;:24 []\n +-(8) MapPartitionsRDD[7] at groupBy at &lt;console&gt;:24 []\n    |  ParallelCollectionRDD[6] at parallelize at &lt;console&gt;:24 []\n"},{"location":"rdd/spark-rdd-HadoopRDD/","text":"== HadoopRDD https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.rdd.HadoopRDD[HadoopRDD ] is an RDD that provides core functionality for reading data stored in HDFS, a local file system (available on all nodes), or any Hadoop-supported file system URI using the older MapReduce API ( https://hadoop.apache.org/docs/current/api/org/apache/hadoop/mapred/package-summary.html[org.apache.hadoop.mapred ]). HadoopRDD is created as a result of calling the following methods in xref:ROOT:SparkContext.adoc[]: hadoopFile textFile (the most often used in examples!) sequenceFile Partitions are of type HadoopPartition . When an HadoopRDD is computed, i.e. an action is called, you should see the INFO message Input split: in the logs. scala> sc.textFile(\"README.md\").count ... 15/10/10 18:03:21 INFO HadoopRDD: Input split: file:/Users/jacek/dev/oss/spark/README.md:0+1784 15/10/10 18:03:21 INFO HadoopRDD: Input split: file:/Users/jacek/dev/oss/spark/README.md:1784+1784 ... The following properties are set upon partition execution: mapred.tip.id - task id of this task's attempt mapred.task.id - task attempt's id mapred.task.is.map as true mapred.task.partition - split id mapred.job.id Spark settings for HadoopRDD : spark.hadoop.cloneConf (default: false ) - shouldCloneJobConf - should a Hadoop job configuration JobConf object be cloned before spawning a Hadoop job. Refer to https://issues.apache.org/jira/browse/SPARK-2546[[SPARK-2546 ] Configuration object thread safety issue]. When true , you should see a DEBUG message Cloning Hadoop Configuration . You can register callbacks on link:spark-TaskContext.adoc[TaskContext]. HadoopRDDs are not checkpointed. They do nothing when checkpoint() is called. [CAUTION] \u00b6 FIXME What are InputMetrics ? What is JobConf ? What are the InputSplits: FileSplit and CombineFileSplit ? * What are InputFormat and Configurable subtypes? What's InputFormat's RecordReader? It creates a key and a value. What are they? What's Hadoop Split? input splits for Hadoop reads? See InputFormat.getSplits \u00b6 === [[getPreferredLocations]] getPreferredLocations Method CAUTION: FIXME === [[getPartitions]] getPartitions Method The number of partition for HadoopRDD, i.e. the return value of getPartitions , is calculated using InputFormat.getSplits(jobConf, minPartitions) where minPartitions is only a hint of how many partitions one may want at minimum. As a hint it does not mean the number of partitions will be exactly the number given. For SparkContext.textFile the input format class is https://hadoop.apache.org/docs/current/api/org/apache/hadoop/mapred/TextInputFormat.html[org.apache.hadoop.mapred.TextInputFormat ]. The https://hadoop.apache.org/docs/current/api/org/apache/hadoop/mapred/FileInputFormat.html[javadoc of org.apache.hadoop.mapred.FileInputFormat] says: FileInputFormat is the base class for all file-based InputFormats. This provides a generic implementation of getSplits(JobConf, int). Subclasses of FileInputFormat can also override the isSplitable(FileSystem, Path) method to ensure input-files are not split-up and are processed as a whole by Mappers. TIP: You may find https://github.com/apache/hadoop/blob/trunk/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/FileInputFormat.java#L319[the sources of org.apache.hadoop.mapred.FileInputFormat.getSplits] enlightening.","title":"HadoopRDD"},{"location":"rdd/spark-rdd-HadoopRDD/#caution","text":"FIXME What are InputMetrics ? What is JobConf ? What are the InputSplits: FileSplit and CombineFileSplit ? * What are InputFormat and Configurable subtypes? What's InputFormat's RecordReader? It creates a key and a value. What are they?","title":"[CAUTION]"},{"location":"rdd/spark-rdd-HadoopRDD/#whats-hadoop-split-input-splits-for-hadoop-reads-see-inputformatgetsplits","text":"=== [[getPreferredLocations]] getPreferredLocations Method CAUTION: FIXME === [[getPartitions]] getPartitions Method The number of partition for HadoopRDD, i.e. the return value of getPartitions , is calculated using InputFormat.getSplits(jobConf, minPartitions) where minPartitions is only a hint of how many partitions one may want at minimum. As a hint it does not mean the number of partitions will be exactly the number given. For SparkContext.textFile the input format class is https://hadoop.apache.org/docs/current/api/org/apache/hadoop/mapred/TextInputFormat.html[org.apache.hadoop.mapred.TextInputFormat ]. The https://hadoop.apache.org/docs/current/api/org/apache/hadoop/mapred/FileInputFormat.html[javadoc of org.apache.hadoop.mapred.FileInputFormat] says: FileInputFormat is the base class for all file-based InputFormats. This provides a generic implementation of getSplits(JobConf, int). Subclasses of FileInputFormat can also override the isSplitable(FileSystem, Path) method to ensure input-files are not split-up and are processed as a whole by Mappers. TIP: You may find https://github.com/apache/hadoop/blob/trunk/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/FileInputFormat.java#L319[the sources of org.apache.hadoop.mapred.FileInputFormat.getSplits] enlightening.","title":"What's Hadoop Split? input splits for Hadoop reads? See InputFormat.getSplits"},{"location":"rdd/spark-rdd-MapPartitionsRDD/","text":"== [[MapPartitionsRDD]] MapPartitionsRDD MapPartitionsRDD is an xref:rdd:RDD.adoc[RDD] that has exactly xref:rdd:spark-rdd-NarrowDependency.adoc#OneToOneDependency[one-to-one narrow dependency] on the < > and \"describes\" a distributed computation of the given < > to every RDD partition. MapPartitionsRDD is < > when: PairRDDFunctions ( RDD[(K, V)] ) is requested to xref:rdd:PairRDDFunctions.adoc#mapValues[mapValues] and xref:rdd:PairRDDFunctions.adoc#flatMapValues[flatMapValues] (with the < > flag enabled) RDD[T] is requested to < >, < >, < >, < >, < >, < >, < >, and < > RDDBarrier[T] is requested to < > (with the < > flag enabled) By default, it does not preserve partitioning -- the last input parameter preservesPartitioning is false . If it is true , it retains the original RDD's partitioning. MapPartitionsRDD is the result of the following transformations: filter glom link:spark-rdd-transformations.adoc#mapPartitions[mapPartitions] mapPartitionsWithIndex xref:rdd:PairRDDFunctions.adoc#mapValues[PairRDDFunctions.mapValues] xref:rdd:PairRDDFunctions.adoc#flatMapValues[PairRDDFunctions.flatMapValues] [[isBarrier_]] When requested for the xref:rdd:RDD.adoc#isBarrier_[isBarrier_] flag, MapPartitionsRDD gives the < > flag or check whether any of the RDDs of the xref:rdd:RDD.adoc#dependencies[RDD dependencies] are xref:rdd:RDD.adoc#isBarrier[barrier-enabled]. === [[creating-instance]] Creating MapPartitionsRDD Instance MapPartitionsRDD takes the following to be created: [[prev]] Parent xref:rdd:RDD.adoc[RDD] ( RDD[T] ) [[f]] Function to execute on partitions + (TaskContext, partitionID, Iterator[T]) => Iterator[U] [[preservesPartitioning]] preservesPartitioning flag (default: false ) [[isFromBarrier]] isFromBarrier flag for < > (default: false ) [[isOrderSensitive]] isOrderSensitive flag (default: false )","title":"MapPartitionsRDD"},{"location":"rdd/spark-rdd-NarrowDependency/","text":"== [[NarrowDependency]] NarrowDependency -- Narrow Dependencies NarrowDependency is a base (abstract) link:spark-rdd-Dependency.adoc[Dependency] with narrow (limited) number of link:spark-rdd-Partition.adoc[partitions] of the parent RDD that are required to compute a partition of the child RDD. NOTE: Narrow dependencies allow for pipelined execution. .Concrete NarrowDependency -ies [cols=\"1,2\",options=\"header\",width=\"100%\"] |=== | Name | Description | < > | | < > | | < > | |=== === [[contract]] NarrowDependency Contract NarrowDependency contract assumes that extensions implement getParents method. [source, scala] \u00b6 def getParents(partitionId: Int): Seq[Int] \u00b6 getParents returns the partitions of the parent RDD that the input partitionId depends on. === [[OneToOneDependency]] OneToOneDependency OneToOneDependency is a narrow dependency that represents a one-to-one dependency between partitions of the parent and child RDDs. scala> val r1 = sc.parallelize(0 to 9) r1: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[13] at parallelize at <console>:18 scala> val r3 = r1.map((_, 1)) r3: org.apache.spark.rdd.RDD[(Int, Int)] = MapPartitionsRDD[19] at map at <console>:20 scala> r3.dependencies res32: Seq[org.apache.spark.Dependency[_]] = List(org.apache.spark.OneToOneDependency@7353a0fb) scala> r3.toDebugString res33: String = (8) MapPartitionsRDD[19] at map at <console>:20 [] | ParallelCollectionRDD[13] at parallelize at <console>:18 [] === [[PruneDependency]] PruneDependency PruneDependency is a narrow dependency that represents a dependency between the PartitionPruningRDD and its parent RDD. === [[RangeDependency]] RangeDependency RangeDependency is a narrow dependency that represents a one-to-one dependency between ranges of partitions in the parent and child RDDs. It is used in UnionRDD for SparkContext.union , RDD.union transformation to list only a few. scala> val r1 = sc.parallelize(0 to 9) r1: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[13] at parallelize at <console>:18 scala> val r2 = sc.parallelize(10 to 19) r2: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[14] at parallelize at <console>:18 scala> val unioned = sc.union(r1, r2) unioned: org.apache.spark.rdd.RDD[Int] = UnionRDD[16] at union at <console>:22 scala> unioned.dependencies res19: Seq[org.apache.spark.Dependency[_]] = ArrayBuffer(org.apache.spark.RangeDependency@28408ad7, org.apache.spark.RangeDependency@6e1d2e9f) scala> unioned.toDebugString res18: String = (16) UnionRDD[16] at union at <console>:22 [] | ParallelCollectionRDD[13] at parallelize at <console>:18 [] | ParallelCollectionRDD[14] at parallelize at <console>:18 []","title":"NarrowDependency"},{"location":"rdd/spark-rdd-NarrowDependency/#source-scala","text":"","title":"[source, scala]"},{"location":"rdd/spark-rdd-NarrowDependency/#def-getparentspartitionid-int-seqint","text":"getParents returns the partitions of the parent RDD that the input partitionId depends on. === [[OneToOneDependency]] OneToOneDependency OneToOneDependency is a narrow dependency that represents a one-to-one dependency between partitions of the parent and child RDDs. scala> val r1 = sc.parallelize(0 to 9) r1: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[13] at parallelize at <console>:18 scala> val r3 = r1.map((_, 1)) r3: org.apache.spark.rdd.RDD[(Int, Int)] = MapPartitionsRDD[19] at map at <console>:20 scala> r3.dependencies res32: Seq[org.apache.spark.Dependency[_]] = List(org.apache.spark.OneToOneDependency@7353a0fb) scala> r3.toDebugString res33: String = (8) MapPartitionsRDD[19] at map at <console>:20 [] | ParallelCollectionRDD[13] at parallelize at <console>:18 [] === [[PruneDependency]] PruneDependency PruneDependency is a narrow dependency that represents a dependency between the PartitionPruningRDD and its parent RDD. === [[RangeDependency]] RangeDependency RangeDependency is a narrow dependency that represents a one-to-one dependency between ranges of partitions in the parent and child RDDs. It is used in UnionRDD for SparkContext.union , RDD.union transformation to list only a few. scala> val r1 = sc.parallelize(0 to 9) r1: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[13] at parallelize at <console>:18 scala> val r2 = sc.parallelize(10 to 19) r2: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[14] at parallelize at <console>:18 scala> val unioned = sc.union(r1, r2) unioned: org.apache.spark.rdd.RDD[Int] = UnionRDD[16] at union at <console>:22 scala> unioned.dependencies res19: Seq[org.apache.spark.Dependency[_]] = ArrayBuffer(org.apache.spark.RangeDependency@28408ad7, org.apache.spark.RangeDependency@6e1d2e9f) scala> unioned.toDebugString res18: String = (16) UnionRDD[16] at union at <console>:22 [] | ParallelCollectionRDD[13] at parallelize at <console>:18 [] | ParallelCollectionRDD[14] at parallelize at <console>:18 []","title":"def getParents(partitionId: Int): Seq[Int]"},{"location":"rdd/spark-rdd-NewHadoopRDD/","text":"== [[NewHadoopRDD]] NewHadoopRDD NewHadoopRDD is an xref:rdd:index.adoc[RDD] of K keys and V values. < NewHadoopRDD is created>> when: SparkContext.newAPIHadoopFile SparkContext.newAPIHadoopRDD (indirectly) SparkContext.binaryFiles (indirectly) SparkContext.wholeTextFiles NOTE: NewHadoopRDD is the base RDD of BinaryFileRDD and WholeTextFileRDD . === [[getPreferredLocations]] getPreferredLocations Method CAUTION: FIXME === [[creating-instance]] Creating NewHadoopRDD Instance NewHadoopRDD takes the following when created: [[sc]] xref:ROOT:SparkContext.adoc[] [[inputFormatClass]] HDFS' InputFormat[K, V] [[keyClass]] K class name [[valueClass]] V class name [[_conf]] transient HDFS' Configuration NewHadoopRDD initializes the < >.","title":"NewHadoopRDD"},{"location":"rdd/spark-rdd-OrderedRDDFunctions/","text":"== [[OrderedRDDFunctions]] OrderedRDDFunctions === [[repartitionAndSortWithinPartitions]] repartitionAndSortWithinPartitions Operator CAUTION: FIXME === [[sortByKey]] sortByKey Operator CAUTION: FIXME","title":"OrderedRDDFunctions"},{"location":"rdd/spark-rdd-ParallelCollectionRDD/","text":"== ParallelCollectionRDD ParallelCollectionRDD is an RDD of a collection of elements with numSlices partitions and optional locationPrefs . ParallelCollectionRDD is the result of SparkContext.parallelize and SparkContext.makeRDD methods. The data collection is split on to numSlices slices. It uses ParallelCollectionPartition .","title":"ParallelCollectionRDD"},{"location":"rdd/spark-rdd-Partition/","text":"== [[Partition]] Partition Partition is a < > of a < > of a RDD. NOTE: A partition is missing when it has not be computed yet. [[contract]] [[index]] Partition is identified by an partition index that is a unique identifier of a partition of a RDD. [source, scala] \u00b6 index: Int \u00b6","title":"Partition"},{"location":"rdd/spark-rdd-Partition/#source-scala","text":"","title":"[source, scala]"},{"location":"rdd/spark-rdd-Partition/#index-int","text":"","title":"index: Int"},{"location":"rdd/spark-rdd-SubtractedRDD/","text":"== [[SubtractedRDD]] SubtractedRDD CAUTION: FIXME === [[compute]] Computing Partition (in TaskContext) -- compute Method [source, scala] \u00b6 compute(p: Partition, context: TaskContext): Iterator[(K, V)] \u00b6 NOTE: compute is part of xref:rdd:RDD.adoc#compute[RDD Contract] to compute a link:spark-rdd-Partition.adoc[partition] (in a link:spark-TaskContext.adoc[TaskContext]). compute ...FIXME === [[getDependencies]] getDependencies Method CAUTION: FIXME","title":"SubtractedRDD"},{"location":"rdd/spark-rdd-SubtractedRDD/#source-scala","text":"","title":"[source, scala]"},{"location":"rdd/spark-rdd-SubtractedRDD/#computep-partition-context-taskcontext-iteratork-v","text":"NOTE: compute is part of xref:rdd:RDD.adoc#compute[RDD Contract] to compute a link:spark-rdd-Partition.adoc[partition] (in a link:spark-TaskContext.adoc[TaskContext]). compute ...FIXME === [[getDependencies]] getDependencies Method CAUTION: FIXME","title":"compute(p: Partition, context: TaskContext): Iterator[(K, V)]"},{"location":"rdd/spark-rdd-actions/","text":"== Actions Actions are link:spark-rdd-operations.adoc[RDD operations] that produce non-RDD values. They materialize a value in a Spark program. In other words, a RDD operation that returns a value of any type but RDD[T] is an action. action: RDD => a value NOTE: Actions are synchronous. You can use < > to release a calling thread while calling actions. They trigger execution of < > to return values. Simply put, an action evaluates the link:spark-rdd-lineage.adoc[RDD lineage graph]. You can think of actions as a valve and until action is fired, the data to be processed is not even in the pipes, i.e. transformations. Only actions can materialize the entire processing pipeline with real data. Actions are one of two ways to send data from xref:executor:Executor.adoc[executors] to the link:spark-driver.adoc[driver] (the other being link:spark-accumulators.adoc[accumulators]). Actions in http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.rdd.RDD[org.apache.spark.rdd.RDD ]: aggregate collect count countApprox* countByValue* first fold foreach foreachPartition max min reduce link:spark-io.adoc#saving-rdds-to-files[saveAs* actions], e.g. saveAsTextFile , saveAsHadoopFile take takeOrdered takeSample toLocalIterator top treeAggregate treeReduce Actions run link:spark-scheduler-ActiveJob.adoc[jobs] using xref:ROOT:SparkContext.adoc#runJob[SparkContext.runJob] or directly xref:scheduler:DAGScheduler.adoc#runJob[DAGScheduler.runJob]. [source,scala] \u00b6 scala> words.count // <1> res0: Long = 502 <1> words is an RDD of String . TIP: You should cache RDDs you work with when you want to execute two or more actions on it for a better performance. Refer to link:spark-rdd-caching.adoc[RDD Caching and Persistence]. Before calling an action, Spark does closure/function cleaning (using SparkContext.clean ) to make it ready for serialization and sending over the wire to executors. Cleaning can throw a SparkException if the computation cannot be cleaned. NOTE: Spark uses ClosureCleaner to clean closures. === [[AsyncRDDActions]] AsyncRDDActions AsyncRDDActions class offers asynchronous actions that you can use on RDDs (thanks to the implicit conversion rddToAsyncRDDActions in RDD class). The methods return a < >. The following asynchronous methods are available: countAsync collectAsync takeAsync foreachAsync foreachPartitionAsync === [[FutureAction]] FutureActions CAUTION: FIXME","title":"Actions"},{"location":"rdd/spark-rdd-actions/#sourcescala","text":"scala> words.count // <1> res0: Long = 502 <1> words is an RDD of String . TIP: You should cache RDDs you work with when you want to execute two or more actions on it for a better performance. Refer to link:spark-rdd-caching.adoc[RDD Caching and Persistence]. Before calling an action, Spark does closure/function cleaning (using SparkContext.clean ) to make it ready for serialization and sending over the wire to executors. Cleaning can throw a SparkException if the computation cannot be cleaned. NOTE: Spark uses ClosureCleaner to clean closures. === [[AsyncRDDActions]] AsyncRDDActions AsyncRDDActions class offers asynchronous actions that you can use on RDDs (thanks to the implicit conversion rddToAsyncRDDActions in RDD class). The methods return a < >. The following asynchronous methods are available: countAsync collectAsync takeAsync foreachAsync foreachPartitionAsync === [[FutureAction]] FutureActions CAUTION: FIXME","title":"[source,scala]"},{"location":"rdd/spark-rdd-caching/","text":"== RDD Caching and Persistence Caching or persistence are optimisation techniques for (iterative and interactive) Spark computations. They help saving interim partial results so they can be reused in subsequent stages. These interim results as RDDs are thus kept in memory (default) or more solid storages like disk and/or replicated. RDDs can be cached using < > operation. They can also be persisted using < > operation. The difference between cache and persist operations is purely syntactic. cache is a synonym of persist or persist(MEMORY_ONLY) , i.e. cache is merely persist with the default storage level MEMORY_ONLY . NOTE: Due to the very small and purely syntactic difference between caching and persistence of RDDs the two terms are often used interchangeably and I will follow the \"pattern\" here. RDDs can also be < > to remove RDD from a permanent storage like memory and/or disk. === [[cache]] Caching RDD -- cache Method [source, scala] \u00b6 cache(): this.type = persist() \u00b6 cache is a synonym of < > with xref:storage:StorageLevel.adoc[ MEMORY_ONLY storage level]. === [[persist]] Persisting RDD -- persist Methods [source, scala] \u00b6 persist(): this.type persist(newLevel: StorageLevel): this.type persist marks a RDD for persistence using newLevel xref:storage:StorageLevel.adoc[storage level]. You can only change the storage level once or persist reports an UnsupportedOperationException : Cannot change storage level of an RDD after it was already assigned a level NOTE: You can pretend to change the storage level of an RDD with already-assigned storage level only if the storage level is the same as it is currently assigned. If the RDD is marked as persistent the first time, the RDD is xref:core:ContextCleaner.adoc#registerRDDForCleanup[registered to ContextCleaner ] (if available) and xref:ROOT:SparkContext.adoc#persistRDD[ SparkContext ]. The internal storageLevel attribute is set to the input newLevel storage level. === [[unpersist]] Unpersisting RDDs (Clearing Blocks) -- unpersist Method [source, scala] \u00b6 unpersist(blocking: Boolean = true): this.type \u00b6 When called, unpersist prints the following INFO message to the logs: INFO [RddName]: Removing RDD [id] from persistence list It then calls xref:ROOT:SparkContext.adoc#unpersist[SparkContext.unpersistRDD(id, blocking)] and sets xref:storage:StorageLevel.adoc[ NONE storage level] as the current storage level.","title":"Caching and Persistence"},{"location":"rdd/spark-rdd-caching/#source-scala","text":"","title":"[source, scala]"},{"location":"rdd/spark-rdd-caching/#cache-thistype-persist","text":"cache is a synonym of < > with xref:storage:StorageLevel.adoc[ MEMORY_ONLY storage level]. === [[persist]] Persisting RDD -- persist Methods","title":"cache(): this.type = persist()"},{"location":"rdd/spark-rdd-caching/#source-scala_1","text":"persist(): this.type persist(newLevel: StorageLevel): this.type persist marks a RDD for persistence using newLevel xref:storage:StorageLevel.adoc[storage level]. You can only change the storage level once or persist reports an UnsupportedOperationException : Cannot change storage level of an RDD after it was already assigned a level NOTE: You can pretend to change the storage level of an RDD with already-assigned storage level only if the storage level is the same as it is currently assigned. If the RDD is marked as persistent the first time, the RDD is xref:core:ContextCleaner.adoc#registerRDDForCleanup[registered to ContextCleaner ] (if available) and xref:ROOT:SparkContext.adoc#persistRDD[ SparkContext ]. The internal storageLevel attribute is set to the input newLevel storage level. === [[unpersist]] Unpersisting RDDs (Clearing Blocks) -- unpersist Method","title":"[source, scala]"},{"location":"rdd/spark-rdd-caching/#source-scala_2","text":"","title":"[source, scala]"},{"location":"rdd/spark-rdd-caching/#unpersistblocking-boolean-true-thistype","text":"When called, unpersist prints the following INFO message to the logs: INFO [RddName]: Removing RDD [id] from persistence list It then calls xref:ROOT:SparkContext.adoc#unpersist[SparkContext.unpersistRDD(id, blocking)] and sets xref:storage:StorageLevel.adoc[ NONE storage level] as the current storage level.","title":"unpersist(blocking: Boolean = true): this.type"},{"location":"rdd/spark-rdd-lineage/","text":"== RDD Lineage -- Logical Execution Plan RDD Lineage (aka RDD operator graph or RDD dependency graph ) is a graph of all the parent RDDs of a RDD. It is built as a result of applying transformations to the RDD and creates a < >. NOTE: The execution DAG or physical execution plan is the xref:scheduler:DAGScheduler.adoc[DAG of stages]. NOTE: The following diagram uses cartesian or zip for learning purposes only. You may use other operators to build a RDD graph. .RDD lineage image::rdd-lineage.png[align=\"center\"] The above RDD graph could be the result of the following series of transformations: val r00 = sc.parallelize(0 to 9) val r01 = sc.parallelize(0 to 90 by 10) val r10 = r00 cartesian r01 val r11 = r00.map(n => (n, n)) val r12 = r00 zip r01 val r13 = r01.keyBy(_ / 20) val r20 = Seq(r11, r12, r13).foldLeft(r10)(_ union _) A RDD lineage graph is hence a graph of what transformations need to be executed after an action has been called. You can learn about a RDD lineage graph using < > method. === [[logical-execution-plan]] Logical Execution Plan Logical Execution Plan starts with the earliest RDDs (those with no dependencies on other RDDs or reference cached data) and ends with the RDD that produces the result of the action that has been called to execute. NOTE: A logical plan, i.e. a DAG, is materialized and executed when xref:ROOT:SparkContext.adoc#runJob[ SparkContext is requested to run a Spark job]. === [[toDebugString]] Getting RDD Lineage Graph -- toDebugString Method [source, scala] \u00b6 toDebugString: String \u00b6 You can learn about a < > using toDebugString method. scala> val wordCount = sc.textFile(\"README.md\").flatMap(_.split(\"\\\\s+\")).map((_, 1)).reduceByKey(_ + _) wordCount: org.apache.spark.rdd.RDD[(String, Int)] = ShuffledRDD[21] at reduceByKey at <console>:24 scala> wordCount.toDebugString res13: String = (2) ShuffledRDD[21] at reduceByKey at <console>:24 [] +-(2) MapPartitionsRDD[20] at map at <console>:24 [] | MapPartitionsRDD[19] at flatMap at <console>:24 [] | README.md MapPartitionsRDD[18] at textFile at <console>:24 [] | README.md HadoopRDD[17] at textFile at <console>:24 [] toDebugString uses indentations to indicate a shuffle boundary. The numbers in round brackets show the level of parallelism at each stage, e.g. (2) in the above output. scala> wordCount.getNumPartitions res14: Int = 2 With < > property enabled, toDebugString is included when executing an action. $ ./bin/spark-shell --conf spark.logLineage=true scala> sc.textFile(\"README.md\", 4).count ... 15/10/17 14:46:42 INFO SparkContext: Starting job: count at <console>:25 15/10/17 14:46:42 INFO SparkContext: RDD's recursive dependencies: (4) MapPartitionsRDD[1] at textFile at <console>:25 [] | README.md HadoopRDD[0] at textFile at <console>:25 [] === [[settings]] Settings .Spark Properties [cols=\"1,1,2\",options=\"header\",width=\"100%\"] |=== | Spark Property | Default Value | Description | [[spark_logLineage]] spark.logLineage | false | When enabled (i.e. true ), executing an action (and hence xref:ROOT:SparkContext.adoc#runJob[running a job]) will also print out the RDD lineage graph using < >. |===","title":"RDD Lineage"},{"location":"rdd/spark-rdd-lineage/#source-scala","text":"","title":"[source, scala]"},{"location":"rdd/spark-rdd-lineage/#todebugstring-string","text":"You can learn about a < > using toDebugString method. scala> val wordCount = sc.textFile(\"README.md\").flatMap(_.split(\"\\\\s+\")).map((_, 1)).reduceByKey(_ + _) wordCount: org.apache.spark.rdd.RDD[(String, Int)] = ShuffledRDD[21] at reduceByKey at <console>:24 scala> wordCount.toDebugString res13: String = (2) ShuffledRDD[21] at reduceByKey at <console>:24 [] +-(2) MapPartitionsRDD[20] at map at <console>:24 [] | MapPartitionsRDD[19] at flatMap at <console>:24 [] | README.md MapPartitionsRDD[18] at textFile at <console>:24 [] | README.md HadoopRDD[17] at textFile at <console>:24 [] toDebugString uses indentations to indicate a shuffle boundary. The numbers in round brackets show the level of parallelism at each stage, e.g. (2) in the above output. scala> wordCount.getNumPartitions res14: Int = 2 With < > property enabled, toDebugString is included when executing an action. $ ./bin/spark-shell --conf spark.logLineage=true scala> sc.textFile(\"README.md\", 4).count ... 15/10/17 14:46:42 INFO SparkContext: Starting job: count at <console>:25 15/10/17 14:46:42 INFO SparkContext: RDD's recursive dependencies: (4) MapPartitionsRDD[1] at textFile at <console>:25 [] | README.md HadoopRDD[0] at textFile at <console>:25 [] === [[settings]] Settings .Spark Properties [cols=\"1,1,2\",options=\"header\",width=\"100%\"] |=== | Spark Property | Default Value | Description | [[spark_logLineage]] spark.logLineage | false | When enabled (i.e. true ), executing an action (and hence xref:ROOT:SparkContext.adoc#runJob[running a job]) will also print out the RDD lineage graph using < >. |===","title":"toDebugString: String"},{"location":"rdd/spark-rdd-operations/","text":"== Operators - Transformations and Actions RDDs have two types of operations: link:spark-rdd-transformations.adoc[transformations] and link:spark-rdd-actions.adoc[actions]. NOTE: Operators are also called operations . === Gotchas - things to watch for Even if you don't access it explicitly it cannot be referenced inside a closure as it is serialized and carried around across executors. See https://issues.apache.org/jira/browse/SPARK-5063","title":"Operators"},{"location":"rdd/spark-rdd-partitions/","text":"== Partitions and Partitioning === Introduction Depending on how you look at Spark (programmer, devop, admin), an RDD is about the content (developer's and data scientist's perspective) or how it gets spread out over a cluster (performance), i.e. how many partitions an RDD represents. A partition (aka split ) is a logical chunk of a large distributed data set. [CAUTION] \u00b6 FIXME How does the number of partitions map to the number of tasks? How to verify it? How does the mapping between partitions and tasks correspond to data locality if any? \u00b6 Spark manages data using partitions that helps parallelize distributed data processing with minimal network traffic for sending data between executors. By default, Spark tries to read data into an RDD from the nodes that are close to it. Since Spark usually accesses distributed partitioned data, to optimize transformation operations it creates partitions to hold the data chunks. There is a one-to-one correspondence between how data is laid out in data storage like HDFS or Cassandra (it is partitioned for the same reasons). Features: size number partitioning scheme node distribution repartitioning [TIP] \u00b6 Read the following documentations to learn what experts say on the topic: https://databricks.gitbooks.io/databricks-spark-knowledge-base/content/performance_optimization/how_many_partitions_does_an_rdd_have.html[How Many Partitions Does An RDD Have?] https://spark.apache.org/docs/latest/tuning.html[Tuning Spark] (the official documentation of Spark) \u00b6 By default, a partition is created for each HDFS partition, which by default is 64MB (from http://spark.apache.org/docs/latest/programming-guide.html#external-datasets[Spark's Programming Guide]). RDDs get partitioned automatically without programmer intervention. However, there are times when you'd like to adjust the size and number of partitions or the partitioning scheme according to the needs of your application. You use def getPartitions: Array[Partition] method on a RDD to know the set of partitions in this RDD. As noted in https://github.com/databricks/spark-knowledgebase/blob/master/performance_optimization/how_many_partitions_does_an_rdd_have.md#view-task-execution-against-partitions-using-the-ui[View Task Execution Against Partitions Using the UI]: When a stage executes, you can see the number of partitions for a given stage in the Spark UI. Start spark-shell and see it yourself! scala> sc.parallelize(1 to 100).count res0: Long = 100 When you execute the Spark job, i.e. sc.parallelize(1 to 100).count , you should see the following in http://localhost:4040/jobs[Spark shell application UI]. .The number of partition as Total tasks in UI image::spark-partitions-ui-stages.png[align=\"center\"] The reason for 8 Tasks in Total is that I'm on a 8-core laptop and by default the number of partitions is the number of all available cores. $ sysctl -n hw.ncpu 8 You can request for the minimum number of partitions, using the second input parameter to many transformations. scala> sc.parallelize(1 to 100, 2).count res1: Long = 100 .Total tasks in UI shows 2 partitions image::spark-partitions-ui-stages-2-partitions.png[align=\"center\"] You can always ask for the number of partitions using partitions method of a RDD: scala> val ints = sc.parallelize(1 to 100, 4) ints: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[1] at parallelize at <console>:24 scala> ints.partitions.size res2: Int = 4 In general, smaller/more numerous partitions allow work to be distributed among more workers, but larger/fewer partitions allow work to be done in larger chunks, which may result in the work getting done more quickly as long as all workers are kept busy, due to reduced overhead. Increasing partitions count will make each partition to have less data (or not at all!) Spark can only run 1 concurrent task for every partition of an RDD, up to the number of cores in your cluster. So if you have a cluster with 50 cores, you want your RDDs to at least have 50 partitions (and probably http://spark.apache.org/docs/latest/tuning.html#level-of-parallelism[2-3x times that]). As far as choosing a \"good\" number of partitions, you generally want at least as many as the number of executors for parallelism. You can get this computed value by calling sc.defaultParallelism . Also, the number of partitions determines how many files get generated by actions that save RDDs to files. The maximum size of a partition is ultimately limited by the available memory of an executor. In the first RDD transformation, e.g. reading from a file using sc.textFile(path, partition) , the partition parameter will be applied to all further transformations and actions on this RDD. Partitions get redistributed among nodes whenever shuffle occurs. Repartitioning may cause shuffle to occur in some situations, but it is not guaranteed to occur in all cases. And it usually happens during action stage. When creating an RDD by reading a file using rdd = SparkContext().textFile(\"hdfs://.../file.txt\") the number of partitions may be smaller. Ideally, you would get the same number of blocks as you see in HDFS, but if the lines in your file are too long (longer than the block size), there will be fewer partitions. Preferred way to set up the number of partitions for an RDD is to directly pass it as the second input parameter in the call like rdd = sc.textFile(\"hdfs://.../file.txt\", 400) , where 400 is the number of partitions. In this case, the partitioning makes for 400 splits that would be done by the Hadoop's TextInputFormat , not Spark and it would work much faster. It's also that the code spawns 400 concurrent tasks to try to load file.txt directly into 400 partitions. It will only work as described for uncompressed files. When using textFile with compressed files ( file.txt.gz not file.txt or similar), Spark disables splitting that makes for an RDD with only 1 partition (as reads against gzipped files cannot be parallelized). In this case, to change the number of partitions you should do < >. Some operations, e.g. map , flatMap , filter , don't preserve partitioning. map , flatMap , filter operations apply a function to every partition. === [[repartitioning]][[repartition]] Repartitioning RDD -- repartition Transformation [source, scala] \u00b6 repartition(numPartitions: Int)(implicit ord: Ordering[T] = null): RDD[T] \u00b6 repartition is < > with numPartitions and shuffle enabled. With the following computation you can see that repartition(5) causes 5 tasks to be started using NODE_LOCAL link:spark-data-locality.adoc[data locality]. scala> lines.repartition(5).count ... 15/10/07 08:10:00 INFO DAGScheduler: Submitting 5 missing tasks from ResultStage 7 (MapPartitionsRDD[19] at repartition at <console>:27) 15/10/07 08:10:00 INFO TaskSchedulerImpl: Adding task set 7.0 with 5 tasks 15/10/07 08:10:00 INFO TaskSetManager: Starting task 0.0 in stage 7.0 (TID 17, localhost, partition 0,NODE_LOCAL, 2089 bytes) 15/10/07 08:10:00 INFO TaskSetManager: Starting task 1.0 in stage 7.0 (TID 18, localhost, partition 1,NODE_LOCAL, 2089 bytes) 15/10/07 08:10:00 INFO TaskSetManager: Starting task 2.0 in stage 7.0 (TID 19, localhost, partition 2,NODE_LOCAL, 2089 bytes) 15/10/07 08:10:00 INFO TaskSetManager: Starting task 3.0 in stage 7.0 (TID 20, localhost, partition 3,NODE_LOCAL, 2089 bytes) 15/10/07 08:10:00 INFO TaskSetManager: Starting task 4.0 in stage 7.0 (TID 21, localhost, partition 4,NODE_LOCAL, 2089 bytes) ... You can see a change after executing repartition(1) causes 2 tasks to be started using PROCESS_LOCAL link:spark-data-locality.adoc[data locality]. scala> lines.repartition(1).count ... 15/10/07 08:14:09 INFO DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 8 (MapPartitionsRDD[20] at repartition at <console>:27) 15/10/07 08:14:09 INFO TaskSchedulerImpl: Adding task set 8.0 with 2 tasks 15/10/07 08:14:09 INFO TaskSetManager: Starting task 0.0 in stage 8.0 (TID 22, localhost, partition 0,PROCESS_LOCAL, 2058 bytes) 15/10/07 08:14:09 INFO TaskSetManager: Starting task 1.0 in stage 8.0 (TID 23, localhost, partition 1,PROCESS_LOCAL, 2058 bytes) ... Please note that Spark disables splitting for compressed files and creates RDDs with only 1 partition. In such cases, it's helpful to use sc.textFile('demo.gz') and do repartitioning using rdd.repartition(100) as follows: rdd = sc.textFile('demo.gz') rdd = rdd.repartition(100) With the lines, you end up with rdd to be exactly 100 partitions of roughly equal in size. rdd.repartition(N) does a shuffle to split data to match N ** partitioning is done on round robin basis TIP: If partitioning scheme doesn't work for you, you can write your own custom partitioner. TIP: It's useful to get familiar with https://hadoop.apache.org/docs/current/api/org/apache/hadoop/mapred/TextInputFormat.html[Hadoop's TextInputFormat]. === [[coalesce]] coalesce Transformation [source, scala] \u00b6 coalesce(numPartitions: Int, shuffle: Boolean = false)(implicit ord: Ordering[T] = null): RDD[T] \u00b6 The coalesce transformation is used to change the number of partitions. It can trigger link:spark-rdd-shuffle.adoc[RDD shuffling] depending on the shuffle flag (disabled by default, i.e. false ). In the following sample, you parallelize a local 10-number sequence and coalesce it first without and then with shuffling (note the shuffle parameter being false and true , respectively). TIP: Use link:spark-rdd-lineage.adoc#toDebugString[toDebugString] to check out the link:spark-rdd-lineage.adoc[RDD lineage graph]. scala> val rdd = sc.parallelize(0 to 10, 8) rdd: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[0] at parallelize at <console>:24 scala> rdd.partitions.size res0: Int = 8 scala> rdd.coalesce(numPartitions=8, shuffle=false) // <1> res1: org.apache.spark.rdd.RDD[Int] = CoalescedRDD[1] at coalesce at <console>:27 scala> res1.toDebugString res2: String = (8) CoalescedRDD[1] at coalesce at <console>:27 [] | ParallelCollectionRDD[0] at parallelize at <console>:24 [] scala> rdd.coalesce(numPartitions=8, shuffle=true) res3: org.apache.spark.rdd.RDD[Int] = MapPartitionsRDD[5] at coalesce at <console>:27 scala> res3.toDebugString res4: String = (8) MapPartitionsRDD[5] at coalesce at <console>:27 [] | CoalescedRDD[4] at coalesce at <console>:27 [] | ShuffledRDD[3] at coalesce at <console>:27 [] +-(8) MapPartitionsRDD[2] at coalesce at <console>:27 [] | ParallelCollectionRDD[0] at parallelize at <console>:24 [] <1> shuffle is false by default and it's explicitly used here for demo purposes. Note the number of partitions that remains the same as the number of partitions in the source RDD rdd .","title":"Partitions and Partitioning"},{"location":"rdd/spark-rdd-partitions/#caution","text":"FIXME How does the number of partitions map to the number of tasks? How to verify it?","title":"[CAUTION]"},{"location":"rdd/spark-rdd-partitions/#how-does-the-mapping-between-partitions-and-tasks-correspond-to-data-locality-if-any","text":"Spark manages data using partitions that helps parallelize distributed data processing with minimal network traffic for sending data between executors. By default, Spark tries to read data into an RDD from the nodes that are close to it. Since Spark usually accesses distributed partitioned data, to optimize transformation operations it creates partitions to hold the data chunks. There is a one-to-one correspondence between how data is laid out in data storage like HDFS or Cassandra (it is partitioned for the same reasons). Features: size number partitioning scheme node distribution repartitioning","title":"How does the mapping between partitions and tasks correspond to data locality if any?"},{"location":"rdd/spark-rdd-partitions/#tip","text":"Read the following documentations to learn what experts say on the topic: https://databricks.gitbooks.io/databricks-spark-knowledge-base/content/performance_optimization/how_many_partitions_does_an_rdd_have.html[How Many Partitions Does An RDD Have?]","title":"[TIP]"},{"location":"rdd/spark-rdd-partitions/#httpssparkapacheorgdocslatesttuninghtmltuning-spark-the-official-documentation-of-spark","text":"By default, a partition is created for each HDFS partition, which by default is 64MB (from http://spark.apache.org/docs/latest/programming-guide.html#external-datasets[Spark's Programming Guide]). RDDs get partitioned automatically without programmer intervention. However, there are times when you'd like to adjust the size and number of partitions or the partitioning scheme according to the needs of your application. You use def getPartitions: Array[Partition] method on a RDD to know the set of partitions in this RDD. As noted in https://github.com/databricks/spark-knowledgebase/blob/master/performance_optimization/how_many_partitions_does_an_rdd_have.md#view-task-execution-against-partitions-using-the-ui[View Task Execution Against Partitions Using the UI]: When a stage executes, you can see the number of partitions for a given stage in the Spark UI. Start spark-shell and see it yourself! scala> sc.parallelize(1 to 100).count res0: Long = 100 When you execute the Spark job, i.e. sc.parallelize(1 to 100).count , you should see the following in http://localhost:4040/jobs[Spark shell application UI]. .The number of partition as Total tasks in UI image::spark-partitions-ui-stages.png[align=\"center\"] The reason for 8 Tasks in Total is that I'm on a 8-core laptop and by default the number of partitions is the number of all available cores. $ sysctl -n hw.ncpu 8 You can request for the minimum number of partitions, using the second input parameter to many transformations. scala> sc.parallelize(1 to 100, 2).count res1: Long = 100 .Total tasks in UI shows 2 partitions image::spark-partitions-ui-stages-2-partitions.png[align=\"center\"] You can always ask for the number of partitions using partitions method of a RDD: scala> val ints = sc.parallelize(1 to 100, 4) ints: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[1] at parallelize at <console>:24 scala> ints.partitions.size res2: Int = 4 In general, smaller/more numerous partitions allow work to be distributed among more workers, but larger/fewer partitions allow work to be done in larger chunks, which may result in the work getting done more quickly as long as all workers are kept busy, due to reduced overhead. Increasing partitions count will make each partition to have less data (or not at all!) Spark can only run 1 concurrent task for every partition of an RDD, up to the number of cores in your cluster. So if you have a cluster with 50 cores, you want your RDDs to at least have 50 partitions (and probably http://spark.apache.org/docs/latest/tuning.html#level-of-parallelism[2-3x times that]). As far as choosing a \"good\" number of partitions, you generally want at least as many as the number of executors for parallelism. You can get this computed value by calling sc.defaultParallelism . Also, the number of partitions determines how many files get generated by actions that save RDDs to files. The maximum size of a partition is ultimately limited by the available memory of an executor. In the first RDD transformation, e.g. reading from a file using sc.textFile(path, partition) , the partition parameter will be applied to all further transformations and actions on this RDD. Partitions get redistributed among nodes whenever shuffle occurs. Repartitioning may cause shuffle to occur in some situations, but it is not guaranteed to occur in all cases. And it usually happens during action stage. When creating an RDD by reading a file using rdd = SparkContext().textFile(\"hdfs://.../file.txt\") the number of partitions may be smaller. Ideally, you would get the same number of blocks as you see in HDFS, but if the lines in your file are too long (longer than the block size), there will be fewer partitions. Preferred way to set up the number of partitions for an RDD is to directly pass it as the second input parameter in the call like rdd = sc.textFile(\"hdfs://.../file.txt\", 400) , where 400 is the number of partitions. In this case, the partitioning makes for 400 splits that would be done by the Hadoop's TextInputFormat , not Spark and it would work much faster. It's also that the code spawns 400 concurrent tasks to try to load file.txt directly into 400 partitions. It will only work as described for uncompressed files. When using textFile with compressed files ( file.txt.gz not file.txt or similar), Spark disables splitting that makes for an RDD with only 1 partition (as reads against gzipped files cannot be parallelized). In this case, to change the number of partitions you should do < >. Some operations, e.g. map , flatMap , filter , don't preserve partitioning. map , flatMap , filter operations apply a function to every partition. === [[repartitioning]][[repartition]] Repartitioning RDD -- repartition Transformation","title":"https://spark.apache.org/docs/latest/tuning.html[Tuning Spark] (the official documentation of Spark)"},{"location":"rdd/spark-rdd-partitions/#source-scala","text":"","title":"[source, scala]"},{"location":"rdd/spark-rdd-partitions/#repartitionnumpartitions-intimplicit-ord-orderingt-null-rddt","text":"repartition is < > with numPartitions and shuffle enabled. With the following computation you can see that repartition(5) causes 5 tasks to be started using NODE_LOCAL link:spark-data-locality.adoc[data locality]. scala> lines.repartition(5).count ... 15/10/07 08:10:00 INFO DAGScheduler: Submitting 5 missing tasks from ResultStage 7 (MapPartitionsRDD[19] at repartition at <console>:27) 15/10/07 08:10:00 INFO TaskSchedulerImpl: Adding task set 7.0 with 5 tasks 15/10/07 08:10:00 INFO TaskSetManager: Starting task 0.0 in stage 7.0 (TID 17, localhost, partition 0,NODE_LOCAL, 2089 bytes) 15/10/07 08:10:00 INFO TaskSetManager: Starting task 1.0 in stage 7.0 (TID 18, localhost, partition 1,NODE_LOCAL, 2089 bytes) 15/10/07 08:10:00 INFO TaskSetManager: Starting task 2.0 in stage 7.0 (TID 19, localhost, partition 2,NODE_LOCAL, 2089 bytes) 15/10/07 08:10:00 INFO TaskSetManager: Starting task 3.0 in stage 7.0 (TID 20, localhost, partition 3,NODE_LOCAL, 2089 bytes) 15/10/07 08:10:00 INFO TaskSetManager: Starting task 4.0 in stage 7.0 (TID 21, localhost, partition 4,NODE_LOCAL, 2089 bytes) ... You can see a change after executing repartition(1) causes 2 tasks to be started using PROCESS_LOCAL link:spark-data-locality.adoc[data locality]. scala> lines.repartition(1).count ... 15/10/07 08:14:09 INFO DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 8 (MapPartitionsRDD[20] at repartition at <console>:27) 15/10/07 08:14:09 INFO TaskSchedulerImpl: Adding task set 8.0 with 2 tasks 15/10/07 08:14:09 INFO TaskSetManager: Starting task 0.0 in stage 8.0 (TID 22, localhost, partition 0,PROCESS_LOCAL, 2058 bytes) 15/10/07 08:14:09 INFO TaskSetManager: Starting task 1.0 in stage 8.0 (TID 23, localhost, partition 1,PROCESS_LOCAL, 2058 bytes) ... Please note that Spark disables splitting for compressed files and creates RDDs with only 1 partition. In such cases, it's helpful to use sc.textFile('demo.gz') and do repartitioning using rdd.repartition(100) as follows: rdd = sc.textFile('demo.gz') rdd = rdd.repartition(100) With the lines, you end up with rdd to be exactly 100 partitions of roughly equal in size. rdd.repartition(N) does a shuffle to split data to match N ** partitioning is done on round robin basis TIP: If partitioning scheme doesn't work for you, you can write your own custom partitioner. TIP: It's useful to get familiar with https://hadoop.apache.org/docs/current/api/org/apache/hadoop/mapred/TextInputFormat.html[Hadoop's TextInputFormat]. === [[coalesce]] coalesce Transformation","title":"repartition(numPartitions: Int)(implicit ord: Ordering[T] = null): RDD[T]"},{"location":"rdd/spark-rdd-partitions/#source-scala_1","text":"","title":"[source, scala]"},{"location":"rdd/spark-rdd-partitions/#coalescenumpartitions-int-shuffle-boolean-falseimplicit-ord-orderingt-null-rddt","text":"The coalesce transformation is used to change the number of partitions. It can trigger link:spark-rdd-shuffle.adoc[RDD shuffling] depending on the shuffle flag (disabled by default, i.e. false ). In the following sample, you parallelize a local 10-number sequence and coalesce it first without and then with shuffling (note the shuffle parameter being false and true , respectively). TIP: Use link:spark-rdd-lineage.adoc#toDebugString[toDebugString] to check out the link:spark-rdd-lineage.adoc[RDD lineage graph]. scala> val rdd = sc.parallelize(0 to 10, 8) rdd: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[0] at parallelize at <console>:24 scala> rdd.partitions.size res0: Int = 8 scala> rdd.coalesce(numPartitions=8, shuffle=false) // <1> res1: org.apache.spark.rdd.RDD[Int] = CoalescedRDD[1] at coalesce at <console>:27 scala> res1.toDebugString res2: String = (8) CoalescedRDD[1] at coalesce at <console>:27 [] | ParallelCollectionRDD[0] at parallelize at <console>:24 [] scala> rdd.coalesce(numPartitions=8, shuffle=true) res3: org.apache.spark.rdd.RDD[Int] = MapPartitionsRDD[5] at coalesce at <console>:27 scala> res3.toDebugString res4: String = (8) MapPartitionsRDD[5] at coalesce at <console>:27 [] | CoalescedRDD[4] at coalesce at <console>:27 [] | ShuffledRDD[3] at coalesce at <console>:27 [] +-(8) MapPartitionsRDD[2] at coalesce at <console>:27 [] | ParallelCollectionRDD[0] at parallelize at <console>:24 [] <1> shuffle is false by default and it's explicitly used here for demo purposes. Note the number of partitions that remains the same as the number of partitions in the source RDD rdd .","title":"coalesce(numPartitions: Int, shuffle: Boolean = false)(implicit ord: Ordering[T] = null): RDD[T]"},{"location":"rdd/spark-rdd-shuffle/","text":"= RDD shuffling :url-spark-docs: https://spark.apache.org/docs/{spark-version } TIP: Read the official documentation about the topic {url-spark-docs}/rdd-programming-guide.html#shuffle-operations[Shuffle operations]. It is still better than this page. Shuffling is a process of link:spark-rdd-partitions.adoc[redistributing data across partitions] (aka repartitioning ) that may or may not cause moving data across JVM processes or even over the wire (between executors on separate machines). Shuffling is the process of data transfer between stages. TIP: Avoid shuffling at all cost. Think about ways to leverage existing partitions. Leverage partial aggregation to reduce data transfer. By default, shuffling doesn't change the number of partitions, but their content. Avoid groupByKey and use reduceByKey or combineByKey instead. ** groupByKey shuffles all the data, which is slow. ** reduceByKey shuffles only the results of sub-aggregations in each partition of the data. == Example - join PairRDD offers http://people.apache.org/~pwendell/spark-nightly/spark-master-docs/latest/programming-guide.html#JoinLink[join ] transformation that (quoting the official documentation): When called on datasets of type (K, V) and (K, W), returns a dataset of (K, (V, W)) pairs with all pairs of elements for each key. Let's have a look at an example and see how it works under the covers: scala> val kv = (0 to 5) zip Stream.continually(5) kv: scala.collection.immutable.IndexedSeq[(Int, Int)] = Vector((0,5), (1,5), (2,5), (3,5), (4,5), (5,5)) scala> val kw = (0 to 5) zip Stream.continually(10) kw: scala.collection.immutable.IndexedSeq[(Int, Int)] = Vector((0,10), (1,10), (2,10), (3,10), (4,10), (5,10)) scala> val kvR = sc.parallelize(kv) kvR: org.apache.spark.rdd.RDD[(Int, Int)] = ParallelCollectionRDD[3] at parallelize at <console>:26 scala> val kwR = sc.parallelize(kw) kwR: org.apache.spark.rdd.RDD[(Int, Int)] = ParallelCollectionRDD[4] at parallelize at <console>:26 scala> val joined = kvR join kwR joined: org.apache.spark.rdd.RDD[(Int, (Int, Int))] = MapPartitionsRDD[10] at join at <console>:32 scala> joined.toDebugString res7: String = (8) MapPartitionsRDD[10] at join at <console>:32 [] | MapPartitionsRDD[9] at join at <console>:32 [] | CoGroupedRDD[8] at join at <console>:32 [] +-(8) ParallelCollectionRDD[3] at parallelize at <console>:26 [] +-(8) ParallelCollectionRDD[4] at parallelize at <console>:26 [] It doesn't look good when there is an \"angle\" between \"nodes\" in an operation graph. It appears before the join operation so shuffle is expected. Here is how the job of executing joined.count looks in Web UI. .Executing joined.count image::spark-shuffle-join-webui.png[align=\"center\"] The screenshot of Web UI shows 3 stages with two parallelize to Shuffle Write and count to Shuffle Read. It means shuffling has indeed happened. CAUTION: FIXME Just learnt about sc.range(0, 5) as a shorter version of sc.parallelize(0 to 5) join operation is one of the cogroup operations that uses defaultPartitioner , i.e. walks through link:spark-rdd-lineage.adoc[the RDD lineage graph] (sorted by the number of partitions decreasing) and picks the partitioner with positive number of output partitions. Otherwise, it checks xref:ROOT:configuration-properties.adoc#spark.default.parallelism[spark.default.parallelism] configuration and if defined picks xref:rdd:HashPartitioner.adoc[HashPartitioner] with the default parallelism of the xref:scheduler:SchedulerBackend.adoc[SchedulerBackend]. join is almost CoGroupedRDD.mapValues . CAUTION: FIXME the default parallelism of scheduler backend","title":"Shuffling"},{"location":"rdd/spark-rdd-transformations/","text":"== Transformations -- Lazy Operations on RDD (to Create One or More RDDs) Transformations are lazy operations on an xref:rdd:RDD.adoc[RDD] that create one or many new RDDs. // T and U are Scala types transformation: RDD[T] => RDD[U] transformation: RDD[T] => Seq[RDD[U]] In other words, transformations are functions that take an RDD as the input and produce one or many RDDs as the output. Transformations do not change the input RDD (since xref:rdd:index.adoc#introduction[RDDs are immutable] and hence cannot be modified), but produce one or more new RDDs by applying the computations they represent. [[methods]] .(Subset of) RDD Transformations (Public API) [cols=\"1m,3\",options=\"header\",width=\"100%\"] |=== | Method | Description | aggregate a| [[aggregate]] [source, scala] \u00b6 aggregate U ( seqOp: (U, T) => U, combOp: (U, U) => U): U | barrier a| [[barrier]] [source, scala] \u00b6 barrier(): RDDBarrier[T] \u00b6 ( New in 2.4.0 ) Marks the current stage as a < > in < >, where Spark must launch all tasks together Internally, barrier creates a < > over the RDD | cache a| [[cache]] [source, scala] \u00b6 cache(): this.type \u00b6 Persists the RDD with the xref:storage:StorageLevel.adoc#MEMORY_ONLY[MEMORY_ONLY] storage level Synonym of < > | coalesce a| [[coalesce]] [source, scala] \u00b6 coalesce( numPartitions: Int, shuffle: Boolean = false, partitionCoalescer: Option[PartitionCoalescer] = Option.empty) (implicit ord: Ordering[T] = null): RDD[T] | filter a| [[filter]] [source, scala] \u00b6 filter(f: T => Boolean): RDD[T] \u00b6 | flatMap a| [[flatMap]] [source, scala] \u00b6 flatMap U : RDD[U] \u00b6 | map a| [[map]] [source, scala] \u00b6 map U : RDD[U] \u00b6 | mapPartitions a| [[mapPartitions]] [source, scala] \u00b6 mapPartitions U : RDD[U] | mapPartitionsWithIndex a| [[mapPartitionsWithIndex]] [source, scala] \u00b6 mapPartitionsWithIndex U : RDD[U] | randomSplit a| [[randomSplit]] [source, scala] \u00b6 randomSplit( weights: Array[Double], seed: Long = Utils.random.nextLong): Array[RDD[T]] | union a| [[union]] [source, scala] \u00b6 ++(other: RDD[T]): RDD[T] union(other: RDD[T]): RDD[T] | persist a| [[persist]] [source, scala] \u00b6 persist(): this.type persist(newLevel: StorageLevel): this.type |=== By applying transformations you incrementally build a link:spark-rdd-lineage.adoc[RDD lineage] with all the parent RDDs of the final RDD(s). Transformations are lazy, i.e. are not executed immediately. Only after calling an action are transformations executed. After executing a transformation, the result RDD(s) will always be different from their parents and can be smaller (e.g. filter , count , distinct , sample ), bigger (e.g. flatMap , union , cartesian ) or the same size (e.g. map ). CAUTION: There are transformations that may trigger jobs, e.g. sortBy , < >, etc. .From SparkContext by transformations to the result image::rdd-sparkcontext-transformations-action.png[align=\"center\"] Certain transformations can be pipelined which is an optimization that Spark uses to improve performance of computations. [source,scala] \u00b6 scala> val file = sc.textFile(\"README.md\") file: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[54] at textFile at :24 scala> val allWords = file.flatMap(_.split(\"\\W+\")) allWords: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[55] at flatMap at :26 scala> val words = allWords.filter(!_.isEmpty) words: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[56] at filter at :28 scala> val pairs = words.map((_,1)) pairs: org.apache.spark.rdd.RDD[(String, Int)] = MapPartitionsRDD[57] at map at :30 scala> val reducedByKey = pairs.reduceByKey(_ + _) reducedByKey: org.apache.spark.rdd.RDD[(String, Int)] = ShuffledRDD[59] at reduceByKey at :32 scala> val top10words = reducedByKey.takeOrdered(10)(Ordering[Int].reverse.on(_._2)) INFO SparkContext: Starting job: takeOrdered at :34 ... INFO DAGScheduler: Job 18 finished: takeOrdered at :34, took 0.074386 s top10words: Array[(String, Int)] = Array((the,21), (to,14), (Spark,13), (for,11), (and,10), (##,8), (a,8), (run,7), (can,6), (is,6)) There are two kinds of transformations: < > < > === [[narrow-transformations]] Narrow Transformations Narrow transformations are the result of map , filter and such that is from the data from a single partition only, i.e. it is self-sustained. An output RDD has partitions with records that originate from a single partition in the parent RDD. Only a limited subset of partitions used to calculate the result. Spark groups narrow transformations as a stage which is called pipelining . === [[wide-transformations]] Wide Transformations Wide transformations are the result of groupByKey and reduceByKey . The data required to compute the records in a single partition may reside in many partitions of the parent RDD. NOTE: Wide transformations are also called shuffle transformations as they may or may not depend on a shuffle. All of the tuples with the same key must end up in the same partition, processed by the same task. To satisfy these operations, Spark must execute link:spark-rdd-shuffle.adoc[RDD shuffle], which transfers data across cluster and results in a new stage with a new set of partitions. === [[zipWithIndex]] zipWithIndex [source, scala] \u00b6 zipWithIndex(): RDD[(T, Long)] \u00b6 zipWithIndex zips this RDD[T] with its element indices. [CAUTION] \u00b6 If the number of partitions of the source RDD is greater than 1, it will submit an additional job to calculate start indices. [source, scala] \u00b6 val onePartition = sc.parallelize(0 to 9, 1) scala> onePartition.partitions.length res0: Int = 1 // no job submitted onePartition.zipWithIndex val eightPartitions = sc.parallelize(0 to 9, 8) scala> eightPartitions.partitions.length res1: Int = 8 // submits a job eightPartitions.zipWithIndex .Spark job submitted by zipWithIndex transformation image::spark-transformations-zipWithIndex-webui.png[align=\"center\"] ====","title":"Transformations"},{"location":"rdd/spark-rdd-transformations/#source-scala","text":"aggregate U ( seqOp: (U, T) => U, combOp: (U, U) => U): U | barrier a| [[barrier]]","title":"[source, scala]"},{"location":"rdd/spark-rdd-transformations/#source-scala_1","text":"","title":"[source, scala]"},{"location":"rdd/spark-rdd-transformations/#barrier-rddbarriert","text":"( New in 2.4.0 ) Marks the current stage as a < > in < >, where Spark must launch all tasks together Internally, barrier creates a < > over the RDD | cache a| [[cache]]","title":"barrier(): RDDBarrier[T]"},{"location":"rdd/spark-rdd-transformations/#source-scala_2","text":"","title":"[source, scala]"},{"location":"rdd/spark-rdd-transformations/#cache-thistype","text":"Persists the RDD with the xref:storage:StorageLevel.adoc#MEMORY_ONLY[MEMORY_ONLY] storage level Synonym of < > | coalesce a| [[coalesce]]","title":"cache(): this.type"},{"location":"rdd/spark-rdd-transformations/#source-scala_3","text":"coalesce( numPartitions: Int, shuffle: Boolean = false, partitionCoalescer: Option[PartitionCoalescer] = Option.empty) (implicit ord: Ordering[T] = null): RDD[T] | filter a| [[filter]]","title":"[source, scala]"},{"location":"rdd/spark-rdd-transformations/#source-scala_4","text":"","title":"[source, scala]"},{"location":"rdd/spark-rdd-transformations/#filterf-t-boolean-rddt","text":"| flatMap a| [[flatMap]]","title":"filter(f: T =&gt; Boolean): RDD[T]"},{"location":"rdd/spark-rdd-transformations/#source-scala_5","text":"","title":"[source, scala]"},{"location":"rdd/spark-rdd-transformations/#flatmapu-rddu","text":"| map a| [[map]]","title":"flatMapU: RDD[U]"},{"location":"rdd/spark-rdd-transformations/#source-scala_6","text":"","title":"[source, scala]"},{"location":"rdd/spark-rdd-transformations/#mapu-rddu","text":"| mapPartitions a| [[mapPartitions]]","title":"mapU: RDD[U]"},{"location":"rdd/spark-rdd-transformations/#source-scala_7","text":"mapPartitions U : RDD[U] | mapPartitionsWithIndex a| [[mapPartitionsWithIndex]]","title":"[source, scala]"},{"location":"rdd/spark-rdd-transformations/#source-scala_8","text":"mapPartitionsWithIndex U : RDD[U] | randomSplit a| [[randomSplit]]","title":"[source, scala]"},{"location":"rdd/spark-rdd-transformations/#source-scala_9","text":"randomSplit( weights: Array[Double], seed: Long = Utils.random.nextLong): Array[RDD[T]] | union a| [[union]]","title":"[source, scala]"},{"location":"rdd/spark-rdd-transformations/#source-scala_10","text":"++(other: RDD[T]): RDD[T] union(other: RDD[T]): RDD[T] | persist a| [[persist]]","title":"[source, scala]"},{"location":"rdd/spark-rdd-transformations/#source-scala_11","text":"persist(): this.type persist(newLevel: StorageLevel): this.type |=== By applying transformations you incrementally build a link:spark-rdd-lineage.adoc[RDD lineage] with all the parent RDDs of the final RDD(s). Transformations are lazy, i.e. are not executed immediately. Only after calling an action are transformations executed. After executing a transformation, the result RDD(s) will always be different from their parents and can be smaller (e.g. filter , count , distinct , sample ), bigger (e.g. flatMap , union , cartesian ) or the same size (e.g. map ). CAUTION: There are transformations that may trigger jobs, e.g. sortBy , < >, etc. .From SparkContext by transformations to the result image::rdd-sparkcontext-transformations-action.png[align=\"center\"] Certain transformations can be pipelined which is an optimization that Spark uses to improve performance of computations.","title":"[source, scala]"},{"location":"rdd/spark-rdd-transformations/#sourcescala","text":"scala> val file = sc.textFile(\"README.md\") file: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[54] at textFile at :24 scala> val allWords = file.flatMap(_.split(\"\\W+\")) allWords: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[55] at flatMap at :26 scala> val words = allWords.filter(!_.isEmpty) words: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[56] at filter at :28 scala> val pairs = words.map((_,1)) pairs: org.apache.spark.rdd.RDD[(String, Int)] = MapPartitionsRDD[57] at map at :30 scala> val reducedByKey = pairs.reduceByKey(_ + _) reducedByKey: org.apache.spark.rdd.RDD[(String, Int)] = ShuffledRDD[59] at reduceByKey at :32 scala> val top10words = reducedByKey.takeOrdered(10)(Ordering[Int].reverse.on(_._2)) INFO SparkContext: Starting job: takeOrdered at :34 ... INFO DAGScheduler: Job 18 finished: takeOrdered at :34, took 0.074386 s top10words: Array[(String, Int)] = Array((the,21), (to,14), (Spark,13), (for,11), (and,10), (##,8), (a,8), (run,7), (can,6), (is,6)) There are two kinds of transformations: < > < > === [[narrow-transformations]] Narrow Transformations Narrow transformations are the result of map , filter and such that is from the data from a single partition only, i.e. it is self-sustained. An output RDD has partitions with records that originate from a single partition in the parent RDD. Only a limited subset of partitions used to calculate the result. Spark groups narrow transformations as a stage which is called pipelining . === [[wide-transformations]] Wide Transformations Wide transformations are the result of groupByKey and reduceByKey . The data required to compute the records in a single partition may reside in many partitions of the parent RDD. NOTE: Wide transformations are also called shuffle transformations as they may or may not depend on a shuffle. All of the tuples with the same key must end up in the same partition, processed by the same task. To satisfy these operations, Spark must execute link:spark-rdd-shuffle.adoc[RDD shuffle], which transfers data across cluster and results in a new stage with a new set of partitions. === [[zipWithIndex]] zipWithIndex","title":"[source,scala]"},{"location":"rdd/spark-rdd-transformations/#source-scala_12","text":"","title":"[source, scala]"},{"location":"rdd/spark-rdd-transformations/#zipwithindex-rddt-long","text":"zipWithIndex zips this RDD[T] with its element indices.","title":"zipWithIndex(): RDD[(T, Long)]"},{"location":"rdd/spark-rdd-transformations/#caution","text":"If the number of partitions of the source RDD is greater than 1, it will submit an additional job to calculate start indices.","title":"[CAUTION]"},{"location":"rdd/spark-rdd-transformations/#source-scala_13","text":"val onePartition = sc.parallelize(0 to 9, 1) scala> onePartition.partitions.length res0: Int = 1 // no job submitted onePartition.zipWithIndex val eightPartitions = sc.parallelize(0 to 9, 8) scala> eightPartitions.partitions.length res1: Int = 8 // submits a job eightPartitions.zipWithIndex .Spark job submitted by zipWithIndex transformation image::spark-transformations-zipWithIndex-webui.png[align=\"center\"] ====","title":"[source, scala]"},{"location":"scheduler/DAGScheduler/","text":"= [[DAGScheduler]] DAGScheduler [NOTE] \u00b6 The introduction that follows was highly influenced by the scaladoc of https://github.com/apache/spark/blob/master/core/src/main/scala/org/apache/spark/scheduler/DAGScheduler.scala[org.apache.spark.scheduler.DAGScheduler ]. As DAGScheduler is a private class it does not appear in the official API documentation. You are strongly encouraged to read https://github.com/apache/spark/blob/master/core/src/main/scala/org/apache/spark/scheduler/DAGScheduler.scala[the sources] and only then read this and the related pages afterwards. \u00b6 == [[introduction]] Introduction DAGScheduler is the scheduling layer of Apache Spark that implements stage-oriented scheduling . DAGScheduler transforms a logical execution plan (i.e. xref:rdd:spark-rdd-lineage.adoc[RDD lineage] of dependencies built using xref:rdd:spark-rdd-transformations.adoc[RDD transformations]) to a physical execution plan (using xref:scheduler:Stage.adoc[stages]). .DAGScheduler Transforming RDD Lineage Into Stage DAG image::dagscheduler-rdd-lineage-stage-dag.png[align=\"center\"] After an xref:rdd:spark-rdd-actions.adoc[action] has been called, xref:ROOT:SparkContext.adoc[SparkContext] hands over a logical plan to DAGScheduler that it in turn translates to a set of stages that are submitted as xref:scheduler:TaskSet.adoc[TaskSets] for execution. .Executing action leads to new ResultStage and ActiveJob in DAGScheduler image::dagscheduler-rdd-partitions-job-resultstage.png[align=\"center\"] The fundamental concepts of DAGScheduler are jobs and stages (refer to xref:scheduler:spark-scheduler-ActiveJob.adoc[Jobs] and xref:scheduler:Stage.adoc[Stages] respectively) that it tracks through < >. DAGScheduler works solely on the driver and is created as part of xref:ROOT:SparkContext.adoc#creating-instance[SparkContext's initialization] (right after xref:scheduler:TaskScheduler.adoc[TaskScheduler] and xref:scheduler:SchedulerBackend.adoc[SchedulerBackend] are ready). .DAGScheduler as created by SparkContext with other services image::dagscheduler-new-instance.png[align=\"center\"] DAGScheduler does three things in Spark (thorough explanations follow): Computes an execution DAG , i.e. DAG of stages, for a job. Determines the < > to run each task on. Handles failures due to shuffle output files being lost. DAGScheduler computes https://en.wikipedia.org/wiki/Directed_acyclic_graph[a directed acyclic graph (DAG)] of stages for each job, keeps track of which RDDs and stage outputs are materialized, and finds a minimal schedule to run jobs. It then submits stages to xref:scheduler:TaskScheduler.adoc[TaskScheduler]. .DAGScheduler.submitJob image::dagscheduler-submitjob.png[align=\"center\"] In addition to coming up with the execution DAG, DAGScheduler also determines the preferred locations to run each task on, based on the current cache status, and passes the information to xref:scheduler:TaskScheduler.adoc[TaskScheduler]. DAGScheduler tracks which xref:rdd:spark-rdd-caching.adoc[RDDs are cached (or persisted)] to avoid \"recomputing\" them, i.e. redoing the map side of a shuffle. DAGScheduler remembers what xref:scheduler:ShuffleMapStage.adoc[ShuffleMapStage]s have already produced output files (that are stored in xref:storage:BlockManager.adoc[BlockManager]s). DAGScheduler is only interested in cache location coordinates, i.e. host and executor id, per partition of a RDD. Furthermore, it handles failures due to shuffle output files being lost, in which case old stages may need to be resubmitted. Failures within a stage that are not caused by shuffle file loss are handled by the TaskScheduler itself, which will retry each task a small number of times before cancelling the whole stage. DAGScheduler uses an event queue architecture in which a thread can post DAGSchedulerEvent events, e.g. a new job or stage being submitted, that DAGScheduler reads and executes sequentially. See the section < >. DAGScheduler runs stages in topological order. DAGScheduler uses xref:ROOT:SparkContext.adoc[SparkContext], xref:scheduler:TaskScheduler.adoc[TaskScheduler], xref:scheduler:LiveListenerBus.adoc[], xref:scheduler:MapOutputTracker.adoc[MapOutputTracker] and xref:storage:BlockManager.adoc[BlockManager] for its services. However, at the very minimum, DAGScheduler takes a SparkContext only (and requests SparkContext for the other services). When DAGScheduler schedules a job as a result of xref:rdd:index.adoc#actions[executing an action on a RDD] or xref:ROOT:SparkContext.adoc#runJob[calling SparkContext.runJob() method directly], it spawns parallel tasks to compute (partial) results per partition. == [[creating-instance]][[initialization]] Creating Instance DAGScheduler takes the following to be created: [[sc]] xref:ROOT:SparkContext.adoc[] < > [[listenerBus]] xref:scheduler:LiveListenerBus.adoc[] [[mapOutputTracker]] xref:scheduler:MapOutputTrackerMaster.adoc[MapOutputTrackerMaster] [[blockManagerMaster]] xref:storage:BlockManagerMaster.adoc[BlockManagerMaster] [[env]] xref:core:SparkEnv.adoc[] [[clock]] Clock (default: SystemClock) While being created, DAGScheduler xref:scheduler:TaskScheduler.adoc#setDAGScheduler[associates itself] with the < > and starts < >. == [[event-loop]][[eventProcessLoop]] DAGScheduler Event Bus DAGScheduler uses an xref:scheduler:DAGSchedulerEventProcessLoop.adoc[event bus] to process scheduling-related events on a separate thread (one by one and asynchronously). DAGScheduler starts the event bus when created and stops it when requested to < >. DAGScheduler defines < > that allow posting DAGSchedulerEvent events to the event bus. [[event-posting-methods]] .DAGScheduler Event Posting Methods [cols=\"20m,20m,60\",options=\"header\",width=\"100%\"] |=== | Method | Event Posted | Trigger | [[cancelAllJobs]] cancelAllJobs | xref:scheduler:DAGSchedulerEvent.adoc#AllJobsCancelled[AllJobsCancelled] | SparkContext is requested to xref:ROOT:SparkContext.adoc#cancelAllJobs[cancel all running or scheduled Spark jobs] | [[cancelJob]] cancelJob | xref:scheduler:DAGSchedulerEvent.adoc#JobCancelled[JobCancelled] | xref:ROOT:SparkContext.adoc#cancelJob[SparkContext] or xref:scheduler:spark-scheduler-JobWaiter.adoc[JobWaiter] are requested to cancel a Spark job | [[cancelJobGroup]] cancelJobGroup | xref:scheduler:DAGSchedulerEvent.adoc#JobGroupCancelled[JobGroupCancelled] | SparkContext is requested to xref:ROOT:SparkContext.adoc#cancelJobGroup[cancel a job group] | [[cancelStage]] cancelStage | xref:scheduler:DAGSchedulerEvent.adoc#StageCancelled[StageCancelled] | SparkContext is requested to xref:ROOT:SparkContext.adoc#cancelStage[cancel a stage] | [[executorAdded]] executorAdded | xref:scheduler:DAGSchedulerEvent.adoc#ExecutorAdded[ExecutorAdded] | TaskSchedulerImpl is requested to xref:scheduler:TaskSchedulerImpl.adoc#resourceOffers[handle resource offers] (and a new executor is found in the resource offers) | [[executorLost]] executorLost | xref:scheduler:DAGSchedulerEvent.adoc#ExecutorLost[ExecutorLost] | TaskSchedulerImpl is requested to xref:scheduler:TaskSchedulerImpl.adoc#statusUpdate[handle a task status update] (and a task gets lost which is used to indicate that the executor got broken and hence should be considered lost) or xref:scheduler:TaskSchedulerImpl.adoc#executorLost[executorLost] | [[runApproximateJob]] runApproximateJob | xref:scheduler:DAGSchedulerEvent.adoc#JobSubmitted[JobSubmitted] | SparkContext is requested to xref:ROOT:SparkContext.adoc#runApproximateJob[run an approximate job] | [[speculativeTaskSubmitted]] speculativeTaskSubmitted | xref:scheduler:DAGSchedulerEvent.adoc#SpeculativeTaskSubmitted[SpeculativeTaskSubmitted] | | [[submitJob]] submitJob | xref:scheduler:DAGSchedulerEvent.adoc#JobSubmitted[JobSubmitted] a| SparkContext is requested to xref:ROOT:SparkContext.adoc#submitJob[submits a job] DAGScheduler is requested to < > | [[submitMapStage]] submitMapStage | xref:scheduler:DAGSchedulerEvent.adoc#MapStageSubmitted[MapStageSubmitted] | SparkContext is requested to xref:ROOT:SparkContext.adoc#submitMapStage[submit a MapStage for execution]. | [[taskEnded]] taskEnded | xref:scheduler:DAGSchedulerEvent.adoc#CompletionEvent[CompletionEvent] | TaskSetManager is requested to xref:scheduler:TaskSetManager.adoc#handleSuccessfulTask[handleSuccessfulTask], xref:scheduler:TaskSetManager.adoc#handleFailedTask[handleFailedTask], and xref:scheduler:TaskSetManager.adoc#executorLost[executorLost] | [[taskGettingResult]] taskGettingResult | xref:scheduler:DAGSchedulerEvent.adoc#GettingResultEvent[GettingResultEvent] | TaskSetManager is requested to xref:scheduler:TaskSetManager.adoc#handleTaskGettingResult[handle a task fetching result] | [[taskSetFailed]] taskSetFailed | xref:scheduler:DAGSchedulerEvent.adoc#TaskSetFailed[TaskSetFailed] | TaskSetManager is requested to xref:scheduler:TaskSetManager.adoc#abort[abort] | [[taskStarted]] taskStarted | xref:scheduler:DAGSchedulerEvent.adoc#BeginEvent[BeginEvent] | TaskSetManager is requested to xref:scheduler:TaskSetManager.adoc#resourceOffer[start a task] | [[workerRemoved]] workerRemoved | xref:scheduler:DAGSchedulerEvent.adoc#WorkerRemoved[WorkerRemoved] | TaskSchedulerImpl is requested to xref:scheduler:TaskSchedulerImpl.adoc#workerRemoved[handle a removed worker event] |=== == [[taskScheduler]] DAGScheduler and TaskScheduler DAGScheduler is given a xref:scheduler:TaskScheduler.adoc[TaskScheduler] when < >. DAGScheduler uses the TaskScheduler for the following: < > < > < > < > < > == [[runJob]] Running Job [source, scala] \u00b6 runJob T, U : Unit runJob submits an action job to the DAGScheduler and waits for a result. Internally, runJob executes < > and then waits until a result comes using xref:scheduler:spark-scheduler-JobWaiter.adoc[JobWaiter]. When the job succeeds, you should see the following INFO message in the logs: Job [jobId] finished: [callSite], took [time] s When the job fails, you should see the following INFO message in the logs and the exception (that led to the failure) is thrown. Job [jobId] failed: [callSite], took [time] s runJob is used when SparkContext is requested to xref:ROOT:SparkContext.adoc#runJob[run a job]. == [[cacheLocs]][[clearCacheLocs]] Partition Placement Preferences DAGScheduler keeps track of block locations per RDD and partition. DAGScheduler uses xref:scheduler:TaskLocation.adoc[TaskLocation] that includes a host name and an executor id on that host (as ExecutorCacheTaskLocation ). The keys are RDDs (their ids) and the values are arrays indexed by partition numbers. Each entry is a set of block locations where a RDD partition is cached, i.e. the xref:storage:BlockManager.adoc[BlockManager]s of the blocks. Initialized empty when < >. Used when DAGScheduler is requested for the < > or < >. == [[activeJobs]] ActiveJobs DAGScheduler tracks xref:scheduler:spark-scheduler-ActiveJob.adoc[ActiveJobs]: Adds a new ActiveJob when requested to handle < > or < > events Removes an ActiveJob when requested to < >. Removes all ActiveJobs when requested to < >. DAGScheduler uses ActiveJobs registry when requested to handle < > or < > events, to < > and to < >. The number of ActiveJobs is available using xref:metrics:spark-scheduler-DAGSchedulerSource.adoc#job.activeJobs[job.activeJobs] performance metric. == [[createResultStage]] Creating ResultStage for RDD [source, scala] \u00b6 createResultStage( rdd: RDD[ ], func: (TaskContext, Iterator[ ]) => _, partitions: Array[Int], jobId: Int, callSite: CallSite): ResultStage createResultStage...FIXME createResultStage is used when DAGScheduler is requested to < >. == [[createShuffleMapStage]] Creating ShuffleMapStage for ShuffleDependency [source, scala] \u00b6 createShuffleMapStage( shuffleDep: ShuffleDependency[_, _, _], jobId: Int): ShuffleMapStage createShuffleMapStage creates a xref:scheduler:ShuffleMapStage.adoc[ShuffleMapStage] for the given xref:rdd:ShuffleDependency.adoc[ShuffleDependency] as follows: Stage ID is generated based on < > internal counter RDD is taken from the given xref:rdd:ShuffleDependency.adoc#rdd[ShuffleDependency] Number of tasks is the number of xref:rdd:RDD.adoc#partitions[partitions] of the RDD < > < > createShuffleMapStage registers the ShuffleMapStage in the < > and < > internal registries. createShuffleMapStage < >. createShuffleMapStage requests the < > to xref:scheduler:MapOutputTrackerMaster.adoc#containsShuffle[check whether it contains the shuffle ID or not]. If not, createShuffleMapStage prints out the following INFO message to the logs and requests the < > to xref:scheduler:MapOutputTrackerMaster.adoc#registerShuffle[register the shuffle]. [source,plaintext] \u00b6 Registering RDD [id] ([creationSite]) as input to shuffle [shuffleId] \u00b6 .DAGScheduler Asks MapOutputTrackerMaster Whether Shuffle Map Output Is Already Tracked image::DAGScheduler-MapOutputTrackerMaster-containsShuffle.png[align=\"center\"] createShuffleMapStage is used when DAGScheduler is requested to < >. == [[cleanupStateForJobAndIndependentStages]] Cleaning Up After Job and Independent Stages [source, scala] \u00b6 cleanupStateForJobAndIndependentStages( job: ActiveJob): Unit cleanupStateForJobAndIndependentStages cleans up the state for job and any stages that are not part of any other job. cleanupStateForJobAndIndependentStages looks the job up in the internal < > registry. If no stages are found, the following ERROR is printed out to the logs: No stages registered for job [jobId] Oterwise, cleanupStateForJobAndIndependentStages uses < > registry to find the stages (the real objects not ids!). For each stage, cleanupStateForJobAndIndependentStages reads the jobs the stage belongs to. If the job does not belong to the jobs of the stage, the following ERROR is printed out to the logs: Job [jobId] not registered for stage [stageId] even though that stage was registered for the job If the job was the only job for the stage, the stage (and the stage id) gets cleaned up from the registries, i.e. < >, < >, < >, < > and < >. While removing from < >, you should see the following DEBUG message in the logs: Removing running stage [stageId] While removing from < >, you should see the following DEBUG message in the logs: Removing stage [stageId] from waiting set. While removing from < >, you should see the following DEBUG message in the logs: Removing stage [stageId] from failed set. After all cleaning (using < > as the source registry), if the stage belonged to the one and only job , you should see the following DEBUG message in the logs: After removal of stage [stageId], remaining stages = [stageIdToStage.size] The job is removed from < >, < >, < > registries. The final stage of the job is removed, i.e. xref:scheduler:ResultStage.adoc#removeActiveJob[ResultStage] or xref:scheduler:ShuffleMapStage.adoc#removeActiveJob[ShuffleMapStage]. cleanupStateForJobAndIndependentStages is used in xref:scheduler:DAGSchedulerEventProcessLoop.adoc#handleTaskCompletion-Success-ResultTask[handleTaskCompletion when a ResultTask has completed successfully], < > and < >. == [[markMapStageJobAsFinished]] Marking ShuffleMapStage Job Finished [source, scala] \u00b6 markMapStageJobAsFinished( job: ActiveJob, stats: MapOutputStatistics): Unit markMapStageJobAsFinished marks the active job finished and notifies Spark listeners. Internally, markMapStageJobAsFinished marks the zeroth partition finished and increases the number of tasks finished in job . The xref:scheduler:spark-scheduler-JobListener.adoc#taskSucceeded[ job listener is notified about the 0 th task succeeded]. The < job and independent stages are cleaned up>>. Ultimately, xref:ROOT:SparkListener.adoc#SparkListenerJobEnd[SparkListenerJobEnd] is posted to xref:scheduler:LiveListenerBus.adoc[] (as < >) for the job , the current time (in millis) and JobSucceeded job result. markMapStageJobAsFinished is used in xref:scheduler:DAGSchedulerEventProcessLoop.adoc#handleMapStageSubmitted[handleMapStageSubmitted] and xref:scheduler:DAGSchedulerEventProcessLoop.adoc#handleTaskCompletion[handleTaskCompletion]. == [[getOrCreateParentStages]] Finding Or Creating Missing Direct Parent ShuffleMapStages (For ShuffleDependencies) of RDD [source, scala] \u00b6 getOrCreateParentStages( rdd: RDD[_], firstJobId: Int): List[Stage] getOrCreateParentStages < ShuffleDependencies >> of the input rdd and then < ShuffleMapStage stages>> for each xref:rdd:ShuffleDependency.adoc[ShuffleDependency]. getOrCreateParentStages is used when DAGScheduler is requested to create a < > or a < >. == [[markStageAsFinished]] Marking Stage Finished [source, scala] \u00b6 markStageAsFinished( stage: Stage, errorMessage: Option[String] = None, willRetry: Boolean = false): Unit markStageAsFinished...FIXME markStageAsFinished is used when...FIXME == [[getOrCreateShuffleMapStage]] Finding or Creating ShuffleMapStage for ShuffleDependency [source, scala] \u00b6 getOrCreateShuffleMapStage( shuffleDep: ShuffleDependency[_, _, _], firstJobId: Int): ShuffleMapStage getOrCreateShuffleMapStage finds the xref:scheduler:ShuffleMapStage.adoc[ShuffleMapStage] in the < > internal registry and returns it if available. If not found, getOrCreateShuffleMapStage < > and < > (including one for the input ShuffleDependency). getOrCreateShuffleMapStage is used when DAGScheduler is requested to < >, < >, < >, and < >. == [[getMissingAncestorShuffleDependencies]] Finding Missing ShuffleDependencies For RDD [source, scala] \u00b6 getMissingAncestorShuffleDependencies( rdd: RDD[ ]): Stack[ShuffleDependency[ , _, _]] getMissingAncestorShuffleDependencies finds all missing xref:rdd:ShuffleDependency.adoc[shuffle dependencies] for the given xref:rdd:index.adoc[RDD] traversing its xref:rdd:spark-rdd-lineage.adoc[RDD lineage]. NOTE: A missing shuffle dependency of a RDD is a dependency not registered in < shuffleIdToMapStage internal registry>>. Internally, getMissingAncestorShuffleDependencies < >\u2009of the input RDD and collects the ones that are not registered in < shuffleIdToMapStage internal registry>>. It repeats the process for the RDDs of the parent shuffle dependencies. getMissingAncestorShuffleDependencies is used when DAGScheduler is requested to < >. == [[getShuffleDependencies]] Finding Direct Parent Shuffle Dependencies of RDD [source, scala] \u00b6 getShuffleDependencies( rdd: RDD[ ]): HashSet[ShuffleDependency[ , _, _]] getShuffleDependencies finds direct parent xref:rdd:ShuffleDependency.adoc[shuffle dependencies] for the given xref:rdd:index.adoc[RDD]. .getShuffleDependencies Finds Direct Parent ShuffleDependencies (shuffle1 and shuffle2) image::spark-DAGScheduler-getShuffleDependencies.png[align=\"center\"] Internally, getShuffleDependencies takes the direct xref:rdd:index.adoc#dependencies[shuffle dependencies of the input RDD] and direct shuffle dependencies of all the parent non- ShuffleDependencies in the xref:rdd:spark-rdd-lineage.adoc[dependency chain] (aka RDD lineage ). getShuffleDependencies is used when DAGScheduler is requested to < > (for ShuffleDependencies of a RDD) and < >. == [[failJobAndIndependentStages]] Failing Job and Independent Single-Job Stages [source, scala] \u00b6 failJobAndIndependentStages( job: ActiveJob, failureReason: String, exception: Option[Throwable] = None): Unit failJobAndIndependentStages fails the input job and all the stages that are only used by the job. Internally, failJobAndIndependentStages uses < jobIdToStageIds internal registry>> to look up the stages registered for the job. If no stages could be found, you should see the following ERROR message in the logs: No stages registered for job [id] Otherwise, for every stage, failJobAndIndependentStages finds the job ids the stage belongs to. If no stages could be found or the job is not referenced by the stages, you should see the following ERROR message in the logs: Job [id] not registered for stage [id] even though that stage was registered for the job Only when there is exactly one job registered for the stage and the stage is in RUNNING state (in runningStages internal registry), xref:scheduler:TaskScheduler.adoc#contract[ TaskScheduler is requested to cancel the stage's tasks] and < >. NOTE: failJobAndIndependentStages uses < >, < >, and < > internal registries. failJobAndIndependentStages is used when...FIXME == [[abortStage]] Aborting Stage [source, scala] \u00b6 abortStage( failedStage: Stage, reason: String, exception: Option[Throwable]): Unit abortStage is an internal method that finds all the active jobs that depend on the failedStage stage and fails them. Internally, abortStage looks the failedStage stage up in the internal < > registry and exits if there the stage was not registered earlier. If it was, abortStage finds all the active jobs (in the internal < > registry) with the < failedStage stage>>. At this time, the completionTime property (of the failed stage's xref:scheduler:spark-scheduler-StageInfo.adoc[StageInfo]) is assigned to the current time (millis). All the active jobs that depend on the failed stage (as calculated above) and the stages that do not belong to other jobs (aka independent stages ) are < > (with the failure reason being \"Job aborted due to stage failure: [reason]\" and the input exception ). If there are no jobs depending on the failed stage, you should see the following INFO message in the logs: [source,plaintext] \u00b6 Ignoring failure of [failedStage] because all jobs depending on it are done \u00b6 abortStage is used when DAGScheduler is requested to < >, < >, < >, < >. == [[stageDependsOn]] Checking Out Stage Dependency on Given Stage [source, scala] \u00b6 stageDependsOn( stage: Stage, target: Stage): Boolean stageDependsOn compares two stages and returns whether the stage depends on target stage (i.e. true ) or not (i.e. false ). NOTE: A stage A depends on stage B if B is among the ancestors of A . Internally, stageDependsOn walks through the graph of RDDs of the input stage . For every RDD in the RDD's dependencies (using RDD.dependencies ) stageDependsOn adds the RDD of a xref:rdd:spark-rdd-NarrowDependency.adoc[NarrowDependency] to a stack of RDDs to visit while for a xref:rdd:ShuffleDependency.adoc[ShuffleDependency] it < ShuffleMapStage stages for a ShuffleDependency >> for the dependency and the stage 's first job id that it later adds to a stack of RDDs to visit if the map stage is ready, i.e. all the partitions have shuffle outputs. After all the RDDs of the input stage are visited, stageDependsOn checks if the target 's RDD is among the RDDs of the stage , i.e. whether the stage depends on target stage. stageDependsOn is used when DAGScheduler is requested to < >. == [[submitWaitingChildStages]] Submitting Waiting Child Stages for Execution [source, scala] \u00b6 submitWaitingChildStages( parent: Stage): Unit submitWaitingChildStages submits for execution all waiting stages for which the input parent xref:scheduler:Stage.adoc[Stage] is the direct parent. NOTE: Waiting stages are the stages registered in < waitingStages internal registry>>. When executed, you should see the following TRACE messages in the logs: Checking if any dependencies of [parent] are now runnable running: [runningStages] waiting: [waitingStages] failed: [failedStages] submitWaitingChildStages finds child stages of the input parent stage, removes them from waitingStages internal registry, and < > one by one sorted by their job ids. submitWaitingChildStages is used when DAGScheduler is requested to < > and < >. == [[submitStage]] Submitting Stage (with Missing Parents) for Execution [source, scala] \u00b6 submitStage( stage: Stage): Unit submitStage submits the input stage or its missing parents (if there any stages not computed yet before the input stage could). NOTE: submitStage is also used to xref:scheduler:DAGSchedulerEventProcessLoop.adoc#resubmitFailedStages[resubmit failed stages]. submitStage recursively submits any missing parents of the stage . Internally, submitStage first finds the earliest-created job id that needs the stage . NOTE: A stage itself tracks the jobs (their ids) it belongs to (using the internal jobIds registry). The following steps depend on whether there is a job or not. If there are no jobs that require the stage , submitStage < > with the reason: No active job for stage [id] If however there is a job for the stage , you should see the following DEBUG message in the logs: submitStage([stage]) submitStage checks the status of the stage and continues when it was not recorded in < >, < > or < > internal registries. It simply exits otherwise. With the stage ready for submission, submitStage calculates the < stage >> (sorted by their job ids). You should see the following DEBUG message in the logs: missing: [missing] When the stage has no parent stages missing, you should see the following INFO message in the logs: Submitting [stage] ([stage.rdd]), which has no missing parents submitStage < stage >> (with the earliest-created job id) and finishes. If however there are missing parent stages for the stage , submitStage < >, and the stage is recorded in the internal < > registry. submitStage is used recursively for missing parents of the given stage and when DAGScheduler is requested for the following: < > (ResubmitFailedStages event) < > (CompletionEvent event) Handle < >, < > and < > events == [[stage-attempts]] Stage Attempts A single stage can be re-executed in multiple attempts due to fault recovery. The number of attempts is configured (FIXME). If TaskScheduler reports that a task failed because a map output file from a previous stage was lost, the DAGScheduler resubmits the lost stage. This is detected through a xref:scheduler:DAGSchedulerEventProcessLoop.adoc#handleTaskCompletion-FetchFailed[ CompletionEvent with FetchFailed ], or an < > event. DAGScheduler will wait a small amount of time to see whether other nodes or tasks fail, then resubmit TaskSets for any lost stage(s) that compute the missing tasks. Please note that tasks from the old attempts of a stage could still be running. A stage object tracks multiple xref:scheduler:spark-scheduler-StageInfo.adoc[StageInfo] objects to pass to Spark listeners or the web UI. The latest StageInfo for the most recent attempt for a stage is accessible through latestInfo . == [[preferred-locations]] Preferred Locations DAGScheduler computes where to run each task in a stage based on the xref:rdd:index.adoc#getPreferredLocations[preferred locations of its underlying RDDs], or < >. == [[adaptive-query-planning]] Adaptive Query Planning / Adaptive Scheduling See https://issues.apache.org/jira/browse/SPARK-9850[SPARK-9850 Adaptive execution in Spark] for the design document. The work is currently in progress. https://github.com/apache/spark/blob/master/core/src/main/scala/org/apache/spark/scheduler/DAGScheduler.scala#L661[DAGScheduler.submitMapStage ] method is used for adaptive query planning, to run map stages and look at statistics about their outputs before submitting downstream stages. == ScheduledExecutorService daemon services DAGScheduler uses the following ScheduledThreadPoolExecutors (with the policy of removing cancelled tasks from a work queue at time of cancellation): dag-scheduler-message - a daemon thread pool using j.u.c.ScheduledThreadPoolExecutor with core pool size 1 . It is used to post a xref:scheduler:DAGSchedulerEventProcessLoop.adoc#ResubmitFailedStages[ResubmitFailedStages] event when xref:scheduler:DAGSchedulerEventProcessLoop.adoc#handleTaskCompletion-FetchFailed[ FetchFailed is reported]. They are created using ThreadUtils.newDaemonSingleThreadScheduledExecutor method that uses Guava DSL to instantiate a ThreadFactory. == [[getMissingParentStages]] Finding Missing Parent ShuffleMapStages For Stage [source, scala] \u00b6 getMissingParentStages( stage: Stage): List[Stage] getMissingParentStages finds missing parent xref:scheduler:ShuffleMapStage.adoc[ShuffleMapStage]s in the dependency graph of the input stage (using the https://en.wikipedia.org/wiki/Breadth-first_search[breadth-first search algorithm]). Internally, getMissingParentStages starts with the stage 's RDD and walks up the tree of all parent RDDs to find < >. NOTE: A Stage tracks the associated RDD using xref:scheduler:Stage.adoc#rdd[ rdd property]. NOTE: An uncached partition of a RDD is a partition that has Nil in the < > (which results in no RDD blocks in any of the active xref:storage:BlockManager.adoc[BlockManager]s on executors). getMissingParentStages traverses the xref:rdd:index.adoc#dependencies[parent dependencies of the RDD] and acts according to their type, i.e. xref:rdd:ShuffleDependency.adoc[ShuffleDependency] or xref:rdd:spark-rdd-NarrowDependency.adoc[NarrowDependency]. NOTE: xref:rdd:ShuffleDependency.adoc[ShuffleDependency] and xref:rdd:spark-rdd-NarrowDependency.adoc[NarrowDependency] are the main top-level xref:rdd:spark-rdd-Dependency.adoc[Dependencies]. For each NarrowDependency , getMissingParentStages simply marks the corresponding RDD to visit and moves on to a next dependency of a RDD or works on another unvisited parent RDD. NOTE: xref:rdd:spark-rdd-NarrowDependency.adoc[NarrowDependency] is a RDD dependency that allows for pipelined execution. getMissingParentStages focuses on ShuffleDependency dependencies. NOTE: xref:rdd:ShuffleDependency.adoc[ShuffleDependency] is a RDD dependency that represents a dependency on the output of a xref:scheduler:ShuffleMapStage.adoc[ShuffleMapStage], i.e. shuffle map stage . For each ShuffleDependency , getMissingParentStages < ShuffleMapStage stages>>. If the ShuffleMapStage is not available , it is added to the set of missing (map) stages. NOTE: A ShuffleMapStage is available when all its partitions are computed, i.e. results are available (as blocks). CAUTION: FIXME...IMAGE with ShuffleDependencies queried getMissingParentStages is used when DAGScheduler is requested to < > and handle < > and < > events. == [[submitMissingTasks]] Submitting Missing Tasks of Stage [source, scala] \u00b6 submitMissingTasks( stage: Stage, jobId: Int): Unit submitMissingTasks prints out the following DEBUG message to the logs: submitMissingTasks([stage]) submitMissingTasks requests the given xref:scheduler:Stage.adoc[Stage] for the xref:scheduler:Stage.adoc#findMissingPartitions[missing partitions] (partitions that need to be computed). submitMissingTasks adds the stage to the < > internal registry. submitMissingTasks notifies the < > that xref:scheduler:OutputCommitCoordinator.adoc#stageStart[stage execution started]. [[submitMissingTasks-taskIdToLocations]] submitMissingTasks < > ( task locality preferences ) of the missing partitions. submitMissingTasks requests the stage for a xref:scheduler:Stage.adoc#makeNewStageAttempt[new stage attempt]. submitMissingTasks requests the < > to xref:scheduler:LiveListenerBus.adoc#post[post] a xref:ROOT:SparkListener.adoc#SparkListenerStageSubmitted[SparkListenerStageSubmitted] event. submitMissingTasks uses the < > to xref:serializer:Serializer.adoc#serialize[serialize] the stage and create a so-called task binary. submitMissingTasks serializes the RDD (of the stage) and either the ShuffleDependency or the compute function based on the type of the stage, i.e. ShuffleMapStage and ResultStage, respectively. submitMissingTasks creates a xref:ROOT:SparkContext.adoc#broadcast[broadcast variable] for the task binary. NOTE: That shows how important xref:ROOT:Broadcast.adoc[]s are for Spark itself to distribute data among executors in a Spark application in the most efficient way. submitMissingTasks creates xref:scheduler:Task.adoc[tasks] for every missing partition: xref:scheduler:ShuffleMapTask.adoc[ShuffleMapTasks] for a xref:scheduler:ShuffleMapStage.adoc[ShuffleMapStage] xref:scheduler:ResultTask.adoc[ResultTasks] for a xref:scheduler:ResultStage.adoc[ResultStage] If there are tasks to submit for execution (i.e. there are missing partitions in the stage), submitMissingTasks prints out the following INFO message to the logs: Submitting [size] missing tasks from [stage] ([rdd]) (first 15 tasks are for partitions [partitionIds]) submitMissingTasks requests the < > to xref:scheduler:TaskScheduler.adoc#submitTasks[submit the tasks for execution] (as a new xref:scheduler:TaskSet.adoc[TaskSet]). With no tasks to submit for execution, submitMissingTasks < >. submitMissingTasks prints out the following DEBUG messages based on the type of the stage: Stage [stage] is actually done; (available: [isAvailable],available outputs: [numAvailableOutputs],partitions: [numPartitions]) or Stage [stage] is actually done; (partitions: [numPartitions]) for ShuffleMapStage and ResultStage , respectively. In the end, with no tasks to submit for execution, submitMissingTasks < > and exits. submitMissingTasks is used when DAGScheduler is requested to < >. == [[getPreferredLocs]] Finding Preferred Locations for Missing Partitions [source, scala] \u00b6 getPreferredLocs( rdd: RDD[_], partition: Int): Seq[TaskLocation] getPreferredLocs is simply an alias for the internal (recursive) < >. getPreferredLocs is used when...FIXME == [[getCacheLocs]] Finding BlockManagers (Executors) for Cached RDD Partitions (aka Block Location Discovery) [source, scala] \u00b6 getCacheLocs( rdd: RDD[_]): IndexedSeq[Seq[TaskLocation]] getCacheLocs gives xref:scheduler:TaskLocation.adoc[TaskLocations] (block locations) for the partitions of the input rdd . getCacheLocs caches lookup results in < > internal registry. NOTE: The size of the collection from getCacheLocs is exactly the number of partitions in rdd RDD. NOTE: The size of every xref:scheduler:TaskLocation.adoc[TaskLocation] collection (i.e. every entry in the result of getCacheLocs) is exactly the number of blocks managed using xref:storage:BlockManager.adoc[BlockManagers] on executors. Internally, getCacheLocs finds rdd in the < > internal registry (of partition locations per RDD). If rdd is not in < > internal registry, getCacheLocs branches per its xref:storage:StorageLevel.adoc[storage level]. For NONE storage level (i.e. no caching), the result is an empty locations (i.e. no location preference). For other non- NONE storage levels, getCacheLocs xref:storage:BlockManagerMaster.adoc#getLocations-block-array[requests BlockManagerMaster for block locations] that are then mapped to xref:scheduler:TaskLocation.adoc[TaskLocations] with the hostname of the owning BlockManager for a block (of a partition) and the executor id. NOTE: getCacheLocs uses < > that was defined when < >. getCacheLocs records the computed block locations per partition (as xref:scheduler:TaskLocation.adoc[TaskLocation]) in < > internal registry. NOTE: getCacheLocs requests locations from BlockManagerMaster using xref:storage:BlockId.adoc#RDDBlockId[RDDBlockId] with the RDD id and the partition indices (which implies that the order of the partitions matters to request proper blocks). NOTE: DAGScheduler uses xref:scheduler:TaskLocation.adoc[TaskLocations] (with host and executor) while xref:storage:BlockManagerMaster.adoc[BlockManagerMaster] uses xref:storage:BlockManagerId.adoc[] (to track similar information, i.e. block locations). getCacheLocs is used when DAGScheduler is requested to finds < > and < >. == [[getPreferredLocsInternal]] Finding Placement Preferences for RDD Partition (recursively) [source, scala] \u00b6 getPreferredLocsInternal( rdd: RDD[ ], partition: Int, visited: HashSet[(RDD[ ], Int)]): Seq[TaskLocation] getPreferredLocsInternal first < TaskLocations for the partition of the rdd >> (using < > internal cache) and returns them. Otherwise, if not found, getPreferredLocsInternal xref:rdd:index.adoc#preferredLocations[requests rdd for the preferred locations of partition ] and returns them. NOTE: Preferred locations of the partitions of a RDD are also called placement preferences or locality preferences . Otherwise, if not found, getPreferredLocsInternal finds the first parent xref:rdd:spark-rdd-NarrowDependency.adoc[NarrowDependency] and (recursively) < TaskLocations >>. If all the attempts fail to yield any non-empty result, getPreferredLocsInternal returns an empty collection of xref:scheduler:TaskLocation.adoc[TaskLocations]. getPreferredLocsInternal is used when DAGScheduler is requested for the < >. == [[stop]] Stopping DAGScheduler [source, scala] \u00b6 stop(): Unit \u00b6 stop stops the internal dag-scheduler-message thread pool, < >, and xref:scheduler:TaskScheduler.adoc#stop[TaskScheduler]. stop is used when...FIXME == [[updateAccumulators]] Updating Accumulators with Partial Values from Completed Tasks [source, scala] \u00b6 updateAccumulators( event: CompletionEvent): Unit updateAccumulators merges the partial values of accumulators from a completed task into their \"source\" accumulators on the driver. NOTE: It is called by < >. For each xref:ROOT:spark-accumulators.adoc#AccumulableInfo[AccumulableInfo] in the CompletionEvent , a partial value from a task is obtained (from AccumulableInfo.update ) and added to the driver's accumulator (using Accumulable.++= method). For named accumulators with the update value being a non-zero value, i.e. not Accumulable.zero : stage.latestInfo.accumulables for the AccumulableInfo.id is set CompletionEvent.taskInfo.accumulables has a new xref:ROOT:spark-accumulators.adoc#AccumulableInfo[AccumulableInfo] added. CAUTION: FIXME Where are Stage.latestInfo.accumulables and CompletionEvent.taskInfo.accumulables used? updateAccumulators is used when DAGScheduler is requested to < >. == [[checkBarrierStageWithNumSlots]] checkBarrierStageWithNumSlots Method [source, scala] \u00b6 checkBarrierStageWithNumSlots( rdd: RDD[_]): Unit checkBarrierStageWithNumSlots...FIXME checkBarrierStageWithNumSlots is used when DAGScheduler is requested to create < > and < > stages. == [[killTaskAttempt]] Killing Task [source, scala] \u00b6 killTaskAttempt( taskId: Long, interruptThread: Boolean, reason: String): Boolean killTaskAttempt requests the < > to xref:scheduler:TaskScheduler.adoc#killTaskAttempt[kill a task]. killTaskAttempt is used when SparkContext is requested to xref:ROOT:SparkContext.adoc#killTaskAttempt[kill a task]. == [[cleanUpAfterSchedulerStop]] cleanUpAfterSchedulerStop Method [source, scala] \u00b6 cleanUpAfterSchedulerStop(): Unit \u00b6 cleanUpAfterSchedulerStop...FIXME cleanUpAfterSchedulerStop is used when DAGSchedulerEventProcessLoop is requested to xref:scheduler:DAGSchedulerEventProcessLoop.adoc#onStop[onStop]. == [[removeExecutorAndUnregisterOutputs]] removeExecutorAndUnregisterOutputs Method [source, scala] \u00b6 removeExecutorAndUnregisterOutputs( execId: String, fileLost: Boolean, hostToUnregisterOutputs: Option[String], maybeEpoch: Option[Long] = None): Unit removeExecutorAndUnregisterOutputs...FIXME removeExecutorAndUnregisterOutputs is used when DAGScheduler is requested to handle < > (due to a fetch failure) and < > events. == [[markMapStageJobsAsFinished]] markMapStageJobsAsFinished Method [source, scala] \u00b6 markMapStageJobsAsFinished( shuffleStage: ShuffleMapStage): Unit markMapStageJobsAsFinished...FIXME markMapStageJobsAsFinished is used when DAGScheduler is requested to < > (of a ShuffleMapStage that has just been computed) and < > (of a ShuffleMapStage). == [[updateJobIdStageIdMaps]] updateJobIdStageIdMaps Method [source, scala] \u00b6 updateJobIdStageIdMaps( jobId: Int, stage: Stage): Unit updateJobIdStageIdMaps...FIXME updateJobIdStageIdMaps is used when DAGScheduler is requested to create < > and < > stages. == [[executorHeartbeatReceived]] executorHeartbeatReceived Method [source, scala] \u00b6 executorHeartbeatReceived( execId: String, // (taskId, stageId, stageAttemptId, accumUpdates) accumUpdates: Array[(Long, Int, Int, Seq[AccumulableInfo])], blockManagerId: BlockManagerId): Boolean executorHeartbeatReceived posts a xref:ROOT:SparkListener.adoc#SparkListenerExecutorMetricsUpdate[SparkListenerExecutorMetricsUpdate] (to < >) and informs xref:storage:BlockManagerMaster.adoc[BlockManagerMaster] that blockManagerId block manager is alive (by posting xref:storage:BlockManagerMaster.adoc#BlockManagerHeartbeat[BlockManagerHeartbeat]). executorHeartbeatReceived is used when TaskSchedulerImpl is requested to xref:scheduler:TaskSchedulerImpl.adoc#executorHeartbeatReceived[handle an executor heartbeat]. == [[postTaskEnd]] postTaskEnd Method [source, scala] \u00b6 postTaskEnd( event: CompletionEvent): Unit postTaskEnd...FIXME postTaskEnd is used when DAGScheduler is requested to < >. == Event Handlers === [[doCancelAllJobs]] AllJobsCancelled Event Handler [source, scala] \u00b6 doCancelAllJobs(): Unit \u00b6 doCancelAllJobs...FIXME doCancelAllJobs is used when DAGSchedulerEventProcessLoop is requested to handle an xref:scheduler:DAGSchedulerEventProcessLoop.adoc#AllJobsCancelled[AllJobsCancelled] event and xref:scheduler:DAGSchedulerEventProcessLoop.adoc#onError[onError]. === [[handleBeginEvent]] BeginEvent Event Handler [source, scala] \u00b6 handleBeginEvent( task: Task[_], taskInfo: TaskInfo): Unit handleBeginEvent...FIXME handleBeginEvent is used when DAGSchedulerEventProcessLoop is requested to handle a xref:scheduler:DAGSchedulerEvent.adoc#BeginEvent[BeginEvent] event. === [[handleTaskCompletion]] CompletionEvent Event Handler [source, scala] \u00b6 handleTaskCompletion( event: CompletionEvent): Unit handleTaskCompletion...FIXME handleTaskCompletion is used when DAGSchedulerEventProcessLoop is requested to handle a xref:scheduler:DAGSchedulerEvent.adoc#CompletionEvent[CompletionEvent] event. === [[handleExecutorAdded]] ExecutorAdded Event Handler [source, scala] \u00b6 handleExecutorAdded( execId: String, host: String): Unit handleExecutorAdded...FIXME handleExecutorAdded is used when DAGSchedulerEventProcessLoop is requested to handle an xref:scheduler:DAGSchedulerEvent.adoc#ExecutorAdded[ExecutorAdded] event. === [[handleExecutorLost]] ExecutorLost Event Handler [source, scala] \u00b6 handleExecutorLost( execId: String, workerLost: Boolean): Unit handleExecutorLost...FIXME handleExecutorLost is used when DAGSchedulerEventProcessLoop is requested to handle an xref:scheduler:DAGSchedulerEvent.adoc#ExecutorLost[ExecutorLost] event. === [[handleGetTaskResult]] GettingResultEvent Event Handler [source, scala] \u00b6 handleGetTaskResult( taskInfo: TaskInfo): Unit handleGetTaskResult...FIXME handleGetTaskResult is used when DAGSchedulerEventProcessLoop is requested to handle a xref:scheduler:DAGSchedulerEvent.adoc#GettingResultEvent[GettingResultEvent] event. === [[handleJobCancellation]] JobCancelled Event Handler [source, scala] \u00b6 handleJobCancellation( jobId: Int, reason: Option[String]): Unit handleJobCancellation...FIXME handleJobCancellation is used when DAGScheduler is requested to handle a xref:scheduler:DAGSchedulerEvent.adoc#JobCancelled[JobCancelled] event, < >, < >, < >. === [[handleJobGroupCancelled]] JobGroupCancelled Event Handler [source, scala] \u00b6 handleJobGroupCancelled( groupId: String): Unit handleJobGroupCancelled...FIXME handleJobGroupCancelled is used when DAGScheduler is requested to handle xref:scheduler:DAGSchedulerEvent.adoc#JobGroupCancelled[JobGroupCancelled] event. === [[handleJobSubmitted]] JobSubmitted Event Handler [source, scala] \u00b6 handleJobSubmitted( jobId: Int, finalRDD: RDD[ ], func: (TaskContext, Iterator[ ]) => _, partitions: Array[Int], callSite: CallSite, listener: JobListener, properties: Properties): Unit handleJobSubmitted xref:scheduler:DAGScheduler.adoc#createResultStage[creates a new ResultStage ] (as finalStage in the picture below) given the input finalRDD , func , partitions , jobId and callSite . . DAGScheduler.handleJobSubmitted Method image::dagscheduler-handleJobSubmitted.png[align=\"center\"] handleJobSubmitted creates an xref:scheduler:spark-scheduler-ActiveJob.adoc[ActiveJob] (with the input jobId , callSite , listener , properties , and the xref:scheduler:ResultStage.adoc[ResultStage]). handleJobSubmitted xref:scheduler:DAGScheduler.adoc#clearCacheLocs[clears the internal cache of RDD partition locations]. CAUTION: FIXME Why is this clearing here so important? You should see the following INFO messages in the logs: Got job [id] ([callSite]) with [number] output partitions Final stage: [stage] ([name]) Parents of final stage: [parents] Missing parents: [missingStages] handleJobSubmitted then registers the new job in xref:scheduler:DAGScheduler.adoc#jobIdToActiveJob[jobIdToActiveJob] and xref:scheduler:DAGScheduler.adoc#activeJobs[activeJobs] internal registries, and xref:scheduler:ResultStage.adoc#setActiveJob[with the final ResultStage ]. NOTE: ResultStage can only have one ActiveJob registered. handleJobSubmitted xref:scheduler:DAGScheduler.adoc#jobIdToStageIds[finds all the registered stages for the input jobId ] and collects xref:scheduler:Stage.adoc#latestInfo[their latest StageInfo ]. In the end, handleJobSubmitted posts xref:ROOT:SparkListener.adoc#SparkListenerJobStart[SparkListenerJobStart] message to xref:scheduler:LiveListenerBus.adoc[] and xref:scheduler:DAGScheduler.adoc#submitStage[submits the stage]. handleJobSubmitted is used when DAGSchedulerEventProcessLoop is requested to handle a xref:scheduler:DAGSchedulerEvent.adoc#JobSubmitted[JobSubmitted] event. === [[handleMapStageSubmitted]] MapStageSubmitted Event Handler [source, scala] \u00b6 handleMapStageSubmitted( jobId: Int, dependency: ShuffleDependency[_, _, _], callSite: CallSite, listener: JobListener, properties: Properties): Unit handleMapStageSubmitted...FIXME handleMapStageSubmitted is used when DAGSchedulerEventProcessLoop is requested to handle a xref:scheduler:DAGSchedulerEvent.adoc#MapStageSubmitted[MapStageSubmitted] event. === [[resubmitFailedStages]] ResubmitFailedStages Event Handler [source, scala] \u00b6 resubmitFailedStages(): Unit \u00b6 resubmitFailedStages...FIXME resubmitFailedStages is used when DAGSchedulerEventProcessLoop is requested to handle a xref:scheduler:DAGSchedulerEvent.adoc#ResubmitFailedStages[ResubmitFailedStages] event. === [[handleSpeculativeTaskSubmitted]] SpeculativeTaskSubmitted Event Handler [source, scala] \u00b6 handleSpeculativeTaskSubmitted(): Unit \u00b6 handleSpeculativeTaskSubmitted...FIXME handleSpeculativeTaskSubmitted is used when DAGSchedulerEventProcessLoop is requested to handle a xref:scheduler:DAGSchedulerEvent.adoc#SpeculativeTaskSubmitted[SpeculativeTaskSubmitted] event. === [[handleStageCancellation]] StageCancelled Event Handler [source, scala] \u00b6 handleStageCancellation(): Unit \u00b6 handleStageCancellation...FIXME handleStageCancellation is used when DAGSchedulerEventProcessLoop is requested to handle a xref:scheduler:DAGSchedulerEvent.adoc#StageCancelled[StageCancelled] event. === [[handleTaskSetFailed]] TaskSetFailed Event Handler [source, scala] \u00b6 handleTaskSetFailed(): Unit \u00b6 handleTaskSetFailed...FIXME handleTaskSetFailed is used when DAGSchedulerEventProcessLoop is requested to handle a xref:scheduler:DAGSchedulerEvent.adoc#TaskSetFailed[TaskSetFailed] event. === [[handleWorkerRemoved]] WorkerRemoved Event Handler [source, scala] \u00b6 handleWorkerRemoved( workerId: String, host: String, message: String): Unit handleWorkerRemoved...FIXME handleWorkerRemoved is used when DAGSchedulerEventProcessLoop is requested to handle a xref:scheduler:DAGSchedulerEvent.adoc#WorkerRemoved[WorkerRemoved] event. == [[logging]] Logging Enable ALL logging level for org.apache.spark.scheduler.DAGScheduler logger to see what happens inside. Add the following line to conf/log4j.properties : [source] \u00b6 log4j.logger.org.apache.spark.scheduler.DAGScheduler=ALL \u00b6 Refer to xref:ROOT:spark-logging.adoc[Logging]. == [[internal-properties]] Internal Properties [cols=\"30m,70\",options=\"header\",width=\"100%\"] |=== | Name | Description | failedEpoch | [[failedEpoch]] The lookup table of lost executors and the epoch of the event. | failedStages | [[failedStages]] Stages that failed due to fetch failures (when a xref:scheduler:DAGSchedulerEventProcessLoop.adoc#handleTaskCompletion-FetchFailed[task fails with FetchFailed exception]). | jobIdToActiveJob | [[jobIdToActiveJob]] The lookup table of ActiveJob s per job id. | jobIdToStageIds | [[jobIdToStageIds]] The lookup table of all stages per ActiveJob id | metricsSource | [[metricsSource]] xref:metrics:spark-scheduler-DAGSchedulerSource.adoc[DAGSchedulerSource] | nextJobId | [[nextJobId]] The next job id counting from 0 . Used when DAGScheduler < > and < >, and < >. | nextStageId | [[nextStageId]] The next stage id counting from 0 . Used when DAGScheduler creates a < > and a < >. It is the key in < >. | runningStages | [[runningStages]] The set of stages that are currently \"running\". A stage is added when < > gets executed (without first checking if the stage has not already been added). | shuffleIdToMapStage | [[shuffleIdToMapStage]] The lookup table of xref:scheduler:ShuffleMapStage.adoc[ShuffleMapStage]s per xref:rdd:ShuffleDependency.adoc[ShuffleDependency]. | stageIdToStage | [[stageIdToStage]] The lookup table for stages per their ids. Used when DAGScheduler < >, < >, < >, is informed that xref:scheduler:DAGSchedulerEventProcessLoop.adoc#handleBeginEvent[a task is started], xref:scheduler:DAGSchedulerEventProcessLoop.adoc#handleTaskSetFailed[a taskset has failed], xref:scheduler:DAGSchedulerEventProcessLoop.adoc#handleJobSubmitted[a job is submitted (to compute a ResultStage )], xref:scheduler:DAGSchedulerEventProcessLoop.adoc#handleMapStageSubmitted[a map stage was submitted], xref:scheduler:DAGSchedulerEventProcessLoop.adoc#handleTaskCompletion[a task has completed] or xref:scheduler:DAGSchedulerEventProcessLoop.adoc#handleStageCancellation[a stage was cancelled], < >, < > and < >. | waitingStages | [[waitingStages]] The stages with parents to be computed |===","title":"DAGScheduler"},{"location":"scheduler/DAGScheduler/#note","text":"","title":"[NOTE]"},{"location":"scheduler/DAGScheduler/#the-introduction-that-follows-was-highly-influenced-by-the-scaladoc-of-httpsgithubcomapachesparkblobmastercoresrcmainscalaorgapachesparkschedulerdagschedulerscalaorgapachesparkschedulerdagscheduler-as-dagscheduler-is-a-private-class-it-does-not-appear-in-the-official-api-documentation-you-are-strongly-encouraged-to-read-httpsgithubcomapachesparkblobmastercoresrcmainscalaorgapachesparkschedulerdagschedulerscalathe-sources-and-only-then-read-this-and-the-related-pages-afterwards","text":"== [[introduction]] Introduction DAGScheduler is the scheduling layer of Apache Spark that implements stage-oriented scheduling . DAGScheduler transforms a logical execution plan (i.e. xref:rdd:spark-rdd-lineage.adoc[RDD lineage] of dependencies built using xref:rdd:spark-rdd-transformations.adoc[RDD transformations]) to a physical execution plan (using xref:scheduler:Stage.adoc[stages]). .DAGScheduler Transforming RDD Lineage Into Stage DAG image::dagscheduler-rdd-lineage-stage-dag.png[align=\"center\"] After an xref:rdd:spark-rdd-actions.adoc[action] has been called, xref:ROOT:SparkContext.adoc[SparkContext] hands over a logical plan to DAGScheduler that it in turn translates to a set of stages that are submitted as xref:scheduler:TaskSet.adoc[TaskSets] for execution. .Executing action leads to new ResultStage and ActiveJob in DAGScheduler image::dagscheduler-rdd-partitions-job-resultstage.png[align=\"center\"] The fundamental concepts of DAGScheduler are jobs and stages (refer to xref:scheduler:spark-scheduler-ActiveJob.adoc[Jobs] and xref:scheduler:Stage.adoc[Stages] respectively) that it tracks through < >. DAGScheduler works solely on the driver and is created as part of xref:ROOT:SparkContext.adoc#creating-instance[SparkContext's initialization] (right after xref:scheduler:TaskScheduler.adoc[TaskScheduler] and xref:scheduler:SchedulerBackend.adoc[SchedulerBackend] are ready). .DAGScheduler as created by SparkContext with other services image::dagscheduler-new-instance.png[align=\"center\"] DAGScheduler does three things in Spark (thorough explanations follow): Computes an execution DAG , i.e. DAG of stages, for a job. Determines the < > to run each task on. Handles failures due to shuffle output files being lost. DAGScheduler computes https://en.wikipedia.org/wiki/Directed_acyclic_graph[a directed acyclic graph (DAG)] of stages for each job, keeps track of which RDDs and stage outputs are materialized, and finds a minimal schedule to run jobs. It then submits stages to xref:scheduler:TaskScheduler.adoc[TaskScheduler]. .DAGScheduler.submitJob image::dagscheduler-submitjob.png[align=\"center\"] In addition to coming up with the execution DAG, DAGScheduler also determines the preferred locations to run each task on, based on the current cache status, and passes the information to xref:scheduler:TaskScheduler.adoc[TaskScheduler]. DAGScheduler tracks which xref:rdd:spark-rdd-caching.adoc[RDDs are cached (or persisted)] to avoid \"recomputing\" them, i.e. redoing the map side of a shuffle. DAGScheduler remembers what xref:scheduler:ShuffleMapStage.adoc[ShuffleMapStage]s have already produced output files (that are stored in xref:storage:BlockManager.adoc[BlockManager]s). DAGScheduler is only interested in cache location coordinates, i.e. host and executor id, per partition of a RDD. Furthermore, it handles failures due to shuffle output files being lost, in which case old stages may need to be resubmitted. Failures within a stage that are not caused by shuffle file loss are handled by the TaskScheduler itself, which will retry each task a small number of times before cancelling the whole stage. DAGScheduler uses an event queue architecture in which a thread can post DAGSchedulerEvent events, e.g. a new job or stage being submitted, that DAGScheduler reads and executes sequentially. See the section < >. DAGScheduler runs stages in topological order. DAGScheduler uses xref:ROOT:SparkContext.adoc[SparkContext], xref:scheduler:TaskScheduler.adoc[TaskScheduler], xref:scheduler:LiveListenerBus.adoc[], xref:scheduler:MapOutputTracker.adoc[MapOutputTracker] and xref:storage:BlockManager.adoc[BlockManager] for its services. However, at the very minimum, DAGScheduler takes a SparkContext only (and requests SparkContext for the other services). When DAGScheduler schedules a job as a result of xref:rdd:index.adoc#actions[executing an action on a RDD] or xref:ROOT:SparkContext.adoc#runJob[calling SparkContext.runJob() method directly], it spawns parallel tasks to compute (partial) results per partition. == [[creating-instance]][[initialization]] Creating Instance DAGScheduler takes the following to be created: [[sc]] xref:ROOT:SparkContext.adoc[] < > [[listenerBus]] xref:scheduler:LiveListenerBus.adoc[] [[mapOutputTracker]] xref:scheduler:MapOutputTrackerMaster.adoc[MapOutputTrackerMaster] [[blockManagerMaster]] xref:storage:BlockManagerMaster.adoc[BlockManagerMaster] [[env]] xref:core:SparkEnv.adoc[] [[clock]] Clock (default: SystemClock) While being created, DAGScheduler xref:scheduler:TaskScheduler.adoc#setDAGScheduler[associates itself] with the < > and starts < >. == [[event-loop]][[eventProcessLoop]] DAGScheduler Event Bus DAGScheduler uses an xref:scheduler:DAGSchedulerEventProcessLoop.adoc[event bus] to process scheduling-related events on a separate thread (one by one and asynchronously). DAGScheduler starts the event bus when created and stops it when requested to < >. DAGScheduler defines < > that allow posting DAGSchedulerEvent events to the event bus. [[event-posting-methods]] .DAGScheduler Event Posting Methods [cols=\"20m,20m,60\",options=\"header\",width=\"100%\"] |=== | Method | Event Posted | Trigger | [[cancelAllJobs]] cancelAllJobs | xref:scheduler:DAGSchedulerEvent.adoc#AllJobsCancelled[AllJobsCancelled] | SparkContext is requested to xref:ROOT:SparkContext.adoc#cancelAllJobs[cancel all running or scheduled Spark jobs] | [[cancelJob]] cancelJob | xref:scheduler:DAGSchedulerEvent.adoc#JobCancelled[JobCancelled] | xref:ROOT:SparkContext.adoc#cancelJob[SparkContext] or xref:scheduler:spark-scheduler-JobWaiter.adoc[JobWaiter] are requested to cancel a Spark job | [[cancelJobGroup]] cancelJobGroup | xref:scheduler:DAGSchedulerEvent.adoc#JobGroupCancelled[JobGroupCancelled] | SparkContext is requested to xref:ROOT:SparkContext.adoc#cancelJobGroup[cancel a job group] | [[cancelStage]] cancelStage | xref:scheduler:DAGSchedulerEvent.adoc#StageCancelled[StageCancelled] | SparkContext is requested to xref:ROOT:SparkContext.adoc#cancelStage[cancel a stage] | [[executorAdded]] executorAdded | xref:scheduler:DAGSchedulerEvent.adoc#ExecutorAdded[ExecutorAdded] | TaskSchedulerImpl is requested to xref:scheduler:TaskSchedulerImpl.adoc#resourceOffers[handle resource offers] (and a new executor is found in the resource offers) | [[executorLost]] executorLost | xref:scheduler:DAGSchedulerEvent.adoc#ExecutorLost[ExecutorLost] | TaskSchedulerImpl is requested to xref:scheduler:TaskSchedulerImpl.adoc#statusUpdate[handle a task status update] (and a task gets lost which is used to indicate that the executor got broken and hence should be considered lost) or xref:scheduler:TaskSchedulerImpl.adoc#executorLost[executorLost] | [[runApproximateJob]] runApproximateJob | xref:scheduler:DAGSchedulerEvent.adoc#JobSubmitted[JobSubmitted] | SparkContext is requested to xref:ROOT:SparkContext.adoc#runApproximateJob[run an approximate job] | [[speculativeTaskSubmitted]] speculativeTaskSubmitted | xref:scheduler:DAGSchedulerEvent.adoc#SpeculativeTaskSubmitted[SpeculativeTaskSubmitted] | | [[submitJob]] submitJob | xref:scheduler:DAGSchedulerEvent.adoc#JobSubmitted[JobSubmitted] a| SparkContext is requested to xref:ROOT:SparkContext.adoc#submitJob[submits a job] DAGScheduler is requested to < > | [[submitMapStage]] submitMapStage | xref:scheduler:DAGSchedulerEvent.adoc#MapStageSubmitted[MapStageSubmitted] | SparkContext is requested to xref:ROOT:SparkContext.adoc#submitMapStage[submit a MapStage for execution]. | [[taskEnded]] taskEnded | xref:scheduler:DAGSchedulerEvent.adoc#CompletionEvent[CompletionEvent] | TaskSetManager is requested to xref:scheduler:TaskSetManager.adoc#handleSuccessfulTask[handleSuccessfulTask], xref:scheduler:TaskSetManager.adoc#handleFailedTask[handleFailedTask], and xref:scheduler:TaskSetManager.adoc#executorLost[executorLost] | [[taskGettingResult]] taskGettingResult | xref:scheduler:DAGSchedulerEvent.adoc#GettingResultEvent[GettingResultEvent] | TaskSetManager is requested to xref:scheduler:TaskSetManager.adoc#handleTaskGettingResult[handle a task fetching result] | [[taskSetFailed]] taskSetFailed | xref:scheduler:DAGSchedulerEvent.adoc#TaskSetFailed[TaskSetFailed] | TaskSetManager is requested to xref:scheduler:TaskSetManager.adoc#abort[abort] | [[taskStarted]] taskStarted | xref:scheduler:DAGSchedulerEvent.adoc#BeginEvent[BeginEvent] | TaskSetManager is requested to xref:scheduler:TaskSetManager.adoc#resourceOffer[start a task] | [[workerRemoved]] workerRemoved | xref:scheduler:DAGSchedulerEvent.adoc#WorkerRemoved[WorkerRemoved] | TaskSchedulerImpl is requested to xref:scheduler:TaskSchedulerImpl.adoc#workerRemoved[handle a removed worker event] |=== == [[taskScheduler]] DAGScheduler and TaskScheduler DAGScheduler is given a xref:scheduler:TaskScheduler.adoc[TaskScheduler] when < >. DAGScheduler uses the TaskScheduler for the following: < > < > < > < > < > == [[runJob]] Running Job","title":"The introduction that follows was highly influenced by the scaladoc of https://github.com/apache/spark/blob/master/core/src/main/scala/org/apache/spark/scheduler/DAGScheduler.scala[org.apache.spark.scheduler.DAGScheduler]. As DAGScheduler is a private class it does not appear in the official API documentation. You are strongly encouraged to read https://github.com/apache/spark/blob/master/core/src/main/scala/org/apache/spark/scheduler/DAGScheduler.scala[the sources] and only then read this and the related pages afterwards."},{"location":"scheduler/DAGScheduler/#source-scala","text":"runJob T, U : Unit runJob submits an action job to the DAGScheduler and waits for a result. Internally, runJob executes < > and then waits until a result comes using xref:scheduler:spark-scheduler-JobWaiter.adoc[JobWaiter]. When the job succeeds, you should see the following INFO message in the logs: Job [jobId] finished: [callSite], took [time] s When the job fails, you should see the following INFO message in the logs and the exception (that led to the failure) is thrown. Job [jobId] failed: [callSite], took [time] s runJob is used when SparkContext is requested to xref:ROOT:SparkContext.adoc#runJob[run a job]. == [[cacheLocs]][[clearCacheLocs]] Partition Placement Preferences DAGScheduler keeps track of block locations per RDD and partition. DAGScheduler uses xref:scheduler:TaskLocation.adoc[TaskLocation] that includes a host name and an executor id on that host (as ExecutorCacheTaskLocation ). The keys are RDDs (their ids) and the values are arrays indexed by partition numbers. Each entry is a set of block locations where a RDD partition is cached, i.e. the xref:storage:BlockManager.adoc[BlockManager]s of the blocks. Initialized empty when < >. Used when DAGScheduler is requested for the < > or < >. == [[activeJobs]] ActiveJobs DAGScheduler tracks xref:scheduler:spark-scheduler-ActiveJob.adoc[ActiveJobs]: Adds a new ActiveJob when requested to handle < > or < > events Removes an ActiveJob when requested to < >. Removes all ActiveJobs when requested to < >. DAGScheduler uses ActiveJobs registry when requested to handle < > or < > events, to < > and to < >. The number of ActiveJobs is available using xref:metrics:spark-scheduler-DAGSchedulerSource.adoc#job.activeJobs[job.activeJobs] performance metric. == [[createResultStage]] Creating ResultStage for RDD","title":"[source, scala]"},{"location":"scheduler/DAGScheduler/#source-scala_1","text":"createResultStage( rdd: RDD[ ], func: (TaskContext, Iterator[ ]) => _, partitions: Array[Int], jobId: Int, callSite: CallSite): ResultStage createResultStage...FIXME createResultStage is used when DAGScheduler is requested to < >. == [[createShuffleMapStage]] Creating ShuffleMapStage for ShuffleDependency","title":"[source, scala]"},{"location":"scheduler/DAGScheduler/#source-scala_2","text":"createShuffleMapStage( shuffleDep: ShuffleDependency[_, _, _], jobId: Int): ShuffleMapStage createShuffleMapStage creates a xref:scheduler:ShuffleMapStage.adoc[ShuffleMapStage] for the given xref:rdd:ShuffleDependency.adoc[ShuffleDependency] as follows: Stage ID is generated based on < > internal counter RDD is taken from the given xref:rdd:ShuffleDependency.adoc#rdd[ShuffleDependency] Number of tasks is the number of xref:rdd:RDD.adoc#partitions[partitions] of the RDD < > < > createShuffleMapStage registers the ShuffleMapStage in the < > and < > internal registries. createShuffleMapStage < >. createShuffleMapStage requests the < > to xref:scheduler:MapOutputTrackerMaster.adoc#containsShuffle[check whether it contains the shuffle ID or not]. If not, createShuffleMapStage prints out the following INFO message to the logs and requests the < > to xref:scheduler:MapOutputTrackerMaster.adoc#registerShuffle[register the shuffle].","title":"[source, scala]"},{"location":"scheduler/DAGScheduler/#sourceplaintext","text":"","title":"[source,plaintext]"},{"location":"scheduler/DAGScheduler/#registering-rdd-id-creationsite-as-input-to-shuffle-shuffleid","text":".DAGScheduler Asks MapOutputTrackerMaster Whether Shuffle Map Output Is Already Tracked image::DAGScheduler-MapOutputTrackerMaster-containsShuffle.png[align=\"center\"] createShuffleMapStage is used when DAGScheduler is requested to < >. == [[cleanupStateForJobAndIndependentStages]] Cleaning Up After Job and Independent Stages","title":"Registering RDD [id] ([creationSite]) as input to shuffle [shuffleId]"},{"location":"scheduler/DAGScheduler/#source-scala_3","text":"cleanupStateForJobAndIndependentStages( job: ActiveJob): Unit cleanupStateForJobAndIndependentStages cleans up the state for job and any stages that are not part of any other job. cleanupStateForJobAndIndependentStages looks the job up in the internal < > registry. If no stages are found, the following ERROR is printed out to the logs: No stages registered for job [jobId] Oterwise, cleanupStateForJobAndIndependentStages uses < > registry to find the stages (the real objects not ids!). For each stage, cleanupStateForJobAndIndependentStages reads the jobs the stage belongs to. If the job does not belong to the jobs of the stage, the following ERROR is printed out to the logs: Job [jobId] not registered for stage [stageId] even though that stage was registered for the job If the job was the only job for the stage, the stage (and the stage id) gets cleaned up from the registries, i.e. < >, < >, < >, < > and < >. While removing from < >, you should see the following DEBUG message in the logs: Removing running stage [stageId] While removing from < >, you should see the following DEBUG message in the logs: Removing stage [stageId] from waiting set. While removing from < >, you should see the following DEBUG message in the logs: Removing stage [stageId] from failed set. After all cleaning (using < > as the source registry), if the stage belonged to the one and only job , you should see the following DEBUG message in the logs: After removal of stage [stageId], remaining stages = [stageIdToStage.size] The job is removed from < >, < >, < > registries. The final stage of the job is removed, i.e. xref:scheduler:ResultStage.adoc#removeActiveJob[ResultStage] or xref:scheduler:ShuffleMapStage.adoc#removeActiveJob[ShuffleMapStage]. cleanupStateForJobAndIndependentStages is used in xref:scheduler:DAGSchedulerEventProcessLoop.adoc#handleTaskCompletion-Success-ResultTask[handleTaskCompletion when a ResultTask has completed successfully], < > and < >. == [[markMapStageJobAsFinished]] Marking ShuffleMapStage Job Finished","title":"[source, scala]"},{"location":"scheduler/DAGScheduler/#source-scala_4","text":"markMapStageJobAsFinished( job: ActiveJob, stats: MapOutputStatistics): Unit markMapStageJobAsFinished marks the active job finished and notifies Spark listeners. Internally, markMapStageJobAsFinished marks the zeroth partition finished and increases the number of tasks finished in job . The xref:scheduler:spark-scheduler-JobListener.adoc#taskSucceeded[ job listener is notified about the 0 th task succeeded]. The < job and independent stages are cleaned up>>. Ultimately, xref:ROOT:SparkListener.adoc#SparkListenerJobEnd[SparkListenerJobEnd] is posted to xref:scheduler:LiveListenerBus.adoc[] (as < >) for the job , the current time (in millis) and JobSucceeded job result. markMapStageJobAsFinished is used in xref:scheduler:DAGSchedulerEventProcessLoop.adoc#handleMapStageSubmitted[handleMapStageSubmitted] and xref:scheduler:DAGSchedulerEventProcessLoop.adoc#handleTaskCompletion[handleTaskCompletion]. == [[getOrCreateParentStages]] Finding Or Creating Missing Direct Parent ShuffleMapStages (For ShuffleDependencies) of RDD","title":"[source, scala]"},{"location":"scheduler/DAGScheduler/#source-scala_5","text":"getOrCreateParentStages( rdd: RDD[_], firstJobId: Int): List[Stage] getOrCreateParentStages < ShuffleDependencies >> of the input rdd and then < ShuffleMapStage stages>> for each xref:rdd:ShuffleDependency.adoc[ShuffleDependency]. getOrCreateParentStages is used when DAGScheduler is requested to create a < > or a < >. == [[markStageAsFinished]] Marking Stage Finished","title":"[source, scala]"},{"location":"scheduler/DAGScheduler/#source-scala_6","text":"markStageAsFinished( stage: Stage, errorMessage: Option[String] = None, willRetry: Boolean = false): Unit markStageAsFinished...FIXME markStageAsFinished is used when...FIXME == [[getOrCreateShuffleMapStage]] Finding or Creating ShuffleMapStage for ShuffleDependency","title":"[source, scala]"},{"location":"scheduler/DAGScheduler/#source-scala_7","text":"getOrCreateShuffleMapStage( shuffleDep: ShuffleDependency[_, _, _], firstJobId: Int): ShuffleMapStage getOrCreateShuffleMapStage finds the xref:scheduler:ShuffleMapStage.adoc[ShuffleMapStage] in the < > internal registry and returns it if available. If not found, getOrCreateShuffleMapStage < > and < > (including one for the input ShuffleDependency). getOrCreateShuffleMapStage is used when DAGScheduler is requested to < >, < >, < >, and < >. == [[getMissingAncestorShuffleDependencies]] Finding Missing ShuffleDependencies For RDD","title":"[source, scala]"},{"location":"scheduler/DAGScheduler/#source-scala_8","text":"getMissingAncestorShuffleDependencies( rdd: RDD[ ]): Stack[ShuffleDependency[ , _, _]] getMissingAncestorShuffleDependencies finds all missing xref:rdd:ShuffleDependency.adoc[shuffle dependencies] for the given xref:rdd:index.adoc[RDD] traversing its xref:rdd:spark-rdd-lineage.adoc[RDD lineage]. NOTE: A missing shuffle dependency of a RDD is a dependency not registered in < shuffleIdToMapStage internal registry>>. Internally, getMissingAncestorShuffleDependencies < >\u2009of the input RDD and collects the ones that are not registered in < shuffleIdToMapStage internal registry>>. It repeats the process for the RDDs of the parent shuffle dependencies. getMissingAncestorShuffleDependencies is used when DAGScheduler is requested to < >. == [[getShuffleDependencies]] Finding Direct Parent Shuffle Dependencies of RDD","title":"[source, scala]"},{"location":"scheduler/DAGScheduler/#source-scala_9","text":"getShuffleDependencies( rdd: RDD[ ]): HashSet[ShuffleDependency[ , _, _]] getShuffleDependencies finds direct parent xref:rdd:ShuffleDependency.adoc[shuffle dependencies] for the given xref:rdd:index.adoc[RDD]. .getShuffleDependencies Finds Direct Parent ShuffleDependencies (shuffle1 and shuffle2) image::spark-DAGScheduler-getShuffleDependencies.png[align=\"center\"] Internally, getShuffleDependencies takes the direct xref:rdd:index.adoc#dependencies[shuffle dependencies of the input RDD] and direct shuffle dependencies of all the parent non- ShuffleDependencies in the xref:rdd:spark-rdd-lineage.adoc[dependency chain] (aka RDD lineage ). getShuffleDependencies is used when DAGScheduler is requested to < > (for ShuffleDependencies of a RDD) and < >. == [[failJobAndIndependentStages]] Failing Job and Independent Single-Job Stages","title":"[source, scala]"},{"location":"scheduler/DAGScheduler/#source-scala_10","text":"failJobAndIndependentStages( job: ActiveJob, failureReason: String, exception: Option[Throwable] = None): Unit failJobAndIndependentStages fails the input job and all the stages that are only used by the job. Internally, failJobAndIndependentStages uses < jobIdToStageIds internal registry>> to look up the stages registered for the job. If no stages could be found, you should see the following ERROR message in the logs: No stages registered for job [id] Otherwise, for every stage, failJobAndIndependentStages finds the job ids the stage belongs to. If no stages could be found or the job is not referenced by the stages, you should see the following ERROR message in the logs: Job [id] not registered for stage [id] even though that stage was registered for the job Only when there is exactly one job registered for the stage and the stage is in RUNNING state (in runningStages internal registry), xref:scheduler:TaskScheduler.adoc#contract[ TaskScheduler is requested to cancel the stage's tasks] and < >. NOTE: failJobAndIndependentStages uses < >, < >, and < > internal registries. failJobAndIndependentStages is used when...FIXME == [[abortStage]] Aborting Stage","title":"[source, scala]"},{"location":"scheduler/DAGScheduler/#source-scala_11","text":"abortStage( failedStage: Stage, reason: String, exception: Option[Throwable]): Unit abortStage is an internal method that finds all the active jobs that depend on the failedStage stage and fails them. Internally, abortStage looks the failedStage stage up in the internal < > registry and exits if there the stage was not registered earlier. If it was, abortStage finds all the active jobs (in the internal < > registry) with the < failedStage stage>>. At this time, the completionTime property (of the failed stage's xref:scheduler:spark-scheduler-StageInfo.adoc[StageInfo]) is assigned to the current time (millis). All the active jobs that depend on the failed stage (as calculated above) and the stages that do not belong to other jobs (aka independent stages ) are < > (with the failure reason being \"Job aborted due to stage failure: [reason]\" and the input exception ). If there are no jobs depending on the failed stage, you should see the following INFO message in the logs:","title":"[source, scala]"},{"location":"scheduler/DAGScheduler/#sourceplaintext_1","text":"","title":"[source,plaintext]"},{"location":"scheduler/DAGScheduler/#ignoring-failure-of-failedstage-because-all-jobs-depending-on-it-are-done","text":"abortStage is used when DAGScheduler is requested to < >, < >, < >, < >. == [[stageDependsOn]] Checking Out Stage Dependency on Given Stage","title":"Ignoring failure of [failedStage] because all jobs depending on it are done"},{"location":"scheduler/DAGScheduler/#source-scala_12","text":"stageDependsOn( stage: Stage, target: Stage): Boolean stageDependsOn compares two stages and returns whether the stage depends on target stage (i.e. true ) or not (i.e. false ). NOTE: A stage A depends on stage B if B is among the ancestors of A . Internally, stageDependsOn walks through the graph of RDDs of the input stage . For every RDD in the RDD's dependencies (using RDD.dependencies ) stageDependsOn adds the RDD of a xref:rdd:spark-rdd-NarrowDependency.adoc[NarrowDependency] to a stack of RDDs to visit while for a xref:rdd:ShuffleDependency.adoc[ShuffleDependency] it < ShuffleMapStage stages for a ShuffleDependency >> for the dependency and the stage 's first job id that it later adds to a stack of RDDs to visit if the map stage is ready, i.e. all the partitions have shuffle outputs. After all the RDDs of the input stage are visited, stageDependsOn checks if the target 's RDD is among the RDDs of the stage , i.e. whether the stage depends on target stage. stageDependsOn is used when DAGScheduler is requested to < >. == [[submitWaitingChildStages]] Submitting Waiting Child Stages for Execution","title":"[source, scala]"},{"location":"scheduler/DAGScheduler/#source-scala_13","text":"submitWaitingChildStages( parent: Stage): Unit submitWaitingChildStages submits for execution all waiting stages for which the input parent xref:scheduler:Stage.adoc[Stage] is the direct parent. NOTE: Waiting stages are the stages registered in < waitingStages internal registry>>. When executed, you should see the following TRACE messages in the logs: Checking if any dependencies of [parent] are now runnable running: [runningStages] waiting: [waitingStages] failed: [failedStages] submitWaitingChildStages finds child stages of the input parent stage, removes them from waitingStages internal registry, and < > one by one sorted by their job ids. submitWaitingChildStages is used when DAGScheduler is requested to < > and < >. == [[submitStage]] Submitting Stage (with Missing Parents) for Execution","title":"[source, scala]"},{"location":"scheduler/DAGScheduler/#source-scala_14","text":"submitStage( stage: Stage): Unit submitStage submits the input stage or its missing parents (if there any stages not computed yet before the input stage could). NOTE: submitStage is also used to xref:scheduler:DAGSchedulerEventProcessLoop.adoc#resubmitFailedStages[resubmit failed stages]. submitStage recursively submits any missing parents of the stage . Internally, submitStage first finds the earliest-created job id that needs the stage . NOTE: A stage itself tracks the jobs (their ids) it belongs to (using the internal jobIds registry). The following steps depend on whether there is a job or not. If there are no jobs that require the stage , submitStage < > with the reason: No active job for stage [id] If however there is a job for the stage , you should see the following DEBUG message in the logs: submitStage([stage]) submitStage checks the status of the stage and continues when it was not recorded in < >, < > or < > internal registries. It simply exits otherwise. With the stage ready for submission, submitStage calculates the < stage >> (sorted by their job ids). You should see the following DEBUG message in the logs: missing: [missing] When the stage has no parent stages missing, you should see the following INFO message in the logs: Submitting [stage] ([stage.rdd]), which has no missing parents submitStage < stage >> (with the earliest-created job id) and finishes. If however there are missing parent stages for the stage , submitStage < >, and the stage is recorded in the internal < > registry. submitStage is used recursively for missing parents of the given stage and when DAGScheduler is requested for the following: < > (ResubmitFailedStages event) < > (CompletionEvent event) Handle < >, < > and < > events == [[stage-attempts]] Stage Attempts A single stage can be re-executed in multiple attempts due to fault recovery. The number of attempts is configured (FIXME). If TaskScheduler reports that a task failed because a map output file from a previous stage was lost, the DAGScheduler resubmits the lost stage. This is detected through a xref:scheduler:DAGSchedulerEventProcessLoop.adoc#handleTaskCompletion-FetchFailed[ CompletionEvent with FetchFailed ], or an < > event. DAGScheduler will wait a small amount of time to see whether other nodes or tasks fail, then resubmit TaskSets for any lost stage(s) that compute the missing tasks. Please note that tasks from the old attempts of a stage could still be running. A stage object tracks multiple xref:scheduler:spark-scheduler-StageInfo.adoc[StageInfo] objects to pass to Spark listeners or the web UI. The latest StageInfo for the most recent attempt for a stage is accessible through latestInfo . == [[preferred-locations]] Preferred Locations DAGScheduler computes where to run each task in a stage based on the xref:rdd:index.adoc#getPreferredLocations[preferred locations of its underlying RDDs], or < >. == [[adaptive-query-planning]] Adaptive Query Planning / Adaptive Scheduling See https://issues.apache.org/jira/browse/SPARK-9850[SPARK-9850 Adaptive execution in Spark] for the design document. The work is currently in progress. https://github.com/apache/spark/blob/master/core/src/main/scala/org/apache/spark/scheduler/DAGScheduler.scala#L661[DAGScheduler.submitMapStage ] method is used for adaptive query planning, to run map stages and look at statistics about their outputs before submitting downstream stages. == ScheduledExecutorService daemon services DAGScheduler uses the following ScheduledThreadPoolExecutors (with the policy of removing cancelled tasks from a work queue at time of cancellation): dag-scheduler-message - a daemon thread pool using j.u.c.ScheduledThreadPoolExecutor with core pool size 1 . It is used to post a xref:scheduler:DAGSchedulerEventProcessLoop.adoc#ResubmitFailedStages[ResubmitFailedStages] event when xref:scheduler:DAGSchedulerEventProcessLoop.adoc#handleTaskCompletion-FetchFailed[ FetchFailed is reported]. They are created using ThreadUtils.newDaemonSingleThreadScheduledExecutor method that uses Guava DSL to instantiate a ThreadFactory. == [[getMissingParentStages]] Finding Missing Parent ShuffleMapStages For Stage","title":"[source, scala]"},{"location":"scheduler/DAGScheduler/#source-scala_15","text":"getMissingParentStages( stage: Stage): List[Stage] getMissingParentStages finds missing parent xref:scheduler:ShuffleMapStage.adoc[ShuffleMapStage]s in the dependency graph of the input stage (using the https://en.wikipedia.org/wiki/Breadth-first_search[breadth-first search algorithm]). Internally, getMissingParentStages starts with the stage 's RDD and walks up the tree of all parent RDDs to find < >. NOTE: A Stage tracks the associated RDD using xref:scheduler:Stage.adoc#rdd[ rdd property]. NOTE: An uncached partition of a RDD is a partition that has Nil in the < > (which results in no RDD blocks in any of the active xref:storage:BlockManager.adoc[BlockManager]s on executors). getMissingParentStages traverses the xref:rdd:index.adoc#dependencies[parent dependencies of the RDD] and acts according to their type, i.e. xref:rdd:ShuffleDependency.adoc[ShuffleDependency] or xref:rdd:spark-rdd-NarrowDependency.adoc[NarrowDependency]. NOTE: xref:rdd:ShuffleDependency.adoc[ShuffleDependency] and xref:rdd:spark-rdd-NarrowDependency.adoc[NarrowDependency] are the main top-level xref:rdd:spark-rdd-Dependency.adoc[Dependencies]. For each NarrowDependency , getMissingParentStages simply marks the corresponding RDD to visit and moves on to a next dependency of a RDD or works on another unvisited parent RDD. NOTE: xref:rdd:spark-rdd-NarrowDependency.adoc[NarrowDependency] is a RDD dependency that allows for pipelined execution. getMissingParentStages focuses on ShuffleDependency dependencies. NOTE: xref:rdd:ShuffleDependency.adoc[ShuffleDependency] is a RDD dependency that represents a dependency on the output of a xref:scheduler:ShuffleMapStage.adoc[ShuffleMapStage], i.e. shuffle map stage . For each ShuffleDependency , getMissingParentStages < ShuffleMapStage stages>>. If the ShuffleMapStage is not available , it is added to the set of missing (map) stages. NOTE: A ShuffleMapStage is available when all its partitions are computed, i.e. results are available (as blocks). CAUTION: FIXME...IMAGE with ShuffleDependencies queried getMissingParentStages is used when DAGScheduler is requested to < > and handle < > and < > events. == [[submitMissingTasks]] Submitting Missing Tasks of Stage","title":"[source, scala]"},{"location":"scheduler/DAGScheduler/#source-scala_16","text":"submitMissingTasks( stage: Stage, jobId: Int): Unit submitMissingTasks prints out the following DEBUG message to the logs: submitMissingTasks([stage]) submitMissingTasks requests the given xref:scheduler:Stage.adoc[Stage] for the xref:scheduler:Stage.adoc#findMissingPartitions[missing partitions] (partitions that need to be computed). submitMissingTasks adds the stage to the < > internal registry. submitMissingTasks notifies the < > that xref:scheduler:OutputCommitCoordinator.adoc#stageStart[stage execution started]. [[submitMissingTasks-taskIdToLocations]] submitMissingTasks < > ( task locality preferences ) of the missing partitions. submitMissingTasks requests the stage for a xref:scheduler:Stage.adoc#makeNewStageAttempt[new stage attempt]. submitMissingTasks requests the < > to xref:scheduler:LiveListenerBus.adoc#post[post] a xref:ROOT:SparkListener.adoc#SparkListenerStageSubmitted[SparkListenerStageSubmitted] event. submitMissingTasks uses the < > to xref:serializer:Serializer.adoc#serialize[serialize] the stage and create a so-called task binary. submitMissingTasks serializes the RDD (of the stage) and either the ShuffleDependency or the compute function based on the type of the stage, i.e. ShuffleMapStage and ResultStage, respectively. submitMissingTasks creates a xref:ROOT:SparkContext.adoc#broadcast[broadcast variable] for the task binary. NOTE: That shows how important xref:ROOT:Broadcast.adoc[]s are for Spark itself to distribute data among executors in a Spark application in the most efficient way. submitMissingTasks creates xref:scheduler:Task.adoc[tasks] for every missing partition: xref:scheduler:ShuffleMapTask.adoc[ShuffleMapTasks] for a xref:scheduler:ShuffleMapStage.adoc[ShuffleMapStage] xref:scheduler:ResultTask.adoc[ResultTasks] for a xref:scheduler:ResultStage.adoc[ResultStage] If there are tasks to submit for execution (i.e. there are missing partitions in the stage), submitMissingTasks prints out the following INFO message to the logs: Submitting [size] missing tasks from [stage] ([rdd]) (first 15 tasks are for partitions [partitionIds]) submitMissingTasks requests the < > to xref:scheduler:TaskScheduler.adoc#submitTasks[submit the tasks for execution] (as a new xref:scheduler:TaskSet.adoc[TaskSet]). With no tasks to submit for execution, submitMissingTasks < >. submitMissingTasks prints out the following DEBUG messages based on the type of the stage: Stage [stage] is actually done; (available: [isAvailable],available outputs: [numAvailableOutputs],partitions: [numPartitions]) or Stage [stage] is actually done; (partitions: [numPartitions]) for ShuffleMapStage and ResultStage , respectively. In the end, with no tasks to submit for execution, submitMissingTasks < > and exits. submitMissingTasks is used when DAGScheduler is requested to < >. == [[getPreferredLocs]] Finding Preferred Locations for Missing Partitions","title":"[source, scala]"},{"location":"scheduler/DAGScheduler/#source-scala_17","text":"getPreferredLocs( rdd: RDD[_], partition: Int): Seq[TaskLocation] getPreferredLocs is simply an alias for the internal (recursive) < >. getPreferredLocs is used when...FIXME == [[getCacheLocs]] Finding BlockManagers (Executors) for Cached RDD Partitions (aka Block Location Discovery)","title":"[source, scala]"},{"location":"scheduler/DAGScheduler/#source-scala_18","text":"getCacheLocs( rdd: RDD[_]): IndexedSeq[Seq[TaskLocation]] getCacheLocs gives xref:scheduler:TaskLocation.adoc[TaskLocations] (block locations) for the partitions of the input rdd . getCacheLocs caches lookup results in < > internal registry. NOTE: The size of the collection from getCacheLocs is exactly the number of partitions in rdd RDD. NOTE: The size of every xref:scheduler:TaskLocation.adoc[TaskLocation] collection (i.e. every entry in the result of getCacheLocs) is exactly the number of blocks managed using xref:storage:BlockManager.adoc[BlockManagers] on executors. Internally, getCacheLocs finds rdd in the < > internal registry (of partition locations per RDD). If rdd is not in < > internal registry, getCacheLocs branches per its xref:storage:StorageLevel.adoc[storage level]. For NONE storage level (i.e. no caching), the result is an empty locations (i.e. no location preference). For other non- NONE storage levels, getCacheLocs xref:storage:BlockManagerMaster.adoc#getLocations-block-array[requests BlockManagerMaster for block locations] that are then mapped to xref:scheduler:TaskLocation.adoc[TaskLocations] with the hostname of the owning BlockManager for a block (of a partition) and the executor id. NOTE: getCacheLocs uses < > that was defined when < >. getCacheLocs records the computed block locations per partition (as xref:scheduler:TaskLocation.adoc[TaskLocation]) in < > internal registry. NOTE: getCacheLocs requests locations from BlockManagerMaster using xref:storage:BlockId.adoc#RDDBlockId[RDDBlockId] with the RDD id and the partition indices (which implies that the order of the partitions matters to request proper blocks). NOTE: DAGScheduler uses xref:scheduler:TaskLocation.adoc[TaskLocations] (with host and executor) while xref:storage:BlockManagerMaster.adoc[BlockManagerMaster] uses xref:storage:BlockManagerId.adoc[] (to track similar information, i.e. block locations). getCacheLocs is used when DAGScheduler is requested to finds < > and < >. == [[getPreferredLocsInternal]] Finding Placement Preferences for RDD Partition (recursively)","title":"[source, scala]"},{"location":"scheduler/DAGScheduler/#source-scala_19","text":"getPreferredLocsInternal( rdd: RDD[ ], partition: Int, visited: HashSet[(RDD[ ], Int)]): Seq[TaskLocation] getPreferredLocsInternal first < TaskLocations for the partition of the rdd >> (using < > internal cache) and returns them. Otherwise, if not found, getPreferredLocsInternal xref:rdd:index.adoc#preferredLocations[requests rdd for the preferred locations of partition ] and returns them. NOTE: Preferred locations of the partitions of a RDD are also called placement preferences or locality preferences . Otherwise, if not found, getPreferredLocsInternal finds the first parent xref:rdd:spark-rdd-NarrowDependency.adoc[NarrowDependency] and (recursively) < TaskLocations >>. If all the attempts fail to yield any non-empty result, getPreferredLocsInternal returns an empty collection of xref:scheduler:TaskLocation.adoc[TaskLocations]. getPreferredLocsInternal is used when DAGScheduler is requested for the < >. == [[stop]] Stopping DAGScheduler","title":"[source, scala]"},{"location":"scheduler/DAGScheduler/#source-scala_20","text":"","title":"[source, scala]"},{"location":"scheduler/DAGScheduler/#stop-unit","text":"stop stops the internal dag-scheduler-message thread pool, < >, and xref:scheduler:TaskScheduler.adoc#stop[TaskScheduler]. stop is used when...FIXME == [[updateAccumulators]] Updating Accumulators with Partial Values from Completed Tasks","title":"stop(): Unit"},{"location":"scheduler/DAGScheduler/#source-scala_21","text":"updateAccumulators( event: CompletionEvent): Unit updateAccumulators merges the partial values of accumulators from a completed task into their \"source\" accumulators on the driver. NOTE: It is called by < >. For each xref:ROOT:spark-accumulators.adoc#AccumulableInfo[AccumulableInfo] in the CompletionEvent , a partial value from a task is obtained (from AccumulableInfo.update ) and added to the driver's accumulator (using Accumulable.++= method). For named accumulators with the update value being a non-zero value, i.e. not Accumulable.zero : stage.latestInfo.accumulables for the AccumulableInfo.id is set CompletionEvent.taskInfo.accumulables has a new xref:ROOT:spark-accumulators.adoc#AccumulableInfo[AccumulableInfo] added. CAUTION: FIXME Where are Stage.latestInfo.accumulables and CompletionEvent.taskInfo.accumulables used? updateAccumulators is used when DAGScheduler is requested to < >. == [[checkBarrierStageWithNumSlots]] checkBarrierStageWithNumSlots Method","title":"[source, scala]"},{"location":"scheduler/DAGScheduler/#source-scala_22","text":"checkBarrierStageWithNumSlots( rdd: RDD[_]): Unit checkBarrierStageWithNumSlots...FIXME checkBarrierStageWithNumSlots is used when DAGScheduler is requested to create < > and < > stages. == [[killTaskAttempt]] Killing Task","title":"[source, scala]"},{"location":"scheduler/DAGScheduler/#source-scala_23","text":"killTaskAttempt( taskId: Long, interruptThread: Boolean, reason: String): Boolean killTaskAttempt requests the < > to xref:scheduler:TaskScheduler.adoc#killTaskAttempt[kill a task]. killTaskAttempt is used when SparkContext is requested to xref:ROOT:SparkContext.adoc#killTaskAttempt[kill a task]. == [[cleanUpAfterSchedulerStop]] cleanUpAfterSchedulerStop Method","title":"[source, scala]"},{"location":"scheduler/DAGScheduler/#source-scala_24","text":"","title":"[source, scala]"},{"location":"scheduler/DAGScheduler/#cleanupafterschedulerstop-unit","text":"cleanUpAfterSchedulerStop...FIXME cleanUpAfterSchedulerStop is used when DAGSchedulerEventProcessLoop is requested to xref:scheduler:DAGSchedulerEventProcessLoop.adoc#onStop[onStop]. == [[removeExecutorAndUnregisterOutputs]] removeExecutorAndUnregisterOutputs Method","title":"cleanUpAfterSchedulerStop(): Unit"},{"location":"scheduler/DAGScheduler/#source-scala_25","text":"removeExecutorAndUnregisterOutputs( execId: String, fileLost: Boolean, hostToUnregisterOutputs: Option[String], maybeEpoch: Option[Long] = None): Unit removeExecutorAndUnregisterOutputs...FIXME removeExecutorAndUnregisterOutputs is used when DAGScheduler is requested to handle < > (due to a fetch failure) and < > events. == [[markMapStageJobsAsFinished]] markMapStageJobsAsFinished Method","title":"[source, scala]"},{"location":"scheduler/DAGScheduler/#source-scala_26","text":"markMapStageJobsAsFinished( shuffleStage: ShuffleMapStage): Unit markMapStageJobsAsFinished...FIXME markMapStageJobsAsFinished is used when DAGScheduler is requested to < > (of a ShuffleMapStage that has just been computed) and < > (of a ShuffleMapStage). == [[updateJobIdStageIdMaps]] updateJobIdStageIdMaps Method","title":"[source, scala]"},{"location":"scheduler/DAGScheduler/#source-scala_27","text":"updateJobIdStageIdMaps( jobId: Int, stage: Stage): Unit updateJobIdStageIdMaps...FIXME updateJobIdStageIdMaps is used when DAGScheduler is requested to create < > and < > stages. == [[executorHeartbeatReceived]] executorHeartbeatReceived Method","title":"[source, scala]"},{"location":"scheduler/DAGScheduler/#source-scala_28","text":"executorHeartbeatReceived( execId: String, // (taskId, stageId, stageAttemptId, accumUpdates) accumUpdates: Array[(Long, Int, Int, Seq[AccumulableInfo])], blockManagerId: BlockManagerId): Boolean executorHeartbeatReceived posts a xref:ROOT:SparkListener.adoc#SparkListenerExecutorMetricsUpdate[SparkListenerExecutorMetricsUpdate] (to < >) and informs xref:storage:BlockManagerMaster.adoc[BlockManagerMaster] that blockManagerId block manager is alive (by posting xref:storage:BlockManagerMaster.adoc#BlockManagerHeartbeat[BlockManagerHeartbeat]). executorHeartbeatReceived is used when TaskSchedulerImpl is requested to xref:scheduler:TaskSchedulerImpl.adoc#executorHeartbeatReceived[handle an executor heartbeat]. == [[postTaskEnd]] postTaskEnd Method","title":"[source, scala]"},{"location":"scheduler/DAGScheduler/#source-scala_29","text":"postTaskEnd( event: CompletionEvent): Unit postTaskEnd...FIXME postTaskEnd is used when DAGScheduler is requested to < >. == Event Handlers === [[doCancelAllJobs]] AllJobsCancelled Event Handler","title":"[source, scala]"},{"location":"scheduler/DAGScheduler/#source-scala_30","text":"","title":"[source, scala]"},{"location":"scheduler/DAGScheduler/#docancelalljobs-unit","text":"doCancelAllJobs...FIXME doCancelAllJobs is used when DAGSchedulerEventProcessLoop is requested to handle an xref:scheduler:DAGSchedulerEventProcessLoop.adoc#AllJobsCancelled[AllJobsCancelled] event and xref:scheduler:DAGSchedulerEventProcessLoop.adoc#onError[onError]. === [[handleBeginEvent]] BeginEvent Event Handler","title":"doCancelAllJobs(): Unit"},{"location":"scheduler/DAGScheduler/#source-scala_31","text":"handleBeginEvent( task: Task[_], taskInfo: TaskInfo): Unit handleBeginEvent...FIXME handleBeginEvent is used when DAGSchedulerEventProcessLoop is requested to handle a xref:scheduler:DAGSchedulerEvent.adoc#BeginEvent[BeginEvent] event. === [[handleTaskCompletion]] CompletionEvent Event Handler","title":"[source, scala]"},{"location":"scheduler/DAGScheduler/#source-scala_32","text":"handleTaskCompletion( event: CompletionEvent): Unit handleTaskCompletion...FIXME handleTaskCompletion is used when DAGSchedulerEventProcessLoop is requested to handle a xref:scheduler:DAGSchedulerEvent.adoc#CompletionEvent[CompletionEvent] event. === [[handleExecutorAdded]] ExecutorAdded Event Handler","title":"[source, scala]"},{"location":"scheduler/DAGScheduler/#source-scala_33","text":"handleExecutorAdded( execId: String, host: String): Unit handleExecutorAdded...FIXME handleExecutorAdded is used when DAGSchedulerEventProcessLoop is requested to handle an xref:scheduler:DAGSchedulerEvent.adoc#ExecutorAdded[ExecutorAdded] event. === [[handleExecutorLost]] ExecutorLost Event Handler","title":"[source, scala]"},{"location":"scheduler/DAGScheduler/#source-scala_34","text":"handleExecutorLost( execId: String, workerLost: Boolean): Unit handleExecutorLost...FIXME handleExecutorLost is used when DAGSchedulerEventProcessLoop is requested to handle an xref:scheduler:DAGSchedulerEvent.adoc#ExecutorLost[ExecutorLost] event. === [[handleGetTaskResult]] GettingResultEvent Event Handler","title":"[source, scala]"},{"location":"scheduler/DAGScheduler/#source-scala_35","text":"handleGetTaskResult( taskInfo: TaskInfo): Unit handleGetTaskResult...FIXME handleGetTaskResult is used when DAGSchedulerEventProcessLoop is requested to handle a xref:scheduler:DAGSchedulerEvent.adoc#GettingResultEvent[GettingResultEvent] event. === [[handleJobCancellation]] JobCancelled Event Handler","title":"[source, scala]"},{"location":"scheduler/DAGScheduler/#source-scala_36","text":"handleJobCancellation( jobId: Int, reason: Option[String]): Unit handleJobCancellation...FIXME handleJobCancellation is used when DAGScheduler is requested to handle a xref:scheduler:DAGSchedulerEvent.adoc#JobCancelled[JobCancelled] event, < >, < >, < >. === [[handleJobGroupCancelled]] JobGroupCancelled Event Handler","title":"[source, scala]"},{"location":"scheduler/DAGScheduler/#source-scala_37","text":"handleJobGroupCancelled( groupId: String): Unit handleJobGroupCancelled...FIXME handleJobGroupCancelled is used when DAGScheduler is requested to handle xref:scheduler:DAGSchedulerEvent.adoc#JobGroupCancelled[JobGroupCancelled] event. === [[handleJobSubmitted]] JobSubmitted Event Handler","title":"[source, scala]"},{"location":"scheduler/DAGScheduler/#source-scala_38","text":"handleJobSubmitted( jobId: Int, finalRDD: RDD[ ], func: (TaskContext, Iterator[ ]) => _, partitions: Array[Int], callSite: CallSite, listener: JobListener, properties: Properties): Unit handleJobSubmitted xref:scheduler:DAGScheduler.adoc#createResultStage[creates a new ResultStage ] (as finalStage in the picture below) given the input finalRDD , func , partitions , jobId and callSite . . DAGScheduler.handleJobSubmitted Method image::dagscheduler-handleJobSubmitted.png[align=\"center\"] handleJobSubmitted creates an xref:scheduler:spark-scheduler-ActiveJob.adoc[ActiveJob] (with the input jobId , callSite , listener , properties , and the xref:scheduler:ResultStage.adoc[ResultStage]). handleJobSubmitted xref:scheduler:DAGScheduler.adoc#clearCacheLocs[clears the internal cache of RDD partition locations]. CAUTION: FIXME Why is this clearing here so important? You should see the following INFO messages in the logs: Got job [id] ([callSite]) with [number] output partitions Final stage: [stage] ([name]) Parents of final stage: [parents] Missing parents: [missingStages] handleJobSubmitted then registers the new job in xref:scheduler:DAGScheduler.adoc#jobIdToActiveJob[jobIdToActiveJob] and xref:scheduler:DAGScheduler.adoc#activeJobs[activeJobs] internal registries, and xref:scheduler:ResultStage.adoc#setActiveJob[with the final ResultStage ]. NOTE: ResultStage can only have one ActiveJob registered. handleJobSubmitted xref:scheduler:DAGScheduler.adoc#jobIdToStageIds[finds all the registered stages for the input jobId ] and collects xref:scheduler:Stage.adoc#latestInfo[their latest StageInfo ]. In the end, handleJobSubmitted posts xref:ROOT:SparkListener.adoc#SparkListenerJobStart[SparkListenerJobStart] message to xref:scheduler:LiveListenerBus.adoc[] and xref:scheduler:DAGScheduler.adoc#submitStage[submits the stage]. handleJobSubmitted is used when DAGSchedulerEventProcessLoop is requested to handle a xref:scheduler:DAGSchedulerEvent.adoc#JobSubmitted[JobSubmitted] event. === [[handleMapStageSubmitted]] MapStageSubmitted Event Handler","title":"[source, scala]"},{"location":"scheduler/DAGScheduler/#source-scala_39","text":"handleMapStageSubmitted( jobId: Int, dependency: ShuffleDependency[_, _, _], callSite: CallSite, listener: JobListener, properties: Properties): Unit handleMapStageSubmitted...FIXME handleMapStageSubmitted is used when DAGSchedulerEventProcessLoop is requested to handle a xref:scheduler:DAGSchedulerEvent.adoc#MapStageSubmitted[MapStageSubmitted] event. === [[resubmitFailedStages]] ResubmitFailedStages Event Handler","title":"[source, scala]"},{"location":"scheduler/DAGScheduler/#source-scala_40","text":"","title":"[source, scala]"},{"location":"scheduler/DAGScheduler/#resubmitfailedstages-unit","text":"resubmitFailedStages...FIXME resubmitFailedStages is used when DAGSchedulerEventProcessLoop is requested to handle a xref:scheduler:DAGSchedulerEvent.adoc#ResubmitFailedStages[ResubmitFailedStages] event. === [[handleSpeculativeTaskSubmitted]] SpeculativeTaskSubmitted Event Handler","title":"resubmitFailedStages(): Unit"},{"location":"scheduler/DAGScheduler/#source-scala_41","text":"","title":"[source, scala]"},{"location":"scheduler/DAGScheduler/#handlespeculativetasksubmitted-unit","text":"handleSpeculativeTaskSubmitted...FIXME handleSpeculativeTaskSubmitted is used when DAGSchedulerEventProcessLoop is requested to handle a xref:scheduler:DAGSchedulerEvent.adoc#SpeculativeTaskSubmitted[SpeculativeTaskSubmitted] event. === [[handleStageCancellation]] StageCancelled Event Handler","title":"handleSpeculativeTaskSubmitted(): Unit"},{"location":"scheduler/DAGScheduler/#source-scala_42","text":"","title":"[source, scala]"},{"location":"scheduler/DAGScheduler/#handlestagecancellation-unit","text":"handleStageCancellation...FIXME handleStageCancellation is used when DAGSchedulerEventProcessLoop is requested to handle a xref:scheduler:DAGSchedulerEvent.adoc#StageCancelled[StageCancelled] event. === [[handleTaskSetFailed]] TaskSetFailed Event Handler","title":"handleStageCancellation(): Unit"},{"location":"scheduler/DAGScheduler/#source-scala_43","text":"","title":"[source, scala]"},{"location":"scheduler/DAGScheduler/#handletasksetfailed-unit","text":"handleTaskSetFailed...FIXME handleTaskSetFailed is used when DAGSchedulerEventProcessLoop is requested to handle a xref:scheduler:DAGSchedulerEvent.adoc#TaskSetFailed[TaskSetFailed] event. === [[handleWorkerRemoved]] WorkerRemoved Event Handler","title":"handleTaskSetFailed(): Unit"},{"location":"scheduler/DAGScheduler/#source-scala_44","text":"handleWorkerRemoved( workerId: String, host: String, message: String): Unit handleWorkerRemoved...FIXME handleWorkerRemoved is used when DAGSchedulerEventProcessLoop is requested to handle a xref:scheduler:DAGSchedulerEvent.adoc#WorkerRemoved[WorkerRemoved] event. == [[logging]] Logging Enable ALL logging level for org.apache.spark.scheduler.DAGScheduler logger to see what happens inside. Add the following line to conf/log4j.properties :","title":"[source, scala]"},{"location":"scheduler/DAGScheduler/#source","text":"","title":"[source]"},{"location":"scheduler/DAGScheduler/#log4jloggerorgapachesparkschedulerdagschedulerall","text":"Refer to xref:ROOT:spark-logging.adoc[Logging]. == [[internal-properties]] Internal Properties [cols=\"30m,70\",options=\"header\",width=\"100%\"] |=== | Name | Description | failedEpoch | [[failedEpoch]] The lookup table of lost executors and the epoch of the event. | failedStages | [[failedStages]] Stages that failed due to fetch failures (when a xref:scheduler:DAGSchedulerEventProcessLoop.adoc#handleTaskCompletion-FetchFailed[task fails with FetchFailed exception]). | jobIdToActiveJob | [[jobIdToActiveJob]] The lookup table of ActiveJob s per job id. | jobIdToStageIds | [[jobIdToStageIds]] The lookup table of all stages per ActiveJob id | metricsSource | [[metricsSource]] xref:metrics:spark-scheduler-DAGSchedulerSource.adoc[DAGSchedulerSource] | nextJobId | [[nextJobId]] The next job id counting from 0 . Used when DAGScheduler < > and < >, and < >. | nextStageId | [[nextStageId]] The next stage id counting from 0 . Used when DAGScheduler creates a < > and a < >. It is the key in < >. | runningStages | [[runningStages]] The set of stages that are currently \"running\". A stage is added when < > gets executed (without first checking if the stage has not already been added). | shuffleIdToMapStage | [[shuffleIdToMapStage]] The lookup table of xref:scheduler:ShuffleMapStage.adoc[ShuffleMapStage]s per xref:rdd:ShuffleDependency.adoc[ShuffleDependency]. | stageIdToStage | [[stageIdToStage]] The lookup table for stages per their ids. Used when DAGScheduler < >, < >, < >, is informed that xref:scheduler:DAGSchedulerEventProcessLoop.adoc#handleBeginEvent[a task is started], xref:scheduler:DAGSchedulerEventProcessLoop.adoc#handleTaskSetFailed[a taskset has failed], xref:scheduler:DAGSchedulerEventProcessLoop.adoc#handleJobSubmitted[a job is submitted (to compute a ResultStage )], xref:scheduler:DAGSchedulerEventProcessLoop.adoc#handleMapStageSubmitted[a map stage was submitted], xref:scheduler:DAGSchedulerEventProcessLoop.adoc#handleTaskCompletion[a task has completed] or xref:scheduler:DAGSchedulerEventProcessLoop.adoc#handleStageCancellation[a stage was cancelled], < >, < > and < >. | waitingStages | [[waitingStages]] The stages with parents to be computed |===","title":"log4j.logger.org.apache.spark.scheduler.DAGScheduler=ALL"},{"location":"storage/BlockData/","text":"= BlockData BlockData is...FIXME","title":"BlockData"},{"location":"storage/BlockDataManager/","text":"= BlockDataManager BlockDataManager is an < > of < > that manage storage for blocks of data (aka block storage management API ). BlockDataManager uses xref:storage:BlockId.adoc[] to uniquely identify blocks of data and xref:network:ManagedBuffer.adoc[] to represent them. BlockDataManager is used to initialize a xref:storage:BlockTransferService.adoc#init[]. BlockDataManager is used to create a xref:storage:NettyBlockRpcServer.adoc[]. == [[contract]] Contract === [[getBlockData]] getBlockData [source,scala] \u00b6 getBlockData( blockId: BlockId): ManagedBuffer Fetches a block data (as a xref:network:ManagedBuffer.adoc[]) for the given xref:storage:BlockId.adoc[] Used when: NettyBlockRpcServer is requested to xref:storage:NettyBlockRpcServer.adoc#OpenBlocks[handle a OpenBlocks message] ShuffleBlockFetcherIterator is requested to xref:storage:ShuffleBlockFetcherIterator.adoc#fetchLocalBlocks[fetchLocalBlocks] === [[putBlockData]] putBlockData [source, scala] \u00b6 putBlockData( blockId: BlockId, data: ManagedBuffer, level: StorageLevel, classTag: ClassTag[_]): Boolean Stores ( puts ) a block data (as a xref:network:ManagedBuffer.adoc[]) for the given xref:storage:BlockId.adoc[]. Returns true when completed successfully or false when failed. Used when NettyBlockRpcServer is requested to xref:storage:NettyBlockRpcServer.adoc#UploadBlock[handle an UploadBlock message] === [[putBlockDataAsStream]] putBlockDataAsStream [source, scala] \u00b6 putBlockDataAsStream( blockId: BlockId, level: StorageLevel, classTag: ClassTag[_]): StreamCallbackWithID Stores a block data that will be received as a stream Used when NettyBlockRpcServer is requested to xref:storage:NettyBlockRpcServer.adoc#receiveStream[receiveStream] === [[releaseLock]] releaseLock [source, scala] \u00b6 releaseLock( blockId: BlockId, taskAttemptId: Option[Long]): Unit Releases a lock Used when: TorrentBroadcast is requested to xref:core:TorrentBroadcast.adoc#releaseLock[releaseLock] BlockManager is requested to xref:storage:BlockManager.adoc#handleLocalReadFailure[handleLocalReadFailure], xref:storage:BlockManager.adoc#getLocalValues[getLocalValues], xref:storage:BlockManager.adoc#getOrElseUpdate[getOrElseUpdate], xref:storage:BlockManager.adoc#doPut[doPut], and xref:storage:BlockManager.adoc#releaseLockAndDispose[releaseLockAndDispose] == [[implementations]] Available BlockDataManagers xref:storage:BlockManager.adoc[] is the default and only known BlockDataManager in Apache Spark.","title":"BlockDataManager"},{"location":"storage/BlockDataManager/#sourcescala","text":"getBlockData( blockId: BlockId): ManagedBuffer Fetches a block data (as a xref:network:ManagedBuffer.adoc[]) for the given xref:storage:BlockId.adoc[] Used when: NettyBlockRpcServer is requested to xref:storage:NettyBlockRpcServer.adoc#OpenBlocks[handle a OpenBlocks message] ShuffleBlockFetcherIterator is requested to xref:storage:ShuffleBlockFetcherIterator.adoc#fetchLocalBlocks[fetchLocalBlocks] === [[putBlockData]] putBlockData","title":"[source,scala]"},{"location":"storage/BlockDataManager/#source-scala","text":"putBlockData( blockId: BlockId, data: ManagedBuffer, level: StorageLevel, classTag: ClassTag[_]): Boolean Stores ( puts ) a block data (as a xref:network:ManagedBuffer.adoc[]) for the given xref:storage:BlockId.adoc[]. Returns true when completed successfully or false when failed. Used when NettyBlockRpcServer is requested to xref:storage:NettyBlockRpcServer.adoc#UploadBlock[handle an UploadBlock message] === [[putBlockDataAsStream]] putBlockDataAsStream","title":"[source, scala]"},{"location":"storage/BlockDataManager/#source-scala_1","text":"putBlockDataAsStream( blockId: BlockId, level: StorageLevel, classTag: ClassTag[_]): StreamCallbackWithID Stores a block data that will be received as a stream Used when NettyBlockRpcServer is requested to xref:storage:NettyBlockRpcServer.adoc#receiveStream[receiveStream] === [[releaseLock]] releaseLock","title":"[source, scala]"},{"location":"storage/BlockDataManager/#source-scala_2","text":"releaseLock( blockId: BlockId, taskAttemptId: Option[Long]): Unit Releases a lock Used when: TorrentBroadcast is requested to xref:core:TorrentBroadcast.adoc#releaseLock[releaseLock] BlockManager is requested to xref:storage:BlockManager.adoc#handleLocalReadFailure[handleLocalReadFailure], xref:storage:BlockManager.adoc#getLocalValues[getLocalValues], xref:storage:BlockManager.adoc#getOrElseUpdate[getOrElseUpdate], xref:storage:BlockManager.adoc#doPut[doPut], and xref:storage:BlockManager.adoc#releaseLockAndDispose[releaseLockAndDispose] == [[implementations]] Available BlockDataManagers xref:storage:BlockManager.adoc[] is the default and only known BlockDataManager in Apache Spark.","title":"[source, scala]"},{"location":"storage/BlockEvictionHandler/","text":"= BlockEvictionHandler BlockEvictionHandler is an < > of < > that can < >. == [[contract]] Contract === [[dropFromMemory]] dropFromMemory Method [source,scala] \u00b6 dropFromMemory T: ClassTag : StorageLevel Used when MemoryStore is requested to xref:storage:MemoryStore.adoc#evictBlocksToFreeSpace[evictBlocksToFreeSpace]. == [[implementations]] Available BlockEvictionHandlers xref:storage:BlockManager.adoc[] is the default and only known BlockEvictionHandler in Apache Spark.","title":"BlockEvictionHandler"},{"location":"storage/BlockEvictionHandler/#sourcescala","text":"dropFromMemory T: ClassTag : StorageLevel Used when MemoryStore is requested to xref:storage:MemoryStore.adoc#evictBlocksToFreeSpace[evictBlocksToFreeSpace]. == [[implementations]] Available BlockEvictionHandlers xref:storage:BlockManager.adoc[] is the default and only known BlockEvictionHandler in Apache Spark.","title":"[source,scala]"},{"location":"storage/BlockId/","text":"= BlockId BlockId is an < > of < > based on an unique < >. BlockId is a Scala sealed abstract class and so all the possible < > are in the single Scala file alongside BlockId. == [[contract]] Contract === [[name]][[toString]] Unique Name [source, scala] \u00b6 name: String \u00b6 Used when: NettyBlockTransferService is requested to xref:storage:NettyBlockTransferService.adoc#uploadBlock[upload a block] AppStatusListener is requested to xref:core:AppStatusListener.adoc#updateRDDBlock[updateRDDBlock], xref:core:AppStatusListener.adoc#updateStreamBlock[updateStreamBlock] BlockManager is requested to xref:storage:BlockManager.adoc#putBlockDataAsStream[putBlockDataAsStream] UpdateBlockInfo is requested to xref:storage:BlockManagerMasterEndpoint.adoc#UpdateBlockInfo[writeExternal] DiskBlockManager is requested to xref:storage:DiskBlockManager.adoc#getFile[getFile] and xref:storage:DiskBlockManager.adoc#containsBlock[containsBlock] DiskStore is requested to xref:storage:DiskStore.adoc#getBytes[getBytes] == [[implementations]] Available BlockIds === [[BroadcastBlockId]] BroadcastBlockId BlockId for xref:ROOT:Broadcast.adoc[]s with broadcastId identifier and optional field name (default: empty ) Uses broadcast_ prefix for the < > Used when: TorrentBroadcast is xref:core:TorrentBroadcast.adoc#broadcastId[created], requested to xref:core:TorrentBroadcast.adoc#writeBlocks[store a broadcast and the blocks in a local BlockManager], and < > BlockManager is requested to xref:storage:BlockManager.adoc#removeBroadcast[remove all the blocks of a broadcast variable] AppStatusListener is requested to xref:core:AppStatusListener.adoc#updateBroadcastBlock[updateBroadcastBlock] (when xref:core:AppStatusListener.adoc#onBlockUpdated[onBlockUpdated] for a BroadcastBlockId ) xref:serializer:SerializerManager.adoc#shouldCompress[Compressed] when xref:core:BroadcastManager.adoc#spark.broadcast.compress[spark.broadcast.compress] configuration property is enabled === [[RDDBlockId]] RDDBlockId BlockId for RDD partitions with rddId and splitIndex identifiers Uses rdd_ prefix for the < > Used when: StorageStatus is requested to < >, < >, < > LocalCheckpointRDD is requested to compute a partition LocalRDDCheckpointData is requested to xref:rdd:LocalRDDCheckpointData.adoc#doCheckpoint[doCheckpoint] RDD is requested to xref:rdd:RDD.adoc#getOrCompute[getOrCompute] DAGScheduler is requested for the xref:scheduler:DAGScheduler.adoc#getCacheLocs[BlockManagers (executors) for cached RDD partitions] AppStatusListener is requested to xref:core:AppStatusListener.adoc#updateRDDBlock[updateRDDBlock] (when xref:core:AppStatusListener.adoc#onBlockUpdated[onBlockUpdated] for a RDDBlockId ) xref:serializer:SerializerManager.adoc#shouldCompress[Compressed] when xref:ROOT:configuration-properties.adoc#spark.rdd.compress[spark.rdd.compress] configuration property is enabled (default: false ) === [[ShuffleBlockId]] ShuffleBlockId BlockId for FIXME with shuffleId , mapId , and reduceId identifiers Uses shuffle_ prefix for the < > Used when: ShuffleBlockFetcherIterator is requested to xref:storage:ShuffleBlockFetcherIterator.adoc#throwFetchFailedException[throwFetchFailedException] MapOutputTracker object is requested to xref:scheduler:MapOutputTracker.adoc#convertMapStatuses[convertMapStatuses] SortShuffleWriter is requested to xref:shuffle:SortShuffleWriter.adoc#write[write partition records] ShuffleBlockResolver is requested for a xref:shuffle:ShuffleBlockResolver.adoc#getBlockData[ManagedBuffer to read shuffle block data file] xref:serializer:SerializerManager.adoc#shouldCompress[Compressed] when xref:ROOT:configuration-properties.adoc#spark.shuffle.compress[spark.shuffle.compress] configuration property is enabled (default: true ) === [[ShuffleDataBlockId]] ShuffleDataBlockId === [[ShuffleIndexBlockId]] ShuffleIndexBlockId === [[StreamBlockId]] StreamBlockId === [[TaskResultBlockId]] TaskResultBlockId === [[TempLocalBlockId]] TempLocalBlockId === [[TempShuffleBlockId]] TempShuffleBlockId == [[apply]] apply Factory Method [source, scala] \u00b6 apply( name: String): BlockId apply creates one of the available < > by the given name (that uses a prefix to differentiate between different BlockIds). apply is used when: NettyBlockRpcServer is requested to xref:storage:NettyBlockRpcServer.adoc#receive[handle an RPC message] and xref:storage:NettyBlockRpcServer.adoc#receiveStream[receiveStream] UpdateBlockInfo is requested to deserialize (readExternal) DiskBlockManager is requested for xref:storage:DiskBlockManager.adoc#getAllBlocks[all the blocks (from files stored on disk)] ShuffleBlockFetcherIterator is requested to xref:storage:ShuffleBlockFetcherIterator.adoc#sendRequest[sendRequest] JsonProtocol utility is used to xref:spark-history-server:JsonProtocol.adoc#accumValueFromJson[accumValueFromJson], xref:spark-history-server:JsonProtocol.adoc#taskMetricsFromJson[taskMetricsFromJson] and xref:spark-history-server:JsonProtocol.adoc#blockUpdatedInfoFromJson[blockUpdatedInfoFromJson]","title":"BlockId"},{"location":"storage/BlockId/#source-scala","text":"","title":"[source, scala]"},{"location":"storage/BlockId/#name-string","text":"Used when: NettyBlockTransferService is requested to xref:storage:NettyBlockTransferService.adoc#uploadBlock[upload a block] AppStatusListener is requested to xref:core:AppStatusListener.adoc#updateRDDBlock[updateRDDBlock], xref:core:AppStatusListener.adoc#updateStreamBlock[updateStreamBlock] BlockManager is requested to xref:storage:BlockManager.adoc#putBlockDataAsStream[putBlockDataAsStream] UpdateBlockInfo is requested to xref:storage:BlockManagerMasterEndpoint.adoc#UpdateBlockInfo[writeExternal] DiskBlockManager is requested to xref:storage:DiskBlockManager.adoc#getFile[getFile] and xref:storage:DiskBlockManager.adoc#containsBlock[containsBlock] DiskStore is requested to xref:storage:DiskStore.adoc#getBytes[getBytes] == [[implementations]] Available BlockIds === [[BroadcastBlockId]] BroadcastBlockId BlockId for xref:ROOT:Broadcast.adoc[]s with broadcastId identifier and optional field name (default: empty ) Uses broadcast_ prefix for the < > Used when: TorrentBroadcast is xref:core:TorrentBroadcast.adoc#broadcastId[created], requested to xref:core:TorrentBroadcast.adoc#writeBlocks[store a broadcast and the blocks in a local BlockManager], and < > BlockManager is requested to xref:storage:BlockManager.adoc#removeBroadcast[remove all the blocks of a broadcast variable] AppStatusListener is requested to xref:core:AppStatusListener.adoc#updateBroadcastBlock[updateBroadcastBlock] (when xref:core:AppStatusListener.adoc#onBlockUpdated[onBlockUpdated] for a BroadcastBlockId ) xref:serializer:SerializerManager.adoc#shouldCompress[Compressed] when xref:core:BroadcastManager.adoc#spark.broadcast.compress[spark.broadcast.compress] configuration property is enabled === [[RDDBlockId]] RDDBlockId BlockId for RDD partitions with rddId and splitIndex identifiers Uses rdd_ prefix for the < > Used when: StorageStatus is requested to < >, < >, < > LocalCheckpointRDD is requested to compute a partition LocalRDDCheckpointData is requested to xref:rdd:LocalRDDCheckpointData.adoc#doCheckpoint[doCheckpoint] RDD is requested to xref:rdd:RDD.adoc#getOrCompute[getOrCompute] DAGScheduler is requested for the xref:scheduler:DAGScheduler.adoc#getCacheLocs[BlockManagers (executors) for cached RDD partitions] AppStatusListener is requested to xref:core:AppStatusListener.adoc#updateRDDBlock[updateRDDBlock] (when xref:core:AppStatusListener.adoc#onBlockUpdated[onBlockUpdated] for a RDDBlockId ) xref:serializer:SerializerManager.adoc#shouldCompress[Compressed] when xref:ROOT:configuration-properties.adoc#spark.rdd.compress[spark.rdd.compress] configuration property is enabled (default: false ) === [[ShuffleBlockId]] ShuffleBlockId BlockId for FIXME with shuffleId , mapId , and reduceId identifiers Uses shuffle_ prefix for the < > Used when: ShuffleBlockFetcherIterator is requested to xref:storage:ShuffleBlockFetcherIterator.adoc#throwFetchFailedException[throwFetchFailedException] MapOutputTracker object is requested to xref:scheduler:MapOutputTracker.adoc#convertMapStatuses[convertMapStatuses] SortShuffleWriter is requested to xref:shuffle:SortShuffleWriter.adoc#write[write partition records] ShuffleBlockResolver is requested for a xref:shuffle:ShuffleBlockResolver.adoc#getBlockData[ManagedBuffer to read shuffle block data file] xref:serializer:SerializerManager.adoc#shouldCompress[Compressed] when xref:ROOT:configuration-properties.adoc#spark.shuffle.compress[spark.shuffle.compress] configuration property is enabled (default: true ) === [[ShuffleDataBlockId]] ShuffleDataBlockId === [[ShuffleIndexBlockId]] ShuffleIndexBlockId === [[StreamBlockId]] StreamBlockId === [[TaskResultBlockId]] TaskResultBlockId === [[TempLocalBlockId]] TempLocalBlockId === [[TempShuffleBlockId]] TempShuffleBlockId == [[apply]] apply Factory Method","title":"name: String"},{"location":"storage/BlockId/#source-scala_1","text":"apply( name: String): BlockId apply creates one of the available < > by the given name (that uses a prefix to differentiate between different BlockIds). apply is used when: NettyBlockRpcServer is requested to xref:storage:NettyBlockRpcServer.adoc#receive[handle an RPC message] and xref:storage:NettyBlockRpcServer.adoc#receiveStream[receiveStream] UpdateBlockInfo is requested to deserialize (readExternal) DiskBlockManager is requested for xref:storage:DiskBlockManager.adoc#getAllBlocks[all the blocks (from files stored on disk)] ShuffleBlockFetcherIterator is requested to xref:storage:ShuffleBlockFetcherIterator.adoc#sendRequest[sendRequest] JsonProtocol utility is used to xref:spark-history-server:JsonProtocol.adoc#accumValueFromJson[accumValueFromJson], xref:spark-history-server:JsonProtocol.adoc#taskMetricsFromJson[taskMetricsFromJson] and xref:spark-history-server:JsonProtocol.adoc#blockUpdatedInfoFromJson[blockUpdatedInfoFromJson]","title":"[source, scala]"},{"location":"storage/BlockInfo/","text":"= BlockInfo BlockInfo is a metadata of xref:storage:BlockId.adoc[memory block] -- the memory block's < >, the < > and the < >. BlockInfo has a xref:storage:StorageLevel.adoc[], ClassTag and tellMaster flag. == [[size]] Size size attribute is the size of the memory block. It starts with 0 . It represents the number of bytes that xref:storage:BlockManager.adoc#putBytes[ BlockManager saved] or xref:storage:BlockManager.adoc#doPutIterator[BlockManager.doPutIterator]. == [[readerCount]] Reader Count -- readerCount Counter readerCount counter is the number of readers of the memory block, i.e. the number of read locks. It starts with 0 . readerCount is incremented when a xref:storage:BlockInfoManager.adoc#lockForReading[read lock is acquired] and decreases when the following happens: The xref:storage:BlockManager.adoc#unlock[memory block is unlocked] xref:storage:BlockInfoManager.adoc#releaseAllLocksForTask[All locks for the memory block obtained by a task are released]. xref:storage:BlockInfoManager.adoc#removeBlock[memory block is removed] xref:storage:BlockInfoManager.adoc#clear[Clearing the current state of BlockInfoManager ]. == [[writerTask]] Writer Task -- writerTask Attribute writerTask attribute is the task that owns the write lock for the memory block. A writer task can be one of the three possible identifiers: [[NO_WRITER]] NO_WRITER (i.e. -1 ) to denote no writers and hence no write lock in use. [[NON_TASK_WRITER]] NON_TASK_WRITER (i.e. -1024 ) for non-task threads, e.g. by a driver thread or by unit test code. the task attempt id of the task which currently holds the write lock for this block. The writer task is assigned in the following scenarios: A xref:storage:BlockInfoManager.adoc#lockForWriting[write lock is requested for a memory block (with no writer and readers)] A xref:storage:BlockInfoManager.adoc#unlock[memory block is unlocked] xref:storage:BlockInfoManager.adoc#releaseAllLocksForTask[All locks obtained by a task are released] A xref:storage:BlockInfoManager.adoc#removeBlock[memory block is removed] xref:storage:BlockInfoManager.adoc#clear[Clearing the current state of BlockInfoManager ].","title":"BlockInfo"},{"location":"storage/BlockInfoManager/","text":"= BlockInfoManager BlockInfoManager is used by xref:storage:BlockManager.adoc[] (and xref:storage:MemoryStore.adoc#blockInfoManager[MemoryStore]) to manage < > and control concurrent access by locks for < > and < >. NOTE: Locks are the mechanism to control concurrent access to data and prevent destructive interaction between operations that use the same resource. BlockInfoManager is used to create a xref:storage:MemoryStore.adoc#blockInfoManager[MemoryStore] and a BlockManagerManagedBuffer. == [[creating-instance]] Creating Instance BlockInfoManager takes no parameters to be created. BlockInfoManager is created for xref:storage:BlockManager.adoc#blockInfoManager[BlockManager]. .BlockInfoManager and BlockManager image::BlockInfoManager-BlockManager.png[align=\"center\"] == [[infos]] Block Metadata [source,scala] \u00b6 infos: Map[BlockId, BlockInfo] \u00b6 BlockInfoManager uses a registry of xref:storage:BlockInfo.adoc[block metadata]s per xref:storage:BlockId.adoc[block]. == [[readLocksByTask]][[writeLocksByTask]] Read and Write Locks By Task Tracks tasks (by TaskAttemptId) and the blocks they locked for reading (as xref:storage:BlockId.adoc[]). Tracks tasks (by TaskAttemptId ) and the blocks they locked for writing (as xref:storage:BlockId.adoc[]). == [[registerTask]] Registering Task (Start of Execution) [source,scala] \u00b6 registerTask( taskAttemptId: Long): Unit registerTask merely adds a new \"empty\" entry for the given task (by the task attempt ID) to < > internal registry. registerTask is used when: BlockInfoManager is < > BlockManager is requested to xref:storage:BlockManager.adoc#registerTask[registerTask] == [[downgradeLock]] Downgrading Exclusive Write Lock For Block to Shared Read Lock [source, scala] \u00b6 downgradeLock( blockId: BlockId): Unit downgradeLock prints out the following TRACE message to the logs: [source,plaintext] \u00b6 Task [currentTaskAttemptId] downgrading write lock for [blockId] \u00b6 downgradeLock...FIXME downgradeLock is used when BlockManager is requested to xref:storage:BlockManager.adoc#doPut[doPut] and xref:storage:BlockManager.adoc#downgradeLock[downgradeLock]. == [[lockForReading]] Obtaining Read Lock For Block [source, scala] \u00b6 lockForReading( blockId: BlockId, blocking: Boolean = true): Option[BlockInfo] lockForReading locks blockId memory block for reading when the block was registered earlier and no writer tasks use it. When executed, lockForReading prints out the following TRACE message to the logs: [source,plaintext] \u00b6 Task [currentTaskAttemptId] trying to acquire read lock for [blockId] \u00b6 lockForReading looks up the metadata of the blockId block (in < > registry). If no metadata could be found, it returns None which means that the block does not exist or was removed (and anybody could acquire a write lock). Otherwise, when the metadata was found, i.e. registered, it checks so-called writerTask . Only when the xref:storage:BlockInfo.adoc#NO_WRITER[block has no writer tasks], a read lock can be acquired. If so, the readerCount of the block metadata is incremented and the block is recorded (in the internal < > registry). You should see the following TRACE message in the logs: [source,plaintext] \u00b6 Task [taskAttemptId] acquired read lock for [blockId] \u00b6 The BlockInfo for the blockId block is returned. NOTE: -1024 is a special taskAttemptId , aka xref:storage:BlockInfo.adoc#NON_TASK_WRITER[NON_TASK_WRITER], used to mark a non-task thread, e.g. by a driver thread or by unit test code. For blocks with xref:storage:BlockInfo.adoc#NO_WRITER[ writerTask other than NO_WRITER ], when blocking is enabled, lockForReading waits (until another thread invokes the Object.notify method or the Object.notifyAll methods for this object). With blocking enabled, it will repeat the waiting-for-read-lock sequence until either None or the lock is obtained. When blocking is disabled and the lock could not be obtained, None is returned immediately. NOTE: lockForReading is a synchronized method, i.e. no two objects can use this and other instance methods. lockForReading is used when: BlockInfoManager is requested to < > and < > BlockManager is requested to xref:storage:BlockManager.adoc#getLocalValues[getLocalValues], xref:storage:BlockManager.adoc#getLocalBytes[getLocalBytes] and xref:storage:BlockManager.adoc#replicateBlock[replicateBlock] BlockManagerManagedBuffer is requested to retain == [[lockForWriting]] Obtaining Write Lock for Block [source, scala] \u00b6 lockForWriting( blockId: BlockId, blocking: Boolean = true): Option[BlockInfo] lockForWriting prints out the following TRACE message to the logs: [source,plaintext] \u00b6 Task [currentTaskAttemptId] trying to acquire write lock for [blockId] \u00b6 lockForWriting looks up blockId in the internal < > registry. When no xref:storage:BlockInfo.adoc[] could be found, None is returned. Otherwise, xref:storage:BlockInfo.adoc#NO_WRITER[ blockId block is checked for writerTask to be BlockInfo.NO_WRITER ] with no readers (i.e. readerCount is 0 ) and only then the lock is returned. When the write lock can be returned, BlockInfo.writerTask is set to currentTaskAttemptId and a new binding is added to the internal < > registry. You should see the following TRACE message in the logs: [source,plaintext] \u00b6 Task [currentTaskAttemptId] acquired write lock for [blockId] \u00b6 If, for some reason, xref:storage:BlockInfo.adoc#writerTask[ blockId has a writer] or the number of readers is positive (i.e. BlockInfo.readerCount is greater than 0 ), the method will wait (based on the input blocking flag) and attempt the write lock acquisition process until it finishes with a write lock. NOTE: (deadlock possible) The method is synchronized and can block, i.e. wait that causes the current thread to wait until another thread invokes Object.notify or Object.notifyAll methods for this object. lockForWriting returns None for no blockId in the internal < > registry or when blocking flag is disabled and the write lock could not be acquired. lockForWriting is used when: BlockInfoManager is requested to < > BlockManager is requested to xref:storage:BlockManager.adoc#removeBlock[removeBlock] MemoryStore is requested to xref:storage:MemoryStore.adoc#evictBlocksToFreeSpace[evictBlocksToFreeSpace] == [[lockNewBlockForWriting]] Obtaining Write Lock for New Block [source, scala] \u00b6 lockNewBlockForWriting( blockId: BlockId, newBlockInfo: BlockInfo): Boolean lockNewBlockForWriting obtains a write lock for blockId but only when the method could register the block. NOTE: lockNewBlockForWriting is similar to < > method but for brand new blocks. When executed, lockNewBlockForWriting prints out the following TRACE message to the logs: [source,plaintext] \u00b6 Task [currentTaskAttemptId] trying to put [blockId] \u00b6 If < >, it finishes returning false . Otherwise, when the block does not exist, newBlockInfo is recorded in the internal < > registry and < >. It then returns true . NOTE: lockNewBlockForWriting executes itself in synchronized block so once the BlockInfoManager is locked the other internal registries should be available only for the currently-executing thread. lockNewBlockForWriting is used when BlockManager is requested to xref:storage:BlockManager.adoc#doPut[doPut]. == [[unlock]] Releasing Lock on Block [source, scala] \u00b6 unlock( blockId: BlockId): Unit unlock prints out the following TRACE message to the logs: [source,plaintext] \u00b6 Task [currentTaskAttemptId] releasing lock for [blockId] \u00b6 unlock gets the metadata for blockId . It may throw a IllegalStateException if the block was not found. If the xref:storage:BlockInfo.adoc#writerTask[writer task] for the block is not xref:storage:BlockInfo.adoc#NO_WRITER[NO_WRITER], it becomes so and the blockId block is removed from the internal < > registry for the < >. Otherwise, if the writer task is indeed NO_WRITER , it is assumed that the xref:storage:BlockInfo.adoc#readerCount[ blockId block is locked for reading]. The readerCount counter is decremented for the blockId block and the read lock removed from the internal < > registry for the < >. In the end, unlock wakes up all the threads waiting for the BlockInfoManager (using Java's link:++ https://docs.oracle.com/javase/8/docs/api/java/lang/Object.html#notifyAll--++[Object.notifyAll ]). CAUTION: FIXME What threads could wait? unlock is used when: BlockInfoManager is requested to < > BlockManager is requested to xref:storage:BlockManager.adoc#releaseLock[releaseLock] and xref:storage:BlockManager.adoc#doPut[doPut] BlockManagerManagedBuffer is requested to release MemoryStore is requested to xref:storage:MemoryStore.adoc#evictBlocksToFreeSpace[evictBlocksToFreeSpace] == [[releaseAllLocksForTask]] Releasing All Locks Obtained by Task [source,scala] \u00b6 releaseAllLocksForTask( taskAttemptId: TaskAttemptId): Seq[BlockId] releaseAllLocksForTask...FIXME releaseAllLocksForTask is used when BlockManager is requested to xref:storage:BlockManager.adoc#releaseAllLocksForTask[releaseAllLocksForTask]. == [[removeBlock]] Removing Block [source,scala] \u00b6 removeBlock( blockId: BlockId): Unit removeBlock...FIXME removeBlock is used when: BlockManager is requested to xref:storage:BlockManager.adoc#removeBlockInternal[removeBlockInternal] MemoryStore is requested to xref:storage:MemoryStore.adoc#evictBlocksToFreeSpace[evictBlocksToFreeSpace] == [[assertBlockIsLockedForWriting]] assertBlockIsLockedForWriting Method [source,scala] \u00b6 assertBlockIsLockedForWriting( blockId: BlockId): BlockInfo assertBlockIsLockedForWriting...FIXME assertBlockIsLockedForWriting is used when BlockManager is requested to xref:storage:BlockManager.adoc#dropFromMemory[dropFromMemory] and xref:storage:BlockManager.adoc#removeBlockInternal[removeBlockInternal]. == [[currentTaskAttemptId]] currentTaskAttemptId Internal Method [source, scala] \u00b6 currentTaskAttemptId: Long /* TaskAttemptId */ \u00b6 currentTaskAttemptId...FIXME currentTaskAttemptId is used when...FIXME == [[clear]] Deleting All State [source,scala] \u00b6 clear(): Unit \u00b6 clear...FIXME clear is used when BlockManager is requested to < >. == [[logging]] Logging Enable ALL logging level for org.apache.spark.storage.BlockInfoManager logger to see what happens inside. Add the following line to conf/log4j.properties : [source] \u00b6 log4j.logger.org.apache.spark.storage.BlockInfoManager=ALL \u00b6 Refer to xref:ROOT:spark-logging.adoc[Logging].","title":"BlockInfoManager"},{"location":"storage/BlockInfoManager/#sourcescala","text":"","title":"[source,scala]"},{"location":"storage/BlockInfoManager/#infos-mapblockid-blockinfo","text":"BlockInfoManager uses a registry of xref:storage:BlockInfo.adoc[block metadata]s per xref:storage:BlockId.adoc[block]. == [[readLocksByTask]][[writeLocksByTask]] Read and Write Locks By Task Tracks tasks (by TaskAttemptId) and the blocks they locked for reading (as xref:storage:BlockId.adoc[]). Tracks tasks (by TaskAttemptId ) and the blocks they locked for writing (as xref:storage:BlockId.adoc[]). == [[registerTask]] Registering Task (Start of Execution)","title":"infos: Map[BlockId, BlockInfo]"},{"location":"storage/BlockInfoManager/#sourcescala_1","text":"registerTask( taskAttemptId: Long): Unit registerTask merely adds a new \"empty\" entry for the given task (by the task attempt ID) to < > internal registry. registerTask is used when: BlockInfoManager is < > BlockManager is requested to xref:storage:BlockManager.adoc#registerTask[registerTask] == [[downgradeLock]] Downgrading Exclusive Write Lock For Block to Shared Read Lock","title":"[source,scala]"},{"location":"storage/BlockInfoManager/#source-scala","text":"downgradeLock( blockId: BlockId): Unit downgradeLock prints out the following TRACE message to the logs:","title":"[source, scala]"},{"location":"storage/BlockInfoManager/#sourceplaintext","text":"","title":"[source,plaintext]"},{"location":"storage/BlockInfoManager/#task-currenttaskattemptid-downgrading-write-lock-for-blockid","text":"downgradeLock...FIXME downgradeLock is used when BlockManager is requested to xref:storage:BlockManager.adoc#doPut[doPut] and xref:storage:BlockManager.adoc#downgradeLock[downgradeLock]. == [[lockForReading]] Obtaining Read Lock For Block","title":"Task [currentTaskAttemptId] downgrading write lock for [blockId]"},{"location":"storage/BlockInfoManager/#source-scala_1","text":"lockForReading( blockId: BlockId, blocking: Boolean = true): Option[BlockInfo] lockForReading locks blockId memory block for reading when the block was registered earlier and no writer tasks use it. When executed, lockForReading prints out the following TRACE message to the logs:","title":"[source, scala]"},{"location":"storage/BlockInfoManager/#sourceplaintext_1","text":"","title":"[source,plaintext]"},{"location":"storage/BlockInfoManager/#task-currenttaskattemptid-trying-to-acquire-read-lock-for-blockid","text":"lockForReading looks up the metadata of the blockId block (in < > registry). If no metadata could be found, it returns None which means that the block does not exist or was removed (and anybody could acquire a write lock). Otherwise, when the metadata was found, i.e. registered, it checks so-called writerTask . Only when the xref:storage:BlockInfo.adoc#NO_WRITER[block has no writer tasks], a read lock can be acquired. If so, the readerCount of the block metadata is incremented and the block is recorded (in the internal < > registry). You should see the following TRACE message in the logs:","title":"Task [currentTaskAttemptId] trying to acquire read lock for [blockId]"},{"location":"storage/BlockInfoManager/#sourceplaintext_2","text":"","title":"[source,plaintext]"},{"location":"storage/BlockInfoManager/#task-taskattemptid-acquired-read-lock-for-blockid","text":"The BlockInfo for the blockId block is returned. NOTE: -1024 is a special taskAttemptId , aka xref:storage:BlockInfo.adoc#NON_TASK_WRITER[NON_TASK_WRITER], used to mark a non-task thread, e.g. by a driver thread or by unit test code. For blocks with xref:storage:BlockInfo.adoc#NO_WRITER[ writerTask other than NO_WRITER ], when blocking is enabled, lockForReading waits (until another thread invokes the Object.notify method or the Object.notifyAll methods for this object). With blocking enabled, it will repeat the waiting-for-read-lock sequence until either None or the lock is obtained. When blocking is disabled and the lock could not be obtained, None is returned immediately. NOTE: lockForReading is a synchronized method, i.e. no two objects can use this and other instance methods. lockForReading is used when: BlockInfoManager is requested to < > and < > BlockManager is requested to xref:storage:BlockManager.adoc#getLocalValues[getLocalValues], xref:storage:BlockManager.adoc#getLocalBytes[getLocalBytes] and xref:storage:BlockManager.adoc#replicateBlock[replicateBlock] BlockManagerManagedBuffer is requested to retain == [[lockForWriting]] Obtaining Write Lock for Block","title":"Task [taskAttemptId] acquired read lock for [blockId]"},{"location":"storage/BlockInfoManager/#source-scala_2","text":"lockForWriting( blockId: BlockId, blocking: Boolean = true): Option[BlockInfo] lockForWriting prints out the following TRACE message to the logs:","title":"[source, scala]"},{"location":"storage/BlockInfoManager/#sourceplaintext_3","text":"","title":"[source,plaintext]"},{"location":"storage/BlockInfoManager/#task-currenttaskattemptid-trying-to-acquire-write-lock-for-blockid","text":"lockForWriting looks up blockId in the internal < > registry. When no xref:storage:BlockInfo.adoc[] could be found, None is returned. Otherwise, xref:storage:BlockInfo.adoc#NO_WRITER[ blockId block is checked for writerTask to be BlockInfo.NO_WRITER ] with no readers (i.e. readerCount is 0 ) and only then the lock is returned. When the write lock can be returned, BlockInfo.writerTask is set to currentTaskAttemptId and a new binding is added to the internal < > registry. You should see the following TRACE message in the logs:","title":"Task [currentTaskAttemptId] trying to acquire write lock for [blockId]"},{"location":"storage/BlockInfoManager/#sourceplaintext_4","text":"","title":"[source,plaintext]"},{"location":"storage/BlockInfoManager/#task-currenttaskattemptid-acquired-write-lock-for-blockid","text":"If, for some reason, xref:storage:BlockInfo.adoc#writerTask[ blockId has a writer] or the number of readers is positive (i.e. BlockInfo.readerCount is greater than 0 ), the method will wait (based on the input blocking flag) and attempt the write lock acquisition process until it finishes with a write lock. NOTE: (deadlock possible) The method is synchronized and can block, i.e. wait that causes the current thread to wait until another thread invokes Object.notify or Object.notifyAll methods for this object. lockForWriting returns None for no blockId in the internal < > registry or when blocking flag is disabled and the write lock could not be acquired. lockForWriting is used when: BlockInfoManager is requested to < > BlockManager is requested to xref:storage:BlockManager.adoc#removeBlock[removeBlock] MemoryStore is requested to xref:storage:MemoryStore.adoc#evictBlocksToFreeSpace[evictBlocksToFreeSpace] == [[lockNewBlockForWriting]] Obtaining Write Lock for New Block","title":"Task [currentTaskAttemptId] acquired write lock for [blockId]"},{"location":"storage/BlockInfoManager/#source-scala_3","text":"lockNewBlockForWriting( blockId: BlockId, newBlockInfo: BlockInfo): Boolean lockNewBlockForWriting obtains a write lock for blockId but only when the method could register the block. NOTE: lockNewBlockForWriting is similar to < > method but for brand new blocks. When executed, lockNewBlockForWriting prints out the following TRACE message to the logs:","title":"[source, scala]"},{"location":"storage/BlockInfoManager/#sourceplaintext_5","text":"","title":"[source,plaintext]"},{"location":"storage/BlockInfoManager/#task-currenttaskattemptid-trying-to-put-blockid","text":"If < >, it finishes returning false . Otherwise, when the block does not exist, newBlockInfo is recorded in the internal < > registry and < >. It then returns true . NOTE: lockNewBlockForWriting executes itself in synchronized block so once the BlockInfoManager is locked the other internal registries should be available only for the currently-executing thread. lockNewBlockForWriting is used when BlockManager is requested to xref:storage:BlockManager.adoc#doPut[doPut]. == [[unlock]] Releasing Lock on Block","title":"Task [currentTaskAttemptId] trying to put [blockId]"},{"location":"storage/BlockInfoManager/#source-scala_4","text":"unlock( blockId: BlockId): Unit unlock prints out the following TRACE message to the logs:","title":"[source, scala]"},{"location":"storage/BlockInfoManager/#sourceplaintext_6","text":"","title":"[source,plaintext]"},{"location":"storage/BlockInfoManager/#task-currenttaskattemptid-releasing-lock-for-blockid","text":"unlock gets the metadata for blockId . It may throw a IllegalStateException if the block was not found. If the xref:storage:BlockInfo.adoc#writerTask[writer task] for the block is not xref:storage:BlockInfo.adoc#NO_WRITER[NO_WRITER], it becomes so and the blockId block is removed from the internal < > registry for the < >. Otherwise, if the writer task is indeed NO_WRITER , it is assumed that the xref:storage:BlockInfo.adoc#readerCount[ blockId block is locked for reading]. The readerCount counter is decremented for the blockId block and the read lock removed from the internal < > registry for the < >. In the end, unlock wakes up all the threads waiting for the BlockInfoManager (using Java's link:++ https://docs.oracle.com/javase/8/docs/api/java/lang/Object.html#notifyAll--++[Object.notifyAll ]). CAUTION: FIXME What threads could wait? unlock is used when: BlockInfoManager is requested to < > BlockManager is requested to xref:storage:BlockManager.adoc#releaseLock[releaseLock] and xref:storage:BlockManager.adoc#doPut[doPut] BlockManagerManagedBuffer is requested to release MemoryStore is requested to xref:storage:MemoryStore.adoc#evictBlocksToFreeSpace[evictBlocksToFreeSpace] == [[releaseAllLocksForTask]] Releasing All Locks Obtained by Task","title":"Task [currentTaskAttemptId] releasing lock for [blockId]"},{"location":"storage/BlockInfoManager/#sourcescala_2","text":"releaseAllLocksForTask( taskAttemptId: TaskAttemptId): Seq[BlockId] releaseAllLocksForTask...FIXME releaseAllLocksForTask is used when BlockManager is requested to xref:storage:BlockManager.adoc#releaseAllLocksForTask[releaseAllLocksForTask]. == [[removeBlock]] Removing Block","title":"[source,scala]"},{"location":"storage/BlockInfoManager/#sourcescala_3","text":"removeBlock( blockId: BlockId): Unit removeBlock...FIXME removeBlock is used when: BlockManager is requested to xref:storage:BlockManager.adoc#removeBlockInternal[removeBlockInternal] MemoryStore is requested to xref:storage:MemoryStore.adoc#evictBlocksToFreeSpace[evictBlocksToFreeSpace] == [[assertBlockIsLockedForWriting]] assertBlockIsLockedForWriting Method","title":"[source,scala]"},{"location":"storage/BlockInfoManager/#sourcescala_4","text":"assertBlockIsLockedForWriting( blockId: BlockId): BlockInfo assertBlockIsLockedForWriting...FIXME assertBlockIsLockedForWriting is used when BlockManager is requested to xref:storage:BlockManager.adoc#dropFromMemory[dropFromMemory] and xref:storage:BlockManager.adoc#removeBlockInternal[removeBlockInternal]. == [[currentTaskAttemptId]] currentTaskAttemptId Internal Method","title":"[source,scala]"},{"location":"storage/BlockInfoManager/#source-scala_5","text":"","title":"[source, scala]"},{"location":"storage/BlockInfoManager/#currenttaskattemptid-long-taskattemptid","text":"currentTaskAttemptId...FIXME currentTaskAttemptId is used when...FIXME == [[clear]] Deleting All State","title":"currentTaskAttemptId: Long /* TaskAttemptId */"},{"location":"storage/BlockInfoManager/#sourcescala_5","text":"","title":"[source,scala]"},{"location":"storage/BlockInfoManager/#clear-unit","text":"clear...FIXME clear is used when BlockManager is requested to < >. == [[logging]] Logging Enable ALL logging level for org.apache.spark.storage.BlockInfoManager logger to see what happens inside. Add the following line to conf/log4j.properties :","title":"clear(): Unit"},{"location":"storage/BlockInfoManager/#source","text":"","title":"[source]"},{"location":"storage/BlockInfoManager/#log4jloggerorgapachesparkstorageblockinfomanagerall","text":"Refer to xref:ROOT:spark-logging.adoc[Logging].","title":"log4j.logger.org.apache.spark.storage.BlockInfoManager=ALL"},{"location":"storage/BlockManager/","text":"BlockManager \u00b6 BlockManager manages the storage for blocks ( chunks of data ) that can be stored in < > and on < >. .BlockManager and Stores image::BlockManager.png[align=\"center\"] BlockManager runs on the xref:ROOT:spark-driver.adoc[driver] and xref:executor:Executor.adoc[executors]. BlockManager provides interface for uploading and fetching blocks both locally and remotely using various stores, i.e. < >. [[futureExecutionContext]] BlockManager uses a Scala https://www.scala-lang.org/api/current/scala/concurrent/ExecutionContextExecutorService.html[ExecutionContextExecutorService ] to execute FIXME asynchronously (on a thread pool with block-manager-future prefix and maximum of 128 threads). Cached blocks are blocks with non-zero sum of memory and disk sizes. TIP: Use xref:webui:index.adoc[Web UI], esp. xref:webui:spark-webui-storage.adoc[Storage] and xref:webui:spark-webui-executors.adoc[Executors] tabs, to monitor the memory used. TIP: Use xref spark-submit.adoc[spark-submit]'s command-line options, i.e. xref spark-submit.adoc#driver-memory[--driver-memory] for the driver and xref spark-submit.adoc#executor-memory[--executor-memory] for executors or their equivalents as Spark properties, i.e. xref spark-submit.adoc#spark.executor.memory[spark.executor.memory] and xref spark-submit.adoc#spark_driver_memory[spark.driver.memory], to control the memory for storage memory. When < >, BlockManager uses xref:storage:ExternalShuffleClient.adoc[ExternalShuffleClient] to read other executors' shuffle files. == [[creating-instance]] Creating Instance BlockManager takes the following to be created: < > < > < > [[serializerManager]] xref:serializer:SerializerManager.adoc[] [[conf]] xref:ROOT:SparkConf.adoc[] < > < > < > < > [[securityManager]] SecurityManager [[numUsableCores]] Number of CPU cores (for an xref:storage:ExternalShuffleClient.adoc[] with < >) When created, BlockManager sets < > internal flag based on xref:ROOT:configuration-properties.adoc#spark.shuffle.service.enabled[spark.shuffle.service.enabled] configuration property. BlockManager then creates an instance of xref:DiskBlockManager.adoc[DiskBlockManager] (requesting deleteFilesOnStop when an external shuffle service is not in use). BlockManager creates block-manager-future daemon cached thread pool with 128 threads maximum (as futureExecutionContext ). BlockManager calculates the maximum memory to use (as maxMemory ) by requesting the maximum xref:memory:MemoryManager.adoc#maxOnHeapStorageMemory[on-heap] and xref:memory:MemoryManager.adoc#maxOffHeapStorageMemory[off-heap] storage memory from the assigned MemoryManager . BlockManager calculates the port used by the external shuffle service (as externalShuffleServicePort ). NOTE: It is computed specially in Spark on YARN. BlockManager creates a client to read other executors' shuffle files (as shuffleClient ). If the external shuffle service is used an xref:storage:ExternalShuffleClient.adoc[ExternalShuffleClient] is created or the input xref:storage:BlockTransferService.adoc[BlockTransferService] is used. BlockManager sets the xref:ROOT:configuration-properties.adoc#spark.block.failures.beforeLocationRefresh[maximum number of failures] before this block manager refreshes the block locations from the driver (as maxFailuresBeforeLocationRefresh ). BlockManager registers a xref:storage:BlockManagerSlaveEndpoint.adoc[] with the input xref:ROOT:index.adoc[RpcEnv], itself, and xref:scheduler:MapOutputTracker.adoc[MapOutputTracker] (as slaveEndpoint ). BlockManager is created when SparkEnv is xref:core:SparkEnv.adoc#create-BlockManager[created] (for the driver and executors) when a Spark application starts. .BlockManager and SparkEnv image::BlockManager-SparkEnv.png[align=\"center\"] == [[BlockEvictionHandler]] BlockEvictionHandler BlockManager is a xref:storage:BlockEvictionHandler.adoc[] that can < > (and store it on a disk when needed). == [[shuffleClient]][[externalShuffleServiceEnabled]] ShuffleClient and External Shuffle Service BlockManager manages the lifecycle of a xref:storage:ShuffleClient.adoc[]: Creates when < > xref:storage:ShuffleClient.adoc#init[Inits] (and possibly < >) when requested to < > Closes when requested to < > The ShuffleClient can be an xref:storage:ExternalShuffleClient.adoc[] or the given < > based on xref:ROOT:configuration-properties.adoc#spark.shuffle.service.enabled[spark.shuffle.service.enabled] configuration property. When enabled, BlockManager uses the xref:storage:ExternalShuffleClient.adoc[]. The ShuffleClient is available to other Spark services (using shuffleClient value) and is used when BlockStoreShuffleReader is requested to xref:shuffle:BlockStoreShuffleReader.adoc#read[read combined key-value records for a reduce task]. When requested for < >, BlockManager simply requests xref:storage:ShuffleClient.adoc#shuffleMetrics[them] from the ShuffleClient. == [[rpcEnv]] BlockManager and RpcEnv BlockManager is given a xref:rpc:RpcEnv.adoc[] when < >. The RpcEnv is used to set up a < >. == [[blockInfoManager]] BlockInfoManager BlockManager creates a xref:storage:BlockInfoManager.adoc[] when < >. BlockManager requests the BlockInfoManager to xref:storage:BlockInfoManager.adoc#clear[clear] when requested to < >. BlockManager uses the BlockInfoManager to create a < >. BlockManager uses the BlockInfoManager when requested for the following: < > < > < > < > and < > < > < > < > < >, < >, < >, < > < >, < >, < >, < > == [[master]] BlockManager and BlockManagerMaster BlockManager is given a xref:storage:BlockManagerMaster.adoc[] when < >. == [[BlockDataManager]] BlockManager as BlockDataManager BlockManager is a xref:storage:BlockDataManager.adoc[]. == [[mapOutputTracker]] BlockManager and MapOutputTracker BlockManager is given a xref:scheduler:MapOutputTracker.adoc[] when < >. == [[executorId]] Executor ID BlockManager is given an Executor ID when < >. The Executor ID is one of the following: driver ( SparkContext.DRIVER_IDENTIFIER ) for the driver Value of xref:executor:CoarseGrainedExecutorBackend.adoc#executor-id[--executor-id] command-line argument for xref:executor:CoarseGrainedExecutorBackend.adoc[] executors (or xref:spark-on-mesos:spark-executor-backends-MesosExecutorBackend.adoc[MesosExecutorBackend]) == [[slaveEndpoint]] BlockManagerEndpoint RPC Endpoint BlockManager requests the < > to xref:rpc:RpcEnv.adoc#setupEndpoint[register] a xref:storage:BlockManagerSlaveEndpoint.adoc[] under the name BlockManagerEndpoint[ID] . The RPC endpoint is used when BlockManager is requested to < > and < > (to register the BlockManager on an executor with the < > on the driver). The endpoint is stopped (by requesting the < > to xref:rpc:RpcEnv.adoc#stop[stop the reference]) when BlockManager is requested to < >. == [[SparkEnv]] Accessing BlockManager Using SparkEnv BlockManager is available using xref:core:SparkEnv.adoc#blockManager[SparkEnv] on the driver and executors. [source,plaintext] \u00b6 import org.apache.spark.SparkEnv val bm = SparkEnv.get.blockManager scala> :type bm org.apache.spark.storage.BlockManager == [[blockTransferService]] BlockTransferService BlockManager is given a xref:storage:BlockTransferService.adoc[BlockTransferService] when < >. BlockTransferService is used as the < > when BlockManager is configured with no external shuffle service (based on xref:ROOT:configuration-properties.adoc#spark.shuffle.service.enabled[spark.shuffle.service.enabled] configuration property). BlockTransferService is xref:storage:BlockTransferService.adoc#init[initialized] when BlockManager < >. BlockTransferService is xref:storage:BlockTransferService.adoc#close[closed] when BlockManager is requested to < >. BlockTransferService is used when BlockManager is requested to < > or < > remote block managers. == [[memoryManager]] MemoryManager BlockManager is given a xref:memory:MemoryManager.adoc[MemoryManager] when < >. BlockManager uses the MemoryManager for the following: Create the < > (that is then assigned to xref:memory:MemoryManager.adoc#setMemoryStore[MemoryManager] as a \"circular dependency\") Initialize < > and < > (for reporting) == [[shuffleManager]] ShuffleManager BlockManager is given a xref:shuffle:ShuffleManager.adoc[ShuffleManager] when < >. BlockManager uses the ShuffleManager for the following: < > (for shuffle blocks) < > (for shuffle blocks anyway) < > (when < > on an executor with < >) == [[diskBlockManager]] DiskBlockManager BlockManager creates a xref:DiskBlockManager.adoc[DiskBlockManager] when < >. .DiskBlockManager and BlockManager image::DiskBlockManager-BlockManager.png[align=\"center\"] BlockManager uses the BlockManager for the following: Creating a < > < > (when < > on an executor with < >) The BlockManager is available as diskBlockManager reference to other Spark systems. [source, scala] \u00b6 import org.apache.spark.SparkEnv SparkEnv.get.blockManager.diskBlockManager == [[memoryStore]] MemoryStore BlockManager creates a xref:storage:MemoryStore.adoc[] when < > (with the < >, the < >, the < > and itself as a xref:storage:BlockEvictionHandler.adoc[]). .MemoryStore and BlockManager image::MemoryStore-BlockManager.png[align=\"center\"] BlockManager requests the < > to xref:memory:MemoryManager.adoc#setMemoryStore[use] the MemoryStore. BlockManager uses the MemoryStore for the following: < > and < > < > < > < > and < > < > and < > < > < > The MemoryStore is requested to xref:storage:MemoryStore.adoc#clear[clear] when BlockManager is requested to < >. The MemoryStore is available as memoryStore private reference to other Spark services. [source, scala] \u00b6 import org.apache.spark.SparkEnv SparkEnv.get.blockManager.memoryStore The MemoryStore is used (via SparkEnv.get.blockManager.memoryStore reference) when Task is requested to xref:scheduler:Task.adoc#run[run] (that has finished and requests the MemoryStore to xref:storage:MemoryStore.adoc#releaseUnrollMemoryForThisTask[releaseUnrollMemoryForThisTask]). == [[diskStore]] DiskStore BlockManager creates a xref:DiskStore.adoc[DiskStore] (with the < >) when < >. .DiskStore and BlockManager image::DiskStore-BlockManager.png[align=\"center\"] BlockManager uses the DiskStore when requested to < >, < >, < >, < >, < >, < >, < >, < >. == [[metrics]] Performance Metrics BlockManager uses link:spark-BlockManager-BlockManagerSource.adoc[BlockManagerSource] to report metrics under the name BlockManager . == [[getPeers]] getPeers Internal Method [source,scala] \u00b6 getPeers( forceFetch: Boolean): Seq[BlockManagerId] getPeers...FIXME getPeers is used when BlockManager is requested to < > and < >. == [[releaseAllLocksForTask]] Releasing All Locks For Task [source,scala] \u00b6 releaseAllLocksForTask( taskAttemptId: Long): Seq[BlockId] releaseAllLocksForTask...FIXME releaseAllLocksForTask is used when TaskRunner is requested to xref:executor:TaskRunner.adoc#run[run] (at the end of a task). == [[stop]] Stopping BlockManager [source, scala] \u00b6 stop(): Unit \u00b6 stop...FIXME stop is used when SparkEnv is requested to xref:core:SparkEnv.adoc#stop[stop]. == [[getMatchingBlockIds]] Getting IDs of Existing Blocks (For a Given Filter) [source, scala] \u00b6 getMatchingBlockIds( filter: BlockId => Boolean): Seq[BlockId] getMatchingBlockIds...FIXME getMatchingBlockIds is used when BlockManagerSlaveEndpoint is requested to xref:storage:BlockManagerSlaveEndpoint.adoc#GetMatchingBlockIds[handle a GetMatchingBlockIds message]. == [[getLocalValues]] Getting Local Block [source, scala] \u00b6 getLocalValues( blockId: BlockId): Option[BlockResult] getLocalValues prints out the following DEBUG message to the logs: Getting local block [blockId] getLocalValues xref:storage:BlockInfoManager.adoc#lockForReading[obtains a read lock for blockId ]. When no blockId block was found, you should see the following DEBUG message in the logs and getLocalValues returns \"nothing\" (i.e. NONE ). Block [blockId] was not found When the blockId block was found, you should see the following DEBUG message in the logs: Level for block [blockId] is [level] If blockId block has memory level and xref:storage:MemoryStore.adoc#contains[is registered in MemoryStore ], getLocalValues returns a < > as Memory read method and with a CompletionIterator for an interator: xref:storage:MemoryStore.adoc#getValues[Values iterator from MemoryStore for blockId ] for \"deserialized\" persistence levels. Iterator from xref:serializer:SerializerManager.adoc#dataDeserializeStream[ SerializerManager after the data stream has been deserialized] for the blockId block and xref:storage:MemoryStore.adoc#getBytes[the bytes for blockId block] for \"serialized\" persistence levels. getLocalValues is used when: TorrentBroadcast is requested to xref:core:TorrentBroadcast.adoc#readBroadcastBlock[readBroadcastBlock] BlockManager is requested to < > and < > === [[maybeCacheDiskValuesInMemory]] maybeCacheDiskValuesInMemory Internal Method [source,scala] \u00b6 maybeCacheDiskValuesInMemory T : Iterator[T] maybeCacheDiskValuesInMemory...FIXME maybeCacheDiskValuesInMemory is used when BlockManager is requested to < >. == [[getRemoteValues]] getRemoteValues Internal Method [source, scala] \u00b6 getRemoteValues T: ClassTag : Option[BlockResult] \u00b6 getRemoteValues ...FIXME == [[get]] Retrieving Block from Local or Remote Block Managers [source, scala] \u00b6 get T: ClassTag : Option[BlockResult] \u00b6 get attempts to get the blockId block from a local block manager first before requesting it from remote block managers. Internally, get tries to < >. If the block was found, you should see the following INFO message in the logs and get returns the local < >. INFO Found block [blockId] locally If however the block was not found locally, get tries to < >. If retrieved from a remote block manager, you should see the following INFO message in the logs and get returns the remote < >. INFO Found block [blockId] remotely In the end, get returns \"nothing\" (i.e. NONE ) when the blockId block was not found either in the local BlockManager or any remote BlockManager. [NOTE] \u00b6 get is used when: * BlockManager is requested to < > and < > \u00b6 == [[getBlockData]] Retrieving Block Data [source, scala] \u00b6 getBlockData( blockId: BlockId): ManagedBuffer NOTE: getBlockData is part of the xref:storage:BlockDataManager.adoc#getBlockData[BlockDataManager] contract. For a xref:BlockId.adoc[] of a shuffle (a ShuffleBlockId), getBlockData requests the < > for the xref:shuffle:ShuffleManager.adoc#shuffleBlockResolver[ShuffleBlockResolver] that is then requested for xref:shuffle:ShuffleBlockResolver.adoc#getBlockData[getBlockData]. Otherwise, getBlockData < > for the given BlockId. If found, getBlockData creates a new BlockManagerManagedBuffer (with the < >, the input BlockId, the retrieved BlockData and the dispose flag enabled). If not found, getBlockData < > that the block could not be found (and that the master should no longer assume the block is available on this executor) and throws a BlockNotFoundException. NOTE: getBlockData is executed for shuffle blocks or local blocks that the BlockManagerMaster knows this executor really has (unless BlockManagerMaster is outdated). == [[getLocalBytes]] Retrieving Non-Shuffle Local Block Data [source, scala] \u00b6 getLocalBytes( blockId: BlockId): Option[BlockData] getLocalBytes ...FIXME [NOTE] \u00b6 getLocalBytes is used when: TorrentBroadcast is requested to xref:core:TorrentBroadcast.adoc#readBlocks[readBlocks] * BlockManager is requested for the < > (of a non-shuffle block) \u00b6 == [[removeBlockInternal]] removeBlockInternal Internal Method [source, scala] \u00b6 removeBlockInternal( blockId: BlockId, tellMaster: Boolean): Unit removeBlockInternal...FIXME removeBlockInternal is used when BlockManager is requested to < > and < >. == [[stores]] Stores A Store is the place where blocks are held. There are the following possible stores: xref:storage:MemoryStore.adoc[MemoryStore] for memory storage level. xref:DiskStore.adoc[DiskStore] for disk storage level. ExternalBlockStore for OFF_HEAP storage level. == [[putBlockData]] Storing Block Data Locally [source, scala] \u00b6 putBlockData( blockId: BlockId, data: ManagedBuffer, level: StorageLevel, classTag: ClassTag[_]): Boolean putBlockData simply < blockId locally>> (given the given storage level ). NOTE: putBlockData is part of the xref:storage:BlockDataManager.adoc#putBlockData[BlockDataManager Contract]. Internally, putBlockData wraps ChunkedByteBuffer around data buffer's NIO ByteBuffer and calls < >. == [[putBytes]] Storing Block Bytes Locally [source, scala] \u00b6 putBytes( blockId: BlockId, bytes: ChunkedByteBuffer, level: StorageLevel, tellMaster: Boolean = true): Boolean putBytes makes sure that the bytes are not null and < >. [NOTE] \u00b6 putBytes is used when: BlockManager is requested to < > TaskRunner is requested to xref:executor:TaskRunner.adoc#run-result-sent-via-blockmanager[run] (and the result size is above xref:executor:Executor.adoc#maxDirectResultSize[maxDirectResultSize]) * TorrentBroadcast is requested to xref:core:TorrentBroadcast.adoc#writeBlocks[writeBlocks] and xref:core:TorrentBroadcast.adoc#readBlocks[readBlocks] \u00b6 === [[doPutBytes]] doPutBytes Internal Method [source, scala] \u00b6 doPutBytes T : Boolean doPutBytes calls the internal helper < > with a function that accepts a BlockInfo and does the uploading. Inside the function, if the xref:storage:StorageLevel.adoc[storage level ]'s replication is greater than 1, it immediately starts < > of the blockId block on a separate thread (from futureExecutionContext thread pool). The replication uses the input bytes and level storage level. For a memory storage level, the function checks whether the storage level is deserialized or not. For a deserialized storage level , BlockManager 's xref:serializer:SerializerManager.adoc#dataDeserializeStream[ SerializerManager deserializes bytes into an iterator of values] that xref:storage:MemoryStore.adoc#putIteratorAsValues[ MemoryStore stores]. If however the storage level is not deserialized, the function requests xref:storage:MemoryStore.adoc#putBytes[ MemoryStore to store the bytes] If the put did not succeed and the storage level is to use disk, you should see the following WARN message in the logs: WARN BlockManager: Persisting block [blockId] to disk instead. And xref:DiskStore.adoc#putBytes[ DiskStore stores the bytes]. NOTE: xref:DiskStore.adoc[DiskStore] is requested to store the bytes of a block with memory and disk storage level only when xref:storage:MemoryStore.adoc[MemoryStore] has failed. If the storage level is to use disk only, xref:DiskStore.adoc#putBytes[ DiskStore stores the bytes]. doPutBytes requests < > and if the block was successfully stored, and the driver should know about it ( tellMaster ), the function < >. The xref:executor:TaskMetrics.adoc#incUpdatedBlockStatuses[current TaskContext metrics are updated with the updated block status] (only when executed inside a task where TaskContext is available). You should see the following DEBUG message in the logs: DEBUG BlockManager: Put block [blockId] locally took [time] ms The function waits till the earlier asynchronous replication finishes for a block with replication level greater than 1 . The final result of doPutBytes is the result of storing the block successful or not (as computed earlier). NOTE: doPutBytes is used exclusively when BlockManager is requested to < >. == [[doPut]] doPut Internal Method [source, scala] \u00b6 doPut T (putBody: BlockInfo => Option[T]): Option[T] doPut executes the input putBody function with a xref:storage:BlockInfo.adoc[] being a new BlockInfo object (with level storage level) that xref:storage:BlockInfoManager.adoc#lockNewBlockForWriting[ BlockInfoManager managed to create a write lock for]. If the block has already been created (and xref:storage:BlockInfoManager.adoc#lockNewBlockForWriting[ BlockInfoManager did not manage to create a write lock for]), the following WARN message is printed out to the logs: [source,plaintext] \u00b6 Block [blockId] already exists on this machine; not re-adding it \u00b6 doPut < > when keepReadLock flag is disabled and returns None immediately. If however the write lock has been given, doPut executes putBody . If the result of putBody is None the block is considered saved successfully. For successful save and keepReadLock enabled, xref:storage:BlockInfoManager.adoc#downgradeLock[ BlockInfoManager is requested to downgrade an exclusive write lock for blockId to a shared read lock]. For successful save and keepReadLock disabled, xref:storage:BlockInfoManager.adoc#unlock[ BlockInfoManager is requested to release lock on blockId ]. For unsuccessful save, < > and the following WARN message is printed out to the logs: [source,plaintext] \u00b6 Putting block [blockId] failed \u00b6 In the end, doPut prints out the following DEBUG message to the logs: [source,plaintext] \u00b6 Putting block [blockId] [withOrWithout] replication took [usedTime] ms \u00b6 doPut is used when BlockManager is requested to < > and < >. == [[removeBlock]] Removing Block From Memory and Disk [source, scala] \u00b6 removeBlock( blockId: BlockId, tellMaster: Boolean = true): Unit removeBlock removes the blockId block from the xref:storage:MemoryStore.adoc[MemoryStore] and xref:DiskStore.adoc[DiskStore]. When executed, it prints out the following DEBUG message to the logs: Removing block [blockId] It requests xref:storage:BlockInfoManager.adoc[] for lock for writing for the blockId block. If it receives none, it prints out the following WARN message to the logs and quits. Asked to remove block [blockId], which does not exist Otherwise, with a write lock for the block, the block is removed from xref:storage:MemoryStore.adoc[MemoryStore] and xref:DiskStore.adoc[DiskStore] (see xref:storage:MemoryStore.adoc#remove[Removing Block in MemoryStore ] and xref:DiskStore.adoc#remove[Removing Block in DiskStore ]). If both removals fail, it prints out the following WARN message: Block [blockId] could not be removed as it was not found in either the disk, memory, or external block store The block is removed from xref:storage:BlockInfoManager.adoc[]. removeBlock then < > that is used to < > (if the input tellMaster and the info's tellMaster are both enabled, i.e. true ) and the xref:executor:TaskMetrics.adoc#incUpdatedBlockStatuses[current TaskContext metrics are updated with the change]. removeBlock is used when: BlockManager is requested to < >, < > and < > BlockManagerSlaveEndpoint is requested to handle a xref:storage:BlockManagerSlaveEndpoint.adoc#RemoveBlock[RemoveBlock] message == [[removeRdd]] Removing RDD Blocks [source, scala] \u00b6 removeRdd(rddId: Int): Int \u00b6 removeRdd removes all the blocks that belong to the rddId RDD. It prints out the following INFO message to the logs: INFO Removing RDD [rddId] It then requests RDD blocks from xref:storage:BlockInfoManager.adoc[] and < > (without informing the driver). The number of blocks removed is the final result. NOTE: It is used by xref:storage:BlockManagerSlaveEndpoint.adoc#RemoveRdd[ BlockManagerSlaveEndpoint while handling RemoveRdd messages]. == [[removeBroadcast]] Removing All Blocks of Broadcast Variable [source, scala] \u00b6 removeBroadcast(broadcastId: Long, tellMaster: Boolean): Int \u00b6 removeBroadcast removes all the blocks of the input broadcastId broadcast. Internally, it starts by printing out the following DEBUG message to the logs: Removing broadcast [broadcastId] It then requests all the xref:storage:BlockId.adoc#BroadcastBlockId[BroadcastBlockId] objects that belong to the broadcastId broadcast from xref:storage:BlockInfoManager.adoc[] and < >. The number of blocks removed is the final result. NOTE: It is used by xref:storage:BlockManagerSlaveEndpoint.adoc#RemoveBroadcast[ BlockManagerSlaveEndpoint while handling RemoveBroadcast messages]. == [[shuffleServerId]] BlockManagerId of Shuffle Server BlockManager uses xref:storage:BlockManagerId.adoc[] for the location (address) of the server that serves shuffle files of this executor. The BlockManagerId is either the BlockManagerId of the external shuffle service (when < >) or the < >. The BlockManagerId of the Shuffle Server is used for the location of a xref:scheduler:MapStatus.adoc[shuffle map output] when: BypassMergeSortShuffleWriter is requested to xref:shuffle:BypassMergeSortShuffleWriter.adoc#write[write partition records to a shuffle file] UnsafeShuffleWriter is requested to xref:shuffle:UnsafeShuffleWriter.adoc#closeAndWriteOutput[close and write output] == [[getStatus]] getStatus Method [source,scala] \u00b6 getStatus( blockId: BlockId): Option[BlockStatus] getStatus...FIXME getStatus is used when BlockManagerSlaveEndpoint is requested to handle xref:storage:BlockManagerSlaveEndpoint.adoc#GetBlockStatus[GetBlockStatus] message. == [[initialize]] Initializing BlockManager [source, scala] \u00b6 initialize( appId: String): Unit initialize initializes a BlockManager on the driver and executors (see xref:ROOT:SparkContext.adoc#creating-instance[Creating SparkContext Instance] and xref:executor:Executor.adoc#creating-instance[Creating Executor Instance], respectively). NOTE: The method must be called before a BlockManager can be considered fully operable. initialize does the following in order: Initializes xref:storage:BlockTransferService.adoc#init[BlockTransferService] Initializes the internal shuffle client, be it xref:storage:ExternalShuffleClient.adoc[ExternalShuffleClient] or xref:storage:BlockTransferService.adoc[BlockTransferService]. xref:BlockManagerMaster.adoc#registerBlockManager[Registers itself with the driver's BlockManagerMaster ] (using the id , maxMemory and its slaveEndpoint ). + The BlockManagerMaster reference is passed in when the < > on the driver and executors. Sets < > to an instance of xref:storage:BlockManagerId.adoc[] given an executor id, host name and port for xref:storage:BlockTransferService.adoc[BlockTransferService]. It creates the address of the server that serves this executor's shuffle files (using < >) CAUTION: FIXME Review the initialize procedure again CAUTION: FIXME Describe shuffleServerId . Where is it used? If the < >, initialize prints out the following INFO message to the logs: [source,plaintext] \u00b6 external shuffle service port = [externalShuffleServicePort] \u00b6 It xref:BlockManagerMaster.adoc#registerBlockManager[registers itself to the driver's BlockManagerMaster] passing the xref:storage:BlockManagerId.adoc[], the maximum memory (as maxMemory ), and the xref:storage:BlockManagerSlaveEndpoint.adoc[]. Ultimately, if the initialization happens on an executor and the < >, it < >. initialize is used when the link:spark-SparkContext-creating-instance-internals.adoc#BlockManager-initialization[driver is launched (and SparkContext is created)] and when an xref:executor:Executor.adoc#creating-instance[ Executor is created] (for xref:executor:CoarseGrainedExecutorBackend.adoc#RegisteredExecutor[CoarseGrainedExecutorBackend] and xref:spark-on-mesos:spark-executor-backends-MesosExecutorBackend.adoc[MesosExecutorBackend]). == [[registerWithExternalShuffleServer]] Registering Executor's BlockManager with External Shuffle Server [source, scala] \u00b6 registerWithExternalShuffleServer(): Unit \u00b6 registerWithExternalShuffleServer is an internal helper method to register the BlockManager for an executor with an xref:deploy:ExternalShuffleService.adoc[external shuffle server]. NOTE: It is executed when a < >. When executed, you should see the following INFO message in the logs: Registering executor with local external shuffle service. It uses < > to xref:storage:ExternalShuffleClient.adoc#registerWithShuffleServer[register the block manager] using < > (i.e. the host, the port and the executorId) and a ExecutorShuffleInfo . NOTE: The ExecutorShuffleInfo uses localDirs and subDirsPerLocalDir from xref:DiskBlockManager.adoc[DiskBlockManager] and the class name of the constructor xref:shuffle:ShuffleManager.adoc[ShuffleManager]. It tries to register at most 3 times with 5-second sleeps in-between. NOTE: The maximum number of attempts and the sleep time in-between are hard-coded, i.e. they are not configured. Any issues while connecting to the external shuffle service are reported as ERROR messages in the logs: Failed to connect to external shuffle server, will retry [#attempts] more times after waiting 5 seconds... registerWithExternalShuffleServer is used when BlockManager is requested to < > (when executed on an executor with < >). == [[reregister]] Re-registering BlockManager with Driver and Reporting Blocks [source, scala] \u00b6 reregister(): Unit \u00b6 When executed, reregister prints the following INFO message to the logs: BlockManager [blockManagerId] re-registering with master reregister then xref:BlockManagerMaster.adoc#registerBlockManager[registers itself to the driver's BlockManagerMaster ] (just as it was when < >). It passes the xref:storage:BlockManagerId.adoc[], the maximum memory (as maxMemory ), and the xref:storage:BlockManagerSlaveEndpoint.adoc[]. reregister will then report all the local blocks to the xref:BlockManagerMaster.adoc[BlockManagerMaster]. You should see the following INFO message in the logs: Reporting [blockInfoManager.size] blocks to the master. For each block metadata (in xref:storage:BlockInfoManager.adoc[]) it < > and < >. If there is an issue communicating to the xref:BlockManagerMaster.adoc[BlockManagerMaster], you should see the following ERROR message in the logs: Failed to report [blockId] to master; giving up. After the ERROR message, reregister stops reporting. reregister is used when a xref:executor:Executor.adoc#heartbeats-and-active-task-metrics[ Executor was informed to re-register while sending heartbeats]. == [[getCurrentBlockStatus]] Calculate Current Block Status [source, scala] \u00b6 getCurrentBlockStatus( blockId: BlockId, info: BlockInfo): BlockStatus getCurrentBlockStatus gives the current BlockStatus of the BlockId block (with the block's current xref:storage:StorageLevel.adoc[StorageLevel], memory and disk sizes). It uses xref:storage:MemoryStore.adoc[MemoryStore] and xref:DiskStore.adoc[DiskStore] for size and other information. NOTE: Most of the information to build BlockStatus is already in BlockInfo except that it may not necessarily reflect the current state per xref:storage:MemoryStore.adoc[MemoryStore] and xref:DiskStore.adoc[DiskStore]. Internally, it uses the input xref:storage:BlockInfo.adoc[] to know about the block's storage level. If the storage level is not set (i.e. null ), the returned BlockStatus assumes the xref:storage:StorageLevel.adoc[default NONE storage level] and the memory and disk sizes being 0 . If however the storage level is set, getCurrentBlockStatus uses xref:storage:MemoryStore.adoc[MemoryStore] and xref:DiskStore.adoc[DiskStore] to check whether the block is stored in the storages or not and request for their sizes in the storages respectively (using their getSize or assume 0 ). NOTE: It is acceptable that the BlockInfo says to use memory or disk yet the block is not in the storages (yet or anymore). The method will give current status. getCurrentBlockStatus is used when < >, < > or < > or < >. == [[reportAllBlocks]] reportAllBlocks Internal Method [source, scala] \u00b6 reportAllBlocks(): Unit \u00b6 reportAllBlocks...FIXME reportAllBlocks is used when BlockManager is requested to < >. == [[reportBlockStatus]] Reporting Current Storage Status of Block to Driver [source, scala] \u00b6 reportBlockStatus( blockId: BlockId, info: BlockInfo, status: BlockStatus, droppedMemorySize: Long = 0L): Unit reportBlockStatus is an internal method for < > and if told to re-register it prints out the following INFO message to the logs: Got told to re-register updating block [blockId] It does asynchronous reregistration (using asyncReregister ). In either case, it prints out the following DEBUG message to the logs: Told master about block [blockId] reportBlockStatus is used when BlockManager is requested to < >, < >, < >, < > and < >. == [[tryToReportBlockStatus]] Reporting Block Status Update to Driver [source, scala] \u00b6 def tryToReportBlockStatus( blockId: BlockId, info: BlockInfo, status: BlockStatus, droppedMemorySize: Long = 0L): Boolean tryToReportBlockStatus xref:BlockManagerMaster.adoc#updateBlockInfo[reports block status update] to < > and returns its response. tryToReportBlockStatus is used when BlockManager is requested to < > or < >. == [[execution-context]] Execution Context block-manager-future is the execution context for...FIXME == [[ByteBuffer]] ByteBuffer The underlying abstraction for blocks in Spark is a ByteBuffer that limits the size of a block to 2GB ( Integer.MAX_VALUE - see http://stackoverflow.com/q/8076472/1305344[Why does FileChannel.map take up to Integer.MAX_VALUE of data?] and https://issues.apache.org/jira/browse/SPARK-1476[SPARK-1476 2GB limit in spark for blocks]). This has implication not just for managed blocks in use, but also for shuffle blocks (memory mapped blocks are limited to 2GB, even though the API allows for long ), ser-deser via byte array-backed output streams. == [[BlockResult]] BlockResult BlockResult is a description of a fetched block with the readMethod and bytes . == [[registerTask]] Registering Task [source, scala] \u00b6 registerTask( taskAttemptId: Long): Unit registerTask requests the < > to xref:storage:BlockInfoManager.adoc#registerTask[register a given task]. registerTask is used when Task is requested to xref:scheduler:Task.adoc#run[run] (at the start of a task). == [[getDiskWriter]] Creating DiskBlockObjectWriter [source, scala] \u00b6 getDiskWriter( blockId: BlockId, file: File, serializerInstance: SerializerInstance, bufferSize: Int, writeMetrics: ShuffleWriteMetrics): DiskBlockObjectWriter getDiskWriter creates a xref:storage:DiskBlockObjectWriter.adoc[DiskBlockObjectWriter] (with xref:ROOT:configuration-properties.adoc#spark.shuffle.sync[spark.shuffle.sync] configuration property for syncWrites argument). getDiskWriter uses the < > of the BlockManager. getDiskWriter is used when: BypassMergeSortShuffleWriter is requested to xref:shuffle:BypassMergeSortShuffleWriter.adoc#write[write records (of a partition)] ShuffleExternalSorter is requested to xref:shuffle:ShuffleExternalSorter.adoc#writeSortedFile[writeSortedFile] ExternalAppendOnlyMap is requested to xref:shuffle:ExternalAppendOnlyMap.adoc#spillMemoryIteratorToDisk[spillMemoryIteratorToDisk] ExternalSorter is requested to xref:shuffle:ExternalSorter.adoc#spillMemoryIteratorToDisk[spillMemoryIteratorToDisk] and xref:shuffle:ExternalSorter.adoc#writePartitionedFile[writePartitionedFile] xref:memory:UnsafeSorterSpillWriter.adoc[UnsafeSorterSpillWriter] is created == [[addUpdatedBlockStatusToTaskMetrics]] Recording Updated BlockStatus In Current Task's TaskMetrics [source, scala] \u00b6 addUpdatedBlockStatusToTaskMetrics( blockId: BlockId, status: BlockStatus): Unit addUpdatedBlockStatusToTaskMetrics link:spark-TaskContext.adoc#get[takes an active TaskContext ] (if available) and xref:executor:TaskMetrics.adoc#incUpdatedBlockStatuses[records updated BlockStatus for Block ] (in the link:spark-TaskContext.adoc#taskMetrics[task's TaskMetrics ]). addUpdatedBlockStatusToTaskMetrics is used when BlockManager < > (for a block that was successfully stored), < >, < >, < > (possibly spilling it to disk) and < >. == [[shuffleMetricsSource]] Requesting Shuffle-Related Spark Metrics Source [source, scala] \u00b6 shuffleMetricsSource: Source \u00b6 shuffleMetricsSource requests the < > for the xref:storage:ShuffleClient.adoc#shuffleMetrics[shuffle metrics] and creates a xref:storage:ShuffleMetricsSource.adoc[] with the xref:storage:ShuffleMetricsSource.adoc#sourceName[source name] based on xref:ROOT:configuration-properties.adoc#spark.shuffle.service.enabled[spark.shuffle.service.enabled] configuration property: ExternalShuffle when xref:ROOT:configuration-properties.adoc#spark.shuffle.service.enabled[spark.shuffle.service.enabled] configuration property is on ( true ) NettyBlockTransfer when xref:ROOT:configuration-properties.adoc#spark.shuffle.service.enabled[spark.shuffle.service.enabled] configuration property is off ( false ) shuffleMetricsSource is used when Executor is xref:executor:Executor.adoc#creating-instance[created] (for non-local / cluster modes). == [[replicate]] Replicating Block To Peers [source, scala] \u00b6 replicate( blockId: BlockId, data: BlockData, level: StorageLevel, classTag: ClassTag[_], existingReplicas: Set[BlockManagerId] = Set.empty): Unit replicate...FIXME replicate is used when BlockManager is requested to < >, < > and < >. == [[replicateBlock]] replicateBlock Method [source, scala] \u00b6 replicateBlock( blockId: BlockId, existingReplicas: Set[BlockManagerId], maxReplicas: Int): Unit replicateBlock...FIXME replicateBlock is used when BlockManagerSlaveEndpoint is requested to xref:storage:BlockManagerSlaveEndpoint.adoc#ReplicateBlock[handle a ReplicateBlock message]. == [[putIterator]] putIterator Method [source, scala] \u00b6 putIterator T: ClassTag : Boolean putIterator ...FIXME [NOTE] \u00b6 putIterator is used when: BlockManager is requested to < > * Spark Streaming's BlockManagerBasedBlockHandler is requested to storeBlock \u00b6 == [[putSingle]] putSingle Method [source, scala] \u00b6 putSingle T: ClassTag : Boolean putSingle...FIXME putSingle is used when TorrentBroadcast is requested to xref:core:TorrentBroadcast.adoc#writeBlocks[write the blocks] and xref:core:TorrentBroadcast.adoc#readBroadcastBlock[readBroadcastBlock]. == [[getRemoteBytes]] Fetching Block From Remote Nodes [source, scala] \u00b6 getRemoteBytes(blockId: BlockId): Option[ChunkedByteBuffer] \u00b6 getRemoteBytes ...FIXME [NOTE] \u00b6 getRemoteBytes is used when: BlockManager is requested to < > TorrentBroadcast is requested to xref:core:TorrentBroadcast.adoc#readBlocks[readBlocks] * TaskResultGetter is requested to xref:scheduler:TaskResultGetter.adoc#enqueueSuccessfulTask[enqueuing a successful IndirectTaskResult] \u00b6 == [[getRemoteValues]] getRemoteValues Internal Method [source, scala] \u00b6 getRemoteValues T: ClassTag : Option[BlockResult] \u00b6 getRemoteValues ...FIXME NOTE: getRemoteValues is used exclusively when BlockManager is requested to < >. == [[getSingle]] getSingle Method [source, scala] \u00b6 getSingle T: ClassTag : Option[T] \u00b6 getSingle ...FIXME NOTE: getSingle is used exclusively in Spark tests. == [[getOrElseUpdate]] Getting Block From Block Managers Or Computing and Storing It Otherwise [source, scala] \u00b6 getOrElseUpdate T : Either[BlockResult, Iterator[T]] [NOTE] \u00b6 I think it is fair to say that getOrElseUpdate is like link:++ https://www.scala-lang.org/api/current/scala/collection/mutable/Map.html#getOrElseUpdate(key:K,op:=%3EV):V++[getOrElseUpdate ] of https://www.scala-lang.org/api/current/scala/collection/mutable/Map.html[scala.collection.mutable.Map ] in Scala. [source, scala] \u00b6 getOrElseUpdate(key: K, op: \u21d2 V): V \u00b6 Quoting the official scaladoc: If given key K is already in this map, getOrElseUpdate returns the associated value V . Otherwise, getOrElseUpdate computes a value V from given expression op , stores with the key K in the map and returns that value. Since BlockManager is a key-value store of blocks of data identified by a block ID that works just fine. \u00b6 getOrElseUpdate first attempts to < > by the BlockId (from the local block manager first and, if unavailable, requesting remote peers). [TIP] \u00b6 Enable INFO logging level for org.apache.spark.storage.BlockManager logger to see what happens when BlockManager tries to < >. See < > in this document. \u00b6 getOrElseUpdate gives the BlockResult of the block if found. If however the block was not found (in any block manager in a Spark cluster), getOrElseUpdate < > (for the input BlockId , the makeIterator function and the StorageLevel ). getOrElseUpdate branches off per the result. For None , getOrElseUpdate < > for the BlockId and eventually returns the BlockResult (unless terminated by a SparkException due to some internal error). For Some(iter) , getOrElseUpdate returns an iterator of T values. NOTE: getOrElseUpdate is used exclusively when RDD is requested to xref:rdd:RDD.adoc#getOrCompute[get or compute an RDD partition] (for a RDDBlockId with a RDD ID and a partition index). == [[doPutIterator]] doPutIterator Internal Method [source, scala] \u00b6 doPutIterator T : Option[PartiallyUnrolledIterator[T]] doPutIterator simply < > with the putBody function that accepts a BlockInfo and does the following: . putBody branches off per whether the StorageLevel indicates to use a xref:storage:StorageLevel.adoc#useMemory[memory] or simply a xref:storage:StorageLevel.adoc#useDisk[disk], i.e. When the input StorageLevel indicates to xref:storage:StorageLevel.adoc#useMemory[use a memory] for storage in xref:storage:StorageLevel.adoc#deserialized[deserialized] format, putBody requests < > to xref:storage:MemoryStore.adoc#putIteratorAsValues[putIteratorAsValues] (for the BlockId and with the iterator factory function). + If the < > returned a correct value, the internal size is set to the value. + If however the < > failed to give a correct value, FIXME When the input StorageLevel indicates to xref:storage:StorageLevel.adoc#useMemory[use memory] for storage in xref:storage:StorageLevel.adoc#deserialized[serialized] format, putBody ...FIXME When the input StorageLevel does not indicate to use memory for storage but xref:storage:StorageLevel.adoc#useDisk[disk] instead, putBody ...FIXME . putBody requests the < > . Only when the block was successfully stored in either the memory or disk store: putBody < > to the < > when the input tellMaster flag (default: enabled) and the tellMaster flag of the block info are both enabled. putBody < > (with the BlockId and BlockStatus ) putBody prints out the following DEBUG message to the logs: + Put block [blockId] locally took [time] ms When the input StorageLevel indicates to use xref:storage:StorageLevel.adoc#replication[replication], putBody < > followed by < > (with the input BlockId and the StorageLevel as well as the BlockData to replicate) With a successful replication, putBody prints out the following DEBUG message to the logs: + Put block [blockId] remotely took [time] ms . In the end, putBody may or may not give a PartiallyUnrolledIterator if...FIXME NOTE: doPutIterator is used when BlockManager is requested to < > and < >. == [[dropFromMemory]] Dropping Block from Memory [source,scala] \u00b6 dropFromMemory( blockId: BlockId, data: () => Either[Array[T], ChunkedByteBuffer]): StorageLevel dropFromMemory prints out the following INFO message to the logs: [source,plaintext] \u00b6 Dropping block [blockId] from memory \u00b6 dropFromMemory then asserts that the given block is xref:storage:BlockInfoManager.adoc#assertBlockIsLockedForWriting[locked for writing]. If the block's xref:storage:StorageLevel.adoc[StorageLevel] uses disks and the internal xref:DiskStore.adoc[DiskStore] object ( diskStore ) does not contain the block, it is saved then. You should see the following INFO message in the logs: Writing block [blockId] to disk CAUTION: FIXME Describe the case with saving a block to disk. The block's memory size is fetched and recorded (using MemoryStore.getSize ). The block is xref:storage:MemoryStore.adoc#remove[removed from memory] if exists. If not, you should see the following WARN message in the logs: Block [blockId] could not be dropped from memory as it does not exist It then < > and < >. It only happens when info.tellMaster . CAUTION: FIXME When would info.tellMaster be true ? A block is considered updated when it was written to disk or removed from memory or both. If either happened, the xref:executor:TaskMetrics.adoc#incUpdatedBlockStatuses[current TaskContext metrics are updated with the change]. In the end, dropFromMemory returns the current storage level of the block. dropFromMemory is part of the xref:storage:BlockEvictionHandler.adoc#dropFromMemory[BlockEvictionHandler] abstraction. == [[handleLocalReadFailure]] handleLocalReadFailure Internal Method [source, scala] \u00b6 handleLocalReadFailure(blockId: BlockId): Nothing \u00b6 handleLocalReadFailure ...FIXME NOTE: handleLocalReadFailure is used when...FIXME == [[releaseLockAndDispose]] releaseLockAndDispose Method [source, scala] \u00b6 releaseLockAndDispose( blockId: BlockId, data: BlockData, taskAttemptId: Option[Long] = None): Unit releaseLockAndDispose...FIXME releaseLockAndDispose is used when...FIXME == [[releaseLock]] releaseLock Method [source, scala] \u00b6 releaseLock( blockId: BlockId, taskAttemptId: Option[Long] = None): Unit releaseLock requests the < > to xref:storage:BlockInfoManager.adoc#unlock[unlock the given block]. releaseLock is part of the xref:storage:BlockDataManager.adoc#releaseLock[BlockDataManager] abstraction. == [[putBlockDataAsStream]] putBlockDataAsStream Method [source,scala] \u00b6 putBlockDataAsStream( blockId: BlockId, level: StorageLevel, classTag: ClassTag[_]): StreamCallbackWithID putBlockDataAsStream...FIXME putBlockDataAsStream is part of the xref:storage:BlockDataManager.adoc#putBlockDataAsStream[BlockDataManager] abstraction. == [[downgradeLock]] downgradeLock Method [source, scala] \u00b6 downgradeLock( blockId: BlockId): Unit downgradeLock requests the < > to xref:storage:BlockInfoManager.adoc#downgradeLock[downgradeLock] for the given xref:storage:BlockId.adoc[block]. downgradeLock seems not to be used. == [[blockIdsToLocations]] blockIdsToLocations Utility [source,scala] \u00b6 blockIdsToLocations( blockIds: Array[BlockId], env: SparkEnv, blockManagerMaster: BlockManagerMaster = null): Map[BlockId, Seq[String]] blockIdsToLocations...FIXME blockIdsToLocations is used in the now defunct Spark Streaming (when BlockRDD is requested for _locations). === [[getLocationBlockIds]] getLocationBlockIds Internal Method [source,scala] \u00b6 getLocationBlockIds( blockIds: Array[BlockId]): Array[Seq[BlockManagerId]] getLocationBlockIds...FIXME getLocationBlockIds is used when BlockManager utility is requested to < > (for the now defunct Spark Streaming). == [[logging]] Logging Enable ALL logging level for org.apache.spark.storage.BlockManager logger to see what happens inside. Add the following line to conf/log4j.properties : [source,plaintext] \u00b6 log4j.logger.org.apache.spark.storage.BlockManager=ALL \u00b6 Refer to xref:ROOT:spark-logging.adoc[Logging]. == [[internal-properties]] Internal Properties === [[maxMemory]] Maximum Memory Total maximum value that BlockManager can ever possibly use (that depends on < > and may vary over time). Total available xref:memory:MemoryManager.adoc#maxOnHeapStorageMemory[on-heap] and xref:memory:MemoryManager.adoc#maxOffHeapStorageMemory[off-heap] memory for storage (in bytes) === [[maxOffHeapMemory]] Maximum Off-Heap Memory === [[maxOnHeapMemory]] Maximum On-Heap Memory","title":"BlockManager"},{"location":"storage/BlockManager/#blockmanager","text":"BlockManager manages the storage for blocks ( chunks of data ) that can be stored in < > and on < >. .BlockManager and Stores image::BlockManager.png[align=\"center\"] BlockManager runs on the xref:ROOT:spark-driver.adoc[driver] and xref:executor:Executor.adoc[executors]. BlockManager provides interface for uploading and fetching blocks both locally and remotely using various stores, i.e. < >. [[futureExecutionContext]] BlockManager uses a Scala https://www.scala-lang.org/api/current/scala/concurrent/ExecutionContextExecutorService.html[ExecutionContextExecutorService ] to execute FIXME asynchronously (on a thread pool with block-manager-future prefix and maximum of 128 threads). Cached blocks are blocks with non-zero sum of memory and disk sizes. TIP: Use xref:webui:index.adoc[Web UI], esp. xref:webui:spark-webui-storage.adoc[Storage] and xref:webui:spark-webui-executors.adoc[Executors] tabs, to monitor the memory used. TIP: Use xref spark-submit.adoc[spark-submit]'s command-line options, i.e. xref spark-submit.adoc#driver-memory[--driver-memory] for the driver and xref spark-submit.adoc#executor-memory[--executor-memory] for executors or their equivalents as Spark properties, i.e. xref spark-submit.adoc#spark.executor.memory[spark.executor.memory] and xref spark-submit.adoc#spark_driver_memory[spark.driver.memory], to control the memory for storage memory. When < >, BlockManager uses xref:storage:ExternalShuffleClient.adoc[ExternalShuffleClient] to read other executors' shuffle files. == [[creating-instance]] Creating Instance BlockManager takes the following to be created: < > < > < > [[serializerManager]] xref:serializer:SerializerManager.adoc[] [[conf]] xref:ROOT:SparkConf.adoc[] < > < > < > < > [[securityManager]] SecurityManager [[numUsableCores]] Number of CPU cores (for an xref:storage:ExternalShuffleClient.adoc[] with < >) When created, BlockManager sets < > internal flag based on xref:ROOT:configuration-properties.adoc#spark.shuffle.service.enabled[spark.shuffle.service.enabled] configuration property. BlockManager then creates an instance of xref:DiskBlockManager.adoc[DiskBlockManager] (requesting deleteFilesOnStop when an external shuffle service is not in use). BlockManager creates block-manager-future daemon cached thread pool with 128 threads maximum (as futureExecutionContext ). BlockManager calculates the maximum memory to use (as maxMemory ) by requesting the maximum xref:memory:MemoryManager.adoc#maxOnHeapStorageMemory[on-heap] and xref:memory:MemoryManager.adoc#maxOffHeapStorageMemory[off-heap] storage memory from the assigned MemoryManager . BlockManager calculates the port used by the external shuffle service (as externalShuffleServicePort ). NOTE: It is computed specially in Spark on YARN. BlockManager creates a client to read other executors' shuffle files (as shuffleClient ). If the external shuffle service is used an xref:storage:ExternalShuffleClient.adoc[ExternalShuffleClient] is created or the input xref:storage:BlockTransferService.adoc[BlockTransferService] is used. BlockManager sets the xref:ROOT:configuration-properties.adoc#spark.block.failures.beforeLocationRefresh[maximum number of failures] before this block manager refreshes the block locations from the driver (as maxFailuresBeforeLocationRefresh ). BlockManager registers a xref:storage:BlockManagerSlaveEndpoint.adoc[] with the input xref:ROOT:index.adoc[RpcEnv], itself, and xref:scheduler:MapOutputTracker.adoc[MapOutputTracker] (as slaveEndpoint ). BlockManager is created when SparkEnv is xref:core:SparkEnv.adoc#create-BlockManager[created] (for the driver and executors) when a Spark application starts. .BlockManager and SparkEnv image::BlockManager-SparkEnv.png[align=\"center\"] == [[BlockEvictionHandler]] BlockEvictionHandler BlockManager is a xref:storage:BlockEvictionHandler.adoc[] that can < > (and store it on a disk when needed). == [[shuffleClient]][[externalShuffleServiceEnabled]] ShuffleClient and External Shuffle Service BlockManager manages the lifecycle of a xref:storage:ShuffleClient.adoc[]: Creates when < > xref:storage:ShuffleClient.adoc#init[Inits] (and possibly < >) when requested to < > Closes when requested to < > The ShuffleClient can be an xref:storage:ExternalShuffleClient.adoc[] or the given < > based on xref:ROOT:configuration-properties.adoc#spark.shuffle.service.enabled[spark.shuffle.service.enabled] configuration property. When enabled, BlockManager uses the xref:storage:ExternalShuffleClient.adoc[]. The ShuffleClient is available to other Spark services (using shuffleClient value) and is used when BlockStoreShuffleReader is requested to xref:shuffle:BlockStoreShuffleReader.adoc#read[read combined key-value records for a reduce task]. When requested for < >, BlockManager simply requests xref:storage:ShuffleClient.adoc#shuffleMetrics[them] from the ShuffleClient. == [[rpcEnv]] BlockManager and RpcEnv BlockManager is given a xref:rpc:RpcEnv.adoc[] when < >. The RpcEnv is used to set up a < >. == [[blockInfoManager]] BlockInfoManager BlockManager creates a xref:storage:BlockInfoManager.adoc[] when < >. BlockManager requests the BlockInfoManager to xref:storage:BlockInfoManager.adoc#clear[clear] when requested to < >. BlockManager uses the BlockInfoManager to create a < >. BlockManager uses the BlockInfoManager when requested for the following: < > < > < > < > and < > < > < > < > < >, < >, < >, < > < >, < >, < >, < > == [[master]] BlockManager and BlockManagerMaster BlockManager is given a xref:storage:BlockManagerMaster.adoc[] when < >. == [[BlockDataManager]] BlockManager as BlockDataManager BlockManager is a xref:storage:BlockDataManager.adoc[]. == [[mapOutputTracker]] BlockManager and MapOutputTracker BlockManager is given a xref:scheduler:MapOutputTracker.adoc[] when < >. == [[executorId]] Executor ID BlockManager is given an Executor ID when < >. The Executor ID is one of the following: driver ( SparkContext.DRIVER_IDENTIFIER ) for the driver Value of xref:executor:CoarseGrainedExecutorBackend.adoc#executor-id[--executor-id] command-line argument for xref:executor:CoarseGrainedExecutorBackend.adoc[] executors (or xref:spark-on-mesos:spark-executor-backends-MesosExecutorBackend.adoc[MesosExecutorBackend]) == [[slaveEndpoint]] BlockManagerEndpoint RPC Endpoint BlockManager requests the < > to xref:rpc:RpcEnv.adoc#setupEndpoint[register] a xref:storage:BlockManagerSlaveEndpoint.adoc[] under the name BlockManagerEndpoint[ID] . The RPC endpoint is used when BlockManager is requested to < > and < > (to register the BlockManager on an executor with the < > on the driver). The endpoint is stopped (by requesting the < > to xref:rpc:RpcEnv.adoc#stop[stop the reference]) when BlockManager is requested to < >. == [[SparkEnv]] Accessing BlockManager Using SparkEnv BlockManager is available using xref:core:SparkEnv.adoc#blockManager[SparkEnv] on the driver and executors.","title":"BlockManager"},{"location":"storage/BlockManager/#sourceplaintext","text":"import org.apache.spark.SparkEnv val bm = SparkEnv.get.blockManager scala> :type bm org.apache.spark.storage.BlockManager == [[blockTransferService]] BlockTransferService BlockManager is given a xref:storage:BlockTransferService.adoc[BlockTransferService] when < >. BlockTransferService is used as the < > when BlockManager is configured with no external shuffle service (based on xref:ROOT:configuration-properties.adoc#spark.shuffle.service.enabled[spark.shuffle.service.enabled] configuration property). BlockTransferService is xref:storage:BlockTransferService.adoc#init[initialized] when BlockManager < >. BlockTransferService is xref:storage:BlockTransferService.adoc#close[closed] when BlockManager is requested to < >. BlockTransferService is used when BlockManager is requested to < > or < > remote block managers. == [[memoryManager]] MemoryManager BlockManager is given a xref:memory:MemoryManager.adoc[MemoryManager] when < >. BlockManager uses the MemoryManager for the following: Create the < > (that is then assigned to xref:memory:MemoryManager.adoc#setMemoryStore[MemoryManager] as a \"circular dependency\") Initialize < > and < > (for reporting) == [[shuffleManager]] ShuffleManager BlockManager is given a xref:shuffle:ShuffleManager.adoc[ShuffleManager] when < >. BlockManager uses the ShuffleManager for the following: < > (for shuffle blocks) < > (for shuffle blocks anyway) < > (when < > on an executor with < >) == [[diskBlockManager]] DiskBlockManager BlockManager creates a xref:DiskBlockManager.adoc[DiskBlockManager] when < >. .DiskBlockManager and BlockManager image::DiskBlockManager-BlockManager.png[align=\"center\"] BlockManager uses the BlockManager for the following: Creating a < > < > (when < > on an executor with < >) The BlockManager is available as diskBlockManager reference to other Spark systems.","title":"[source,plaintext]"},{"location":"storage/BlockManager/#source-scala","text":"import org.apache.spark.SparkEnv SparkEnv.get.blockManager.diskBlockManager == [[memoryStore]] MemoryStore BlockManager creates a xref:storage:MemoryStore.adoc[] when < > (with the < >, the < >, the < > and itself as a xref:storage:BlockEvictionHandler.adoc[]). .MemoryStore and BlockManager image::MemoryStore-BlockManager.png[align=\"center\"] BlockManager requests the < > to xref:memory:MemoryManager.adoc#setMemoryStore[use] the MemoryStore. BlockManager uses the MemoryStore for the following: < > and < > < > < > < > and < > < > and < > < > < > The MemoryStore is requested to xref:storage:MemoryStore.adoc#clear[clear] when BlockManager is requested to < >. The MemoryStore is available as memoryStore private reference to other Spark services.","title":"[source, scala]"},{"location":"storage/BlockManager/#source-scala_1","text":"import org.apache.spark.SparkEnv SparkEnv.get.blockManager.memoryStore The MemoryStore is used (via SparkEnv.get.blockManager.memoryStore reference) when Task is requested to xref:scheduler:Task.adoc#run[run] (that has finished and requests the MemoryStore to xref:storage:MemoryStore.adoc#releaseUnrollMemoryForThisTask[releaseUnrollMemoryForThisTask]). == [[diskStore]] DiskStore BlockManager creates a xref:DiskStore.adoc[DiskStore] (with the < >) when < >. .DiskStore and BlockManager image::DiskStore-BlockManager.png[align=\"center\"] BlockManager uses the DiskStore when requested to < >, < >, < >, < >, < >, < >, < >, < >. == [[metrics]] Performance Metrics BlockManager uses link:spark-BlockManager-BlockManagerSource.adoc[BlockManagerSource] to report metrics under the name BlockManager . == [[getPeers]] getPeers Internal Method","title":"[source, scala]"},{"location":"storage/BlockManager/#sourcescala","text":"getPeers( forceFetch: Boolean): Seq[BlockManagerId] getPeers...FIXME getPeers is used when BlockManager is requested to < > and < >. == [[releaseAllLocksForTask]] Releasing All Locks For Task","title":"[source,scala]"},{"location":"storage/BlockManager/#sourcescala_1","text":"releaseAllLocksForTask( taskAttemptId: Long): Seq[BlockId] releaseAllLocksForTask...FIXME releaseAllLocksForTask is used when TaskRunner is requested to xref:executor:TaskRunner.adoc#run[run] (at the end of a task). == [[stop]] Stopping BlockManager","title":"[source,scala]"},{"location":"storage/BlockManager/#source-scala_2","text":"","title":"[source, scala]"},{"location":"storage/BlockManager/#stop-unit","text":"stop...FIXME stop is used when SparkEnv is requested to xref:core:SparkEnv.adoc#stop[stop]. == [[getMatchingBlockIds]] Getting IDs of Existing Blocks (For a Given Filter)","title":"stop(): Unit"},{"location":"storage/BlockManager/#source-scala_3","text":"getMatchingBlockIds( filter: BlockId => Boolean): Seq[BlockId] getMatchingBlockIds...FIXME getMatchingBlockIds is used when BlockManagerSlaveEndpoint is requested to xref:storage:BlockManagerSlaveEndpoint.adoc#GetMatchingBlockIds[handle a GetMatchingBlockIds message]. == [[getLocalValues]] Getting Local Block","title":"[source, scala]"},{"location":"storage/BlockManager/#source-scala_4","text":"getLocalValues( blockId: BlockId): Option[BlockResult] getLocalValues prints out the following DEBUG message to the logs: Getting local block [blockId] getLocalValues xref:storage:BlockInfoManager.adoc#lockForReading[obtains a read lock for blockId ]. When no blockId block was found, you should see the following DEBUG message in the logs and getLocalValues returns \"nothing\" (i.e. NONE ). Block [blockId] was not found When the blockId block was found, you should see the following DEBUG message in the logs: Level for block [blockId] is [level] If blockId block has memory level and xref:storage:MemoryStore.adoc#contains[is registered in MemoryStore ], getLocalValues returns a < > as Memory read method and with a CompletionIterator for an interator: xref:storage:MemoryStore.adoc#getValues[Values iterator from MemoryStore for blockId ] for \"deserialized\" persistence levels. Iterator from xref:serializer:SerializerManager.adoc#dataDeserializeStream[ SerializerManager after the data stream has been deserialized] for the blockId block and xref:storage:MemoryStore.adoc#getBytes[the bytes for blockId block] for \"serialized\" persistence levels. getLocalValues is used when: TorrentBroadcast is requested to xref:core:TorrentBroadcast.adoc#readBroadcastBlock[readBroadcastBlock] BlockManager is requested to < > and < > === [[maybeCacheDiskValuesInMemory]] maybeCacheDiskValuesInMemory Internal Method","title":"[source, scala]"},{"location":"storage/BlockManager/#sourcescala_2","text":"maybeCacheDiskValuesInMemory T : Iterator[T] maybeCacheDiskValuesInMemory...FIXME maybeCacheDiskValuesInMemory is used when BlockManager is requested to < >. == [[getRemoteValues]] getRemoteValues Internal Method","title":"[source,scala]"},{"location":"storage/BlockManager/#source-scala_5","text":"","title":"[source, scala]"},{"location":"storage/BlockManager/#getremotevaluest-classtag-optionblockresult","text":"getRemoteValues ...FIXME == [[get]] Retrieving Block from Local or Remote Block Managers","title":"getRemoteValuesT: ClassTag: Option[BlockResult]"},{"location":"storage/BlockManager/#source-scala_6","text":"","title":"[source, scala]"},{"location":"storage/BlockManager/#gett-classtag-optionblockresult","text":"get attempts to get the blockId block from a local block manager first before requesting it from remote block managers. Internally, get tries to < >. If the block was found, you should see the following INFO message in the logs and get returns the local < >. INFO Found block [blockId] locally If however the block was not found locally, get tries to < >. If retrieved from a remote block manager, you should see the following INFO message in the logs and get returns the remote < >. INFO Found block [blockId] remotely In the end, get returns \"nothing\" (i.e. NONE ) when the blockId block was not found either in the local BlockManager or any remote BlockManager.","title":"getT: ClassTag: Option[BlockResult]"},{"location":"storage/BlockManager/#note","text":"get is used when:","title":"[NOTE]"},{"location":"storage/BlockManager/#blockmanager-is-requested-to-and","text":"== [[getBlockData]] Retrieving Block Data","title":"* BlockManager is requested to &lt;&gt; and &lt;&gt;"},{"location":"storage/BlockManager/#source-scala_7","text":"getBlockData( blockId: BlockId): ManagedBuffer NOTE: getBlockData is part of the xref:storage:BlockDataManager.adoc#getBlockData[BlockDataManager] contract. For a xref:BlockId.adoc[] of a shuffle (a ShuffleBlockId), getBlockData requests the < > for the xref:shuffle:ShuffleManager.adoc#shuffleBlockResolver[ShuffleBlockResolver] that is then requested for xref:shuffle:ShuffleBlockResolver.adoc#getBlockData[getBlockData]. Otherwise, getBlockData < > for the given BlockId. If found, getBlockData creates a new BlockManagerManagedBuffer (with the < >, the input BlockId, the retrieved BlockData and the dispose flag enabled). If not found, getBlockData < > that the block could not be found (and that the master should no longer assume the block is available on this executor) and throws a BlockNotFoundException. NOTE: getBlockData is executed for shuffle blocks or local blocks that the BlockManagerMaster knows this executor really has (unless BlockManagerMaster is outdated). == [[getLocalBytes]] Retrieving Non-Shuffle Local Block Data","title":"[source, scala]"},{"location":"storage/BlockManager/#source-scala_8","text":"getLocalBytes( blockId: BlockId): Option[BlockData] getLocalBytes ...FIXME","title":"[source, scala]"},{"location":"storage/BlockManager/#note_1","text":"getLocalBytes is used when: TorrentBroadcast is requested to xref:core:TorrentBroadcast.adoc#readBlocks[readBlocks]","title":"[NOTE]"},{"location":"storage/BlockManager/#blockmanager-is-requested-for-the-of-a-non-shuffle-block","text":"== [[removeBlockInternal]] removeBlockInternal Internal Method","title":"* BlockManager is requested for the &lt;&gt; (of a non-shuffle block)"},{"location":"storage/BlockManager/#source-scala_9","text":"removeBlockInternal( blockId: BlockId, tellMaster: Boolean): Unit removeBlockInternal...FIXME removeBlockInternal is used when BlockManager is requested to < > and < >. == [[stores]] Stores A Store is the place where blocks are held. There are the following possible stores: xref:storage:MemoryStore.adoc[MemoryStore] for memory storage level. xref:DiskStore.adoc[DiskStore] for disk storage level. ExternalBlockStore for OFF_HEAP storage level. == [[putBlockData]] Storing Block Data Locally","title":"[source, scala]"},{"location":"storage/BlockManager/#source-scala_10","text":"putBlockData( blockId: BlockId, data: ManagedBuffer, level: StorageLevel, classTag: ClassTag[_]): Boolean putBlockData simply < blockId locally>> (given the given storage level ). NOTE: putBlockData is part of the xref:storage:BlockDataManager.adoc#putBlockData[BlockDataManager Contract]. Internally, putBlockData wraps ChunkedByteBuffer around data buffer's NIO ByteBuffer and calls < >. == [[putBytes]] Storing Block Bytes Locally","title":"[source, scala]"},{"location":"storage/BlockManager/#source-scala_11","text":"putBytes( blockId: BlockId, bytes: ChunkedByteBuffer, level: StorageLevel, tellMaster: Boolean = true): Boolean putBytes makes sure that the bytes are not null and < >.","title":"[source, scala]"},{"location":"storage/BlockManager/#note_2","text":"putBytes is used when: BlockManager is requested to < > TaskRunner is requested to xref:executor:TaskRunner.adoc#run-result-sent-via-blockmanager[run] (and the result size is above xref:executor:Executor.adoc#maxDirectResultSize[maxDirectResultSize])","title":"[NOTE]"},{"location":"storage/BlockManager/#torrentbroadcast-is-requested-to-xrefcoretorrentbroadcastadocwriteblockswriteblocks-and-xrefcoretorrentbroadcastadocreadblocksreadblocks","text":"=== [[doPutBytes]] doPutBytes Internal Method","title":"* TorrentBroadcast is requested to xref:core:TorrentBroadcast.adoc#writeBlocks[writeBlocks] and xref:core:TorrentBroadcast.adoc#readBlocks[readBlocks]"},{"location":"storage/BlockManager/#source-scala_12","text":"doPutBytes T : Boolean doPutBytes calls the internal helper < > with a function that accepts a BlockInfo and does the uploading. Inside the function, if the xref:storage:StorageLevel.adoc[storage level ]'s replication is greater than 1, it immediately starts < > of the blockId block on a separate thread (from futureExecutionContext thread pool). The replication uses the input bytes and level storage level. For a memory storage level, the function checks whether the storage level is deserialized or not. For a deserialized storage level , BlockManager 's xref:serializer:SerializerManager.adoc#dataDeserializeStream[ SerializerManager deserializes bytes into an iterator of values] that xref:storage:MemoryStore.adoc#putIteratorAsValues[ MemoryStore stores]. If however the storage level is not deserialized, the function requests xref:storage:MemoryStore.adoc#putBytes[ MemoryStore to store the bytes] If the put did not succeed and the storage level is to use disk, you should see the following WARN message in the logs: WARN BlockManager: Persisting block [blockId] to disk instead. And xref:DiskStore.adoc#putBytes[ DiskStore stores the bytes]. NOTE: xref:DiskStore.adoc[DiskStore] is requested to store the bytes of a block with memory and disk storage level only when xref:storage:MemoryStore.adoc[MemoryStore] has failed. If the storage level is to use disk only, xref:DiskStore.adoc#putBytes[ DiskStore stores the bytes]. doPutBytes requests < > and if the block was successfully stored, and the driver should know about it ( tellMaster ), the function < >. The xref:executor:TaskMetrics.adoc#incUpdatedBlockStatuses[current TaskContext metrics are updated with the updated block status] (only when executed inside a task where TaskContext is available). You should see the following DEBUG message in the logs: DEBUG BlockManager: Put block [blockId] locally took [time] ms The function waits till the earlier asynchronous replication finishes for a block with replication level greater than 1 . The final result of doPutBytes is the result of storing the block successful or not (as computed earlier). NOTE: doPutBytes is used exclusively when BlockManager is requested to < >. == [[doPut]] doPut Internal Method","title":"[source, scala]"},{"location":"storage/BlockManager/#source-scala_13","text":"doPut T (putBody: BlockInfo => Option[T]): Option[T] doPut executes the input putBody function with a xref:storage:BlockInfo.adoc[] being a new BlockInfo object (with level storage level) that xref:storage:BlockInfoManager.adoc#lockNewBlockForWriting[ BlockInfoManager managed to create a write lock for]. If the block has already been created (and xref:storage:BlockInfoManager.adoc#lockNewBlockForWriting[ BlockInfoManager did not manage to create a write lock for]), the following WARN message is printed out to the logs:","title":"[source, scala]"},{"location":"storage/BlockManager/#sourceplaintext_1","text":"","title":"[source,plaintext]"},{"location":"storage/BlockManager/#block-blockid-already-exists-on-this-machine-not-re-adding-it","text":"doPut < > when keepReadLock flag is disabled and returns None immediately. If however the write lock has been given, doPut executes putBody . If the result of putBody is None the block is considered saved successfully. For successful save and keepReadLock enabled, xref:storage:BlockInfoManager.adoc#downgradeLock[ BlockInfoManager is requested to downgrade an exclusive write lock for blockId to a shared read lock]. For successful save and keepReadLock disabled, xref:storage:BlockInfoManager.adoc#unlock[ BlockInfoManager is requested to release lock on blockId ]. For unsuccessful save, < > and the following WARN message is printed out to the logs:","title":"Block [blockId] already exists on this machine; not re-adding it"},{"location":"storage/BlockManager/#sourceplaintext_2","text":"","title":"[source,plaintext]"},{"location":"storage/BlockManager/#putting-block-blockid-failed","text":"In the end, doPut prints out the following DEBUG message to the logs:","title":"Putting block [blockId] failed"},{"location":"storage/BlockManager/#sourceplaintext_3","text":"","title":"[source,plaintext]"},{"location":"storage/BlockManager/#putting-block-blockid-withorwithout-replication-took-usedtime-ms","text":"doPut is used when BlockManager is requested to < > and < >. == [[removeBlock]] Removing Block From Memory and Disk","title":"Putting block [blockId] [withOrWithout] replication took [usedTime] ms"},{"location":"storage/BlockManager/#source-scala_14","text":"removeBlock( blockId: BlockId, tellMaster: Boolean = true): Unit removeBlock removes the blockId block from the xref:storage:MemoryStore.adoc[MemoryStore] and xref:DiskStore.adoc[DiskStore]. When executed, it prints out the following DEBUG message to the logs: Removing block [blockId] It requests xref:storage:BlockInfoManager.adoc[] for lock for writing for the blockId block. If it receives none, it prints out the following WARN message to the logs and quits. Asked to remove block [blockId], which does not exist Otherwise, with a write lock for the block, the block is removed from xref:storage:MemoryStore.adoc[MemoryStore] and xref:DiskStore.adoc[DiskStore] (see xref:storage:MemoryStore.adoc#remove[Removing Block in MemoryStore ] and xref:DiskStore.adoc#remove[Removing Block in DiskStore ]). If both removals fail, it prints out the following WARN message: Block [blockId] could not be removed as it was not found in either the disk, memory, or external block store The block is removed from xref:storage:BlockInfoManager.adoc[]. removeBlock then < > that is used to < > (if the input tellMaster and the info's tellMaster are both enabled, i.e. true ) and the xref:executor:TaskMetrics.adoc#incUpdatedBlockStatuses[current TaskContext metrics are updated with the change]. removeBlock is used when: BlockManager is requested to < >, < > and < > BlockManagerSlaveEndpoint is requested to handle a xref:storage:BlockManagerSlaveEndpoint.adoc#RemoveBlock[RemoveBlock] message == [[removeRdd]] Removing RDD Blocks","title":"[source, scala]"},{"location":"storage/BlockManager/#source-scala_15","text":"","title":"[source, scala]"},{"location":"storage/BlockManager/#removerddrddid-int-int","text":"removeRdd removes all the blocks that belong to the rddId RDD. It prints out the following INFO message to the logs: INFO Removing RDD [rddId] It then requests RDD blocks from xref:storage:BlockInfoManager.adoc[] and < > (without informing the driver). The number of blocks removed is the final result. NOTE: It is used by xref:storage:BlockManagerSlaveEndpoint.adoc#RemoveRdd[ BlockManagerSlaveEndpoint while handling RemoveRdd messages]. == [[removeBroadcast]] Removing All Blocks of Broadcast Variable","title":"removeRdd(rddId: Int): Int"},{"location":"storage/BlockManager/#source-scala_16","text":"","title":"[source, scala]"},{"location":"storage/BlockManager/#removebroadcastbroadcastid-long-tellmaster-boolean-int","text":"removeBroadcast removes all the blocks of the input broadcastId broadcast. Internally, it starts by printing out the following DEBUG message to the logs: Removing broadcast [broadcastId] It then requests all the xref:storage:BlockId.adoc#BroadcastBlockId[BroadcastBlockId] objects that belong to the broadcastId broadcast from xref:storage:BlockInfoManager.adoc[] and < >. The number of blocks removed is the final result. NOTE: It is used by xref:storage:BlockManagerSlaveEndpoint.adoc#RemoveBroadcast[ BlockManagerSlaveEndpoint while handling RemoveBroadcast messages]. == [[shuffleServerId]] BlockManagerId of Shuffle Server BlockManager uses xref:storage:BlockManagerId.adoc[] for the location (address) of the server that serves shuffle files of this executor. The BlockManagerId is either the BlockManagerId of the external shuffle service (when < >) or the < >. The BlockManagerId of the Shuffle Server is used for the location of a xref:scheduler:MapStatus.adoc[shuffle map output] when: BypassMergeSortShuffleWriter is requested to xref:shuffle:BypassMergeSortShuffleWriter.adoc#write[write partition records to a shuffle file] UnsafeShuffleWriter is requested to xref:shuffle:UnsafeShuffleWriter.adoc#closeAndWriteOutput[close and write output] == [[getStatus]] getStatus Method","title":"removeBroadcast(broadcastId: Long, tellMaster: Boolean): Int"},{"location":"storage/BlockManager/#sourcescala_3","text":"getStatus( blockId: BlockId): Option[BlockStatus] getStatus...FIXME getStatus is used when BlockManagerSlaveEndpoint is requested to handle xref:storage:BlockManagerSlaveEndpoint.adoc#GetBlockStatus[GetBlockStatus] message. == [[initialize]] Initializing BlockManager","title":"[source,scala]"},{"location":"storage/BlockManager/#source-scala_17","text":"initialize( appId: String): Unit initialize initializes a BlockManager on the driver and executors (see xref:ROOT:SparkContext.adoc#creating-instance[Creating SparkContext Instance] and xref:executor:Executor.adoc#creating-instance[Creating Executor Instance], respectively). NOTE: The method must be called before a BlockManager can be considered fully operable. initialize does the following in order: Initializes xref:storage:BlockTransferService.adoc#init[BlockTransferService] Initializes the internal shuffle client, be it xref:storage:ExternalShuffleClient.adoc[ExternalShuffleClient] or xref:storage:BlockTransferService.adoc[BlockTransferService]. xref:BlockManagerMaster.adoc#registerBlockManager[Registers itself with the driver's BlockManagerMaster ] (using the id , maxMemory and its slaveEndpoint ). + The BlockManagerMaster reference is passed in when the < > on the driver and executors. Sets < > to an instance of xref:storage:BlockManagerId.adoc[] given an executor id, host name and port for xref:storage:BlockTransferService.adoc[BlockTransferService]. It creates the address of the server that serves this executor's shuffle files (using < >) CAUTION: FIXME Review the initialize procedure again CAUTION: FIXME Describe shuffleServerId . Where is it used? If the < >, initialize prints out the following INFO message to the logs:","title":"[source, scala]"},{"location":"storage/BlockManager/#sourceplaintext_4","text":"","title":"[source,plaintext]"},{"location":"storage/BlockManager/#external-shuffle-service-port-externalshuffleserviceport","text":"It xref:BlockManagerMaster.adoc#registerBlockManager[registers itself to the driver's BlockManagerMaster] passing the xref:storage:BlockManagerId.adoc[], the maximum memory (as maxMemory ), and the xref:storage:BlockManagerSlaveEndpoint.adoc[]. Ultimately, if the initialization happens on an executor and the < >, it < >. initialize is used when the link:spark-SparkContext-creating-instance-internals.adoc#BlockManager-initialization[driver is launched (and SparkContext is created)] and when an xref:executor:Executor.adoc#creating-instance[ Executor is created] (for xref:executor:CoarseGrainedExecutorBackend.adoc#RegisteredExecutor[CoarseGrainedExecutorBackend] and xref:spark-on-mesos:spark-executor-backends-MesosExecutorBackend.adoc[MesosExecutorBackend]). == [[registerWithExternalShuffleServer]] Registering Executor's BlockManager with External Shuffle Server","title":"external shuffle service port = [externalShuffleServicePort]"},{"location":"storage/BlockManager/#source-scala_18","text":"","title":"[source, scala]"},{"location":"storage/BlockManager/#registerwithexternalshuffleserver-unit","text":"registerWithExternalShuffleServer is an internal helper method to register the BlockManager for an executor with an xref:deploy:ExternalShuffleService.adoc[external shuffle server]. NOTE: It is executed when a < >. When executed, you should see the following INFO message in the logs: Registering executor with local external shuffle service. It uses < > to xref:storage:ExternalShuffleClient.adoc#registerWithShuffleServer[register the block manager] using < > (i.e. the host, the port and the executorId) and a ExecutorShuffleInfo . NOTE: The ExecutorShuffleInfo uses localDirs and subDirsPerLocalDir from xref:DiskBlockManager.adoc[DiskBlockManager] and the class name of the constructor xref:shuffle:ShuffleManager.adoc[ShuffleManager]. It tries to register at most 3 times with 5-second sleeps in-between. NOTE: The maximum number of attempts and the sleep time in-between are hard-coded, i.e. they are not configured. Any issues while connecting to the external shuffle service are reported as ERROR messages in the logs: Failed to connect to external shuffle server, will retry [#attempts] more times after waiting 5 seconds... registerWithExternalShuffleServer is used when BlockManager is requested to < > (when executed on an executor with < >). == [[reregister]] Re-registering BlockManager with Driver and Reporting Blocks","title":"registerWithExternalShuffleServer(): Unit"},{"location":"storage/BlockManager/#source-scala_19","text":"","title":"[source, scala]"},{"location":"storage/BlockManager/#reregister-unit","text":"When executed, reregister prints the following INFO message to the logs: BlockManager [blockManagerId] re-registering with master reregister then xref:BlockManagerMaster.adoc#registerBlockManager[registers itself to the driver's BlockManagerMaster ] (just as it was when < >). It passes the xref:storage:BlockManagerId.adoc[], the maximum memory (as maxMemory ), and the xref:storage:BlockManagerSlaveEndpoint.adoc[]. reregister will then report all the local blocks to the xref:BlockManagerMaster.adoc[BlockManagerMaster]. You should see the following INFO message in the logs: Reporting [blockInfoManager.size] blocks to the master. For each block metadata (in xref:storage:BlockInfoManager.adoc[]) it < > and < >. If there is an issue communicating to the xref:BlockManagerMaster.adoc[BlockManagerMaster], you should see the following ERROR message in the logs: Failed to report [blockId] to master; giving up. After the ERROR message, reregister stops reporting. reregister is used when a xref:executor:Executor.adoc#heartbeats-and-active-task-metrics[ Executor was informed to re-register while sending heartbeats]. == [[getCurrentBlockStatus]] Calculate Current Block Status","title":"reregister(): Unit"},{"location":"storage/BlockManager/#source-scala_20","text":"getCurrentBlockStatus( blockId: BlockId, info: BlockInfo): BlockStatus getCurrentBlockStatus gives the current BlockStatus of the BlockId block (with the block's current xref:storage:StorageLevel.adoc[StorageLevel], memory and disk sizes). It uses xref:storage:MemoryStore.adoc[MemoryStore] and xref:DiskStore.adoc[DiskStore] for size and other information. NOTE: Most of the information to build BlockStatus is already in BlockInfo except that it may not necessarily reflect the current state per xref:storage:MemoryStore.adoc[MemoryStore] and xref:DiskStore.adoc[DiskStore]. Internally, it uses the input xref:storage:BlockInfo.adoc[] to know about the block's storage level. If the storage level is not set (i.e. null ), the returned BlockStatus assumes the xref:storage:StorageLevel.adoc[default NONE storage level] and the memory and disk sizes being 0 . If however the storage level is set, getCurrentBlockStatus uses xref:storage:MemoryStore.adoc[MemoryStore] and xref:DiskStore.adoc[DiskStore] to check whether the block is stored in the storages or not and request for their sizes in the storages respectively (using their getSize or assume 0 ). NOTE: It is acceptable that the BlockInfo says to use memory or disk yet the block is not in the storages (yet or anymore). The method will give current status. getCurrentBlockStatus is used when < >, < > or < > or < >. == [[reportAllBlocks]] reportAllBlocks Internal Method","title":"[source, scala]"},{"location":"storage/BlockManager/#source-scala_21","text":"","title":"[source, scala]"},{"location":"storage/BlockManager/#reportallblocks-unit","text":"reportAllBlocks...FIXME reportAllBlocks is used when BlockManager is requested to < >. == [[reportBlockStatus]] Reporting Current Storage Status of Block to Driver","title":"reportAllBlocks(): Unit"},{"location":"storage/BlockManager/#source-scala_22","text":"reportBlockStatus( blockId: BlockId, info: BlockInfo, status: BlockStatus, droppedMemorySize: Long = 0L): Unit reportBlockStatus is an internal method for < > and if told to re-register it prints out the following INFO message to the logs: Got told to re-register updating block [blockId] It does asynchronous reregistration (using asyncReregister ). In either case, it prints out the following DEBUG message to the logs: Told master about block [blockId] reportBlockStatus is used when BlockManager is requested to < >, < >, < >, < > and < >. == [[tryToReportBlockStatus]] Reporting Block Status Update to Driver","title":"[source, scala]"},{"location":"storage/BlockManager/#source-scala_23","text":"def tryToReportBlockStatus( blockId: BlockId, info: BlockInfo, status: BlockStatus, droppedMemorySize: Long = 0L): Boolean tryToReportBlockStatus xref:BlockManagerMaster.adoc#updateBlockInfo[reports block status update] to < > and returns its response. tryToReportBlockStatus is used when BlockManager is requested to < > or < >. == [[execution-context]] Execution Context block-manager-future is the execution context for...FIXME == [[ByteBuffer]] ByteBuffer The underlying abstraction for blocks in Spark is a ByteBuffer that limits the size of a block to 2GB ( Integer.MAX_VALUE - see http://stackoverflow.com/q/8076472/1305344[Why does FileChannel.map take up to Integer.MAX_VALUE of data?] and https://issues.apache.org/jira/browse/SPARK-1476[SPARK-1476 2GB limit in spark for blocks]). This has implication not just for managed blocks in use, but also for shuffle blocks (memory mapped blocks are limited to 2GB, even though the API allows for long ), ser-deser via byte array-backed output streams. == [[BlockResult]] BlockResult BlockResult is a description of a fetched block with the readMethod and bytes . == [[registerTask]] Registering Task","title":"[source, scala]"},{"location":"storage/BlockManager/#source-scala_24","text":"registerTask( taskAttemptId: Long): Unit registerTask requests the < > to xref:storage:BlockInfoManager.adoc#registerTask[register a given task]. registerTask is used when Task is requested to xref:scheduler:Task.adoc#run[run] (at the start of a task). == [[getDiskWriter]] Creating DiskBlockObjectWriter","title":"[source, scala]"},{"location":"storage/BlockManager/#source-scala_25","text":"getDiskWriter( blockId: BlockId, file: File, serializerInstance: SerializerInstance, bufferSize: Int, writeMetrics: ShuffleWriteMetrics): DiskBlockObjectWriter getDiskWriter creates a xref:storage:DiskBlockObjectWriter.adoc[DiskBlockObjectWriter] (with xref:ROOT:configuration-properties.adoc#spark.shuffle.sync[spark.shuffle.sync] configuration property for syncWrites argument). getDiskWriter uses the < > of the BlockManager. getDiskWriter is used when: BypassMergeSortShuffleWriter is requested to xref:shuffle:BypassMergeSortShuffleWriter.adoc#write[write records (of a partition)] ShuffleExternalSorter is requested to xref:shuffle:ShuffleExternalSorter.adoc#writeSortedFile[writeSortedFile] ExternalAppendOnlyMap is requested to xref:shuffle:ExternalAppendOnlyMap.adoc#spillMemoryIteratorToDisk[spillMemoryIteratorToDisk] ExternalSorter is requested to xref:shuffle:ExternalSorter.adoc#spillMemoryIteratorToDisk[spillMemoryIteratorToDisk] and xref:shuffle:ExternalSorter.adoc#writePartitionedFile[writePartitionedFile] xref:memory:UnsafeSorterSpillWriter.adoc[UnsafeSorterSpillWriter] is created == [[addUpdatedBlockStatusToTaskMetrics]] Recording Updated BlockStatus In Current Task's TaskMetrics","title":"[source, scala]"},{"location":"storage/BlockManager/#source-scala_26","text":"addUpdatedBlockStatusToTaskMetrics( blockId: BlockId, status: BlockStatus): Unit addUpdatedBlockStatusToTaskMetrics link:spark-TaskContext.adoc#get[takes an active TaskContext ] (if available) and xref:executor:TaskMetrics.adoc#incUpdatedBlockStatuses[records updated BlockStatus for Block ] (in the link:spark-TaskContext.adoc#taskMetrics[task's TaskMetrics ]). addUpdatedBlockStatusToTaskMetrics is used when BlockManager < > (for a block that was successfully stored), < >, < >, < > (possibly spilling it to disk) and < >. == [[shuffleMetricsSource]] Requesting Shuffle-Related Spark Metrics Source","title":"[source, scala]"},{"location":"storage/BlockManager/#source-scala_27","text":"","title":"[source, scala]"},{"location":"storage/BlockManager/#shufflemetricssource-source","text":"shuffleMetricsSource requests the < > for the xref:storage:ShuffleClient.adoc#shuffleMetrics[shuffle metrics] and creates a xref:storage:ShuffleMetricsSource.adoc[] with the xref:storage:ShuffleMetricsSource.adoc#sourceName[source name] based on xref:ROOT:configuration-properties.adoc#spark.shuffle.service.enabled[spark.shuffle.service.enabled] configuration property: ExternalShuffle when xref:ROOT:configuration-properties.adoc#spark.shuffle.service.enabled[spark.shuffle.service.enabled] configuration property is on ( true ) NettyBlockTransfer when xref:ROOT:configuration-properties.adoc#spark.shuffle.service.enabled[spark.shuffle.service.enabled] configuration property is off ( false ) shuffleMetricsSource is used when Executor is xref:executor:Executor.adoc#creating-instance[created] (for non-local / cluster modes). == [[replicate]] Replicating Block To Peers","title":"shuffleMetricsSource: Source"},{"location":"storage/BlockManager/#source-scala_28","text":"replicate( blockId: BlockId, data: BlockData, level: StorageLevel, classTag: ClassTag[_], existingReplicas: Set[BlockManagerId] = Set.empty): Unit replicate...FIXME replicate is used when BlockManager is requested to < >, < > and < >. == [[replicateBlock]] replicateBlock Method","title":"[source, scala]"},{"location":"storage/BlockManager/#source-scala_29","text":"replicateBlock( blockId: BlockId, existingReplicas: Set[BlockManagerId], maxReplicas: Int): Unit replicateBlock...FIXME replicateBlock is used when BlockManagerSlaveEndpoint is requested to xref:storage:BlockManagerSlaveEndpoint.adoc#ReplicateBlock[handle a ReplicateBlock message]. == [[putIterator]] putIterator Method","title":"[source, scala]"},{"location":"storage/BlockManager/#source-scala_30","text":"putIterator T: ClassTag : Boolean putIterator ...FIXME","title":"[source, scala]"},{"location":"storage/BlockManager/#note_3","text":"putIterator is used when: BlockManager is requested to < >","title":"[NOTE]"},{"location":"storage/BlockManager/#spark-streamings-blockmanagerbasedblockhandler-is-requested-to-storeblock","text":"== [[putSingle]] putSingle Method","title":"* Spark Streaming's BlockManagerBasedBlockHandler is requested to storeBlock"},{"location":"storage/BlockManager/#source-scala_31","text":"putSingle T: ClassTag : Boolean putSingle...FIXME putSingle is used when TorrentBroadcast is requested to xref:core:TorrentBroadcast.adoc#writeBlocks[write the blocks] and xref:core:TorrentBroadcast.adoc#readBroadcastBlock[readBroadcastBlock]. == [[getRemoteBytes]] Fetching Block From Remote Nodes","title":"[source, scala]"},{"location":"storage/BlockManager/#source-scala_32","text":"","title":"[source, scala]"},{"location":"storage/BlockManager/#getremotebytesblockid-blockid-optionchunkedbytebuffer","text":"getRemoteBytes ...FIXME","title":"getRemoteBytes(blockId: BlockId): Option[ChunkedByteBuffer]"},{"location":"storage/BlockManager/#note_4","text":"getRemoteBytes is used when: BlockManager is requested to < > TorrentBroadcast is requested to xref:core:TorrentBroadcast.adoc#readBlocks[readBlocks]","title":"[NOTE]"},{"location":"storage/BlockManager/#taskresultgetter-is-requested-to-xrefschedulertaskresultgetteradocenqueuesuccessfultaskenqueuing-a-successful-indirecttaskresult","text":"== [[getRemoteValues]] getRemoteValues Internal Method","title":"* TaskResultGetter is requested to xref:scheduler:TaskResultGetter.adoc#enqueueSuccessfulTask[enqueuing a successful IndirectTaskResult]"},{"location":"storage/BlockManager/#source-scala_33","text":"","title":"[source, scala]"},{"location":"storage/BlockManager/#getremotevaluest-classtag-optionblockresult_1","text":"getRemoteValues ...FIXME NOTE: getRemoteValues is used exclusively when BlockManager is requested to < >. == [[getSingle]] getSingle Method","title":"getRemoteValuesT: ClassTag: Option[BlockResult]"},{"location":"storage/BlockManager/#source-scala_34","text":"","title":"[source, scala]"},{"location":"storage/BlockManager/#getsinglet-classtag-optiont","text":"getSingle ...FIXME NOTE: getSingle is used exclusively in Spark tests. == [[getOrElseUpdate]] Getting Block From Block Managers Or Computing and Storing It Otherwise","title":"getSingleT: ClassTag: Option[T]"},{"location":"storage/BlockManager/#source-scala_35","text":"getOrElseUpdate T : Either[BlockResult, Iterator[T]]","title":"[source, scala]"},{"location":"storage/BlockManager/#note_5","text":"I think it is fair to say that getOrElseUpdate is like link:++ https://www.scala-lang.org/api/current/scala/collection/mutable/Map.html#getOrElseUpdate(key:K,op:=%3EV):V++[getOrElseUpdate ] of https://www.scala-lang.org/api/current/scala/collection/mutable/Map.html[scala.collection.mutable.Map ] in Scala.","title":"[NOTE]"},{"location":"storage/BlockManager/#source-scala_36","text":"","title":"[source, scala]"},{"location":"storage/BlockManager/#getorelseupdatekey-k-op-v-v","text":"Quoting the official scaladoc: If given key K is already in this map, getOrElseUpdate returns the associated value V . Otherwise, getOrElseUpdate computes a value V from given expression op , stores with the key K in the map and returns that value.","title":"getOrElseUpdate(key: K, op: \u21d2 V): V"},{"location":"storage/BlockManager/#since-blockmanager-is-a-key-value-store-of-blocks-of-data-identified-by-a-block-id-that-works-just-fine","text":"getOrElseUpdate first attempts to < > by the BlockId (from the local block manager first and, if unavailable, requesting remote peers).","title":"Since BlockManager is a key-value store of blocks of data identified by a block ID that works just fine."},{"location":"storage/BlockManager/#tip","text":"Enable INFO logging level for org.apache.spark.storage.BlockManager logger to see what happens when BlockManager tries to < >.","title":"[TIP]"},{"location":"storage/BlockManager/#see-in-this-document","text":"getOrElseUpdate gives the BlockResult of the block if found. If however the block was not found (in any block manager in a Spark cluster), getOrElseUpdate < > (for the input BlockId , the makeIterator function and the StorageLevel ). getOrElseUpdate branches off per the result. For None , getOrElseUpdate < > for the BlockId and eventually returns the BlockResult (unless terminated by a SparkException due to some internal error). For Some(iter) , getOrElseUpdate returns an iterator of T values. NOTE: getOrElseUpdate is used exclusively when RDD is requested to xref:rdd:RDD.adoc#getOrCompute[get or compute an RDD partition] (for a RDDBlockId with a RDD ID and a partition index). == [[doPutIterator]] doPutIterator Internal Method","title":"See &lt;&gt; in this document."},{"location":"storage/BlockManager/#source-scala_37","text":"doPutIterator T : Option[PartiallyUnrolledIterator[T]] doPutIterator simply < > with the putBody function that accepts a BlockInfo and does the following: . putBody branches off per whether the StorageLevel indicates to use a xref:storage:StorageLevel.adoc#useMemory[memory] or simply a xref:storage:StorageLevel.adoc#useDisk[disk], i.e. When the input StorageLevel indicates to xref:storage:StorageLevel.adoc#useMemory[use a memory] for storage in xref:storage:StorageLevel.adoc#deserialized[deserialized] format, putBody requests < > to xref:storage:MemoryStore.adoc#putIteratorAsValues[putIteratorAsValues] (for the BlockId and with the iterator factory function). + If the < > returned a correct value, the internal size is set to the value. + If however the < > failed to give a correct value, FIXME When the input StorageLevel indicates to xref:storage:StorageLevel.adoc#useMemory[use memory] for storage in xref:storage:StorageLevel.adoc#deserialized[serialized] format, putBody ...FIXME When the input StorageLevel does not indicate to use memory for storage but xref:storage:StorageLevel.adoc#useDisk[disk] instead, putBody ...FIXME . putBody requests the < > . Only when the block was successfully stored in either the memory or disk store: putBody < > to the < > when the input tellMaster flag (default: enabled) and the tellMaster flag of the block info are both enabled. putBody < > (with the BlockId and BlockStatus ) putBody prints out the following DEBUG message to the logs: + Put block [blockId] locally took [time] ms When the input StorageLevel indicates to use xref:storage:StorageLevel.adoc#replication[replication], putBody < > followed by < > (with the input BlockId and the StorageLevel as well as the BlockData to replicate) With a successful replication, putBody prints out the following DEBUG message to the logs: + Put block [blockId] remotely took [time] ms . In the end, putBody may or may not give a PartiallyUnrolledIterator if...FIXME NOTE: doPutIterator is used when BlockManager is requested to < > and < >. == [[dropFromMemory]] Dropping Block from Memory","title":"[source, scala]"},{"location":"storage/BlockManager/#sourcescala_4","text":"dropFromMemory( blockId: BlockId, data: () => Either[Array[T], ChunkedByteBuffer]): StorageLevel dropFromMemory prints out the following INFO message to the logs:","title":"[source,scala]"},{"location":"storage/BlockManager/#sourceplaintext_5","text":"","title":"[source,plaintext]"},{"location":"storage/BlockManager/#dropping-block-blockid-from-memory","text":"dropFromMemory then asserts that the given block is xref:storage:BlockInfoManager.adoc#assertBlockIsLockedForWriting[locked for writing]. If the block's xref:storage:StorageLevel.adoc[StorageLevel] uses disks and the internal xref:DiskStore.adoc[DiskStore] object ( diskStore ) does not contain the block, it is saved then. You should see the following INFO message in the logs: Writing block [blockId] to disk CAUTION: FIXME Describe the case with saving a block to disk. The block's memory size is fetched and recorded (using MemoryStore.getSize ). The block is xref:storage:MemoryStore.adoc#remove[removed from memory] if exists. If not, you should see the following WARN message in the logs: Block [blockId] could not be dropped from memory as it does not exist It then < > and < >. It only happens when info.tellMaster . CAUTION: FIXME When would info.tellMaster be true ? A block is considered updated when it was written to disk or removed from memory or both. If either happened, the xref:executor:TaskMetrics.adoc#incUpdatedBlockStatuses[current TaskContext metrics are updated with the change]. In the end, dropFromMemory returns the current storage level of the block. dropFromMemory is part of the xref:storage:BlockEvictionHandler.adoc#dropFromMemory[BlockEvictionHandler] abstraction. == [[handleLocalReadFailure]] handleLocalReadFailure Internal Method","title":"Dropping block [blockId] from memory"},{"location":"storage/BlockManager/#source-scala_38","text":"","title":"[source, scala]"},{"location":"storage/BlockManager/#handlelocalreadfailureblockid-blockid-nothing","text":"handleLocalReadFailure ...FIXME NOTE: handleLocalReadFailure is used when...FIXME == [[releaseLockAndDispose]] releaseLockAndDispose Method","title":"handleLocalReadFailure(blockId: BlockId): Nothing"},{"location":"storage/BlockManager/#source-scala_39","text":"releaseLockAndDispose( blockId: BlockId, data: BlockData, taskAttemptId: Option[Long] = None): Unit releaseLockAndDispose...FIXME releaseLockAndDispose is used when...FIXME == [[releaseLock]] releaseLock Method","title":"[source, scala]"},{"location":"storage/BlockManager/#source-scala_40","text":"releaseLock( blockId: BlockId, taskAttemptId: Option[Long] = None): Unit releaseLock requests the < > to xref:storage:BlockInfoManager.adoc#unlock[unlock the given block]. releaseLock is part of the xref:storage:BlockDataManager.adoc#releaseLock[BlockDataManager] abstraction. == [[putBlockDataAsStream]] putBlockDataAsStream Method","title":"[source, scala]"},{"location":"storage/BlockManager/#sourcescala_5","text":"putBlockDataAsStream( blockId: BlockId, level: StorageLevel, classTag: ClassTag[_]): StreamCallbackWithID putBlockDataAsStream...FIXME putBlockDataAsStream is part of the xref:storage:BlockDataManager.adoc#putBlockDataAsStream[BlockDataManager] abstraction. == [[downgradeLock]] downgradeLock Method","title":"[source,scala]"},{"location":"storage/BlockManager/#source-scala_41","text":"downgradeLock( blockId: BlockId): Unit downgradeLock requests the < > to xref:storage:BlockInfoManager.adoc#downgradeLock[downgradeLock] for the given xref:storage:BlockId.adoc[block]. downgradeLock seems not to be used. == [[blockIdsToLocations]] blockIdsToLocations Utility","title":"[source, scala]"},{"location":"storage/BlockManager/#sourcescala_6","text":"blockIdsToLocations( blockIds: Array[BlockId], env: SparkEnv, blockManagerMaster: BlockManagerMaster = null): Map[BlockId, Seq[String]] blockIdsToLocations...FIXME blockIdsToLocations is used in the now defunct Spark Streaming (when BlockRDD is requested for _locations). === [[getLocationBlockIds]] getLocationBlockIds Internal Method","title":"[source,scala]"},{"location":"storage/BlockManager/#sourcescala_7","text":"getLocationBlockIds( blockIds: Array[BlockId]): Array[Seq[BlockManagerId]] getLocationBlockIds...FIXME getLocationBlockIds is used when BlockManager utility is requested to < > (for the now defunct Spark Streaming). == [[logging]] Logging Enable ALL logging level for org.apache.spark.storage.BlockManager logger to see what happens inside. Add the following line to conf/log4j.properties :","title":"[source,scala]"},{"location":"storage/BlockManager/#sourceplaintext_6","text":"","title":"[source,plaintext]"},{"location":"storage/BlockManager/#log4jloggerorgapachesparkstorageblockmanagerall","text":"Refer to xref:ROOT:spark-logging.adoc[Logging]. == [[internal-properties]] Internal Properties === [[maxMemory]] Maximum Memory Total maximum value that BlockManager can ever possibly use (that depends on < > and may vary over time). Total available xref:memory:MemoryManager.adoc#maxOnHeapStorageMemory[on-heap] and xref:memory:MemoryManager.adoc#maxOffHeapStorageMemory[off-heap] memory for storage (in bytes) === [[maxOffHeapMemory]] Maximum Off-Heap Memory === [[maxOnHeapMemory]] Maximum On-Heap Memory","title":"log4j.logger.org.apache.spark.storage.BlockManager=ALL"},{"location":"storage/BlockManagerId/","text":"= BlockManagerId BlockManagerId is a unique identifier of a xref:storage:BlockManager.adoc[].","title":"BlockManagerId"},{"location":"storage/BlockManagerInfo/","text":"= BlockManagerInfo BlockManagerInfo is...FIXME","title":"BlockManagerInfo"},{"location":"storage/BlockManagerMaster/","text":"= BlockManagerMaster BlockManagerMaster xref:core:SparkEnv.adoc#BlockManagerMaster[runs on the driver]. BlockManagerMaster uses xref:storage:BlockManagerMasterEndpoint.adoc[] registered under BlockManagerMaster RPC endpoint name on the driver (with the endpoint references on executors) to allow executors for sending block status updates to it and hence keep track of block statuses. == [[creating-instance]] Creating Instance BlockManagerMaster takes the following to be created: [[driverEndpoint]] xref:rpc:RpcEndpointRef.adoc[] [[conf]] xref:ROOT:SparkConf.adoc[] [[isDriver]] Flag whether BlockManagerMaster is created for the driver or executors BlockManagerMaster is created when SparkEnv utility is used to xref:core:SparkEnv.adoc#create[create a SparkEnv (for the driver and executors)] (to create a xref:storage:BlockManager.adoc[]). == [[removeExecutorAsync]] removeExecutorAsync Method CAUTION: FIXME == [[contains]] contains Method CAUTION: FIXME == [[removeExecutor]] Removing Executor -- removeExecutor Method [source, scala] \u00b6 removeExecutor(execId: String): Unit \u00b6 removeExecutor posts xref:storage:BlockManagerMasterEndpoint.adoc#RemoveExecutor[ RemoveExecutor to BlockManagerMaster RPC endpoint] and waits for a response. If false in response comes in, a SparkException is thrown with the following message: BlockManagerMasterEndpoint returned false, expected true. If all goes fine, you should see the following INFO message in the logs: INFO BlockManagerMaster: Removed executor [execId] NOTE: removeExecutor is executed when xref:scheduler:DAGSchedulerEventProcessLoop.adoc#handleExecutorLost[ DAGScheduler processes ExecutorLost event]. == [[removeBlock]] Removing Block -- removeBlock Method [source, scala] \u00b6 removeBlock(blockId: BlockId): Unit \u00b6 removeBlock simply posts a RemoveBlock blocking message to xref:storage:BlockManagerMasterEndpoint.adoc[] (and ultimately disregards the reponse). == [[removeRdd]] Removing RDD Blocks -- removeRdd Method [source, scala] \u00b6 removeRdd(rddId: Int, blocking: Boolean) \u00b6 removeRdd removes all the blocks of rddId RDD, possibly in blocking fashion. Internally, removeRdd posts a RemoveRdd(rddId) message to xref:storage:BlockManagerMasterEndpoint.adoc[] on a separate thread. If there is an issue, you should see the following WARN message in the logs and the entire exception: WARN Failed to remove RDD [rddId] - [exception] If it is a blocking operation, it waits for a result for xref:rpc:index.adoc#spark.rpc.askTimeout[spark.rpc.askTimeout], xref:rpc:index.adoc#spark.network.timeout[spark.network.timeout] or 120 secs. == [[removeShuffle]] Removing Shuffle Blocks -- removeShuffle Method [source, scala] \u00b6 removeShuffle(shuffleId: Int, blocking: Boolean) \u00b6 removeShuffle removes all the blocks of shuffleId shuffle, possibly in a blocking fashion. It posts a RemoveShuffle(shuffleId) message to xref:storage:BlockManagerMasterEndpoint.adoc[] on a separate thread. If there is an issue, you should see the following WARN message in the logs and the entire exception: WARN Failed to remove shuffle [shuffleId] - [exception] If it is a blocking operation, it waits for the result for xref:rpc:index.adoc#spark.rpc.askTimeout[spark.rpc.askTimeout], xref:rpc:index.adoc#spark.network.timeout[spark.network.timeout] or 120 secs. NOTE: removeShuffle is used exclusively when xref:core:ContextCleaner.adoc#doCleanupShuffle[ ContextCleaner removes a shuffle]. == [[removeBroadcast]] Removing Broadcast Blocks -- removeBroadcast Method [source, scala] \u00b6 removeBroadcast(broadcastId: Long, removeFromMaster: Boolean, blocking: Boolean) \u00b6 removeBroadcast removes all the blocks of broadcastId broadcast, possibly in a blocking fashion. It posts a RemoveBroadcast(broadcastId, removeFromMaster) message to xref:storage:BlockManagerMasterEndpoint.adoc[] on a separate thread. If there is an issue, you should see the following WARN message in the logs and the entire exception: WARN Failed to remove broadcast [broadcastId] with removeFromMaster = [removeFromMaster] - [exception] If it is a blocking operation, it waits for the result for xref:rpc:index.adoc#spark.rpc.askTimeout[spark.rpc.askTimeout], xref:rpc:index.adoc#spark.network.timeout[spark.network.timeout] or 120 secs. == [[stop]] Stopping BlockManagerMaster -- stop Method [source, scala] \u00b6 stop(): Unit \u00b6 stop sends a StopBlockManagerMaster message to xref:storage:BlockManagerMasterEndpoint.adoc[] and waits for a response. NOTE: It is only executed for the driver. If all goes fine, you should see the following INFO message in the logs: INFO BlockManagerMaster: BlockManagerMaster stopped Otherwise, a SparkException is thrown. BlockManagerMasterEndpoint returned false, expected true. == [[registerBlockManager]] Registering BlockManager with Driver [source, scala] \u00b6 registerBlockManager( blockManagerId: BlockManagerId, maxMemSize: Long, slaveEndpoint: RpcEndpointRef): BlockManagerId registerBlockManager prints the following INFO message to the logs: [source,plaintext] \u00b6 Registering BlockManager [blockManagerId] \u00b6 .Registering BlockManager with the Driver image::BlockManagerMaster-RegisterBlockManager.png[align=\"center\"] registerBlockManager then notifies the driver that the xref:storage:BlockManagerId.adoc[] is registering itself. registerBlockManager posts a xref:storage:BlockManagerMasterEndpoint.adoc#RegisterBlockManager[blocking RegisterBlockManager message to BlockManagerMaster RPC endpoint]. NOTE: The input maxMemSize is the xref:storage:BlockManager.adoc#maxMemory[total available on-heap and off-heap memory for storage on a BlockManager ]. registerBlockManager waits until a confirmation comes (as xref:storage:BlockManagerId.adoc[]). In the end, registerBlockManager prints the following INFO message to the logs and returns the xref:storage:BlockManagerId.adoc[] received. [source,plaintext] \u00b6 Registered BlockManager [updatedId] \u00b6 registerBlockManager is used when BlockManager is requested to xref:storage:BlockManager.adoc#initialize[initialize] and xref:storage:BlockManager.adoc#reregister[re-register itself with the driver]. == [[updateBlockInfo]] Relaying Block Status Update From BlockManager to Driver [source, scala] \u00b6 updateBlockInfo( blockManagerId: BlockManagerId, blockId: BlockId, storageLevel: StorageLevel, memSize: Long, diskSize: Long): Boolean updateBlockInfo sends a blocking xref:storage:BlockManagerMasterEndpoint.adoc#UpdateBlockInfo[UpdateBlockInfo] event to < > (and waits for a response). updateBlockInfo prints out the following DEBUG message to the logs: DEBUG BlockManagerMaster: Updated info of block [blockId] updateBlockInfo returns the response from the < >. NOTE: updateBlockInfo is used exclusively when BlockManager is requested to xref:storage:BlockManager.adoc#tryToReportBlockStatus[report a block status update to the driver]. == [[getLocations-block]] Get Block Locations of One Block -- getLocations Method [source, scala] \u00b6 getLocations(blockId: BlockId): Seq[BlockManagerId] \u00b6 getLocations xref:storage:BlockManagerMasterEndpoint.adoc#GetLocations[posts a blocking GetLocations message to BlockManagerMaster RPC endpoint] and returns the response. NOTE: getLocations is used when < > and xref:storage:BlockManager.adoc#getLocations[ BlockManager getLocations]. == [[getLocations-block-array]] Get Block Locations for Multiple Blocks -- getLocations Method [source, scala] \u00b6 getLocations(blockIds: Array[BlockId]): IndexedSeq[Seq[BlockManagerId]] \u00b6 getLocations xref:storage:BlockManagerMasterEndpoint.adoc#GetLocationsMultipleBlockIds[posts a blocking GetLocationsMultipleBlockIds message to BlockManagerMaster RPC endpoint] and returns the response. NOTE: getLocations is used when xref:scheduler:DAGScheduler.adoc#getCacheLocs[ DAGScheduler finds BlockManagers (and so executors) for cached RDD partitions] and when BlockManager xref:storage:BlockManager.adoc#getLocationBlockIds[getLocationBlockIds] and xref:storage:BlockManager.adoc#blockIdsToHosts[blockIdsToHosts]. == [[getPeers]] Finding Peers of BlockManager -- getPeers Internal Method [source, scala] \u00b6 getPeers(blockManagerId: BlockManagerId): Seq[BlockManagerId] \u00b6 getPeers xref:storage:BlockManagerMasterEndpoint.adoc#GetPeers[posts a blocking GetPeers message to BlockManagerMaster RPC endpoint] and returns the response. NOTE: Peers of a xref:storage:BlockManager.adoc[BlockManager] are the other BlockManagers in a cluster (except the driver's BlockManager). Peers are used to know the available executors in a Spark application. NOTE: getPeers is used when xref:storage:BlockManager.adoc#getPeers[ BlockManager finds the peers of a BlockManager ], Structured Streaming's KafkaSource and Spark Streaming's KafkaRDD . == [[getExecutorEndpointRef]] getExecutorEndpointRef Method [source, scala] \u00b6 getExecutorEndpointRef(executorId: String): Option[RpcEndpointRef] \u00b6 getExecutorEndpointRef posts GetExecutorEndpointRef(executorId) message to xref:storage:BlockManagerMasterEndpoint.adoc[] and waits for a response which becomes the return value. == [[getMemoryStatus]] getMemoryStatus Method [source, scala] \u00b6 getMemoryStatus: Map[BlockManagerId, (Long, Long)] \u00b6 getMemoryStatus posts a GetMemoryStatus message xref:storage:BlockManagerMasterEndpoint.adoc[] and waits for a response which becomes the return value. == [[getStorageStatus]] Storage Status (Posting GetStorageStatus to BlockManagerMaster RPC endpoint) -- getStorageStatus Method [source, scala] \u00b6 getStorageStatus: Array[StorageStatus] \u00b6 getStorageStatus posts a GetStorageStatus message to xref:storage:BlockManagerMasterEndpoint.adoc[] and waits for a response which becomes the return value. == [[getBlockStatus]] getBlockStatus Method [source, scala] \u00b6 getBlockStatus( blockId: BlockId, askSlaves: Boolean = true): Map[BlockManagerId, BlockStatus] getBlockStatus posts a GetBlockStatus(blockId, askSlaves) message to xref:storage:BlockManagerMasterEndpoint.adoc[] and waits for a response (of type Map[BlockManagerId, Future[Option[BlockStatus]]] ). It then builds a sequence of future results that are BlockStatus statuses and waits for a result for xref:rpc:index.adoc#spark.rpc.askTimeout[spark.rpc.askTimeout], xref:rpc:index.adoc#spark.network.timeout[spark.network.timeout] or 120 secs. No result leads to a SparkException with the following message: BlockManager returned null for BlockStatus query: [blockId] == [[getMatchingBlockIds]] getMatchingBlockIds Method [source, scala] \u00b6 getMatchingBlockIds( filter: BlockId => Boolean, askSlaves: Boolean): Seq[BlockId] getMatchingBlockIds posts a GetMatchingBlockIds(filter, askSlaves) message to xref:storage:BlockManagerMasterEndpoint.adoc[] and waits for a response which becomes the result for xref:rpc:index.adoc#spark.rpc.askTimeout[spark.rpc.askTimeout], xref:rpc:index.adoc#spark.network.timeout[spark.network.timeout] or 120 secs. == [[hasCachedBlocks]] hasCachedBlocks Method [source, scala] \u00b6 hasCachedBlocks(executorId: String): Boolean \u00b6 hasCachedBlocks posts a HasCachedBlocks(executorId) message to xref:storage:BlockManagerMasterEndpoint.adoc[] and waits for a response which becomes the result. == [[logging]] Logging Enable ALL logging level for org.apache.spark.storage.BlockManagerMaster logger to see what happens inside. Add the following line to conf/log4j.properties : [source] \u00b6 log4j.logger.org.apache.spark.storage.BlockManagerMaster=ALL \u00b6 Refer to xref:ROOT:spark-logging.adoc[Logging].","title":"BlockManagerMaster"},{"location":"storage/BlockManagerMaster/#source-scala","text":"","title":"[source, scala]"},{"location":"storage/BlockManagerMaster/#removeexecutorexecid-string-unit","text":"removeExecutor posts xref:storage:BlockManagerMasterEndpoint.adoc#RemoveExecutor[ RemoveExecutor to BlockManagerMaster RPC endpoint] and waits for a response. If false in response comes in, a SparkException is thrown with the following message: BlockManagerMasterEndpoint returned false, expected true. If all goes fine, you should see the following INFO message in the logs: INFO BlockManagerMaster: Removed executor [execId] NOTE: removeExecutor is executed when xref:scheduler:DAGSchedulerEventProcessLoop.adoc#handleExecutorLost[ DAGScheduler processes ExecutorLost event]. == [[removeBlock]] Removing Block -- removeBlock Method","title":"removeExecutor(execId: String): Unit"},{"location":"storage/BlockManagerMaster/#source-scala_1","text":"","title":"[source, scala]"},{"location":"storage/BlockManagerMaster/#removeblockblockid-blockid-unit","text":"removeBlock simply posts a RemoveBlock blocking message to xref:storage:BlockManagerMasterEndpoint.adoc[] (and ultimately disregards the reponse). == [[removeRdd]] Removing RDD Blocks -- removeRdd Method","title":"removeBlock(blockId: BlockId): Unit"},{"location":"storage/BlockManagerMaster/#source-scala_2","text":"","title":"[source, scala]"},{"location":"storage/BlockManagerMaster/#removerddrddid-int-blocking-boolean","text":"removeRdd removes all the blocks of rddId RDD, possibly in blocking fashion. Internally, removeRdd posts a RemoveRdd(rddId) message to xref:storage:BlockManagerMasterEndpoint.adoc[] on a separate thread. If there is an issue, you should see the following WARN message in the logs and the entire exception: WARN Failed to remove RDD [rddId] - [exception] If it is a blocking operation, it waits for a result for xref:rpc:index.adoc#spark.rpc.askTimeout[spark.rpc.askTimeout], xref:rpc:index.adoc#spark.network.timeout[spark.network.timeout] or 120 secs. == [[removeShuffle]] Removing Shuffle Blocks -- removeShuffle Method","title":"removeRdd(rddId: Int, blocking: Boolean)"},{"location":"storage/BlockManagerMaster/#source-scala_3","text":"","title":"[source, scala]"},{"location":"storage/BlockManagerMaster/#removeshuffleshuffleid-int-blocking-boolean","text":"removeShuffle removes all the blocks of shuffleId shuffle, possibly in a blocking fashion. It posts a RemoveShuffle(shuffleId) message to xref:storage:BlockManagerMasterEndpoint.adoc[] on a separate thread. If there is an issue, you should see the following WARN message in the logs and the entire exception: WARN Failed to remove shuffle [shuffleId] - [exception] If it is a blocking operation, it waits for the result for xref:rpc:index.adoc#spark.rpc.askTimeout[spark.rpc.askTimeout], xref:rpc:index.adoc#spark.network.timeout[spark.network.timeout] or 120 secs. NOTE: removeShuffle is used exclusively when xref:core:ContextCleaner.adoc#doCleanupShuffle[ ContextCleaner removes a shuffle]. == [[removeBroadcast]] Removing Broadcast Blocks -- removeBroadcast Method","title":"removeShuffle(shuffleId: Int, blocking: Boolean)"},{"location":"storage/BlockManagerMaster/#source-scala_4","text":"","title":"[source, scala]"},{"location":"storage/BlockManagerMaster/#removebroadcastbroadcastid-long-removefrommaster-boolean-blocking-boolean","text":"removeBroadcast removes all the blocks of broadcastId broadcast, possibly in a blocking fashion. It posts a RemoveBroadcast(broadcastId, removeFromMaster) message to xref:storage:BlockManagerMasterEndpoint.adoc[] on a separate thread. If there is an issue, you should see the following WARN message in the logs and the entire exception: WARN Failed to remove broadcast [broadcastId] with removeFromMaster = [removeFromMaster] - [exception] If it is a blocking operation, it waits for the result for xref:rpc:index.adoc#spark.rpc.askTimeout[spark.rpc.askTimeout], xref:rpc:index.adoc#spark.network.timeout[spark.network.timeout] or 120 secs. == [[stop]] Stopping BlockManagerMaster -- stop Method","title":"removeBroadcast(broadcastId: Long, removeFromMaster: Boolean, blocking: Boolean)"},{"location":"storage/BlockManagerMaster/#source-scala_5","text":"","title":"[source, scala]"},{"location":"storage/BlockManagerMaster/#stop-unit","text":"stop sends a StopBlockManagerMaster message to xref:storage:BlockManagerMasterEndpoint.adoc[] and waits for a response. NOTE: It is only executed for the driver. If all goes fine, you should see the following INFO message in the logs: INFO BlockManagerMaster: BlockManagerMaster stopped Otherwise, a SparkException is thrown. BlockManagerMasterEndpoint returned false, expected true. == [[registerBlockManager]] Registering BlockManager with Driver","title":"stop(): Unit"},{"location":"storage/BlockManagerMaster/#source-scala_6","text":"registerBlockManager( blockManagerId: BlockManagerId, maxMemSize: Long, slaveEndpoint: RpcEndpointRef): BlockManagerId registerBlockManager prints the following INFO message to the logs:","title":"[source, scala]"},{"location":"storage/BlockManagerMaster/#sourceplaintext","text":"","title":"[source,plaintext]"},{"location":"storage/BlockManagerMaster/#registering-blockmanager-blockmanagerid","text":".Registering BlockManager with the Driver image::BlockManagerMaster-RegisterBlockManager.png[align=\"center\"] registerBlockManager then notifies the driver that the xref:storage:BlockManagerId.adoc[] is registering itself. registerBlockManager posts a xref:storage:BlockManagerMasterEndpoint.adoc#RegisterBlockManager[blocking RegisterBlockManager message to BlockManagerMaster RPC endpoint]. NOTE: The input maxMemSize is the xref:storage:BlockManager.adoc#maxMemory[total available on-heap and off-heap memory for storage on a BlockManager ]. registerBlockManager waits until a confirmation comes (as xref:storage:BlockManagerId.adoc[]). In the end, registerBlockManager prints the following INFO message to the logs and returns the xref:storage:BlockManagerId.adoc[] received.","title":"Registering BlockManager [blockManagerId]"},{"location":"storage/BlockManagerMaster/#sourceplaintext_1","text":"","title":"[source,plaintext]"},{"location":"storage/BlockManagerMaster/#registered-blockmanager-updatedid","text":"registerBlockManager is used when BlockManager is requested to xref:storage:BlockManager.adoc#initialize[initialize] and xref:storage:BlockManager.adoc#reregister[re-register itself with the driver]. == [[updateBlockInfo]] Relaying Block Status Update From BlockManager to Driver","title":"Registered BlockManager [updatedId]"},{"location":"storage/BlockManagerMaster/#source-scala_7","text":"updateBlockInfo( blockManagerId: BlockManagerId, blockId: BlockId, storageLevel: StorageLevel, memSize: Long, diskSize: Long): Boolean updateBlockInfo sends a blocking xref:storage:BlockManagerMasterEndpoint.adoc#UpdateBlockInfo[UpdateBlockInfo] event to < > (and waits for a response). updateBlockInfo prints out the following DEBUG message to the logs: DEBUG BlockManagerMaster: Updated info of block [blockId] updateBlockInfo returns the response from the < >. NOTE: updateBlockInfo is used exclusively when BlockManager is requested to xref:storage:BlockManager.adoc#tryToReportBlockStatus[report a block status update to the driver]. == [[getLocations-block]] Get Block Locations of One Block -- getLocations Method","title":"[source, scala]"},{"location":"storage/BlockManagerMaster/#source-scala_8","text":"","title":"[source, scala]"},{"location":"storage/BlockManagerMaster/#getlocationsblockid-blockid-seqblockmanagerid","text":"getLocations xref:storage:BlockManagerMasterEndpoint.adoc#GetLocations[posts a blocking GetLocations message to BlockManagerMaster RPC endpoint] and returns the response. NOTE: getLocations is used when < > and xref:storage:BlockManager.adoc#getLocations[ BlockManager getLocations]. == [[getLocations-block-array]] Get Block Locations for Multiple Blocks -- getLocations Method","title":"getLocations(blockId: BlockId): Seq[BlockManagerId]"},{"location":"storage/BlockManagerMaster/#source-scala_9","text":"","title":"[source, scala]"},{"location":"storage/BlockManagerMaster/#getlocationsblockids-arrayblockid-indexedseqseqblockmanagerid","text":"getLocations xref:storage:BlockManagerMasterEndpoint.adoc#GetLocationsMultipleBlockIds[posts a blocking GetLocationsMultipleBlockIds message to BlockManagerMaster RPC endpoint] and returns the response. NOTE: getLocations is used when xref:scheduler:DAGScheduler.adoc#getCacheLocs[ DAGScheduler finds BlockManagers (and so executors) for cached RDD partitions] and when BlockManager xref:storage:BlockManager.adoc#getLocationBlockIds[getLocationBlockIds] and xref:storage:BlockManager.adoc#blockIdsToHosts[blockIdsToHosts]. == [[getPeers]] Finding Peers of BlockManager -- getPeers Internal Method","title":"getLocations(blockIds: Array[BlockId]): IndexedSeq[Seq[BlockManagerId]]"},{"location":"storage/BlockManagerMaster/#source-scala_10","text":"","title":"[source, scala]"},{"location":"storage/BlockManagerMaster/#getpeersblockmanagerid-blockmanagerid-seqblockmanagerid","text":"getPeers xref:storage:BlockManagerMasterEndpoint.adoc#GetPeers[posts a blocking GetPeers message to BlockManagerMaster RPC endpoint] and returns the response. NOTE: Peers of a xref:storage:BlockManager.adoc[BlockManager] are the other BlockManagers in a cluster (except the driver's BlockManager). Peers are used to know the available executors in a Spark application. NOTE: getPeers is used when xref:storage:BlockManager.adoc#getPeers[ BlockManager finds the peers of a BlockManager ], Structured Streaming's KafkaSource and Spark Streaming's KafkaRDD . == [[getExecutorEndpointRef]] getExecutorEndpointRef Method","title":"getPeers(blockManagerId: BlockManagerId): Seq[BlockManagerId]"},{"location":"storage/BlockManagerMaster/#source-scala_11","text":"","title":"[source, scala]"},{"location":"storage/BlockManagerMaster/#getexecutorendpointrefexecutorid-string-optionrpcendpointref","text":"getExecutorEndpointRef posts GetExecutorEndpointRef(executorId) message to xref:storage:BlockManagerMasterEndpoint.adoc[] and waits for a response which becomes the return value. == [[getMemoryStatus]] getMemoryStatus Method","title":"getExecutorEndpointRef(executorId: String): Option[RpcEndpointRef]"},{"location":"storage/BlockManagerMaster/#source-scala_12","text":"","title":"[source, scala]"},{"location":"storage/BlockManagerMaster/#getmemorystatus-mapblockmanagerid-long-long","text":"getMemoryStatus posts a GetMemoryStatus message xref:storage:BlockManagerMasterEndpoint.adoc[] and waits for a response which becomes the return value. == [[getStorageStatus]] Storage Status (Posting GetStorageStatus to BlockManagerMaster RPC endpoint) -- getStorageStatus Method","title":"getMemoryStatus: Map[BlockManagerId, (Long, Long)]"},{"location":"storage/BlockManagerMaster/#source-scala_13","text":"","title":"[source, scala]"},{"location":"storage/BlockManagerMaster/#getstoragestatus-arraystoragestatus","text":"getStorageStatus posts a GetStorageStatus message to xref:storage:BlockManagerMasterEndpoint.adoc[] and waits for a response which becomes the return value. == [[getBlockStatus]] getBlockStatus Method","title":"getStorageStatus: Array[StorageStatus]"},{"location":"storage/BlockManagerMaster/#source-scala_14","text":"getBlockStatus( blockId: BlockId, askSlaves: Boolean = true): Map[BlockManagerId, BlockStatus] getBlockStatus posts a GetBlockStatus(blockId, askSlaves) message to xref:storage:BlockManagerMasterEndpoint.adoc[] and waits for a response (of type Map[BlockManagerId, Future[Option[BlockStatus]]] ). It then builds a sequence of future results that are BlockStatus statuses and waits for a result for xref:rpc:index.adoc#spark.rpc.askTimeout[spark.rpc.askTimeout], xref:rpc:index.adoc#spark.network.timeout[spark.network.timeout] or 120 secs. No result leads to a SparkException with the following message: BlockManager returned null for BlockStatus query: [blockId] == [[getMatchingBlockIds]] getMatchingBlockIds Method","title":"[source, scala]"},{"location":"storage/BlockManagerMaster/#source-scala_15","text":"getMatchingBlockIds( filter: BlockId => Boolean, askSlaves: Boolean): Seq[BlockId] getMatchingBlockIds posts a GetMatchingBlockIds(filter, askSlaves) message to xref:storage:BlockManagerMasterEndpoint.adoc[] and waits for a response which becomes the result for xref:rpc:index.adoc#spark.rpc.askTimeout[spark.rpc.askTimeout], xref:rpc:index.adoc#spark.network.timeout[spark.network.timeout] or 120 secs. == [[hasCachedBlocks]] hasCachedBlocks Method","title":"[source, scala]"},{"location":"storage/BlockManagerMaster/#source-scala_16","text":"","title":"[source, scala]"},{"location":"storage/BlockManagerMaster/#hascachedblocksexecutorid-string-boolean","text":"hasCachedBlocks posts a HasCachedBlocks(executorId) message to xref:storage:BlockManagerMasterEndpoint.adoc[] and waits for a response which becomes the result. == [[logging]] Logging Enable ALL logging level for org.apache.spark.storage.BlockManagerMaster logger to see what happens inside. Add the following line to conf/log4j.properties :","title":"hasCachedBlocks(executorId: String): Boolean"},{"location":"storage/BlockManagerMaster/#source","text":"","title":"[source]"},{"location":"storage/BlockManagerMaster/#log4jloggerorgapachesparkstorageblockmanagermasterall","text":"Refer to xref:ROOT:spark-logging.adoc[Logging].","title":"log4j.logger.org.apache.spark.storage.BlockManagerMaster=ALL"},{"location":"storage/BlockManagerMasterEndpoint/","text":"= BlockManagerMasterEndpoint -- BlockManagerMaster RPC Endpoint :navtitle: BlockManagerMasterEndpoint BlockManagerMasterEndpoint is a xref:rpc:RpcEndpoint.adoc#ThreadSafeRpcEndpoint[ThreadSafeRpcEndpoint] for xref:storage:BlockManagerMaster.adoc[BlockManagerMaster]. BlockManagerMasterEndpoint is registered under BlockManagerMaster name. BlockManagerMasterEndpoint tracks status of the xref:storage:BlockManager.adoc[BlockManagers] (on the executors) in a Spark application. == [[creating-instance]] Creating Instance BlockManagerMasterEndpoint takes the following to be created: [[rpcEnv]] xref:rpc:RpcEnv.adoc[] [[isLocal]] Flag whether BlockManagerMasterEndpoint works in local or cluster mode [[conf]] xref:ROOT:SparkConf.adoc[] [[listenerBus]] xref:scheduler:LiveListenerBus.adoc[] BlockManagerMasterEndpoint is created for the xref:core:SparkEnv.adoc#create[SparkEnv] on the driver (to create a xref:storage:BlockManagerMaster.adoc[] for a xref:storage:BlockManager.adoc#master[BlockManager]). When created, BlockManagerMasterEndpoint prints out the following INFO message to the logs: [source,plaintext] \u00b6 BlockManagerMasterEndpoint up \u00b6 == [[messages]][[receiveAndReply]] Messages As an xref:rpc:RpcEndpoint.adoc[], BlockManagerMasterEndpoint handles RPC messages. === [[BlockManagerHeartbeat]] BlockManagerHeartbeat [source, scala] \u00b6 BlockManagerHeartbeat( blockManagerId: BlockManagerId) When received, BlockManagerMasterEndpoint...FIXME Posted when...FIXME === [[GetLocations]] GetLocations [source, scala] \u00b6 GetLocations( blockId: BlockId) When received, BlockManagerMasterEndpoint replies with the < > of blockId . Posted when xref:BlockManagerMaster.adoc#getLocations-block[ BlockManagerMaster requests the block locations of a single block]. === [[GetLocationsAndStatus]] GetLocationsAndStatus [source, scala] \u00b6 GetLocationsAndStatus( blockId: BlockId) When received, BlockManagerMasterEndpoint...FIXME Posted when...FIXME === [[GetLocationsMultipleBlockIds]] GetLocationsMultipleBlockIds [source, scala] \u00b6 GetLocationsMultipleBlockIds( blockIds: Array[BlockId]) When received, BlockManagerMasterEndpoint replies with the < > for the given xref:storage:BlockId.adoc[]. Posted when xref:BlockManagerMaster.adoc#getLocations[ BlockManagerMaster requests the block locations for multiple blocks]. === [[GetPeers]] GetPeers [source, scala] \u00b6 GetPeers( blockManagerId: BlockManagerId) When received, BlockManagerMasterEndpoint replies with the < > of blockManagerId . Peers of a xref:storage:BlockManager.adoc[BlockManager] are the other BlockManagers in a cluster (except the driver's BlockManager). Peers are used to know the available executors in a Spark application. Posted when xref:BlockManagerMaster.adoc#getPeers[ BlockManagerMaster requests the peers of a BlockManager ]. === [[GetExecutorEndpointRef]] GetExecutorEndpointRef [source, scala] \u00b6 GetExecutorEndpointRef( executorId: String) When received, BlockManagerMasterEndpoint...FIXME Posted when...FIXME === [[GetMemoryStatus]] GetMemoryStatus [source, scala] \u00b6 GetMemoryStatus \u00b6 When received, BlockManagerMasterEndpoint...FIXME Posted when...FIXME === [[GetStorageStatus]] GetStorageStatus [source, scala] \u00b6 GetStorageStatus \u00b6 When received, BlockManagerMasterEndpoint...FIXME Posted when...FIXME === [[GetBlockStatus]] GetBlockStatus [source, scala] \u00b6 GetBlockStatus( blockId: BlockId, askSlaves: Boolean = true) When received, BlockManagerMasterEndpoint is requested to < >. Posted when...FIXME === [[GetMatchingBlockIds]] GetMatchingBlockIds [source, scala] \u00b6 GetMatchingBlockIds( filter: BlockId => Boolean, askSlaves: Boolean = true) When received, BlockManagerMasterEndpoint...FIXME Posted when...FIXME === [[HasCachedBlocks]] HasCachedBlocks [source, scala] \u00b6 HasCachedBlocks( executorId: String) When received, BlockManagerMasterEndpoint...FIXME Posted when...FIXME === [[RegisterBlockManager]] RegisterBlockManager [source,scala] \u00b6 RegisterBlockManager( blockManagerId: BlockManagerId, maxOnHeapMemSize: Long, maxOffHeapMemSize: Long, sender: RpcEndpointRef) When received, BlockManagerMasterEndpoint is requested to < > (by the given xref:storage:BlockManagerId.adoc[]). Posted when BlockManagerMaster is requested to xref:storage:BlockManagerMaster.adoc#registerBlockManager[register a BlockManager] === [[RemoveRdd]] RemoveRdd [source, scala] \u00b6 RemoveRdd( rddId: Int) When received, BlockManagerMasterEndpoint...FIXME Posted when...FIXME === [[RemoveShuffle]] RemoveShuffle [source, scala] \u00b6 RemoveShuffle( shuffleId: Int) When received, BlockManagerMasterEndpoint...FIXME Posted when...FIXME === [[RemoveBroadcast]] RemoveBroadcast [source, scala] \u00b6 RemoveBroadcast( broadcastId: Long, removeFromDriver: Boolean = true) When received, BlockManagerMasterEndpoint...FIXME Posted when...FIXME === [[RemoveBlock]] RemoveBlock [source, scala] \u00b6 RemoveBlock( blockId: BlockId) When received, BlockManagerMasterEndpoint...FIXME Posted when...FIXME === [[RemoveExecutor]] RemoveExecutor [source, scala] \u00b6 RemoveExecutor( execId: String) When received, BlockManagerMasterEndpoint < execId is removed>> and the response true sent back. Posted when xref:BlockManagerMaster.adoc#removeExecutor[ BlockManagerMaster removes an executor]. === [[StopBlockManagerMaster]] StopBlockManagerMaster [source, scala] \u00b6 StopBlockManagerMaster \u00b6 When received, BlockManagerMasterEndpoint...FIXME Posted when...FIXME === [[UpdateBlockInfo]] UpdateBlockInfo [source, scala] \u00b6 UpdateBlockInfo( blockManagerId: BlockManagerId, blockId: BlockId, storageLevel: StorageLevel, memSize: Long, diskSize: Long) When received, BlockManagerMasterEndpoint...FIXME Posted when BlockManagerMaster is requested to xref:storage:BlockManagerMaster.adoc#updateBlockInfo[handle a block status update (from BlockManager on an executor)]. == [[storageStatus]] storageStatus Internal Method [source,scala] \u00b6 storageStatus: Array[StorageStatus] \u00b6 storageStatus...FIXME storageStatus is used when BlockManagerMasterEndpoint is requested to handle < > message. == [[getLocationsMultipleBlockIds]] getLocationsMultipleBlockIds Internal Method [source,scala] \u00b6 getLocationsMultipleBlockIds( blockIds: Array[BlockId]): IndexedSeq[Seq[BlockManagerId]] getLocationsMultipleBlockIds...FIXME getLocationsMultipleBlockIds is used when BlockManagerMasterEndpoint is requested to handle < > message. == [[removeShuffle]] removeShuffle Internal Method [source,scala] \u00b6 removeShuffle( shuffleId: Int): Future[Seq[Boolean]] removeShuffle...FIXME removeShuffle is used when BlockManagerMasterEndpoint is requested to handle < > message. == [[getPeers]] getPeers Internal Method [source, scala] \u00b6 getPeers( blockManagerId: BlockManagerId): Seq[BlockManagerId] getPeers finds all the registered BlockManagers (using < > internal registry) and checks if the input blockManagerId is amongst them. If the input blockManagerId is registered, getPeers returns all the registered BlockManagers but the one on the driver and blockManagerId . Otherwise, getPeers returns no BlockManagers . NOTE: Peers of a xref:storage:BlockManager.adoc[BlockManager] are the other BlockManagers in a cluster (except the driver's BlockManager). Peers are used to know the available executors in a Spark application. getPeers is used when BlockManagerMasterEndpoint is requested to handle < > message. == [[register]] register Internal Method [source, scala] \u00b6 register( idWithoutTopologyInfo: BlockManagerId, maxOnHeapMemSize: Long, maxOffHeapMemSize: Long, slaveEndpoint: RpcEndpointRef): BlockManagerId register registers a xref:storage:BlockManager.adoc[] (based on the given xref:storage:BlockManagerId.adoc[]) in the < > and < > registries and posts a SparkListenerBlockManagerAdded message (to the < >). NOTE: The input maxMemSize is the xref:storage:BlockManager.adoc#maxMemory[total available on-heap and off-heap memory for storage on a BlockManager ]. NOTE: Registering a BlockManager can only happen once for an executor (identified by BlockManagerId.executorId in < > internal registry). If another BlockManager has earlier been registered for the executor, you should see the following ERROR message in the logs: [source,plaintext] \u00b6 Got two different block manager registrations on same executor - will replace old one [oldId] with new one [id] \u00b6 And then < >. register prints out the following INFO message to the logs: [source,plaintext] \u00b6 Registering block manager [hostPort] with [bytes] RAM, [id] \u00b6 The BlockManager is recorded in the internal registries: < > < > In the end, register requests the < > to xref:scheduler:LiveListenerBus.adoc#post[post] a xref:ROOT:SparkListener.adoc#SparkListenerBlockManagerAdded[SparkListenerBlockManagerAdded] message. register is used when BlockManagerMasterEndpoint is requested to handle < > message. == [[removeExecutor]] removeExecutor Internal Method [source, scala] \u00b6 removeExecutor( execId: String): Unit removeExecutor prints the following INFO message to the logs: [source,plaintext] \u00b6 Trying to remove executor [execId] from BlockManagerMaster. \u00b6 If the execId executor is registered (in the internal < > internal registry), removeExecutor < BlockManager >>. removeExecutor is used when BlockManagerMasterEndpoint is requested to handle < > or < > messages. == [[removeBlockManager]] removeBlockManager Internal Method [source, scala] \u00b6 removeBlockManager( blockManagerId: BlockManagerId): Unit removeBlockManager looks up blockManagerId and removes the executor it was working on from the internal registries: < > < > It then goes over all the blocks for the BlockManager , and removes the executor for each block from blockLocations registry. xref:ROOT:SparkListener.adoc#SparkListenerBlockManagerRemoved[SparkListenerBlockManagerRemoved(System.currentTimeMillis(), blockManagerId)] is posted to xref:ROOT:SparkContext.adoc#listenerBus[listenerBus]. You should then see the following INFO message in the logs: [source,plaintext] \u00b6 Removing block manager [blockManagerId] \u00b6 removeBlockManager is used when BlockManagerMasterEndpoint is requested to < > (to handle < > or < > messages). == [[getLocations]] getLocations Internal Method [source, scala] \u00b6 getLocations( blockId: BlockId): Seq[BlockManagerId] getLocations looks up the given xref:storage:BlockId.adoc[] in the blockLocations internal registry and returns the locations (as a collection of BlockManagerId ) or an empty collection. getLocations is used when BlockManagerMasterEndpoint is requested to handle < > and < > messages. == [[logging]] Logging Enable ALL logging level for org.apache.spark.storage.BlockManagerMasterEndpoint logger to see what happens inside. Add the following line to conf/log4j.properties : [source] \u00b6 log4j.logger.org.apache.spark.storage.BlockManagerMasterEndpoint=ALL \u00b6 Refer to xref:ROOT:spark-logging.adoc[Logging]. == [[internal-properties]] Internal Properties === [[blockManagerIdByExecutor]] blockManagerIdByExecutor Lookup Table [source,scala] \u00b6 blockManagerIdByExecutor: Map[String, BlockManagerId] \u00b6 Lookup table of xref:storage:BlockManagerId.adoc[]s by executor ID A new executor is added when BlockManagerMasterEndpoint is requested to handle a < > message (and < >). An executor is removed when BlockManagerMasterEndpoint is requested to handle a < > and a < > messages (via < >) Used when BlockManagerMasterEndpoint is requested to handle < > message, < >, < > and < >. === [[blockManagerInfo]] blockManagerInfo Lookup Table [source,scala] \u00b6 blockManagerIdByExecutor: Map[String, BlockManagerId] \u00b6 Lookup table of xref:storage:BlockManagerInfo.adoc[] by xref:storage:BlockManagerId.adoc[] A new BlockManagerInfo is added when BlockManagerMasterEndpoint is requested to handle a < > message (and < >). A BlockManagerInfo is removed when BlockManagerMasterEndpoint is requested to < > (to handle < > and < > messages). === [[blockLocations]] blockLocations [source,scala] \u00b6 blockLocations: Map[BlockId, Set[BlockManagerId]] \u00b6 Collection of xref:storage:BlockId.adoc[] and their locations (as BlockManagerId ). Used in removeRdd to remove blocks for a RDD, removeBlockManager to remove blocks after a BlockManager gets removed, removeBlockFromWorkers , updateBlockInfo , and < >.","title":"BlockManagerMasterEndpoint"},{"location":"storage/BlockManagerMasterEndpoint/#sourceplaintext","text":"","title":"[source,plaintext]"},{"location":"storage/BlockManagerMasterEndpoint/#blockmanagermasterendpoint-up","text":"== [[messages]][[receiveAndReply]] Messages As an xref:rpc:RpcEndpoint.adoc[], BlockManagerMasterEndpoint handles RPC messages. === [[BlockManagerHeartbeat]] BlockManagerHeartbeat","title":"BlockManagerMasterEndpoint up"},{"location":"storage/BlockManagerMasterEndpoint/#source-scala","text":"BlockManagerHeartbeat( blockManagerId: BlockManagerId) When received, BlockManagerMasterEndpoint...FIXME Posted when...FIXME === [[GetLocations]] GetLocations","title":"[source, scala]"},{"location":"storage/BlockManagerMasterEndpoint/#source-scala_1","text":"GetLocations( blockId: BlockId) When received, BlockManagerMasterEndpoint replies with the < > of blockId . Posted when xref:BlockManagerMaster.adoc#getLocations-block[ BlockManagerMaster requests the block locations of a single block]. === [[GetLocationsAndStatus]] GetLocationsAndStatus","title":"[source, scala]"},{"location":"storage/BlockManagerMasterEndpoint/#source-scala_2","text":"GetLocationsAndStatus( blockId: BlockId) When received, BlockManagerMasterEndpoint...FIXME Posted when...FIXME === [[GetLocationsMultipleBlockIds]] GetLocationsMultipleBlockIds","title":"[source, scala]"},{"location":"storage/BlockManagerMasterEndpoint/#source-scala_3","text":"GetLocationsMultipleBlockIds( blockIds: Array[BlockId]) When received, BlockManagerMasterEndpoint replies with the < > for the given xref:storage:BlockId.adoc[]. Posted when xref:BlockManagerMaster.adoc#getLocations[ BlockManagerMaster requests the block locations for multiple blocks]. === [[GetPeers]] GetPeers","title":"[source, scala]"},{"location":"storage/BlockManagerMasterEndpoint/#source-scala_4","text":"GetPeers( blockManagerId: BlockManagerId) When received, BlockManagerMasterEndpoint replies with the < > of blockManagerId . Peers of a xref:storage:BlockManager.adoc[BlockManager] are the other BlockManagers in a cluster (except the driver's BlockManager). Peers are used to know the available executors in a Spark application. Posted when xref:BlockManagerMaster.adoc#getPeers[ BlockManagerMaster requests the peers of a BlockManager ]. === [[GetExecutorEndpointRef]] GetExecutorEndpointRef","title":"[source, scala]"},{"location":"storage/BlockManagerMasterEndpoint/#source-scala_5","text":"GetExecutorEndpointRef( executorId: String) When received, BlockManagerMasterEndpoint...FIXME Posted when...FIXME === [[GetMemoryStatus]] GetMemoryStatus","title":"[source, scala]"},{"location":"storage/BlockManagerMasterEndpoint/#source-scala_6","text":"","title":"[source, scala]"},{"location":"storage/BlockManagerMasterEndpoint/#getmemorystatus","text":"When received, BlockManagerMasterEndpoint...FIXME Posted when...FIXME === [[GetStorageStatus]] GetStorageStatus","title":"GetMemoryStatus"},{"location":"storage/BlockManagerMasterEndpoint/#source-scala_7","text":"","title":"[source, scala]"},{"location":"storage/BlockManagerMasterEndpoint/#getstoragestatus","text":"When received, BlockManagerMasterEndpoint...FIXME Posted when...FIXME === [[GetBlockStatus]] GetBlockStatus","title":"GetStorageStatus"},{"location":"storage/BlockManagerMasterEndpoint/#source-scala_8","text":"GetBlockStatus( blockId: BlockId, askSlaves: Boolean = true) When received, BlockManagerMasterEndpoint is requested to < >. Posted when...FIXME === [[GetMatchingBlockIds]] GetMatchingBlockIds","title":"[source, scala]"},{"location":"storage/BlockManagerMasterEndpoint/#source-scala_9","text":"GetMatchingBlockIds( filter: BlockId => Boolean, askSlaves: Boolean = true) When received, BlockManagerMasterEndpoint...FIXME Posted when...FIXME === [[HasCachedBlocks]] HasCachedBlocks","title":"[source, scala]"},{"location":"storage/BlockManagerMasterEndpoint/#source-scala_10","text":"HasCachedBlocks( executorId: String) When received, BlockManagerMasterEndpoint...FIXME Posted when...FIXME === [[RegisterBlockManager]] RegisterBlockManager","title":"[source, scala]"},{"location":"storage/BlockManagerMasterEndpoint/#sourcescala","text":"RegisterBlockManager( blockManagerId: BlockManagerId, maxOnHeapMemSize: Long, maxOffHeapMemSize: Long, sender: RpcEndpointRef) When received, BlockManagerMasterEndpoint is requested to < > (by the given xref:storage:BlockManagerId.adoc[]). Posted when BlockManagerMaster is requested to xref:storage:BlockManagerMaster.adoc#registerBlockManager[register a BlockManager] === [[RemoveRdd]] RemoveRdd","title":"[source,scala]"},{"location":"storage/BlockManagerMasterEndpoint/#source-scala_11","text":"RemoveRdd( rddId: Int) When received, BlockManagerMasterEndpoint...FIXME Posted when...FIXME === [[RemoveShuffle]] RemoveShuffle","title":"[source, scala]"},{"location":"storage/BlockManagerMasterEndpoint/#source-scala_12","text":"RemoveShuffle( shuffleId: Int) When received, BlockManagerMasterEndpoint...FIXME Posted when...FIXME === [[RemoveBroadcast]] RemoveBroadcast","title":"[source, scala]"},{"location":"storage/BlockManagerMasterEndpoint/#source-scala_13","text":"RemoveBroadcast( broadcastId: Long, removeFromDriver: Boolean = true) When received, BlockManagerMasterEndpoint...FIXME Posted when...FIXME === [[RemoveBlock]] RemoveBlock","title":"[source, scala]"},{"location":"storage/BlockManagerMasterEndpoint/#source-scala_14","text":"RemoveBlock( blockId: BlockId) When received, BlockManagerMasterEndpoint...FIXME Posted when...FIXME === [[RemoveExecutor]] RemoveExecutor","title":"[source, scala]"},{"location":"storage/BlockManagerMasterEndpoint/#source-scala_15","text":"RemoveExecutor( execId: String) When received, BlockManagerMasterEndpoint < execId is removed>> and the response true sent back. Posted when xref:BlockManagerMaster.adoc#removeExecutor[ BlockManagerMaster removes an executor]. === [[StopBlockManagerMaster]] StopBlockManagerMaster","title":"[source, scala]"},{"location":"storage/BlockManagerMasterEndpoint/#source-scala_16","text":"","title":"[source, scala]"},{"location":"storage/BlockManagerMasterEndpoint/#stopblockmanagermaster","text":"When received, BlockManagerMasterEndpoint...FIXME Posted when...FIXME === [[UpdateBlockInfo]] UpdateBlockInfo","title":"StopBlockManagerMaster"},{"location":"storage/BlockManagerMasterEndpoint/#source-scala_17","text":"UpdateBlockInfo( blockManagerId: BlockManagerId, blockId: BlockId, storageLevel: StorageLevel, memSize: Long, diskSize: Long) When received, BlockManagerMasterEndpoint...FIXME Posted when BlockManagerMaster is requested to xref:storage:BlockManagerMaster.adoc#updateBlockInfo[handle a block status update (from BlockManager on an executor)]. == [[storageStatus]] storageStatus Internal Method","title":"[source, scala]"},{"location":"storage/BlockManagerMasterEndpoint/#sourcescala_1","text":"","title":"[source,scala]"},{"location":"storage/BlockManagerMasterEndpoint/#storagestatus-arraystoragestatus","text":"storageStatus...FIXME storageStatus is used when BlockManagerMasterEndpoint is requested to handle < > message. == [[getLocationsMultipleBlockIds]] getLocationsMultipleBlockIds Internal Method","title":"storageStatus: Array[StorageStatus]"},{"location":"storage/BlockManagerMasterEndpoint/#sourcescala_2","text":"getLocationsMultipleBlockIds( blockIds: Array[BlockId]): IndexedSeq[Seq[BlockManagerId]] getLocationsMultipleBlockIds...FIXME getLocationsMultipleBlockIds is used when BlockManagerMasterEndpoint is requested to handle < > message. == [[removeShuffle]] removeShuffle Internal Method","title":"[source,scala]"},{"location":"storage/BlockManagerMasterEndpoint/#sourcescala_3","text":"removeShuffle( shuffleId: Int): Future[Seq[Boolean]] removeShuffle...FIXME removeShuffle is used when BlockManagerMasterEndpoint is requested to handle < > message. == [[getPeers]] getPeers Internal Method","title":"[source,scala]"},{"location":"storage/BlockManagerMasterEndpoint/#source-scala_18","text":"getPeers( blockManagerId: BlockManagerId): Seq[BlockManagerId] getPeers finds all the registered BlockManagers (using < > internal registry) and checks if the input blockManagerId is amongst them. If the input blockManagerId is registered, getPeers returns all the registered BlockManagers but the one on the driver and blockManagerId . Otherwise, getPeers returns no BlockManagers . NOTE: Peers of a xref:storage:BlockManager.adoc[BlockManager] are the other BlockManagers in a cluster (except the driver's BlockManager). Peers are used to know the available executors in a Spark application. getPeers is used when BlockManagerMasterEndpoint is requested to handle < > message. == [[register]] register Internal Method","title":"[source, scala]"},{"location":"storage/BlockManagerMasterEndpoint/#source-scala_19","text":"register( idWithoutTopologyInfo: BlockManagerId, maxOnHeapMemSize: Long, maxOffHeapMemSize: Long, slaveEndpoint: RpcEndpointRef): BlockManagerId register registers a xref:storage:BlockManager.adoc[] (based on the given xref:storage:BlockManagerId.adoc[]) in the < > and < > registries and posts a SparkListenerBlockManagerAdded message (to the < >). NOTE: The input maxMemSize is the xref:storage:BlockManager.adoc#maxMemory[total available on-heap and off-heap memory for storage on a BlockManager ]. NOTE: Registering a BlockManager can only happen once for an executor (identified by BlockManagerId.executorId in < > internal registry). If another BlockManager has earlier been registered for the executor, you should see the following ERROR message in the logs:","title":"[source, scala]"},{"location":"storage/BlockManagerMasterEndpoint/#sourceplaintext_1","text":"","title":"[source,plaintext]"},{"location":"storage/BlockManagerMasterEndpoint/#got-two-different-block-manager-registrations-on-same-executor-will-replace-old-one-oldid-with-new-one-id","text":"And then < >. register prints out the following INFO message to the logs:","title":"Got two different block manager registrations on same executor - will replace old one [oldId] with new one [id]"},{"location":"storage/BlockManagerMasterEndpoint/#sourceplaintext_2","text":"","title":"[source,plaintext]"},{"location":"storage/BlockManagerMasterEndpoint/#registering-block-manager-hostport-with-bytes-ram-id","text":"The BlockManager is recorded in the internal registries: < > < > In the end, register requests the < > to xref:scheduler:LiveListenerBus.adoc#post[post] a xref:ROOT:SparkListener.adoc#SparkListenerBlockManagerAdded[SparkListenerBlockManagerAdded] message. register is used when BlockManagerMasterEndpoint is requested to handle < > message. == [[removeExecutor]] removeExecutor Internal Method","title":"Registering block manager [hostPort] with [bytes] RAM, [id]"},{"location":"storage/BlockManagerMasterEndpoint/#source-scala_20","text":"removeExecutor( execId: String): Unit removeExecutor prints the following INFO message to the logs:","title":"[source, scala]"},{"location":"storage/BlockManagerMasterEndpoint/#sourceplaintext_3","text":"","title":"[source,plaintext]"},{"location":"storage/BlockManagerMasterEndpoint/#trying-to-remove-executor-execid-from-blockmanagermaster","text":"If the execId executor is registered (in the internal < > internal registry), removeExecutor < BlockManager >>. removeExecutor is used when BlockManagerMasterEndpoint is requested to handle < > or < > messages. == [[removeBlockManager]] removeBlockManager Internal Method","title":"Trying to remove executor [execId] from BlockManagerMaster."},{"location":"storage/BlockManagerMasterEndpoint/#source-scala_21","text":"removeBlockManager( blockManagerId: BlockManagerId): Unit removeBlockManager looks up blockManagerId and removes the executor it was working on from the internal registries: < > < > It then goes over all the blocks for the BlockManager , and removes the executor for each block from blockLocations registry. xref:ROOT:SparkListener.adoc#SparkListenerBlockManagerRemoved[SparkListenerBlockManagerRemoved(System.currentTimeMillis(), blockManagerId)] is posted to xref:ROOT:SparkContext.adoc#listenerBus[listenerBus]. You should then see the following INFO message in the logs:","title":"[source, scala]"},{"location":"storage/BlockManagerMasterEndpoint/#sourceplaintext_4","text":"","title":"[source,plaintext]"},{"location":"storage/BlockManagerMasterEndpoint/#removing-block-manager-blockmanagerid","text":"removeBlockManager is used when BlockManagerMasterEndpoint is requested to < > (to handle < > or < > messages). == [[getLocations]] getLocations Internal Method","title":"Removing block manager [blockManagerId]"},{"location":"storage/BlockManagerMasterEndpoint/#source-scala_22","text":"getLocations( blockId: BlockId): Seq[BlockManagerId] getLocations looks up the given xref:storage:BlockId.adoc[] in the blockLocations internal registry and returns the locations (as a collection of BlockManagerId ) or an empty collection. getLocations is used when BlockManagerMasterEndpoint is requested to handle < > and < > messages. == [[logging]] Logging Enable ALL logging level for org.apache.spark.storage.BlockManagerMasterEndpoint logger to see what happens inside. Add the following line to conf/log4j.properties :","title":"[source, scala]"},{"location":"storage/BlockManagerMasterEndpoint/#source","text":"","title":"[source]"},{"location":"storage/BlockManagerMasterEndpoint/#log4jloggerorgapachesparkstorageblockmanagermasterendpointall","text":"Refer to xref:ROOT:spark-logging.adoc[Logging]. == [[internal-properties]] Internal Properties === [[blockManagerIdByExecutor]] blockManagerIdByExecutor Lookup Table","title":"log4j.logger.org.apache.spark.storage.BlockManagerMasterEndpoint=ALL"},{"location":"storage/BlockManagerMasterEndpoint/#sourcescala_4","text":"","title":"[source,scala]"},{"location":"storage/BlockManagerMasterEndpoint/#blockmanageridbyexecutor-mapstring-blockmanagerid","text":"Lookup table of xref:storage:BlockManagerId.adoc[]s by executor ID A new executor is added when BlockManagerMasterEndpoint is requested to handle a < > message (and < >). An executor is removed when BlockManagerMasterEndpoint is requested to handle a < > and a < > messages (via < >) Used when BlockManagerMasterEndpoint is requested to handle < > message, < >, < > and < >. === [[blockManagerInfo]] blockManagerInfo Lookup Table","title":"blockManagerIdByExecutor: Map[String, BlockManagerId]"},{"location":"storage/BlockManagerMasterEndpoint/#sourcescala_5","text":"","title":"[source,scala]"},{"location":"storage/BlockManagerMasterEndpoint/#blockmanageridbyexecutor-mapstring-blockmanagerid_1","text":"Lookup table of xref:storage:BlockManagerInfo.adoc[] by xref:storage:BlockManagerId.adoc[] A new BlockManagerInfo is added when BlockManagerMasterEndpoint is requested to handle a < > message (and < >). A BlockManagerInfo is removed when BlockManagerMasterEndpoint is requested to < > (to handle < > and < > messages). === [[blockLocations]] blockLocations","title":"blockManagerIdByExecutor: Map[String, BlockManagerId]"},{"location":"storage/BlockManagerMasterEndpoint/#sourcescala_6","text":"","title":"[source,scala]"},{"location":"storage/BlockManagerMasterEndpoint/#blocklocations-mapblockid-setblockmanagerid","text":"Collection of xref:storage:BlockId.adoc[] and their locations (as BlockManagerId ). Used in removeRdd to remove blocks for a RDD, removeBlockManager to remove blocks after a BlockManager gets removed, removeBlockFromWorkers , updateBlockInfo , and < >.","title":"blockLocations: Map[BlockId, Set[BlockManagerId]]"},{"location":"storage/BlockManagerSlaveEndpoint/","text":"= BlockManagerSlaveEndpoint BlockManagerSlaveEndpoint is a xref:rpc:RpcEndpoint.adoc#ThreadSafeRpcEndpoint[ThreadSafeRpcEndpoint] for xref:storage:BlockManager.adoc#slaveEndpoint[BlockManager]. == [[creating-instance]] Creating Instance BlockManagerSlaveEndpoint takes the following to be created: [[rpcEnv]] xref:rpc:RpcEnv.adoc[] [[blockManager]] Parent xref:storage:BlockManager.adoc[] [[mapOutputTracker]] xref:scheduler:MapOutputTracker.adoc[] BlockManagerSlaveEndpoint is created for xref:storage:BlockManager.adoc#slaveEndpoint[BlockManager] (and registered under the name BlockManagerEndpoint[ID] ). == [[messages]] Messages === [[GetBlockStatus]] GetBlockStatus [source, scala] \u00b6 GetBlockStatus( blockId: BlockId, askSlaves: Boolean = true) When received, BlockManagerSlaveEndpoint requests the < > for the xref:storage:BlockManager.adoc#getStatus[status of a given block] (by xref:storage:BlockId.adoc[]) and sends it back to a sender. Posted when...FIXME === [[GetMatchingBlockIds]] GetMatchingBlockIds [source, scala] \u00b6 GetMatchingBlockIds( filter: BlockId => Boolean, askSlaves: Boolean = true) When received, BlockManagerSlaveEndpoint requests the < > to xref:storage:BlockManager.adoc#getMatchingBlockIds[find IDs of existing blocks for a given filter] and sends them back to a sender. Posted when...FIXME === [[RemoveBlock]] RemoveBlock [source, scala] \u00b6 RemoveBlock( blockId: BlockId) When received, BlockManagerSlaveEndpoint prints out the following DEBUG message to the logs: [source,plaintext] \u00b6 removing block [blockId] \u00b6 BlockManagerSlaveEndpoint then < blockId block>>. When the computation is successful, you should see the following DEBUG in the logs: Done removing block [blockId], response is [response] And true response is sent back. You should see the following DEBUG in the logs: Sent response: true to [senderAddress] In case of failure, you should see the following ERROR in the logs and the stack trace. Error in removing block [blockId] === [[RemoveBroadcast]] RemoveBroadcast [source, scala] \u00b6 RemoveBroadcast( broadcastId: Long, removeFromDriver: Boolean = true) When received, BlockManagerSlaveEndpoint prints out the following DEBUG message to the logs: [source,plaintext] \u00b6 removing broadcast [broadcastId] \u00b6 It then calls < broadcastId broadcast>>. When the computation is successful, you should see the following DEBUG in the logs: Done removing broadcast [broadcastId], response is [response] And the result is sent back. You should see the following DEBUG in the logs: Sent response: [response] to [senderAddress] In case of failure, you should see the following ERROR in the logs and the stack trace. Error in removing broadcast [broadcastId] === [[RemoveRdd]] RemoveRdd [source, scala] \u00b6 RemoveRdd( rddId: Int) When received, BlockManagerSlaveEndpoint prints out the following DEBUG message to the logs: removing RDD [rddId] It then calls < rddId RDD>>. NOTE: Handling RemoveRdd messages happens on a separate thread. See < >. When the computation is successful, you should see the following DEBUG in the logs: Done removing RDD [rddId], response is [response] And the number of blocks removed is sent back. You should see the following DEBUG in the logs: Sent response: [#blocks] to [senderAddress] In case of failure, you should see the following ERROR in the logs and the stack trace. Error in removing RDD [rddId] === [[RemoveShuffle]] RemoveShuffle [source, scala] \u00b6 RemoveShuffle( shuffleId: Int) When received, BlockManagerSlaveEndpoint prints out the following DEBUG message to the logs: removing shuffle [shuffleId] If xref:scheduler:MapOutputTracker.adoc[MapOutputTracker] was given (when the RPC endpoint was created), it calls xref:scheduler:MapOutputTracker.adoc#unregisterShuffle[MapOutputTracker to unregister the shuffleId shuffle]. It then calls xref:shuffle:ShuffleManager.adoc#unregisterShuffle[ShuffleManager to unregister the shuffleId shuffle]. NOTE: Handling RemoveShuffle messages happens on a separate thread. See < >. When the computation is successful, you should see the following DEBUG in the logs: Done removing shuffle [shuffleId], response is [response] And the result is sent back. You should see the following DEBUG in the logs: Sent response: [response] to [senderAddress] In case of failure, you should see the following ERROR in the logs and the stack trace. Error in removing shuffle [shuffleId] Posted when xref:BlockManagerMaster.adoc#removeShuffle[BlockManagerMaster] and xref:storage:BlockManagerMasterEndpoint.adoc#removeShuffle[BlockManagerMasterEndpoint] are requested to remove all blocks of a shuffle. === [[ReplicateBlock]] ReplicateBlock [source, scala] \u00b6 ReplicateBlock( blockId: BlockId, replicas: Seq[BlockManagerId], maxReplicas: Int) When received, BlockManagerSlaveEndpoint...FIXME Posted when...FIXME === [[TriggerThreadDump]] TriggerThreadDump When received, BlockManagerSlaveEndpoint is requested for the thread info for all live threads with stack trace and synchronization information. == [[asyncThreadPool]][[asyncExecutionContext]] block-manager-slave-async-thread-pool Thread Pool BlockManagerSlaveEndpoint creates a thread pool of maximum 100 daemon threads with block-manager-slave-async-thread-pool thread prefix (using {java-javadoc-url}/java/util/concurrent/ThreadPoolExecutor.html[java.util.concurrent.ThreadPoolExecutor]). BlockManagerSlaveEndpoint uses the thread pool (as a Scala implicit value) when requested to < > to communicate in a non-blocking, asynchronous way. The thread pool is shut down when BlockManagerSlaveEndpoint is requested to < >. The reason for the async thread pool is that the block-related operations might take quite some time and to release the main RPC thread other threads are spawned to talk to the external services and pass responses on to the clients. == [[doAsync]] doAsync Internal Method [source,scala] \u00b6 doAsync T ( body: => T) doAsync creates a Scala Future to execute the following asynchronously (i.e. on a separate thread from the < >): . Prints out the given actionMessage as a DEBUG message to the logs . Executes the given body When completed successfully, doAsync prints out the following DEBUG messages to the logs and requests the given RpcCallContext to reply the response to the sender. [source,plaintext] \u00b6 Done [actionMessage], response is [response] Sent response: [response] to [senderAddress] In case of a failure, doAsync prints out the following ERROR message to the logs and requests the given RpcCallContext to send the failure to the sender. [source,plaintext] \u00b6 Error in [actionMessage] \u00b6 doAsync is used when BlockManagerSlaveEndpoint is requested to handle < >, < >, < > and < > messages. == [[logging]] Logging Enable ALL logging level for org.apache.spark.storage.BlockManagerSlaveEndpoint logger to see what happens inside. Add the following line to conf/log4j.properties : [source] \u00b6 log4j.logger.org.apache.spark.storage.BlockManagerSlaveEndpoint=ALL \u00b6 Refer to xref:ROOT:spark-logging.adoc[Logging].","title":"BlockManagerSlaveEndpoint"},{"location":"storage/BlockManagerSlaveEndpoint/#source-scala","text":"GetBlockStatus( blockId: BlockId, askSlaves: Boolean = true) When received, BlockManagerSlaveEndpoint requests the < > for the xref:storage:BlockManager.adoc#getStatus[status of a given block] (by xref:storage:BlockId.adoc[]) and sends it back to a sender. Posted when...FIXME === [[GetMatchingBlockIds]] GetMatchingBlockIds","title":"[source, scala]"},{"location":"storage/BlockManagerSlaveEndpoint/#source-scala_1","text":"GetMatchingBlockIds( filter: BlockId => Boolean, askSlaves: Boolean = true) When received, BlockManagerSlaveEndpoint requests the < > to xref:storage:BlockManager.adoc#getMatchingBlockIds[find IDs of existing blocks for a given filter] and sends them back to a sender. Posted when...FIXME === [[RemoveBlock]] RemoveBlock","title":"[source, scala]"},{"location":"storage/BlockManagerSlaveEndpoint/#source-scala_2","text":"RemoveBlock( blockId: BlockId) When received, BlockManagerSlaveEndpoint prints out the following DEBUG message to the logs:","title":"[source, scala]"},{"location":"storage/BlockManagerSlaveEndpoint/#sourceplaintext","text":"","title":"[source,plaintext]"},{"location":"storage/BlockManagerSlaveEndpoint/#removing-block-blockid","text":"BlockManagerSlaveEndpoint then < blockId block>>. When the computation is successful, you should see the following DEBUG in the logs: Done removing block [blockId], response is [response] And true response is sent back. You should see the following DEBUG in the logs: Sent response: true to [senderAddress] In case of failure, you should see the following ERROR in the logs and the stack trace. Error in removing block [blockId] === [[RemoveBroadcast]] RemoveBroadcast","title":"removing block [blockId]"},{"location":"storage/BlockManagerSlaveEndpoint/#source-scala_3","text":"RemoveBroadcast( broadcastId: Long, removeFromDriver: Boolean = true) When received, BlockManagerSlaveEndpoint prints out the following DEBUG message to the logs:","title":"[source, scala]"},{"location":"storage/BlockManagerSlaveEndpoint/#sourceplaintext_1","text":"","title":"[source,plaintext]"},{"location":"storage/BlockManagerSlaveEndpoint/#removing-broadcast-broadcastid","text":"It then calls < broadcastId broadcast>>. When the computation is successful, you should see the following DEBUG in the logs: Done removing broadcast [broadcastId], response is [response] And the result is sent back. You should see the following DEBUG in the logs: Sent response: [response] to [senderAddress] In case of failure, you should see the following ERROR in the logs and the stack trace. Error in removing broadcast [broadcastId] === [[RemoveRdd]] RemoveRdd","title":"removing broadcast [broadcastId]"},{"location":"storage/BlockManagerSlaveEndpoint/#source-scala_4","text":"RemoveRdd( rddId: Int) When received, BlockManagerSlaveEndpoint prints out the following DEBUG message to the logs: removing RDD [rddId] It then calls < rddId RDD>>. NOTE: Handling RemoveRdd messages happens on a separate thread. See < >. When the computation is successful, you should see the following DEBUG in the logs: Done removing RDD [rddId], response is [response] And the number of blocks removed is sent back. You should see the following DEBUG in the logs: Sent response: [#blocks] to [senderAddress] In case of failure, you should see the following ERROR in the logs and the stack trace. Error in removing RDD [rddId] === [[RemoveShuffle]] RemoveShuffle","title":"[source, scala]"},{"location":"storage/BlockManagerSlaveEndpoint/#source-scala_5","text":"RemoveShuffle( shuffleId: Int) When received, BlockManagerSlaveEndpoint prints out the following DEBUG message to the logs: removing shuffle [shuffleId] If xref:scheduler:MapOutputTracker.adoc[MapOutputTracker] was given (when the RPC endpoint was created), it calls xref:scheduler:MapOutputTracker.adoc#unregisterShuffle[MapOutputTracker to unregister the shuffleId shuffle]. It then calls xref:shuffle:ShuffleManager.adoc#unregisterShuffle[ShuffleManager to unregister the shuffleId shuffle]. NOTE: Handling RemoveShuffle messages happens on a separate thread. See < >. When the computation is successful, you should see the following DEBUG in the logs: Done removing shuffle [shuffleId], response is [response] And the result is sent back. You should see the following DEBUG in the logs: Sent response: [response] to [senderAddress] In case of failure, you should see the following ERROR in the logs and the stack trace. Error in removing shuffle [shuffleId] Posted when xref:BlockManagerMaster.adoc#removeShuffle[BlockManagerMaster] and xref:storage:BlockManagerMasterEndpoint.adoc#removeShuffle[BlockManagerMasterEndpoint] are requested to remove all blocks of a shuffle. === [[ReplicateBlock]] ReplicateBlock","title":"[source, scala]"},{"location":"storage/BlockManagerSlaveEndpoint/#source-scala_6","text":"ReplicateBlock( blockId: BlockId, replicas: Seq[BlockManagerId], maxReplicas: Int) When received, BlockManagerSlaveEndpoint...FIXME Posted when...FIXME === [[TriggerThreadDump]] TriggerThreadDump When received, BlockManagerSlaveEndpoint is requested for the thread info for all live threads with stack trace and synchronization information. == [[asyncThreadPool]][[asyncExecutionContext]] block-manager-slave-async-thread-pool Thread Pool BlockManagerSlaveEndpoint creates a thread pool of maximum 100 daemon threads with block-manager-slave-async-thread-pool thread prefix (using {java-javadoc-url}/java/util/concurrent/ThreadPoolExecutor.html[java.util.concurrent.ThreadPoolExecutor]). BlockManagerSlaveEndpoint uses the thread pool (as a Scala implicit value) when requested to < > to communicate in a non-blocking, asynchronous way. The thread pool is shut down when BlockManagerSlaveEndpoint is requested to < >. The reason for the async thread pool is that the block-related operations might take quite some time and to release the main RPC thread other threads are spawned to talk to the external services and pass responses on to the clients. == [[doAsync]] doAsync Internal Method","title":"[source, scala]"},{"location":"storage/BlockManagerSlaveEndpoint/#sourcescala","text":"doAsync T ( body: => T) doAsync creates a Scala Future to execute the following asynchronously (i.e. on a separate thread from the < >): . Prints out the given actionMessage as a DEBUG message to the logs . Executes the given body When completed successfully, doAsync prints out the following DEBUG messages to the logs and requests the given RpcCallContext to reply the response to the sender.","title":"[source,scala]"},{"location":"storage/BlockManagerSlaveEndpoint/#sourceplaintext_2","text":"Done [actionMessage], response is [response] Sent response: [response] to [senderAddress] In case of a failure, doAsync prints out the following ERROR message to the logs and requests the given RpcCallContext to send the failure to the sender.","title":"[source,plaintext]"},{"location":"storage/BlockManagerSlaveEndpoint/#sourceplaintext_3","text":"","title":"[source,plaintext]"},{"location":"storage/BlockManagerSlaveEndpoint/#error-in-actionmessage","text":"doAsync is used when BlockManagerSlaveEndpoint is requested to handle < >, < >, < > and < > messages. == [[logging]] Logging Enable ALL logging level for org.apache.spark.storage.BlockManagerSlaveEndpoint logger to see what happens inside. Add the following line to conf/log4j.properties :","title":"Error in [actionMessage]"},{"location":"storage/BlockManagerSlaveEndpoint/#source","text":"","title":"[source]"},{"location":"storage/BlockManagerSlaveEndpoint/#log4jloggerorgapachesparkstorageblockmanagerslaveendpointall","text":"Refer to xref:ROOT:spark-logging.adoc[Logging].","title":"log4j.logger.org.apache.spark.storage.BlockManagerSlaveEndpoint=ALL"},{"location":"storage/BlockManagerSource/","text":"== [[BlockManagerSource]] BlockManagerSource -- Metrics Source for BlockManager BlockManagerSource is the link:spark-metrics-Source.adoc[metrics source] of a xref:storage:BlockManager.adoc[BlockManager]. [[sourceName]] BlockManagerSource is registered under the name BlockManager (when SparkContext is link:spark-SparkContext-creating-instance-internals.adoc#registerSource[created]). [[metrics]] .BlockManagerSource's Gauge Metrics (in alphabetical order) [width=\"100%\",cols=\"1,1,2\",options=\"header\"] |=== | Name | Type | Description | disk.diskSpaceUsed_MB | long | Requests BlockManagerMaster for xref:BlockManagerMaster.adoc#getStorageStatus[storage status] (for every xref:storage:BlockManager.adoc[BlockManager]) and sums up their disk space used ( diskUsed ). | memory.maxMem_MB | long | Requests BlockManagerMaster for xref:BlockManagerMaster.adoc#getStorageStatus[storage status] (for every xref:storage:BlockManager.adoc[BlockManager]) and sums up their maximum memory limit ( maxMem ). | memory.maxOffHeapMem_MB | long | Requests BlockManagerMaster for xref:BlockManagerMaster.adoc#getStorageStatus[storage status] (for every xref:storage:BlockManager.adoc[BlockManager]) and sums up their off-heap memory remaining ( maxOffHeapMem ). | memory.maxOnHeapMem_MB | long | Requests BlockManagerMaster for xref:BlockManagerMaster.adoc#getStorageStatus[storage status] (for every xref:storage:BlockManager.adoc[BlockManager]) and sums up their on-heap memory remaining ( maxOnHeapMem ). | memory.memUsed_MB | long | Requests BlockManagerMaster for xref:BlockManagerMaster.adoc#getStorageStatus[storage status] (for every xref:storage:BlockManager.adoc[BlockManager]) and sums up their memory used ( memUsed ). | memory.offHeapMemUsed_MB | long | Requests BlockManagerMaster for xref:BlockManagerMaster.adoc#getStorageStatus[storage status] (for every xref:storage:BlockManager.adoc[BlockManager]) and sums up their off-heap memory used ( offHeapMemUsed ). | memory.onHeapMemUsed_MB | long | Requests BlockManagerMaster for xref:BlockManagerMaster.adoc#getStorageStatus[storage status] (for every xref:storage:BlockManager.adoc[BlockManager]) and sums up their on-heap memory used ( onHeapMemUsed ). | memory.remainingMem_MB | long | Requests BlockManagerMaster for xref:BlockManagerMaster.adoc#getStorageStatus[storage status] (for every xref:storage:BlockManager.adoc[BlockManager]) and sums up their memory remaining ( memRemaining ). | memory.remainingOffHeapMem_MB | long | Requests BlockManagerMaster for xref:BlockManagerMaster.adoc#getStorageStatus[storage status] (for every xref:storage:BlockManager.adoc[BlockManager]) and sums up their off-heap memory remaining ( offHeapMemRemaining ). | memory.remainingOnHeapMem_MB | long | Requests BlockManagerMaster for xref:BlockManagerMaster.adoc#getStorageStatus[storage status] (for every xref:storage:BlockManager.adoc[BlockManager]) and sums up their on-heap memory remaining ( onHeapMemRemaining ). |=== You can access the BlockManagerSource < > using the web UI's port (as link:spark-webui-properties.adoc#spark.ui.port[spark.ui.port] configuration property). $ http --follow http://localhost:4040/metrics/json \\ | jq '.gauges | keys | .[] | select(test(\".driver.BlockManager\"; \"g\"))' \"local-1528725411625.driver.BlockManager.disk.diskSpaceUsed_MB\" \"local-1528725411625.driver.BlockManager.memory.maxMem_MB\" \"local-1528725411625.driver.BlockManager.memory.maxOffHeapMem_MB\" \"local-1528725411625.driver.BlockManager.memory.maxOnHeapMem_MB\" \"local-1528725411625.driver.BlockManager.memory.memUsed_MB\" \"local-1528725411625.driver.BlockManager.memory.offHeapMemUsed_MB\" \"local-1528725411625.driver.BlockManager.memory.onHeapMemUsed_MB\" \"local-1528725411625.driver.BlockManager.memory.remainingMem_MB\" \"local-1528725411625.driver.BlockManager.memory.remainingOffHeapMem_MB\" \"local-1528725411625.driver.BlockManager.memory.remainingOnHeapMem_MB\" [[creating-instance]] [[blockManager]] BlockManagerSource takes a xref:storage:BlockManager.adoc[BlockManager] when created. BlockManagerSource is < > when SparkContext is link:spark-SparkContext-creating-instance-internals.adoc#registerSource[created].","title":"BlockManagerSource"},{"location":"storage/BlockTransferService/","text":"= BlockTransferService BlockTransferService is an < > of the xref:storage:ShuffleClient.adoc[] abstraction for < > that can < > and < > blocks of data synchronously or asynchronously. BlockTransferService is a networking service available by a < > and a < >. BlockTransferService was introduced in https://issues.apache.org/jira/browse/SPARK-3019[SPARK-3019 Pluggable block transfer interface (BlockTransferService)]. == [[contract]] Contract === [[close]] close [source,scala] \u00b6 close(): Unit \u00b6 Used when BlockManager is requested to xref:storage:BlockManager.adoc#stop[stop] === [[fetchBlocks]] fetchBlocks [source,scala] \u00b6 fetchBlocks( host: String, port: Int, execId: String, blockIds: Array[String], listener: BlockFetchingListener, tempFileManager: DownloadFileManager): Unit Fetches a sequence of blocks from a remote node asynchronously fetchBlocks is part of the xref:storage:ShuffleClient.adoc#fetchBlocks[ShuffleClient] abstraction. Used when BlockTransferService is requested to < > === [[hostName]] hostName [source,scala] \u00b6 hostName: String \u00b6 Used when BlockManager is requested to xref:storage:BlockManager.adoc#initialize[initialize] === [[init]] init [source,scala] \u00b6 init( blockDataManager: BlockDataManager): Unit Used when BlockManager is requested to xref:storage:BlockManager.adoc#initialize[initialize] (with the xref:storage:BlockDataManager.adoc[] being the BlockManager itself) === [[port]] port [source,scala] \u00b6 port: Int \u00b6 Used when BlockManager is requested to xref:storage:BlockManager.adoc#initialize[initialize] === [[uploadBlock]] uploadBlock [source,scala] \u00b6 uploadBlock( hostname: String, port: Int, execId: String, blockId: BlockId, blockData: ManagedBuffer, level: StorageLevel, classTag: ClassTag[_]): Future[Unit] Used when BlockTransferService is requested to < > == [[implementations]] BlockTransferServices xref:storage:NettyBlockTransferService.adoc[] is the default and only known BlockTransferService. == [[fetchBlockSync]] fetchBlockSync Method [source, scala] \u00b6 fetchBlockSync( host: String, port: Int, execId: String, blockId: String, tempFileManager: TempFileManager): ManagedBuffer fetchBlockSync...FIXME Synchronous (and hence blocking) fetchBlockSync to fetch one block blockId (that corresponds to the xref:storage:ShuffleClient.adoc[] parent's asynchronous xref:storage:ShuffleClient.adoc#fetchBlocks[fetchBlocks]). fetchBlockSync is a mere wrapper around xref:storage:ShuffleClient.adoc#fetchBlocks[fetchBlocks] to fetch one blockId block that waits until the fetch finishes. fetchBlockSync is used when...FIXME == [[uploadBlockSync]] Uploading Single Block to Remote Node (Blocking Fashion) [source, scala] \u00b6 uploadBlockSync( hostname: String, port: Int, execId: String, blockId: BlockId, blockData: ManagedBuffer, level: StorageLevel, classTag: ClassTag[_]): Unit uploadBlockSync...FIXME uploadBlockSync is a mere blocking wrapper around < > that waits until the upload finishes. uploadBlockSync is used when BlockManager is requested to xref:storage:BlockManager.adoc#replicate[replicate] (when a xref:storage:StorageLevel.adoc[replication level is greater than 1]).","title":"BlockTransferService"},{"location":"storage/BlockTransferService/#sourcescala","text":"","title":"[source,scala]"},{"location":"storage/BlockTransferService/#close-unit","text":"Used when BlockManager is requested to xref:storage:BlockManager.adoc#stop[stop] === [[fetchBlocks]] fetchBlocks","title":"close(): Unit"},{"location":"storage/BlockTransferService/#sourcescala_1","text":"fetchBlocks( host: String, port: Int, execId: String, blockIds: Array[String], listener: BlockFetchingListener, tempFileManager: DownloadFileManager): Unit Fetches a sequence of blocks from a remote node asynchronously fetchBlocks is part of the xref:storage:ShuffleClient.adoc#fetchBlocks[ShuffleClient] abstraction. Used when BlockTransferService is requested to < > === [[hostName]] hostName","title":"[source,scala]"},{"location":"storage/BlockTransferService/#sourcescala_2","text":"","title":"[source,scala]"},{"location":"storage/BlockTransferService/#hostname-string","text":"Used when BlockManager is requested to xref:storage:BlockManager.adoc#initialize[initialize] === [[init]] init","title":"hostName: String"},{"location":"storage/BlockTransferService/#sourcescala_3","text":"init( blockDataManager: BlockDataManager): Unit Used when BlockManager is requested to xref:storage:BlockManager.adoc#initialize[initialize] (with the xref:storage:BlockDataManager.adoc[] being the BlockManager itself) === [[port]] port","title":"[source,scala]"},{"location":"storage/BlockTransferService/#sourcescala_4","text":"","title":"[source,scala]"},{"location":"storage/BlockTransferService/#port-int","text":"Used when BlockManager is requested to xref:storage:BlockManager.adoc#initialize[initialize] === [[uploadBlock]] uploadBlock","title":"port: Int"},{"location":"storage/BlockTransferService/#sourcescala_5","text":"uploadBlock( hostname: String, port: Int, execId: String, blockId: BlockId, blockData: ManagedBuffer, level: StorageLevel, classTag: ClassTag[_]): Future[Unit] Used when BlockTransferService is requested to < > == [[implementations]] BlockTransferServices xref:storage:NettyBlockTransferService.adoc[] is the default and only known BlockTransferService. == [[fetchBlockSync]] fetchBlockSync Method","title":"[source,scala]"},{"location":"storage/BlockTransferService/#source-scala","text":"fetchBlockSync( host: String, port: Int, execId: String, blockId: String, tempFileManager: TempFileManager): ManagedBuffer fetchBlockSync...FIXME Synchronous (and hence blocking) fetchBlockSync to fetch one block blockId (that corresponds to the xref:storage:ShuffleClient.adoc[] parent's asynchronous xref:storage:ShuffleClient.adoc#fetchBlocks[fetchBlocks]). fetchBlockSync is a mere wrapper around xref:storage:ShuffleClient.adoc#fetchBlocks[fetchBlocks] to fetch one blockId block that waits until the fetch finishes. fetchBlockSync is used when...FIXME == [[uploadBlockSync]] Uploading Single Block to Remote Node (Blocking Fashion)","title":"[source, scala]"},{"location":"storage/BlockTransferService/#source-scala_1","text":"uploadBlockSync( hostname: String, port: Int, execId: String, blockId: BlockId, blockData: ManagedBuffer, level: StorageLevel, classTag: ClassTag[_]): Unit uploadBlockSync...FIXME uploadBlockSync is a mere blocking wrapper around < > that waits until the upload finishes. uploadBlockSync is used when BlockManager is requested to xref:storage:BlockManager.adoc#replicate[replicate] (when a xref:storage:StorageLevel.adoc[replication level is greater than 1]).","title":"[source, scala]"},{"location":"storage/DiskBlockManager/","text":"= [[DiskBlockManager]] DiskBlockManager DiskBlockManager creates and maintains the logical mapping between logical blocks and physical on-disk locations for a xref:storage:BlockManager.adoc#diskBlockManager[BlockManager]. .DiskBlockManager and BlockManager image::DiskBlockManager-BlockManager.png[align=\"center\"] By default, one block is mapped to one file with a name given by its BlockId . It is however possible to have a block map to only a segment of a file. Block files are hashed among the < >. DiskBlockManager is used to create a xref:DiskStore.adoc[DiskStore]. TIP: Consult xref:demo-diskblockmanager-and-block-data.adoc[Demo: DiskBlockManager and Block Data]. == [[creating-instance]] Creating Instance DiskBlockManager takes the following to be created: [[conf]] xref:ROOT:SparkConf.adoc[SparkConf] [[deleteFilesOnStop]] deleteFilesOnStop flag When created, DiskBlockManager < > and initializes the internal < > collection of locks for every local directory. In the end, DiskBlockManager < > to clean up the local directories for blocks. == [[localDirs]] Local Directories for Blocks [source,scala] \u00b6 localDirs: Array[File] \u00b6 While being created, DiskBlockManager < > for block data. DiskBlockManager expects at least one local directory or prints out the following ERROR message to the logs and exits the JVM (with exit code 53). Failed to create any local dir. localDirs is used when: DiskBlockManager is requested to < >, initialize the < > internal registry, and to < > BlockManager is requested to xref:storage:BlockManager.adoc#registerWithExternalShuffleServer[register with an external shuffle server] == [[subDirsPerLocalDir]][[subDirs]] File Locks for Local Block Store Directories [source, scala] \u00b6 subDirs: Array[Array[File]] \u00b6 subDirs is a lookup table for file locks of every < > (with the first dimension for local directories and the second for locks). The number of block subdirectories is controlled by xref:ROOT:configuration-properties.adoc#spark.diskStore.subDirectories[spark.diskStore.subDirectories] configuration property (default: 64 ). subDirs(dirId)(subDirId) is used to access subDirId subdirectory in dirId local directory. subDirs is used when DiskBlockManager is requested for a < > or < >. == [[createLocalDirs]] Creating Local Directories for Block Data [source, scala] \u00b6 createLocalDirs( conf: SparkConf): Array[File] createLocalDirs creates blockmgr-[random UUID] directory under local directories to store block data. Internally, createLocalDirs < > and creates a subdirectory blockmgr-[UUID] under every configured parent directory. For every local directory, createLocalDirs prints out the following INFO message to the logs: Created local directory at [localDir] In case of an exception, createLocalDirs prints out the following ERROR message to the logs and skips the directory. Failed to create local dir in [rootDir]. Ignoring this directory. createLocalDirs is used when the < > internal registry is initialized. == [[getFile]] Finding Block File (and Creating Parent Directories) [source, scala] \u00b6 getFile( blockId: BlockId): File // <1> getFile( filename: String): File <1> Uses the name of the given BlockId getFile computes a hash of the file name of the input xref:storage:BlockId.adoc[] that is used for the name of the parent directory and subdirectory. getFile creates the subdirectory unless it already exists. getFile is used when: DiskBlockManager is requested to < >, < >, < > DiskStore is requested to xref:DiskStore.adoc#getBytes[getBytes], xref:DiskStore.adoc#remove[remove], xref:DiskStore.adoc#contains[contains], and xref:DiskStore.adoc#put[put] IndexShuffleBlockResolver is requested to xref:shuffle:IndexShuffleBlockResolver.adoc#getDataFile[getDataFile] and xref:shuffle:IndexShuffleBlockResolver.adoc#getIndexFile[getIndexFile] == [[createTempShuffleBlock]] createTempShuffleBlock Method [source, scala] \u00b6 createTempShuffleBlock(): (TempShuffleBlockId, File) \u00b6 createTempShuffleBlock creates a temporary TempShuffleBlockId block. CAUTION: FIXME == [[getAllFiles]] All Block Files [source, scala] \u00b6 getAllFiles(): Seq[File] \u00b6 getAllFiles ...FIXME NOTE: getAllFiles is used exclusively when DiskBlockManager is requested to < >. == [[addShutdownHook]] Registering Shutdown Hook -- addShutdownHook Internal Method [source, scala] \u00b6 addShutdownHook(): AnyRef \u00b6 addShutdownHook registers a shutdown hook to execute < > at shutdown. When executed, you should see the following DEBUG message in the logs: DEBUG DiskBlockManager: Adding shutdown hook addShutdownHook adds the shutdown hook so it prints the following INFO message and executes < >. INFO DiskBlockManager: Shutdown hook called == [[doStop]] Stopping DiskBlockManager (Removing Local Directories for Blocks) -- doStop Internal Method [source, scala] \u00b6 doStop(): Unit \u00b6 doStop deletes the local directories recursively (only when the constructor's deleteFilesOnStop is enabled and the parent directories are not registered to be removed at shutdown). NOTE: doStop is used when DiskBlockManager is requested to < > or < >. == [[getConfiguredLocalDirs]] Getting Local Directories for Spark to Write Files -- Utils.getConfiguredLocalDirs Internal Method [source, scala] \u00b6 getConfiguredLocalDirs(conf: SparkConf): Array[String] \u00b6 getConfiguredLocalDirs returns the local directories where Spark can write files. Internally, getConfiguredLocalDirs uses conf xref:ROOT:SparkConf.adoc[SparkConf] to know if xref:deploy:ExternalShuffleService.adoc[External Shuffle Service] is enabled (based on xref:ROOT:configuration-properties.adoc#spark.shuffle.service.enabled[spark.shuffle.service.enabled] configuration property). getConfiguredLocalDirs checks if < > and if so, returns < LOCAL_DIRS -controlled local directories>>. In non-YARN mode (or for the driver in yarn-client mode), getConfiguredLocalDirs checks the following environment variables (in the order) and returns the value of the first met: SPARK_EXECUTOR_DIRS environment variable SPARK_LOCAL_DIRS environment variable MESOS_DIRECTORY environment variable (only when External Shuffle Service is not used) In the end, when no earlier environment variables were found, getConfiguredLocalDirs uses link:spark-properties.adoc#spark.local.dir[spark.local.dir] Spark property or falls back on java.io.tmpdir System property. [NOTE] \u00b6 getConfiguredLocalDirs is used when: DiskBlockManager is requested to < > * Utils helper is requested to link:spark-Utils.adoc#getLocalDir[getLocalDir] and link:spark-Utils.adoc#getOrCreateLocalRootDirsImpl[getOrCreateLocalRootDirsImpl] \u00b6 == [[getYarnLocalDirs]] Getting Writable Directories in YARN -- getYarnLocalDirs Internal Method [source, scala] \u00b6 getYarnLocalDirs(conf: SparkConf): String \u00b6 getYarnLocalDirs uses conf xref:ROOT:SparkConf.adoc[SparkConf] to read LOCAL_DIRS environment variable with comma-separated local directories (that have already been created and secured so that only the user has access to them). getYarnLocalDirs throws an Exception with the message Yarn Local dirs can't be empty if LOCAL_DIRS environment variable was not set. == [[isRunningInYarnContainer]] Checking If Spark Runs on YARN -- isRunningInYarnContainer Internal Method [source, scala] \u00b6 isRunningInYarnContainer(conf: SparkConf): Boolean \u00b6 isRunningInYarnContainer uses conf xref:ROOT:SparkConf.adoc[SparkConf] to read Hadoop YARN's link: http://hadoop.apache.org/docs/current/hadoop-yarn/hadoop-yarn-api/apidocs/org/apache/hadoop/yarn/api/ApplicationConstants.Environment.html#CONTAINER_ID [ CONTAINER_ID environment variable] to find out if Spark runs in a YARN container. NOTE: CONTAINER_ID environment variable is exported by YARN NodeManager. == [[getAllBlocks]] Getting All Blocks (From Files Stored On Disk) [source, scala] \u00b6 getAllBlocks(): Seq[BlockId] \u00b6 getAllBlocks gets all the blocks stored on disk. Internally, getAllBlocks takes the < > and returns their names (as BlockId ). getAllBlocks is used when BlockManager is requested to xref:storage:BlockManager.adoc#getMatchingBlockIds[find IDs of existing blocks for a given filter]. == [[stop]] stop Internal Method [source, scala] \u00b6 stop(): Unit \u00b6 stop ...FIXME NOTE: stop is used exclusively when BlockManager is requested to xref:storage:BlockManager.adoc#stop[stop]. == [[logging]] Logging Enable ALL logging level for org.apache.spark.storage.DiskBlockManager logger to see what happens inside. Add the following line to conf/log4j.properties : [source] \u00b6 log4j.logger.org.apache.spark.storage.DiskBlockManager=ALL \u00b6 Refer to xref:ROOT:spark-logging.adoc[Logging].","title":"DiskBlockManager"},{"location":"storage/DiskBlockManager/#sourcescala","text":"","title":"[source,scala]"},{"location":"storage/DiskBlockManager/#localdirs-arrayfile","text":"While being created, DiskBlockManager < > for block data. DiskBlockManager expects at least one local directory or prints out the following ERROR message to the logs and exits the JVM (with exit code 53). Failed to create any local dir. localDirs is used when: DiskBlockManager is requested to < >, initialize the < > internal registry, and to < > BlockManager is requested to xref:storage:BlockManager.adoc#registerWithExternalShuffleServer[register with an external shuffle server] == [[subDirsPerLocalDir]][[subDirs]] File Locks for Local Block Store Directories","title":"localDirs: Array[File]"},{"location":"storage/DiskBlockManager/#source-scala","text":"","title":"[source, scala]"},{"location":"storage/DiskBlockManager/#subdirs-arrayarrayfile","text":"subDirs is a lookup table for file locks of every < > (with the first dimension for local directories and the second for locks). The number of block subdirectories is controlled by xref:ROOT:configuration-properties.adoc#spark.diskStore.subDirectories[spark.diskStore.subDirectories] configuration property (default: 64 ). subDirs(dirId)(subDirId) is used to access subDirId subdirectory in dirId local directory. subDirs is used when DiskBlockManager is requested for a < > or < >. == [[createLocalDirs]] Creating Local Directories for Block Data","title":"subDirs: Array[Array[File]]"},{"location":"storage/DiskBlockManager/#source-scala_1","text":"createLocalDirs( conf: SparkConf): Array[File] createLocalDirs creates blockmgr-[random UUID] directory under local directories to store block data. Internally, createLocalDirs < > and creates a subdirectory blockmgr-[UUID] under every configured parent directory. For every local directory, createLocalDirs prints out the following INFO message to the logs: Created local directory at [localDir] In case of an exception, createLocalDirs prints out the following ERROR message to the logs and skips the directory. Failed to create local dir in [rootDir]. Ignoring this directory. createLocalDirs is used when the < > internal registry is initialized. == [[getFile]] Finding Block File (and Creating Parent Directories)","title":"[source, scala]"},{"location":"storage/DiskBlockManager/#source-scala_2","text":"getFile( blockId: BlockId): File // <1> getFile( filename: String): File <1> Uses the name of the given BlockId getFile computes a hash of the file name of the input xref:storage:BlockId.adoc[] that is used for the name of the parent directory and subdirectory. getFile creates the subdirectory unless it already exists. getFile is used when: DiskBlockManager is requested to < >, < >, < > DiskStore is requested to xref:DiskStore.adoc#getBytes[getBytes], xref:DiskStore.adoc#remove[remove], xref:DiskStore.adoc#contains[contains], and xref:DiskStore.adoc#put[put] IndexShuffleBlockResolver is requested to xref:shuffle:IndexShuffleBlockResolver.adoc#getDataFile[getDataFile] and xref:shuffle:IndexShuffleBlockResolver.adoc#getIndexFile[getIndexFile] == [[createTempShuffleBlock]] createTempShuffleBlock Method","title":"[source, scala]"},{"location":"storage/DiskBlockManager/#source-scala_3","text":"","title":"[source, scala]"},{"location":"storage/DiskBlockManager/#createtempshuffleblock-tempshuffleblockid-file","text":"createTempShuffleBlock creates a temporary TempShuffleBlockId block. CAUTION: FIXME == [[getAllFiles]] All Block Files","title":"createTempShuffleBlock(): (TempShuffleBlockId, File)"},{"location":"storage/DiskBlockManager/#source-scala_4","text":"","title":"[source, scala]"},{"location":"storage/DiskBlockManager/#getallfiles-seqfile","text":"getAllFiles ...FIXME NOTE: getAllFiles is used exclusively when DiskBlockManager is requested to < >. == [[addShutdownHook]] Registering Shutdown Hook -- addShutdownHook Internal Method","title":"getAllFiles(): Seq[File]"},{"location":"storage/DiskBlockManager/#source-scala_5","text":"","title":"[source, scala]"},{"location":"storage/DiskBlockManager/#addshutdownhook-anyref","text":"addShutdownHook registers a shutdown hook to execute < > at shutdown. When executed, you should see the following DEBUG message in the logs: DEBUG DiskBlockManager: Adding shutdown hook addShutdownHook adds the shutdown hook so it prints the following INFO message and executes < >. INFO DiskBlockManager: Shutdown hook called == [[doStop]] Stopping DiskBlockManager (Removing Local Directories for Blocks) -- doStop Internal Method","title":"addShutdownHook(): AnyRef"},{"location":"storage/DiskBlockManager/#source-scala_6","text":"","title":"[source, scala]"},{"location":"storage/DiskBlockManager/#dostop-unit","text":"doStop deletes the local directories recursively (only when the constructor's deleteFilesOnStop is enabled and the parent directories are not registered to be removed at shutdown). NOTE: doStop is used when DiskBlockManager is requested to < > or < >. == [[getConfiguredLocalDirs]] Getting Local Directories for Spark to Write Files -- Utils.getConfiguredLocalDirs Internal Method","title":"doStop(): Unit"},{"location":"storage/DiskBlockManager/#source-scala_7","text":"","title":"[source, scala]"},{"location":"storage/DiskBlockManager/#getconfiguredlocaldirsconf-sparkconf-arraystring","text":"getConfiguredLocalDirs returns the local directories where Spark can write files. Internally, getConfiguredLocalDirs uses conf xref:ROOT:SparkConf.adoc[SparkConf] to know if xref:deploy:ExternalShuffleService.adoc[External Shuffle Service] is enabled (based on xref:ROOT:configuration-properties.adoc#spark.shuffle.service.enabled[spark.shuffle.service.enabled] configuration property). getConfiguredLocalDirs checks if < > and if so, returns < LOCAL_DIRS -controlled local directories>>. In non-YARN mode (or for the driver in yarn-client mode), getConfiguredLocalDirs checks the following environment variables (in the order) and returns the value of the first met: SPARK_EXECUTOR_DIRS environment variable SPARK_LOCAL_DIRS environment variable MESOS_DIRECTORY environment variable (only when External Shuffle Service is not used) In the end, when no earlier environment variables were found, getConfiguredLocalDirs uses link:spark-properties.adoc#spark.local.dir[spark.local.dir] Spark property or falls back on java.io.tmpdir System property.","title":"getConfiguredLocalDirs(conf: SparkConf): Array[String]"},{"location":"storage/DiskBlockManager/#note","text":"getConfiguredLocalDirs is used when: DiskBlockManager is requested to < >","title":"[NOTE]"},{"location":"storage/DiskBlockManager/#utils-helper-is-requested-to-linkspark-utilsadocgetlocaldirgetlocaldir-and-linkspark-utilsadocgetorcreatelocalrootdirsimplgetorcreatelocalrootdirsimpl","text":"== [[getYarnLocalDirs]] Getting Writable Directories in YARN -- getYarnLocalDirs Internal Method","title":"* Utils helper is requested to link:spark-Utils.adoc#getLocalDir[getLocalDir] and link:spark-Utils.adoc#getOrCreateLocalRootDirsImpl[getOrCreateLocalRootDirsImpl]"},{"location":"storage/DiskBlockManager/#source-scala_8","text":"","title":"[source, scala]"},{"location":"storage/DiskBlockManager/#getyarnlocaldirsconf-sparkconf-string","text":"getYarnLocalDirs uses conf xref:ROOT:SparkConf.adoc[SparkConf] to read LOCAL_DIRS environment variable with comma-separated local directories (that have already been created and secured so that only the user has access to them). getYarnLocalDirs throws an Exception with the message Yarn Local dirs can't be empty if LOCAL_DIRS environment variable was not set. == [[isRunningInYarnContainer]] Checking If Spark Runs on YARN -- isRunningInYarnContainer Internal Method","title":"getYarnLocalDirs(conf: SparkConf): String"},{"location":"storage/DiskBlockManager/#source-scala_9","text":"","title":"[source, scala]"},{"location":"storage/DiskBlockManager/#isrunninginyarncontainerconf-sparkconf-boolean","text":"isRunningInYarnContainer uses conf xref:ROOT:SparkConf.adoc[SparkConf] to read Hadoop YARN's link: http://hadoop.apache.org/docs/current/hadoop-yarn/hadoop-yarn-api/apidocs/org/apache/hadoop/yarn/api/ApplicationConstants.Environment.html#CONTAINER_ID [ CONTAINER_ID environment variable] to find out if Spark runs in a YARN container. NOTE: CONTAINER_ID environment variable is exported by YARN NodeManager. == [[getAllBlocks]] Getting All Blocks (From Files Stored On Disk)","title":"isRunningInYarnContainer(conf: SparkConf): Boolean"},{"location":"storage/DiskBlockManager/#source-scala_10","text":"","title":"[source, scala]"},{"location":"storage/DiskBlockManager/#getallblocks-seqblockid","text":"getAllBlocks gets all the blocks stored on disk. Internally, getAllBlocks takes the < > and returns their names (as BlockId ). getAllBlocks is used when BlockManager is requested to xref:storage:BlockManager.adoc#getMatchingBlockIds[find IDs of existing blocks for a given filter]. == [[stop]] stop Internal Method","title":"getAllBlocks(): Seq[BlockId]"},{"location":"storage/DiskBlockManager/#source-scala_11","text":"","title":"[source, scala]"},{"location":"storage/DiskBlockManager/#stop-unit","text":"stop ...FIXME NOTE: stop is used exclusively when BlockManager is requested to xref:storage:BlockManager.adoc#stop[stop]. == [[logging]] Logging Enable ALL logging level for org.apache.spark.storage.DiskBlockManager logger to see what happens inside. Add the following line to conf/log4j.properties :","title":"stop(): Unit"},{"location":"storage/DiskBlockManager/#source","text":"","title":"[source]"},{"location":"storage/DiskBlockManager/#log4jloggerorgapachesparkstoragediskblockmanagerall","text":"Refer to xref:ROOT:spark-logging.adoc[Logging].","title":"log4j.logger.org.apache.spark.storage.DiskBlockManager=ALL"},{"location":"storage/DiskBlockObjectWriter/","text":"= [[DiskBlockObjectWriter]] DiskBlockObjectWriter DiskBlockObjectWriter is a custom {java-javadoc-url}/java/io/OutputStream.html[java.io.OutputStream] that xref:storage:BlockManager.adoc#getDiskWriter[BlockManager] offers for < >. DiskBlockObjectWriter is used when: BypassMergeSortShuffleWriter is requested for xref:shuffle:BypassMergeSortShuffleWriter.adoc#partitionWriters[partition writers] UnsafeSorterSpillWriter is requested for a xref:memory:UnsafeSorterSpillWriter.adoc#writer[partition writer] ShuffleExternalSorter is requested to xref:shuffle:ShuffleExternalSorter.adoc#writeSortedFile[writeSortedFile] ExternalSorter is requested to xref:shuffle:ExternalSorter.adoc#spillMemoryIteratorToDisk[spillMemoryIteratorToDisk] == [[creating-instance]] Creating Instance DiskBlockObjectWriter takes the following to be created: [[file]] Java {java-javadoc-url}/java/io/File.html[File] [[serializerManager]] xref:serializer:SerializerManager.adoc[] [[serializerInstance]] xref:serializer:SerializerInstance.adoc[] [[bufferSize]] Buffer size [[syncWrites]] syncWrites flag [[writeMetrics]] xref:executor:ShuffleWriteMetrics.adoc[] [[blockId]] xref:storage:BlockId.adoc[] (default: null ) DiskBlockObjectWriter is created when: BlockManager is requested for xref:storage:BlockManager.adoc#getDiskWriter[one] BypassMergeSortShuffleWriter is requested to xref:shuffle:BypassMergeSortShuffleWriter.adoc#write[write records] (as xref:shuffle:BypassMergeSortShuffleWriter.adoc#partitionWriters[partition writers]) == [[objOut]] SerializationStream DiskBlockObjectWriter manages a xref:serializer:SerializationStream.adoc[SerializationStream] for < >: Opens it when requested to < > Closes it when requested to < > Dereferences it ( null s it) when < > == [[states]][[streamOpen]] States DiskBlockObjectWriter can be in the following states (that match the state of the underlying output streams): . Initialized . Open . Closed == [[write]] Writing Key and Value (of Record) [source, scala] \u00b6 write( key: Any, value: Any): Unit write < > unless < > already. write requests the < > to xref:serializer:SerializationStream.adoc#writeKey[write the key] and then the xref:serializer:SerializationStream.adoc#writeValue[value]. In the end, write < >. write is used when: BypassMergeSortShuffleWriter is requested to xref:shuffle:BypassMergeSortShuffleWriter.adoc#write[write records of a partition] ExternalAppendOnlyMap is requested to xref:shuffle:ExternalAppendOnlyMap.adoc#spillMemoryIteratorToDisk[spillMemoryIteratorToDisk] ExternalSorter is requested to xref:shuffle:ExternalSorter.adoc#writePartitionedFile[write all records into a partitioned file] ** SpillableIterator is requested to spill WritablePartitionedPairCollection is requested for a destructiveSortedWritablePartitionedIterator == [[commitAndGet]] commitAndGet Method [source, scala] \u00b6 commitAndGet(): FileSegment \u00b6 commitAndGet...FIXME commitAndGet is used when...FIXME == [[close]] Committing Writes and Closing Resources [source, scala] \u00b6 close(): Unit \u00b6 close...FIXME close is used when...FIXME == [[revertPartialWritesAndClose]] revertPartialWritesAndClose Method [source, scala] \u00b6 revertPartialWritesAndClose(): File \u00b6 revertPartialWritesAndClose...FIXME revertPartialWritesAndClose is used when...FIXME == [[updateBytesWritten]] updateBytesWritten Method CAUTION: FIXME == [[initialize]] initialize Method CAUTION: FIXME == [[write-bytes]] Writing Bytes (From Byte Array Starting From Offset) [source, scala] \u00b6 write(kvBytes: Array[Byte], offs: Int, len: Int): Unit \u00b6 write...FIXME CAUTION: FIXME == [[recordWritten]] recordWritten Method CAUTION: FIXME == [[open]] Opening DiskBlockObjectWriter [source, scala] \u00b6 open(): DiskBlockObjectWriter \u00b6 open opens DiskBlockObjectWriter, i.e. < > and re-sets < > and < > internal output streams. Internally, open makes sure that DiskBlockObjectWriter is not closed (i.e. < > flag is disabled). If it was, open throws a IllegalStateException : Writer already closed. Cannot be reopened. Unless DiskBlockObjectWriter has already been initialized (i.e. < > flag is enabled), open < > it (and turns < > flag on). Regardless of whether DiskBlockObjectWriter was already initialized or not, open xref:serializer:SerializerManager.adoc#wrapStream[requests SerializerManager to wrap mcs output stream for encryption and compression] (for < >) and sets it as < >. open requests the < > to xref:serializer:SerializerInstance.adoc#serializeStream[serialize bs output stream] and sets it as < >. NOTE: open uses SerializerInstance that was specified when < > In the end, open turns < > flag on. NOTE: open is used exclusively when DiskBlockObjectWriter < > or < > but the < >. == [[internal-properties]] Internal Properties [cols=\"30m,70\",options=\"header\",width=\"100%\"] |=== | Name | Description | initialized | [[initialized]] Internal flag...FIXME Used when...FIXME | hasBeenClosed | [[hasBeenClosed]] Internal flag...FIXME Used when...FIXME | mcs | [[mcs]] FIXME Used when...FIXME | bs | [[bs]] FIXME Used when...FIXME |===","title":"DiskBlockObjectWriter"},{"location":"storage/DiskBlockObjectWriter/#source-scala","text":"write( key: Any, value: Any): Unit write < > unless < > already. write requests the < > to xref:serializer:SerializationStream.adoc#writeKey[write the key] and then the xref:serializer:SerializationStream.adoc#writeValue[value]. In the end, write < >. write is used when: BypassMergeSortShuffleWriter is requested to xref:shuffle:BypassMergeSortShuffleWriter.adoc#write[write records of a partition] ExternalAppendOnlyMap is requested to xref:shuffle:ExternalAppendOnlyMap.adoc#spillMemoryIteratorToDisk[spillMemoryIteratorToDisk] ExternalSorter is requested to xref:shuffle:ExternalSorter.adoc#writePartitionedFile[write all records into a partitioned file] ** SpillableIterator is requested to spill WritablePartitionedPairCollection is requested for a destructiveSortedWritablePartitionedIterator == [[commitAndGet]] commitAndGet Method","title":"[source, scala]"},{"location":"storage/DiskBlockObjectWriter/#source-scala_1","text":"","title":"[source, scala]"},{"location":"storage/DiskBlockObjectWriter/#commitandget-filesegment","text":"commitAndGet...FIXME commitAndGet is used when...FIXME == [[close]] Committing Writes and Closing Resources","title":"commitAndGet(): FileSegment"},{"location":"storage/DiskBlockObjectWriter/#source-scala_2","text":"","title":"[source, scala]"},{"location":"storage/DiskBlockObjectWriter/#close-unit","text":"close...FIXME close is used when...FIXME == [[revertPartialWritesAndClose]] revertPartialWritesAndClose Method","title":"close(): Unit"},{"location":"storage/DiskBlockObjectWriter/#source-scala_3","text":"","title":"[source, scala]"},{"location":"storage/DiskBlockObjectWriter/#revertpartialwritesandclose-file","text":"revertPartialWritesAndClose...FIXME revertPartialWritesAndClose is used when...FIXME == [[updateBytesWritten]] updateBytesWritten Method CAUTION: FIXME == [[initialize]] initialize Method CAUTION: FIXME == [[write-bytes]] Writing Bytes (From Byte Array Starting From Offset)","title":"revertPartialWritesAndClose(): File"},{"location":"storage/DiskBlockObjectWriter/#source-scala_4","text":"","title":"[source, scala]"},{"location":"storage/DiskBlockObjectWriter/#writekvbytes-arraybyte-offs-int-len-int-unit","text":"write...FIXME CAUTION: FIXME == [[recordWritten]] recordWritten Method CAUTION: FIXME == [[open]] Opening DiskBlockObjectWriter","title":"write(kvBytes: Array[Byte], offs: Int, len: Int): Unit"},{"location":"storage/DiskBlockObjectWriter/#source-scala_5","text":"","title":"[source, scala]"},{"location":"storage/DiskBlockObjectWriter/#open-diskblockobjectwriter","text":"open opens DiskBlockObjectWriter, i.e. < > and re-sets < > and < > internal output streams. Internally, open makes sure that DiskBlockObjectWriter is not closed (i.e. < > flag is disabled). If it was, open throws a IllegalStateException : Writer already closed. Cannot be reopened. Unless DiskBlockObjectWriter has already been initialized (i.e. < > flag is enabled), open < > it (and turns < > flag on). Regardless of whether DiskBlockObjectWriter was already initialized or not, open xref:serializer:SerializerManager.adoc#wrapStream[requests SerializerManager to wrap mcs output stream for encryption and compression] (for < >) and sets it as < >. open requests the < > to xref:serializer:SerializerInstance.adoc#serializeStream[serialize bs output stream] and sets it as < >. NOTE: open uses SerializerInstance that was specified when < > In the end, open turns < > flag on. NOTE: open is used exclusively when DiskBlockObjectWriter < > or < > but the < >. == [[internal-properties]] Internal Properties [cols=\"30m,70\",options=\"header\",width=\"100%\"] |=== | Name | Description | initialized | [[initialized]] Internal flag...FIXME Used when...FIXME | hasBeenClosed | [[hasBeenClosed]] Internal flag...FIXME Used when...FIXME | mcs | [[mcs]] FIXME Used when...FIXME | bs | [[bs]] FIXME Used when...FIXME |===","title":"open(): DiskBlockObjectWriter"},{"location":"storage/DiskStore/","text":"= DiskStore DiskStore manages data blocks on disk for xref:storage:BlockManager.adoc#diskStore[BlockManager]. .DiskStore and BlockManager image::DiskStore-BlockManager.png[align=\"center\"] == [[creating-instance]] Creating Instance DiskStore takes the following to be created: [[conf]] xref:ROOT:SparkConf.adoc[] [[diskManager]] xref:storage:DiskBlockManager.adoc[] [[securityManager]] SecurityManager == [[getBytes]] getBytes Method [source,scala] \u00b6 getBytes( blockId: BlockId): BlockData getBytes...FIXME getBytes is used when BlockManager is requested to xref:storage:BlockManager.adoc#getLocalValues[getLocalValues] and xref:storage:BlockManager.adoc#doGetLocalBytes[doGetLocalBytes]. == [[blockSizes]] blockSizes Internal Registry [source, scala] \u00b6 blockSizes: ConcurrentHashMap[BlockId, Long] \u00b6 blockSizes is a Java {java-javadoc-url}/java/util/concurrent/ConcurrentHashMap.html[java.util.concurrent.ConcurrentHashMap] that DiskStore uses to track xref:storage:BlockId.adoc[]s by their size on disk. == [[contains]] Checking if Block File Exists [source, scala] \u00b6 contains( blockId: BlockId): Boolean contains requests the < > for the xref:storage:DiskBlockManager.adoc#getFile[block file] by (the name of) the input xref:storage:BlockId.adoc[] and check whether the file actually exists or not. contains is used when: BlockManager is requested to xref:storage:BlockManager.adoc#getStatus[getStatus], xref:storage:BlockManager.adoc#getCurrentBlockStatus[getCurrentBlockStatus], xref:storage:BlockManager.adoc#getLocalValues[getLocalValues], xref:storage:BlockManager.adoc#doGetLocalBytes[doGetLocalBytes], xref:storage:BlockManager.adoc#dropFromMemory[dropFromMemory] DiskStore is requested to < > == [[put]] Writing Block to Disk [source, scala] \u00b6 put( blockId: BlockId)( writeFunc: WritableByteChannel => Unit): Unit put prints out the following DEBUG message to the logs: Attempting to put block [blockId] put requests the < > for the xref:storage:DiskBlockManager.adoc#getFile[block file] for the input xref:storage:BlockId.adoc[]. put < > (wrapped into a CountingWritableChannel to count the bytes written). put executes the given writeFunc function with the WritableByteChannel of the block file and registers the bytes written to the < > internal registry. In the end, put prints out the following DEBUG message to the logs: Block [fileName] stored as [size] file on disk in [time] ms In case of any exception, put < >. put throws an IllegalStateException when the BlockId is already < >: Block [blockId] is already present in the disk store put is used when: BlockManager is requested to xref:storage:BlockManager.adoc#doPutIterator[doPutIterator] and xref:storage:BlockManager.adoc#dropFromMemory[dropFromMemory] DiskStore is requested to < > == [[putBytes]] putBytes Method [source, scala] \u00b6 putBytes( blockId: BlockId, bytes: ChunkedByteBuffer): Unit putBytes ...FIXME putBytes is used when BlockManager is requested to xref:storage:BlockManager.adoc#doPutBytes[doPutBytes] and xref:storage:BlockManager.adoc#dropFromMemory[dropFromMemory]. == [[remove]] Removing Block [source, scala] \u00b6 remove( blockId: BlockId): Boolean remove ...FIXME remove is used when: BlockManager is requested to xref:storage:BlockManager.adoc#removeBlockInternal[removeBlockInternal] DiskStore is requested to < > (when an exception was thrown) == [[openForWrite]] Opening Block File For Writing [source, scala] \u00b6 openForWrite( file: File): WritableByteChannel openForWrite ...FIXME openForWrite is used when DiskStore is requested to < >. == [[logging]] Logging Enable ALL logging level for org.apache.spark.storage.DiskStore logger to see what happens inside. Add the following line to conf/log4j.properties : [source] \u00b6 log4j.logger.org.apache.spark.storage.DiskStore=ALL \u00b6 Refer to xref:ROOT:spark-logging.adoc[Logging].","title":"DiskStore"},{"location":"storage/DiskStore/#sourcescala","text":"getBytes( blockId: BlockId): BlockData getBytes...FIXME getBytes is used when BlockManager is requested to xref:storage:BlockManager.adoc#getLocalValues[getLocalValues] and xref:storage:BlockManager.adoc#doGetLocalBytes[doGetLocalBytes]. == [[blockSizes]] blockSizes Internal Registry","title":"[source,scala]"},{"location":"storage/DiskStore/#source-scala","text":"","title":"[source, scala]"},{"location":"storage/DiskStore/#blocksizes-concurrenthashmapblockid-long","text":"blockSizes is a Java {java-javadoc-url}/java/util/concurrent/ConcurrentHashMap.html[java.util.concurrent.ConcurrentHashMap] that DiskStore uses to track xref:storage:BlockId.adoc[]s by their size on disk. == [[contains]] Checking if Block File Exists","title":"blockSizes: ConcurrentHashMap[BlockId, Long]"},{"location":"storage/DiskStore/#source-scala_1","text":"contains( blockId: BlockId): Boolean contains requests the < > for the xref:storage:DiskBlockManager.adoc#getFile[block file] by (the name of) the input xref:storage:BlockId.adoc[] and check whether the file actually exists or not. contains is used when: BlockManager is requested to xref:storage:BlockManager.adoc#getStatus[getStatus], xref:storage:BlockManager.adoc#getCurrentBlockStatus[getCurrentBlockStatus], xref:storage:BlockManager.adoc#getLocalValues[getLocalValues], xref:storage:BlockManager.adoc#doGetLocalBytes[doGetLocalBytes], xref:storage:BlockManager.adoc#dropFromMemory[dropFromMemory] DiskStore is requested to < > == [[put]] Writing Block to Disk","title":"[source, scala]"},{"location":"storage/DiskStore/#source-scala_2","text":"put( blockId: BlockId)( writeFunc: WritableByteChannel => Unit): Unit put prints out the following DEBUG message to the logs: Attempting to put block [blockId] put requests the < > for the xref:storage:DiskBlockManager.adoc#getFile[block file] for the input xref:storage:BlockId.adoc[]. put < > (wrapped into a CountingWritableChannel to count the bytes written). put executes the given writeFunc function with the WritableByteChannel of the block file and registers the bytes written to the < > internal registry. In the end, put prints out the following DEBUG message to the logs: Block [fileName] stored as [size] file on disk in [time] ms In case of any exception, put < >. put throws an IllegalStateException when the BlockId is already < >: Block [blockId] is already present in the disk store put is used when: BlockManager is requested to xref:storage:BlockManager.adoc#doPutIterator[doPutIterator] and xref:storage:BlockManager.adoc#dropFromMemory[dropFromMemory] DiskStore is requested to < > == [[putBytes]] putBytes Method","title":"[source, scala]"},{"location":"storage/DiskStore/#source-scala_3","text":"putBytes( blockId: BlockId, bytes: ChunkedByteBuffer): Unit putBytes ...FIXME putBytes is used when BlockManager is requested to xref:storage:BlockManager.adoc#doPutBytes[doPutBytes] and xref:storage:BlockManager.adoc#dropFromMemory[dropFromMemory]. == [[remove]] Removing Block","title":"[source, scala]"},{"location":"storage/DiskStore/#source-scala_4","text":"remove( blockId: BlockId): Boolean remove ...FIXME remove is used when: BlockManager is requested to xref:storage:BlockManager.adoc#removeBlockInternal[removeBlockInternal] DiskStore is requested to < > (when an exception was thrown) == [[openForWrite]] Opening Block File For Writing","title":"[source, scala]"},{"location":"storage/DiskStore/#source-scala_5","text":"openForWrite( file: File): WritableByteChannel openForWrite ...FIXME openForWrite is used when DiskStore is requested to < >. == [[logging]] Logging Enable ALL logging level for org.apache.spark.storage.DiskStore logger to see what happens inside. Add the following line to conf/log4j.properties :","title":"[source, scala]"},{"location":"storage/DiskStore/#source","text":"","title":"[source]"},{"location":"storage/DiskStore/#log4jloggerorgapachesparkstoragediskstoreall","text":"Refer to xref:ROOT:spark-logging.adoc[Logging].","title":"log4j.logger.org.apache.spark.storage.DiskStore=ALL"},{"location":"storage/ExternalShuffleClient/","text":"= ExternalShuffleClient ExternalShuffleClient is a xref:storage:ShuffleClient.adoc[] that...FIXME == [[init]] Initializing ExternalShuffleClient [source,java] \u00b6 void init( String appId) init...FIXME init is part of the xref:storage:ShuffleClient.adoc#init[ShuffleClient] abstraction. == [[registerWithShuffleServer]] Register Block Manager with Shuffle Server [source, java] \u00b6 void registerWithShuffleServer( String host, int port, String execId, ExecutorShuffleInfo executorInfo) registerWithShuffleServer...FIXME registerWithShuffleServer is used when...FIXME == [[fetchBlocks]] Fetching Blocks [source, java] \u00b6 void fetchBlocks( String host, int port, String execId, String[] blockIds, BlockFetchingListener listener, TempFileManager tempFileManager) fetchBlocks...FIXME fetchBlocks is part of xref:storage:ShuffleClient.adoc#fetchBlocks[ShuffleClient] abstraction.","title":"ExternalShuffleClient"},{"location":"storage/ExternalShuffleClient/#sourcejava","text":"void init( String appId) init...FIXME init is part of the xref:storage:ShuffleClient.adoc#init[ShuffleClient] abstraction. == [[registerWithShuffleServer]] Register Block Manager with Shuffle Server","title":"[source,java]"},{"location":"storage/ExternalShuffleClient/#source-java","text":"void registerWithShuffleServer( String host, int port, String execId, ExecutorShuffleInfo executorInfo) registerWithShuffleServer...FIXME registerWithShuffleServer is used when...FIXME == [[fetchBlocks]] Fetching Blocks","title":"[source, java]"},{"location":"storage/ExternalShuffleClient/#source-java_1","text":"void fetchBlocks( String host, int port, String execId, String[] blockIds, BlockFetchingListener listener, TempFileManager tempFileManager) fetchBlocks...FIXME fetchBlocks is part of xref:storage:ShuffleClient.adoc#fetchBlocks[ShuffleClient] abstraction.","title":"[source, java]"},{"location":"storage/MemoryStore/","text":"= MemoryStore MemoryStore manages blocks of data in memory for xref:storage:BlockManager.adoc#memoryStore[BlockManager]. .MemoryStore and BlockManager image::MemoryStore-BlockManager.png[align=\"center\"] == [[creating-instance]] Creating Instance MemoryStore takes the following to be created: [[conf]] xref:ROOT:SparkConf.adoc[] < > [[serializerManager]] xref:serializer:SerializerManager.adoc[] [[memoryManager]] xref:memory:MemoryManager.adoc[] [[blockEvictionHandler]] xref:storage:BlockEvictionHandler.adoc[] MemoryStore is created for xref:storage:BlockManager.adoc#memoryStore[BlockManager]. .Creating MemoryStore image::spark-MemoryStore.png[align=\"center\"] == [[blockInfoManager]] BlockInfoManager MemoryStore is given a xref:storage:BlockInfoManager.adoc[] when < >. MemoryStore uses the BlockInfoManager when requested to < >. == [[memoryStore]] Accessing MemoryStore MemoryStore is available using xref:storage:BlockManager.adoc#memoryStore[BlockManager.memoryStore] reference to other Spark services. [source,scala] \u00b6 import org.apache.spark.SparkEnv SparkEnv.get.blockManager.memoryStore == [[unrollMemoryThreshold]][[spark.storage.unrollMemoryThreshold]] spark.storage.unrollMemoryThreshold Configuration Property MemoryStore uses xref:ROOT:configuration-properties.adoc#spark.storage.unrollMemoryThreshold[spark.storage.unrollMemoryThreshold] configuration property for < > and < >. == [[releaseUnrollMemoryForThisTask]] releaseUnrollMemoryForThisTask Method [source, scala] \u00b6 releaseUnrollMemoryForThisTask( memoryMode: MemoryMode, memory: Long = Long.MaxValue): Unit releaseUnrollMemoryForThisTask...FIXME releaseUnrollMemoryForThisTask is used when: Task is requested to xref:scheduler:Task.adoc#run[run] (and cleans up after itself) MemoryStore is requested to < > PartiallyUnrolledIterator is requested to releaseUnrollMemory PartiallySerializedBlock is requested to discard and finishWritingToStream == [[getValues]] getValues Method [source, scala] \u00b6 getValues( blockId: BlockId): Option[Iterator[_]] getValues...FIXME getValues is used when BlockManager is requested to xref:storage:BlockManager.adoc#doGetLocalBytes[doGetLocalBytes], xref:storage:BlockManager.adoc#getLocalValues[getLocalValues] and xref:storage:BlockManager.adoc#maybeCacheDiskBytesInMemory[maybeCacheDiskBytesInMemory]. == [[getBytes]] getBytes Method [source, scala] \u00b6 getBytes( blockId: BlockId): Option[ChunkedByteBuffer] getBytes...FIXME getBytes is used when BlockManager is requested to xref:storage:BlockManager.adoc#doGetLocalBytes[doGetLocalBytes], xref:storage:BlockManager.adoc#getLocalValues[getLocalValues] and xref:storage:BlockManager.adoc#maybeCacheDiskBytesInMemory[maybeCacheDiskBytesInMemory]. == [[putIteratorAsBytes]] putIteratorAsBytes Method [source, scala] \u00b6 putIteratorAsBytes T : Either[PartiallySerializedBlock[T], Long] putIteratorAsBytes...FIXME putIteratorAsBytes is used when BlockManager is requested to xref:storage:BlockManager.adoc#doPutIterator[doPutIterator]. == [[remove]] Dropping Block from Memory [source, scala] \u00b6 remove( blockId: BlockId): Boolean remove removes the given xref:storage:BlockId.adoc[] from the < > internal registry and branches off based on whether the < > or < >. === [[remove-block-removed]] Block Removed When found and removed, remove requests the < > to xref:memory:MemoryManager.adoc#releaseStorageMemory[releaseStorageMemory] and prints out the following DEBUG message to the logs: [source,plaintext] \u00b6 Block [blockId] of size [size] dropped from memory (free [memory]) \u00b6 remove returns true . === [[remove-no-block]] No Block Removed If no BlockId was registered and removed, remove returns false . === [[remove-usage]] Usage remove is used when BlockManager is requested to xref:storage:BlockManager.adoc#dropFromMemory[dropFromMemory] and xref:storage:BlockManager.adoc#removeBlockInternal[removeBlockInternal]. == [[putBytes]] Acquiring Storage Memory for Blocks [source, scala] \u00b6 putBytes T: ClassTag : Boolean putBytes requests xref:memory:MemoryManager.adoc#acquireStorageMemory[storage memory for blockId from MemoryManager ] and registers the block in < > internal registry. Internally, putBytes first makes sure that blockId block has not been registered already in < > internal registry. putBytes then requests xref:memory:MemoryManager.adoc#acquireStorageMemory[ size memory for the blockId block in a given memoryMode from the current MemoryManager ]. [NOTE] \u00b6 memoryMode can be ON_HEAP or OFF_HEAP and is a property of a xref:storage:StorageLevel.adoc[StorageLevel]. import org.apache.spark.storage.StorageLevel._ scala> MEMORY_AND_DISK.useOffHeap res0: Boolean = false scala> OFF_HEAP.useOffHeap res1: Boolean = true \u00b6 If successful, putBytes \"materializes\" _bytes byte buffer and makes sure that the size is exactly size . It then registers a SerializedMemoryEntry (for the bytes and memoryMode ) for blockId in the internal < > registry. You should see the following INFO message in the logs: Block [blockId] stored as bytes in memory (estimated size [size], free [bytes]) putBytes returns true only after blockId was successfully registered in the internal < > registry. putBytes is used when BlockManager is requested to xref:storage:BlockManager.adoc#doPutBytes[doPutBytes] and xref:storage:BlockManager.adoc#maybeCacheDiskBytesInMemory[maybeCacheDiskBytesInMemory]. == [[evictBlocksToFreeSpace]] Evicting Blocks [source, scala] \u00b6 evictBlocksToFreeSpace( blockId: Option[BlockId], space: Long, memoryMode: MemoryMode): Long evictBlocksToFreeSpace...FIXME evictBlocksToFreeSpace is used when StorageMemoryPool is requested to xref:memory:StorageMemoryPool.adoc#acquireMemory[acquireMemory] and xref:memory:StorageMemoryPool.adoc#freeSpaceToShrinkPool[freeSpaceToShrinkPool]. == [[contains]] Checking Whether Block Exists In MemoryStore [source, scala] \u00b6 contains( blockId: BlockId): Boolean contains is positive ( true ) when the < > internal registry contains blockId key. contains is used when...FIXME == [[putIteratorAsValues]] putIteratorAsValues Method [source, scala] \u00b6 putIteratorAsValues T : Either[PartiallyUnrolledIterator[T], Long] putIteratorAsValues makes sure that the BlockId does not exist or throws an IllegalArgumentException : requirement failed: Block [blockId] is already present in the MemoryStore putIteratorAsValues < > (with the < > and ON_HEAP memory mode). CAUTION: FIXME putIteratorAsValues tries to put the blockId block in memory store as values . putIteratorAsValues is used when BlockManager is requested to store xref:storage:BlockManager.adoc#doPutBytes[bytes] or xref:storage:BlockManager.adoc#doPutIterator[values] of a block or when xref:storage:BlockManager.adoc#maybeCacheDiskValuesInMemory[attempting to cache spilled values read from disk]. == [[reserveUnrollMemoryForThisTask]] reserveUnrollMemoryForThisTask Method [source, scala] \u00b6 reserveUnrollMemoryForThisTask( blockId: BlockId, memory: Long, memoryMode: MemoryMode): Boolean reserveUnrollMemoryForThisTask acquires a lock on < > and requests it to xref:memory:MemoryManager.adoc#acquireUnrollMemory[acquireUnrollMemory]. NOTE: reserveUnrollMemoryForThisTask is used when MemoryStore is requested to < > and < >. == [[maxMemory]] Total Amount Of Memory Available For Storage [source, scala] \u00b6 maxMemory: Long \u00b6 maxMemory requests the < > for the current xref:memory:MemoryManager.adoc#maxOnHeapStorageMemory[maxOnHeapStorageMemory] and xref:memory:MemoryManager.adoc#maxOffHeapStorageMemory[maxOffHeapStorageMemory], and simply returns their sum. [TIP] \u00b6 Enable INFO < > to find the maxMemory in the logs when MemoryStore is < >: MemoryStore started with capacity [maxMemory] MB \u00b6 NOTE: maxMemory is used for < > purposes only. == [[putIterator]] putIterator Internal Method [source, scala] \u00b6 putIterator T : Either[Long, Long] putIterator...FIXME putIterator is used when MemoryStore is requested to < > and < >. == [[logUnrollFailureMessage]] logUnrollFailureMessage Internal Method [source, scala] \u00b6 logUnrollFailureMessage( blockId: BlockId, finalVectorSize: Long): Unit logUnrollFailureMessage...FIXME logUnrollFailureMessage is used when MemoryStore is requested to < >. == [[logMemoryUsage]] logMemoryUsage Internal Method [source, scala] \u00b6 logMemoryUsage(): Unit \u00b6 logMemoryUsage...FIXME logMemoryUsage is used when MemoryStore is requested to < >. == [[memoryUsed]] Total Memory Used [source, scala] \u00b6 memoryUsed: Long \u00b6 memoryUsed requests the < > for the xref:memory:MemoryManager.adoc#storageMemoryUsed[storageMemoryUsed]. memoryUsed is used when MemoryStore is requested for < > and to < >. == [[blocksMemoryUsed]] Memory Used for Caching Blocks [source, scala] \u00b6 blocksMemoryUsed: Long \u00b6 blocksMemoryUsed is the < > without the < >. blocksMemoryUsed is used for logging purposes when MemoryStore is requested to < >, < >, < >, < > and < >. == [[logging]] Logging Enable ALL logging level for org.apache.spark.storage.memory.MemoryStore logger to see what happens inside. Add the following line to conf/log4j.properties : [source] \u00b6 log4j.logger.org.apache.spark.storage.memory.MemoryStore=ALL \u00b6 Refer to xref:ROOT:spark-logging.adoc[Logging]. == [[internal-registries]] Internal Registries === [[entries]] MemoryEntries by BlockId [source, scala] \u00b6 entries: LinkedHashMap[BlockId, MemoryEntry[_]] \u00b6 MemoryStore creates a Java {java-javadoc-url}/java/util/LinkedHashMap.html[LinkedHashMap] of MemoryEntries per xref:storage:BlockId.adoc[] (with the initial capacity of 32 and the load factor of 0.75 ) when < >. entries uses access-order ordering mode where the order of iteration is the order in which the entries were last accessed (from least-recently accessed to most-recently). That gives LRU cache behaviour when MemoryStore is requested to < >.","title":"MemoryStore"},{"location":"storage/MemoryStore/#sourcescala","text":"import org.apache.spark.SparkEnv SparkEnv.get.blockManager.memoryStore == [[unrollMemoryThreshold]][[spark.storage.unrollMemoryThreshold]] spark.storage.unrollMemoryThreshold Configuration Property MemoryStore uses xref:ROOT:configuration-properties.adoc#spark.storage.unrollMemoryThreshold[spark.storage.unrollMemoryThreshold] configuration property for < > and < >. == [[releaseUnrollMemoryForThisTask]] releaseUnrollMemoryForThisTask Method","title":"[source,scala]"},{"location":"storage/MemoryStore/#source-scala","text":"releaseUnrollMemoryForThisTask( memoryMode: MemoryMode, memory: Long = Long.MaxValue): Unit releaseUnrollMemoryForThisTask...FIXME releaseUnrollMemoryForThisTask is used when: Task is requested to xref:scheduler:Task.adoc#run[run] (and cleans up after itself) MemoryStore is requested to < > PartiallyUnrolledIterator is requested to releaseUnrollMemory PartiallySerializedBlock is requested to discard and finishWritingToStream == [[getValues]] getValues Method","title":"[source, scala]"},{"location":"storage/MemoryStore/#source-scala_1","text":"getValues( blockId: BlockId): Option[Iterator[_]] getValues...FIXME getValues is used when BlockManager is requested to xref:storage:BlockManager.adoc#doGetLocalBytes[doGetLocalBytes], xref:storage:BlockManager.adoc#getLocalValues[getLocalValues] and xref:storage:BlockManager.adoc#maybeCacheDiskBytesInMemory[maybeCacheDiskBytesInMemory]. == [[getBytes]] getBytes Method","title":"[source, scala]"},{"location":"storage/MemoryStore/#source-scala_2","text":"getBytes( blockId: BlockId): Option[ChunkedByteBuffer] getBytes...FIXME getBytes is used when BlockManager is requested to xref:storage:BlockManager.adoc#doGetLocalBytes[doGetLocalBytes], xref:storage:BlockManager.adoc#getLocalValues[getLocalValues] and xref:storage:BlockManager.adoc#maybeCacheDiskBytesInMemory[maybeCacheDiskBytesInMemory]. == [[putIteratorAsBytes]] putIteratorAsBytes Method","title":"[source, scala]"},{"location":"storage/MemoryStore/#source-scala_3","text":"putIteratorAsBytes T : Either[PartiallySerializedBlock[T], Long] putIteratorAsBytes...FIXME putIteratorAsBytes is used when BlockManager is requested to xref:storage:BlockManager.adoc#doPutIterator[doPutIterator]. == [[remove]] Dropping Block from Memory","title":"[source, scala]"},{"location":"storage/MemoryStore/#source-scala_4","text":"remove( blockId: BlockId): Boolean remove removes the given xref:storage:BlockId.adoc[] from the < > internal registry and branches off based on whether the < > or < >. === [[remove-block-removed]] Block Removed When found and removed, remove requests the < > to xref:memory:MemoryManager.adoc#releaseStorageMemory[releaseStorageMemory] and prints out the following DEBUG message to the logs:","title":"[source, scala]"},{"location":"storage/MemoryStore/#sourceplaintext","text":"","title":"[source,plaintext]"},{"location":"storage/MemoryStore/#block-blockid-of-size-size-dropped-from-memory-free-memory","text":"remove returns true . === [[remove-no-block]] No Block Removed If no BlockId was registered and removed, remove returns false . === [[remove-usage]] Usage remove is used when BlockManager is requested to xref:storage:BlockManager.adoc#dropFromMemory[dropFromMemory] and xref:storage:BlockManager.adoc#removeBlockInternal[removeBlockInternal]. == [[putBytes]] Acquiring Storage Memory for Blocks","title":"Block [blockId] of size [size] dropped from memory (free [memory])"},{"location":"storage/MemoryStore/#source-scala_5","text":"putBytes T: ClassTag : Boolean putBytes requests xref:memory:MemoryManager.adoc#acquireStorageMemory[storage memory for blockId from MemoryManager ] and registers the block in < > internal registry. Internally, putBytes first makes sure that blockId block has not been registered already in < > internal registry. putBytes then requests xref:memory:MemoryManager.adoc#acquireStorageMemory[ size memory for the blockId block in a given memoryMode from the current MemoryManager ].","title":"[source, scala]"},{"location":"storage/MemoryStore/#note","text":"memoryMode can be ON_HEAP or OFF_HEAP and is a property of a xref:storage:StorageLevel.adoc[StorageLevel].","title":"[NOTE]"},{"location":"storage/MemoryStore/#import-orgapachesparkstoragestoragelevel_-scala-memory_and_diskuseoffheap-res0-boolean-false-scala-off_heapuseoffheap-res1-boolean-true","text":"If successful, putBytes \"materializes\" _bytes byte buffer and makes sure that the size is exactly size . It then registers a SerializedMemoryEntry (for the bytes and memoryMode ) for blockId in the internal < > registry. You should see the following INFO message in the logs: Block [blockId] stored as bytes in memory (estimated size [size], free [bytes]) putBytes returns true only after blockId was successfully registered in the internal < > registry. putBytes is used when BlockManager is requested to xref:storage:BlockManager.adoc#doPutBytes[doPutBytes] and xref:storage:BlockManager.adoc#maybeCacheDiskBytesInMemory[maybeCacheDiskBytesInMemory]. == [[evictBlocksToFreeSpace]] Evicting Blocks","title":"import org.apache.spark.storage.StorageLevel._\nscala&gt; MEMORY_AND_DISK.useOffHeap\nres0: Boolean = false\n\nscala&gt; OFF_HEAP.useOffHeap\nres1: Boolean = true\n"},{"location":"storage/MemoryStore/#source-scala_6","text":"evictBlocksToFreeSpace( blockId: Option[BlockId], space: Long, memoryMode: MemoryMode): Long evictBlocksToFreeSpace...FIXME evictBlocksToFreeSpace is used when StorageMemoryPool is requested to xref:memory:StorageMemoryPool.adoc#acquireMemory[acquireMemory] and xref:memory:StorageMemoryPool.adoc#freeSpaceToShrinkPool[freeSpaceToShrinkPool]. == [[contains]] Checking Whether Block Exists In MemoryStore","title":"[source, scala]"},{"location":"storage/MemoryStore/#source-scala_7","text":"contains( blockId: BlockId): Boolean contains is positive ( true ) when the < > internal registry contains blockId key. contains is used when...FIXME == [[putIteratorAsValues]] putIteratorAsValues Method","title":"[source, scala]"},{"location":"storage/MemoryStore/#source-scala_8","text":"putIteratorAsValues T : Either[PartiallyUnrolledIterator[T], Long] putIteratorAsValues makes sure that the BlockId does not exist or throws an IllegalArgumentException : requirement failed: Block [blockId] is already present in the MemoryStore putIteratorAsValues < > (with the < > and ON_HEAP memory mode). CAUTION: FIXME putIteratorAsValues tries to put the blockId block in memory store as values . putIteratorAsValues is used when BlockManager is requested to store xref:storage:BlockManager.adoc#doPutBytes[bytes] or xref:storage:BlockManager.adoc#doPutIterator[values] of a block or when xref:storage:BlockManager.adoc#maybeCacheDiskValuesInMemory[attempting to cache spilled values read from disk]. == [[reserveUnrollMemoryForThisTask]] reserveUnrollMemoryForThisTask Method","title":"[source, scala]"},{"location":"storage/MemoryStore/#source-scala_9","text":"reserveUnrollMemoryForThisTask( blockId: BlockId, memory: Long, memoryMode: MemoryMode): Boolean reserveUnrollMemoryForThisTask acquires a lock on < > and requests it to xref:memory:MemoryManager.adoc#acquireUnrollMemory[acquireUnrollMemory]. NOTE: reserveUnrollMemoryForThisTask is used when MemoryStore is requested to < > and < >. == [[maxMemory]] Total Amount Of Memory Available For Storage","title":"[source, scala]"},{"location":"storage/MemoryStore/#source-scala_10","text":"","title":"[source, scala]"},{"location":"storage/MemoryStore/#maxmemory-long","text":"maxMemory requests the < > for the current xref:memory:MemoryManager.adoc#maxOnHeapStorageMemory[maxOnHeapStorageMemory] and xref:memory:MemoryManager.adoc#maxOffHeapStorageMemory[maxOffHeapStorageMemory], and simply returns their sum.","title":"maxMemory: Long"},{"location":"storage/MemoryStore/#tip","text":"Enable INFO < > to find the maxMemory in the logs when MemoryStore is < >:","title":"[TIP]"},{"location":"storage/MemoryStore/#memorystore-started-with-capacity-maxmemory-mb","text":"NOTE: maxMemory is used for < > purposes only. == [[putIterator]] putIterator Internal Method","title":"MemoryStore started with capacity [maxMemory] MB\n"},{"location":"storage/MemoryStore/#source-scala_11","text":"putIterator T : Either[Long, Long] putIterator...FIXME putIterator is used when MemoryStore is requested to < > and < >. == [[logUnrollFailureMessage]] logUnrollFailureMessage Internal Method","title":"[source, scala]"},{"location":"storage/MemoryStore/#source-scala_12","text":"logUnrollFailureMessage( blockId: BlockId, finalVectorSize: Long): Unit logUnrollFailureMessage...FIXME logUnrollFailureMessage is used when MemoryStore is requested to < >. == [[logMemoryUsage]] logMemoryUsage Internal Method","title":"[source, scala]"},{"location":"storage/MemoryStore/#source-scala_13","text":"","title":"[source, scala]"},{"location":"storage/MemoryStore/#logmemoryusage-unit","text":"logMemoryUsage...FIXME logMemoryUsage is used when MemoryStore is requested to < >. == [[memoryUsed]] Total Memory Used","title":"logMemoryUsage(): Unit"},{"location":"storage/MemoryStore/#source-scala_14","text":"","title":"[source, scala]"},{"location":"storage/MemoryStore/#memoryused-long","text":"memoryUsed requests the < > for the xref:memory:MemoryManager.adoc#storageMemoryUsed[storageMemoryUsed]. memoryUsed is used when MemoryStore is requested for < > and to < >. == [[blocksMemoryUsed]] Memory Used for Caching Blocks","title":"memoryUsed: Long"},{"location":"storage/MemoryStore/#source-scala_15","text":"","title":"[source, scala]"},{"location":"storage/MemoryStore/#blocksmemoryused-long","text":"blocksMemoryUsed is the < > without the < >. blocksMemoryUsed is used for logging purposes when MemoryStore is requested to < >, < >, < >, < > and < >. == [[logging]] Logging Enable ALL logging level for org.apache.spark.storage.memory.MemoryStore logger to see what happens inside. Add the following line to conf/log4j.properties :","title":"blocksMemoryUsed: Long"},{"location":"storage/MemoryStore/#source","text":"","title":"[source]"},{"location":"storage/MemoryStore/#log4jloggerorgapachesparkstoragememorymemorystoreall","text":"Refer to xref:ROOT:spark-logging.adoc[Logging]. == [[internal-registries]] Internal Registries === [[entries]] MemoryEntries by BlockId","title":"log4j.logger.org.apache.spark.storage.memory.MemoryStore=ALL"},{"location":"storage/MemoryStore/#source-scala_16","text":"","title":"[source, scala]"},{"location":"storage/MemoryStore/#entries-linkedhashmapblockid-memoryentry_","text":"MemoryStore creates a Java {java-javadoc-url}/java/util/LinkedHashMap.html[LinkedHashMap] of MemoryEntries per xref:storage:BlockId.adoc[] (with the initial capacity of 32 and the load factor of 0.75 ) when < >. entries uses access-order ordering mode where the order of iteration is the order in which the entries were last accessed (from least-recently accessed to most-recently). That gives LRU cache behaviour when MemoryStore is requested to < >.","title":"entries: LinkedHashMap[BlockId, MemoryEntry[_]]"},{"location":"storage/NettyBlockRpcServer/","text":"= NettyBlockRpcServer NettyBlockRpcServer is an xref:network:RpcHandler.adoc[] to handle < > for xref:storage:NettyBlockTransferService.adoc[NettyBlockTransferService]. .NettyBlockRpcServer and NettyBlockTransferService image::NettyBlockRpcServer.png[align=\"center\"] == [[creating-instance]] Creating Instance NettyBlockRpcServer takes the following to be created: [[appId]] Application ID [[serializer]] xref:serializer:Serializer.adoc[] [[blockManager]] xref:storage:BlockDataManager.adoc[] NettyBlockRpcServer is created when NettyBlockTransferService is requested to xref:storage:NettyBlockTransferService.adoc#init[initialize]. == [[streamManager]] OneForOneStreamManager NettyBlockRpcServer uses a xref:network:OneForOneStreamManager.adoc[] for...FIXME == [[receive]] Receiving RPC Messages [source, scala] \u00b6 receive( client: TransportClient, rpcMessage: ByteBuffer, responseContext: RpcResponseCallback): Unit receive...FIXME receive is part of xref:network:RpcHandler.adoc#receive[RpcHandler] abstraction. == [[messages]] Messages === [[OpenBlocks]] OpenBlocks [source,java] \u00b6 OpenBlocks( String appId, String execId, String[] blockIds) When received, NettyBlockRpcServer requests the < > for xref:storage:BlockDataManager.adoc#getBlockData[block data] for every block id in the message. The block data is a collection of xref:network:ManagedBuffer.adoc[] for every block id in the incoming message. NettyBlockRpcServer then xref:network:OneForOneStreamManager.adoc#registerStream[registers a stream of ManagedBuffer s (for the blocks) with the internal StreamManager ] under streamId . NOTE: The internal StreamManager is xref:network:OneForOneStreamManager.adoc[OneForOneStreamManager] and is created when < >. You should see the following TRACE message in the logs: [source,plaintext] \u00b6 NettyBlockRpcServer: Registered streamId [streamId] with [size] buffers \u00b6 In the end, NettyBlockRpcServer responds with a StreamHandle (with the streamId and the number of blocks). The response is serialized as a ByteBuffer . Posted when OneForOneBlockFetcher is requested to xref:storage:OneForOneBlockFetcher.adoc#start[start]. === [[UploadBlock]] UploadBlock [source,java] \u00b6 UploadBlock( String appId, String execId, String blockId, byte[] metadata, byte[] blockData) When received, NettyBlockRpcServer deserializes the metadata of the input message to get the xref:storage:StorageLevel.adoc[StorageLevel] and ClassTag of the block being uploaded. NettyBlockRpcServer creates a xref:storage:BlockId.adoc[] for the block id and requests the < > to xref:storage:BlockDataManager.adoc#putBlockData[store the block]. In the end, NettyBlockRpcServer responds with a 0 -capacity ByteBuffer . Posted when NettyBlockTransferService is requested to xref:storage:NettyBlockTransferService.adoc#uploadBlock[upload a block]. == [[receiveStream]] Receiving RPC Message with Streamed Data [source, scala] \u00b6 receiveStream( client: TransportClient, messageHeader: ByteBuffer, responseContext: RpcResponseCallback): StreamCallbackWithID receiveStream...FIXME receiveStream is part of xref:network:RpcHandler.adoc#receive[RpcHandler] abstraction. == [[logging]] Logging Enable ALL logging level for org.apache.spark.network.netty.NettyBlockRpcServer logger to see what happens inside. Add the following line to conf/log4j.properties : [source,plaintext] \u00b6 log4j.logger.org.apache.spark.network.netty.NettyBlockRpcServer=ALL \u00b6 Refer to xref:ROOT:spark-logging.adoc[Logging].","title":"NettyBlockRpcServer"},{"location":"storage/NettyBlockRpcServer/#source-scala","text":"receive( client: TransportClient, rpcMessage: ByteBuffer, responseContext: RpcResponseCallback): Unit receive...FIXME receive is part of xref:network:RpcHandler.adoc#receive[RpcHandler] abstraction. == [[messages]] Messages === [[OpenBlocks]] OpenBlocks","title":"[source, scala]"},{"location":"storage/NettyBlockRpcServer/#sourcejava","text":"OpenBlocks( String appId, String execId, String[] blockIds) When received, NettyBlockRpcServer requests the < > for xref:storage:BlockDataManager.adoc#getBlockData[block data] for every block id in the message. The block data is a collection of xref:network:ManagedBuffer.adoc[] for every block id in the incoming message. NettyBlockRpcServer then xref:network:OneForOneStreamManager.adoc#registerStream[registers a stream of ManagedBuffer s (for the blocks) with the internal StreamManager ] under streamId . NOTE: The internal StreamManager is xref:network:OneForOneStreamManager.adoc[OneForOneStreamManager] and is created when < >. You should see the following TRACE message in the logs:","title":"[source,java]"},{"location":"storage/NettyBlockRpcServer/#sourceplaintext","text":"","title":"[source,plaintext]"},{"location":"storage/NettyBlockRpcServer/#nettyblockrpcserver-registered-streamid-streamid-with-size-buffers","text":"In the end, NettyBlockRpcServer responds with a StreamHandle (with the streamId and the number of blocks). The response is serialized as a ByteBuffer . Posted when OneForOneBlockFetcher is requested to xref:storage:OneForOneBlockFetcher.adoc#start[start]. === [[UploadBlock]] UploadBlock","title":"NettyBlockRpcServer: Registered streamId [streamId]  with [size] buffers"},{"location":"storage/NettyBlockRpcServer/#sourcejava_1","text":"UploadBlock( String appId, String execId, String blockId, byte[] metadata, byte[] blockData) When received, NettyBlockRpcServer deserializes the metadata of the input message to get the xref:storage:StorageLevel.adoc[StorageLevel] and ClassTag of the block being uploaded. NettyBlockRpcServer creates a xref:storage:BlockId.adoc[] for the block id and requests the < > to xref:storage:BlockDataManager.adoc#putBlockData[store the block]. In the end, NettyBlockRpcServer responds with a 0 -capacity ByteBuffer . Posted when NettyBlockTransferService is requested to xref:storage:NettyBlockTransferService.adoc#uploadBlock[upload a block]. == [[receiveStream]] Receiving RPC Message with Streamed Data","title":"[source,java]"},{"location":"storage/NettyBlockRpcServer/#source-scala_1","text":"receiveStream( client: TransportClient, messageHeader: ByteBuffer, responseContext: RpcResponseCallback): StreamCallbackWithID receiveStream...FIXME receiveStream is part of xref:network:RpcHandler.adoc#receive[RpcHandler] abstraction. == [[logging]] Logging Enable ALL logging level for org.apache.spark.network.netty.NettyBlockRpcServer logger to see what happens inside. Add the following line to conf/log4j.properties :","title":"[source, scala]"},{"location":"storage/NettyBlockRpcServer/#sourceplaintext_1","text":"","title":"[source,plaintext]"},{"location":"storage/NettyBlockRpcServer/#log4jloggerorgapachesparknetworknettynettyblockrpcserverall","text":"Refer to xref:ROOT:spark-logging.adoc[Logging].","title":"log4j.logger.org.apache.spark.network.netty.NettyBlockRpcServer=ALL"},{"location":"storage/NettyBlockTransferService/","text":"= NettyBlockTransferService NettyBlockTransferService is a xref:storage:BlockTransferService.adoc[] that uses Netty for < > or < > blocks of data. .NettyBlockTransferService, SparkEnv and BlockManager image::NettyBlockTransferService.png[align=\"center\"] == [[creating-instance]] Creating Instance NettyBlockTransferService takes the following to be created: [[conf]] xref:ROOT:SparkConf.adoc[SparkConf] [[securityManager]] SecurityManager [[bindAddress]] Bind address [[hostName]] Host name [[_port]] Port number [[numCores]] Number of CPU cores NettyBlockTransferService is created when SparkEnv is xref:core:SparkEnv.adoc#create-NettyBlockTransferService[created] for the driver and executors (and xref:core:SparkEnv.adoc#create-BlockManager[creates the BlockManager]). == [[transportConf]][[transportContext]] TransportConf, TransportContext NettyBlockTransferService creates a xref:network:TransportConf.adoc[] for shuffle module (using xref:network:SparkTransportConf.adoc#fromSparkConf[SparkTransportConf] utility) when < >. NettyBlockTransferService uses the TransportConf for the following: Create a xref:network:TransportContext.adoc[] when requested to < > Create a xref:storage:OneForOneBlockFetcher.adoc[] and a xref:core:RetryingBlockFetcher.adoc[RetryingBlockFetcher] when requested to < > NettyBlockTransferService uses the TransportContext to create the < > and the < >. == [[clientFactory]] TransportClientFactory NettyBlockTransferService creates a xref:network:TransportClientFactory.adoc[] when requested to < >. NettyBlockTransferService uses the TransportClientFactory for the following: < > < > < > NettyBlockTransferService requests the TransportClientFactory to xref:network:TransportClientFactory.adoc#close[close] when requested to < >. == [[server]] TransportServer NettyBlockTransferService < > when requested to < >. NettyBlockTransferService uses the TransportServer for the following: < > < > NettyBlockTransferService requests the TransportServer to xref:network:TransportServer.adoc#close[close] when requested to < >. == [[port]] Port NettyBlockTransferService simply requests the < > for the xref:network:TransportServer.adoc#getPort[port]. == [[shuffleMetrics]] Shuffle Metrics [source,scala] \u00b6 shuffleMetrics(): MetricSet \u00b6 shuffleMetrics...FIXME shuffleMetrics is part of the xref:storage:ShuffleClient.adoc#shuffleMetrics[ShuffleClient] abstraction. == [[fetchBlocks]] Fetching Blocks [source, scala] \u00b6 fetchBlocks( host: String, port: Int, execId: String, blockIds: Array[String], listener: BlockFetchingListener): Unit When executed, fetchBlocks prints out the following TRACE message in the logs: TRACE Fetch blocks from [host]:[port] (executor id [execId]) fetchBlocks then creates a RetryingBlockFetcher.BlockFetchStarter where createAndStart method...FIXME Depending on the maximum number of acceptable IO exceptions (such as connection timeouts) per request, if the number is greater than 0 , fetchBlocks creates a xref:core:RetryingBlockFetcher.adoc#creating-instance[RetryingBlockFetcher] and xref:core:RetryingBlockFetcher.adoc#start[starts] it immediately. NOTE: RetryingBlockFetcher is created with the RetryingBlockFetcher.BlockFetchStarter created earlier, the input blockIds and listener . If however the number of retries is not greater than 0 (it could be 0 or less), the RetryingBlockFetcher.BlockFetchStarter created earlier is started (with the input blockIds and listener ). In case of any Exception , you should see the following ERROR message in the logs and the input BlockFetchingListener gets notified (using onBlockFetchFailure for every block id). ERROR Exception while beginning fetchBlocks fetchBlocks is part of xref:storage:BlockTransferService.adoc#fetchBlocks[BlockTransferService] abstraction. == [[appId]] Application Id == [[close]] Closing NettyBlockTransferService [source, scala] \u00b6 close(): Unit \u00b6 close...FIXME close is part of the xref:storage:BlockTransferService.adoc#close[BlockTransferService] abstraction. == [[init]] Initializing NettyBlockTransferService [source, scala] \u00b6 init( blockDataManager: BlockDataManager): Unit init creates a xref:storage:NettyBlockRpcServer.adoc[] (for the xref:ROOT:SparkConf.adoc#getAppId[application id], a JavaSerializer and the given xref:storage:BlockDataManager.adoc[BlockDataManager]) that is used to create a < >. init creates the internal clientFactory and a server. CAUTION: FIXME What's the \"a server\"? In the end, you should see the INFO message in the logs: Server created on [hostName]:[port] NOTE: hostname is given when xref:core:SparkEnv.adoc#NettyBlockTransferService[NettyBlockTransferService is created] and is controlled by link:spark-driver.adoc#spark_driver_host[ spark.driver.host Spark property] for the driver and differs per deployment environment for executors (as controlled by xref:executor:CoarseGrainedExecutorBackend.adoc#main[ --hostname for CoarseGrainedExecutorBackend ]). init is part of the xref:storage:BlockTransferService.adoc#init[BlockTransferService] abstraction. == [[uploadBlock]] Uploading Block [source, scala] \u00b6 uploadBlock( hostname: String, port: Int, execId: String, blockId: BlockId, blockData: ManagedBuffer, level: StorageLevel, classTag: ClassTag[_]): Future[Unit] Internally, uploadBlock creates a TransportClient client to send a < UploadBlock message>> (to the input hostname and port ). NOTE: UploadBlock message is processed by xref:storage:NettyBlockRpcServer.adoc[NettyBlockRpcServer]. The UploadBlock message holds the < >, the input execId and blockId . It also holds the serialized bytes for block metadata with level and classTag serialized (using the internal JavaSerializer ) as well as the serialized bytes for the input blockData itself (this time however the serialization uses xref:storage:BlockDataManager.adoc#ManagedBuffer[ ManagedBuffer.nioByteBuffer method]). The entire UploadBlock message is further serialized before sending (using TransportClient.sendRpc ). CAUTION: FIXME Describe TransportClient and clientFactory.createClient . When blockId block was successfully uploaded, you should see the following TRACE message in the logs: TRACE NettyBlockTransferService: Successfully uploaded block [blockId] When an upload failed, you should see the following ERROR message in the logs: ERROR Error while uploading block [blockId] uploadBlock is part of the xref:storage:BlockTransferService.adoc#uploadBlock[BlockTransferService] abstraction. == [[UploadBlock]] UploadBlock Message UploadBlock is a BlockTransferMessage that describes a block being uploaded, i.e. send over the wire from a < > to a xref:storage:NettyBlockRpcServer.adoc#UploadBlock[NettyBlockRpcServer]. . UploadBlock Attributes [cols=\"1,2\",options=\"header\",width=\"100%\"] |=== | Attribute | Description | appId | The application id (the block belongs to) | execId | The executor id | blockId | The block id | metadata | | blockData | The block data as an array of bytes |=== As an Encodable , UploadBlock can calculate the encoded size and do encoding and decoding itself to or from a ByteBuf , respectively. == [[createServer]] createServer Internal Method [source, scala] \u00b6 createServer( bootstraps: List[TransportServerBootstrap]): TransportServer createServer...FIXME createServer is used when NettyBlockTransferService is requested to < >. == [[logging]] Logging Enable ALL logging level for org.apache.spark.network.netty.NettyBlockTransferService logger to see what happens inside. Add the following line to conf/log4j.properties : [source,plaintext] \u00b6 log4j.logger.org.apache.spark.network.netty.NettyBlockTransferService=ALL \u00b6 Refer to xref:ROOT:spark-logging.adoc[Logging].","title":"NettyBlockTransferService"},{"location":"storage/NettyBlockTransferService/#sourcescala","text":"","title":"[source,scala]"},{"location":"storage/NettyBlockTransferService/#shufflemetrics-metricset","text":"shuffleMetrics...FIXME shuffleMetrics is part of the xref:storage:ShuffleClient.adoc#shuffleMetrics[ShuffleClient] abstraction. == [[fetchBlocks]] Fetching Blocks","title":"shuffleMetrics(): MetricSet"},{"location":"storage/NettyBlockTransferService/#source-scala","text":"fetchBlocks( host: String, port: Int, execId: String, blockIds: Array[String], listener: BlockFetchingListener): Unit When executed, fetchBlocks prints out the following TRACE message in the logs: TRACE Fetch blocks from [host]:[port] (executor id [execId]) fetchBlocks then creates a RetryingBlockFetcher.BlockFetchStarter where createAndStart method...FIXME Depending on the maximum number of acceptable IO exceptions (such as connection timeouts) per request, if the number is greater than 0 , fetchBlocks creates a xref:core:RetryingBlockFetcher.adoc#creating-instance[RetryingBlockFetcher] and xref:core:RetryingBlockFetcher.adoc#start[starts] it immediately. NOTE: RetryingBlockFetcher is created with the RetryingBlockFetcher.BlockFetchStarter created earlier, the input blockIds and listener . If however the number of retries is not greater than 0 (it could be 0 or less), the RetryingBlockFetcher.BlockFetchStarter created earlier is started (with the input blockIds and listener ). In case of any Exception , you should see the following ERROR message in the logs and the input BlockFetchingListener gets notified (using onBlockFetchFailure for every block id). ERROR Exception while beginning fetchBlocks fetchBlocks is part of xref:storage:BlockTransferService.adoc#fetchBlocks[BlockTransferService] abstraction. == [[appId]] Application Id == [[close]] Closing NettyBlockTransferService","title":"[source, scala]"},{"location":"storage/NettyBlockTransferService/#source-scala_1","text":"","title":"[source, scala]"},{"location":"storage/NettyBlockTransferService/#close-unit","text":"close...FIXME close is part of the xref:storage:BlockTransferService.adoc#close[BlockTransferService] abstraction. == [[init]] Initializing NettyBlockTransferService","title":"close(): Unit"},{"location":"storage/NettyBlockTransferService/#source-scala_2","text":"init( blockDataManager: BlockDataManager): Unit init creates a xref:storage:NettyBlockRpcServer.adoc[] (for the xref:ROOT:SparkConf.adoc#getAppId[application id], a JavaSerializer and the given xref:storage:BlockDataManager.adoc[BlockDataManager]) that is used to create a < >. init creates the internal clientFactory and a server. CAUTION: FIXME What's the \"a server\"? In the end, you should see the INFO message in the logs: Server created on [hostName]:[port] NOTE: hostname is given when xref:core:SparkEnv.adoc#NettyBlockTransferService[NettyBlockTransferService is created] and is controlled by link:spark-driver.adoc#spark_driver_host[ spark.driver.host Spark property] for the driver and differs per deployment environment for executors (as controlled by xref:executor:CoarseGrainedExecutorBackend.adoc#main[ --hostname for CoarseGrainedExecutorBackend ]). init is part of the xref:storage:BlockTransferService.adoc#init[BlockTransferService] abstraction. == [[uploadBlock]] Uploading Block","title":"[source, scala]"},{"location":"storage/NettyBlockTransferService/#source-scala_3","text":"uploadBlock( hostname: String, port: Int, execId: String, blockId: BlockId, blockData: ManagedBuffer, level: StorageLevel, classTag: ClassTag[_]): Future[Unit] Internally, uploadBlock creates a TransportClient client to send a < UploadBlock message>> (to the input hostname and port ). NOTE: UploadBlock message is processed by xref:storage:NettyBlockRpcServer.adoc[NettyBlockRpcServer]. The UploadBlock message holds the < >, the input execId and blockId . It also holds the serialized bytes for block metadata with level and classTag serialized (using the internal JavaSerializer ) as well as the serialized bytes for the input blockData itself (this time however the serialization uses xref:storage:BlockDataManager.adoc#ManagedBuffer[ ManagedBuffer.nioByteBuffer method]). The entire UploadBlock message is further serialized before sending (using TransportClient.sendRpc ). CAUTION: FIXME Describe TransportClient and clientFactory.createClient . When blockId block was successfully uploaded, you should see the following TRACE message in the logs: TRACE NettyBlockTransferService: Successfully uploaded block [blockId] When an upload failed, you should see the following ERROR message in the logs: ERROR Error while uploading block [blockId] uploadBlock is part of the xref:storage:BlockTransferService.adoc#uploadBlock[BlockTransferService] abstraction. == [[UploadBlock]] UploadBlock Message UploadBlock is a BlockTransferMessage that describes a block being uploaded, i.e. send over the wire from a < > to a xref:storage:NettyBlockRpcServer.adoc#UploadBlock[NettyBlockRpcServer]. . UploadBlock Attributes [cols=\"1,2\",options=\"header\",width=\"100%\"] |=== | Attribute | Description | appId | The application id (the block belongs to) | execId | The executor id | blockId | The block id | metadata | | blockData | The block data as an array of bytes |=== As an Encodable , UploadBlock can calculate the encoded size and do encoding and decoding itself to or from a ByteBuf , respectively. == [[createServer]] createServer Internal Method","title":"[source, scala]"},{"location":"storage/NettyBlockTransferService/#source-scala_4","text":"createServer( bootstraps: List[TransportServerBootstrap]): TransportServer createServer...FIXME createServer is used when NettyBlockTransferService is requested to < >. == [[logging]] Logging Enable ALL logging level for org.apache.spark.network.netty.NettyBlockTransferService logger to see what happens inside. Add the following line to conf/log4j.properties :","title":"[source, scala]"},{"location":"storage/NettyBlockTransferService/#sourceplaintext","text":"","title":"[source,plaintext]"},{"location":"storage/NettyBlockTransferService/#log4jloggerorgapachesparknetworknettynettyblocktransferserviceall","text":"Refer to xref:ROOT:spark-logging.adoc[Logging].","title":"log4j.logger.org.apache.spark.network.netty.NettyBlockTransferService=ALL"},{"location":"storage/OneForOneBlockFetcher/","text":"= OneForOneBlockFetcher OneForOneBlockFetcher is...FIXME == [[creating-instance]] Creating Instance OneForOneBlockFetcher takes the following to be created: [[client]] TransportClient [[appId]] Application ID [[execId]] Executor ID [[blockIds]] Block IDs [[listener]] xref:core:BlockFetchingListener.adoc[] [[transportConf]] xref:network:TransportConf.adoc[] [[downloadFileManager]] DownloadFileManager OneForOneBlockFetcher is created when xref:storage:NettyBlockTransferService.adoc#fetchBlocks[NettyBlockTransferService] and xref:storage:ExternalShuffleClient.adoc#fetchBlocks[ExternalShuffleClient] are requested to fetch blocks. == [[openMessage]] OpenBlocks Message OneForOneBlockFetcher creates a OpenBlocks message (for the given < >, < > and < >) when < >. The OpenBlocks message is posted when OneForOneBlockFetcher is requested to < >. == [[start]] start Method [source,java] \u00b6 void start() \u00b6 start...FIXME start is used when xref:storage:NettyBlockTransferService.adoc#fetchBlocks[NettyBlockTransferService] and xref:storage:ExternalShuffleClient.adoc#fetchBlocks[ExternalShuffleClient] are requested to fetch blocks.","title":"OneForOneBlockFetcher"},{"location":"storage/OneForOneBlockFetcher/#sourcejava","text":"","title":"[source,java]"},{"location":"storage/OneForOneBlockFetcher/#void-start","text":"start...FIXME start is used when xref:storage:NettyBlockTransferService.adoc#fetchBlocks[NettyBlockTransferService] and xref:storage:ExternalShuffleClient.adoc#fetchBlocks[ExternalShuffleClient] are requested to fetch blocks.","title":"void start()"},{"location":"storage/RDDInfo/","text":"= RDDInfo RDDInfo is...FIXME","title":"RDDInfo"},{"location":"storage/ShuffleBlockFetcherIterator/","text":"= ShuffleBlockFetcherIterator ShuffleBlockFetcherIterator is a Scala http://www.scala-lang.org/api/current/scala/collection/Iterator.html[Iterator ] that fetches shuffle blocks (aka shuffle map outputs ) from block managers. ShuffleBlockFetcherIterator is < > exclusively when BlockStoreShuffleReader is requested to xref:shuffle:BlockStoreShuffleReader.adoc#read[read combined key-value records for a reduce task]. ShuffleBlockFetcherIterator allows for < > as (BlockId, InputStream) pairs so a caller can handle shuffle blocks in a pipelined fashion as they are received. ShuffleBlockFetcherIterator is exhausted (i.e. < >) when the < > is at least the < >. ShuffleBlockFetcherIterator < > to avoid consuming too much memory. [[internal-registries]] .ShuffleBlockFetcherIterator's Internal Registries and Counters [cols=\"1,2\",options=\"header\",width=\"100%\"] |=== | Name | Description | numBlocksProcessed | [[numBlocksProcessed]] The number of blocks < >. | numBlocksToFetch a| [[numBlocksToFetch]] Total number of blocks to < >. ShuffleBlockFetcherIterator can < > up to numBlocksToFetch elements. numBlocksToFetch is increased every time ShuffleBlockFetcherIterator is requested to < > that prints it out as the INFO message to the logs: Getting [numBlocksToFetch] non-empty blocks out of [totalBlocks] blocks | [[results]] results | Internal FIFO blocking queue (using Java's https://docs.oracle.com/javase/8/docs/api/java/util/concurrent/LinkedBlockingQueue.html[java.util.concurrent.LinkedBlockingQueue ]) to hold FetchResult remote and local fetch results. Used in: < > to take one FetchResult off the queue, < > to put SuccessFetchResult or FailureFetchResult remote fetch results (as part of BlockFetchingListener callback), < > (similarly to < >) to put local fetch results, < > to release managed buffers for SuccessFetchResult results. | [[maxBytesInFlight]] maxBytesInFlight | The maximum size (in bytes) of all the remote shuffle blocks to fetch. Set when < >. | [[maxReqsInFlight]] maxReqsInFlight | The maximum number of remote requests to fetch shuffle blocks. Set when < >. | [[bytesInFlight]] bytesInFlight | The bytes of fetched remote shuffle blocks in flight Starts at 0 when < >. Incremented every < > and decremented every < >. ShuffleBlockFetcherIterator makes sure that the invariant of bytesInFlight below < > holds every < >. | [[reqsInFlight]] reqsInFlight | The number of remote shuffle block fetch requests in flight. Starts at 0 when < >. Incremented every < > and decremented every < >. ShuffleBlockFetcherIterator makes sure that the invariant of reqsInFlight below < > holds every < >. | [[isZombie]] isZombie | Flag whether ShuffleBlockFetcherIterator is still active. It is disabled, i.e. false , when < >. < > (when the task using ShuffleBlockFetcherIterator finishes), the < > (registered in sendRequest ) will no longer add fetched remote shuffle blocks into < > internal queue. | [[currentResult]] currentResult | The currently-processed SuccessFetchResult Set when ShuffleBlockFetcherIterator < (BlockId, InputStream) tuple>> and < > (on < >). |=== [TIP] \u00b6 Enable ERROR , WARN , INFO , DEBUG or TRACE logging levels for org.apache.spark.storage.ShuffleBlockFetcherIterator logger to see what happens in ShuffleBlockFetcherIterator. Add the following line to conf/log4j.properties : log4j.logger.org.apache.spark.storage.ShuffleBlockFetcherIterator=TRACE Refer to link:spark-logging.adoc[Logging]. \u00b6 == [[fetchUpToMaxBytes]] fetchUpToMaxBytes Method CAUTION: FIXME == [[creating-instance]] Creating ShuffleBlockFetcherIterator Instance When created, ShuffleBlockFetcherIterator takes the following: [[context]] link:spark-TaskContext.adoc[TaskContext] [[shuffleClient]] xref:storage:ShuffleClient.adoc[] [[blockManager]] xref:storage:BlockManager.adoc[BlockManager] [[blocksByAddress]] Blocks to fetch per xref:storage:BlockManager.adoc[BlockManager] (as Seq[(BlockManagerId, Seq[(BlockId, Long)])] ) [[streamWrapper]] Function to wrap the returned input stream (as (BlockId, InputStream) => InputStream ) < > -- the maximum size (in bytes) of map outputs to fetch simultaneously from each reduce task (controlled by xref:shuffle:BlockStoreShuffleReader.adoc#spark_reducer_maxSizeInFlight[spark.reducer.maxSizeInFlight] Spark property) < > -- the maximum number of remote requests to fetch blocks at any given point (controlled by xref:shuffle:BlockStoreShuffleReader.adoc#spark_reducer_maxReqsInFlight[spark.reducer.maxReqsInFlight] Spark property) [[maxBlocksInFlightPerAddress]] maxBlocksInFlightPerAddress [[maxReqSizeShuffleToMem]] maxReqSizeShuffleToMem [[detectCorrupt]] detectCorrupt flag to detect any corruption in fetched blocks (controlled by xref:shuffle:BlockStoreShuffleReader.adoc#spark_shuffle_detectCorrupt[spark.shuffle.detectCorrupt] Spark property) == [[initialize]] Initializing ShuffleBlockFetcherIterator -- initialize Internal Method [source, scala] \u00b6 initialize(): Unit \u00b6 initialize registers a task cleanup and fetches shuffle blocks from remote and local xref:storage:BlockManager.adoc[BlockManagers]. Internally, initialize link:spark-TaskContext.adoc#addTaskCompletionListener[registers a TaskCompletionListener ] (that will < > right after the task finishes). initialize < >. initialize < fetchRequests internal registry)>>. As ShuffleBlockFetcherIterator is in initialization phase, initialize makes sure that < > and < > internal counters are both 0 . Otherwise, initialize throws an exception. initialize < > (from remote xref:storage:BlockManager.adoc[BlockManagers]). You should see the following INFO message in the logs: INFO ShuffleBlockFetcherIterator: Started [numFetches] remote fetches in [time] ms initialize < >. You should see the following DEBUG message in the logs: DEBUG ShuffleBlockFetcherIterator: Got local blocks in [time] ms NOTE: initialize is used exclusively when ShuffleBlockFetcherIterator is < >. == [[sendRequest]] Sending Remote Shuffle Block Fetch Request -- sendRequest Internal Method [source, scala] \u00b6 sendRequest(req: FetchRequest): Unit \u00b6 Internally, when sendRequest runs, you should see the following DEBUG message in the logs: DEBUG ShuffleBlockFetcherIterator: Sending request for [blocks.size] blocks ([size] B) from [hostPort] sendRequest increments < > and < > internal counters. NOTE: The input FetchRequest contains the remote xref:storage:BlockManagerId.adoc[] address and the shuffle blocks to fetch (as a sequence of xref:storage:BlockId.adoc[] and their sizes). sendRequest xref:storage:ShuffleClient.adoc#fetchBlocks[requests ShuffleClient to fetch shuffle blocks] (from the host, the port, and the executor as defined in the input FetchRequest ). NOTE: ShuffleClient was defined when < >. sendRequest registers a BlockFetchingListener with ShuffleClient that: < > adds it as SuccessFetchResult to < > internal queue. < > adds it as FailureFetchResult to < > internal queue. NOTE: sendRequest is used exclusively when ShuffleBlockFetcherIterator is requested to < >. === [[sendRequest-BlockFetchingListener-onBlockFetchSuccess]] onBlockFetchSuccess Callback [source, scala] \u00b6 onBlockFetchSuccess(blockId: String, buf: ManagedBuffer): Unit \u00b6 Internally, onBlockFetchSuccess checks if the < > and does the further processing if it is not. onBlockFetchSuccess marks the input blockId as received (i.e. removes it from all the blocks to fetch as requested in < >). onBlockFetchSuccess adds the managed buf (as SuccessFetchResult ) to < > internal queue. You should see the following DEBUG message in the logs: DEBUG ShuffleBlockFetcherIterator: remainingBlocks: [blocks] Regardless of zombie state of ShuffleBlockFetcherIterator, you should see the following TRACE message in the logs: TRACE ShuffleBlockFetcherIterator: Got remote block [blockId] after [time] ms === [[sendRequest-BlockFetchingListener-onBlockFetchFailure]] onBlockFetchFailure Callback [source, scala] \u00b6 onBlockFetchFailure(blockId: String, e: Throwable): Unit \u00b6 When onBlockFetchFailure is called, you should see the following ERROR message in the logs: ERROR ShuffleBlockFetcherIterator: Failed to get block(s) from [hostPort] onBlockFetchFailure adds the block (as FailureFetchResult ) to < > internal queue. == [[throwFetchFailedException]] Throwing FetchFailedException (for ShuffleBlockId) -- throwFetchFailedException Internal Method [source, scala] \u00b6 throwFetchFailedException( blockId: BlockId, address: BlockManagerId, e: Throwable): Nothing throwFetchFailedException throws a xref:shuffle:FetchFailedException.adoc[FetchFailedException] when the input blockId is a ShuffleBlockId . NOTE: throwFetchFailedException creates a FetchFailedException passing on the root cause of a failure, i.e. the input e . Otherwise, throwFetchFailedException throws a SparkException : Failed to get block [blockId], which is not a shuffle block NOTE: throwFetchFailedException is used when ShuffleBlockFetcherIterator is requested for the < >. == [[cleanup]] Releasing Resources -- cleanup Internal Method [source, scala] \u00b6 cleanup(): Unit \u00b6 Internally, cleanup marks ShuffleBlockFetcherIterator a < >. cleanup < >. cleanup iterates over < > internal queue and for every SuccessFetchResult , increments remote bytes read and blocks fetched shuffle task metrics, and eventually releases the managed buffer. NOTE: cleanup is used when < >. == [[releaseCurrentResultBuffer]] Decrementing Reference Count Of and Releasing Result Buffer (for SuccessFetchResult) -- releaseCurrentResultBuffer Internal Method [source, scala] \u00b6 releaseCurrentResultBuffer(): Unit \u00b6 releaseCurrentResultBuffer decrements the < SuccessFetchResult reference>>'s buffer reference count if there is any. releaseCurrentResultBuffer releases < >. NOTE: releaseCurrentResultBuffer is used when < > and BufferReleasingInputStream closes. == [[fetchLocalBlocks]] fetchLocalBlocks Internal Method [source, scala] \u00b6 fetchLocalBlocks(): Unit \u00b6 fetchLocalBlocks ...FIXME NOTE: fetchLocalBlocks is used when...FIXME == [[hasNext]] hasNext Method [source, scala] \u00b6 hasNext: Boolean \u00b6 NOTE: hasNext is part of Scala's link:++ https://www.scala-lang.org/api/current/scala/collection/Iterator.html#hasNext:Boolean++[Iterator Contract] to test whether this iterator can provide another element. hasNext is positive ( true ) when < > is less than < >. Otherwise, hasNext is negative ( false ). == [[splitLocalRemoteBlocks]] splitLocalRemoteBlocks Internal Method [source, scala] \u00b6 splitLocalRemoteBlocks(): ArrayBuffer[FetchRequest] \u00b6 splitLocalRemoteBlocks ...FIXME NOTE: splitLocalRemoteBlocks is used exclusively when ShuffleBlockFetcherIterator is requested to < >. == [[next]] Retrieving Next Element -- next Method [source, scala] \u00b6 next(): (BlockId, InputStream) \u00b6 NOTE: next is part of Scala's link:++ https://www.scala-lang.org/api/current/scala/collection/Iterator.html#next():A++[Iterator Contract] to produce the next element of this iterator. next ...FIXME","title":"ShuffleBlockFetcherIterator"},{"location":"storage/ShuffleBlockFetcherIterator/#tip","text":"Enable ERROR , WARN , INFO , DEBUG or TRACE logging levels for org.apache.spark.storage.ShuffleBlockFetcherIterator logger to see what happens in ShuffleBlockFetcherIterator. Add the following line to conf/log4j.properties : log4j.logger.org.apache.spark.storage.ShuffleBlockFetcherIterator=TRACE","title":"[TIP]"},{"location":"storage/ShuffleBlockFetcherIterator/#refer-to-linkspark-loggingadoclogging","text":"== [[fetchUpToMaxBytes]] fetchUpToMaxBytes Method CAUTION: FIXME == [[creating-instance]] Creating ShuffleBlockFetcherIterator Instance When created, ShuffleBlockFetcherIterator takes the following: [[context]] link:spark-TaskContext.adoc[TaskContext] [[shuffleClient]] xref:storage:ShuffleClient.adoc[] [[blockManager]] xref:storage:BlockManager.adoc[BlockManager] [[blocksByAddress]] Blocks to fetch per xref:storage:BlockManager.adoc[BlockManager] (as Seq[(BlockManagerId, Seq[(BlockId, Long)])] ) [[streamWrapper]] Function to wrap the returned input stream (as (BlockId, InputStream) => InputStream ) < > -- the maximum size (in bytes) of map outputs to fetch simultaneously from each reduce task (controlled by xref:shuffle:BlockStoreShuffleReader.adoc#spark_reducer_maxSizeInFlight[spark.reducer.maxSizeInFlight] Spark property) < > -- the maximum number of remote requests to fetch blocks at any given point (controlled by xref:shuffle:BlockStoreShuffleReader.adoc#spark_reducer_maxReqsInFlight[spark.reducer.maxReqsInFlight] Spark property) [[maxBlocksInFlightPerAddress]] maxBlocksInFlightPerAddress [[maxReqSizeShuffleToMem]] maxReqSizeShuffleToMem [[detectCorrupt]] detectCorrupt flag to detect any corruption in fetched blocks (controlled by xref:shuffle:BlockStoreShuffleReader.adoc#spark_shuffle_detectCorrupt[spark.shuffle.detectCorrupt] Spark property) == [[initialize]] Initializing ShuffleBlockFetcherIterator -- initialize Internal Method","title":"Refer to link:spark-logging.adoc[Logging]."},{"location":"storage/ShuffleBlockFetcherIterator/#source-scala","text":"","title":"[source, scala]"},{"location":"storage/ShuffleBlockFetcherIterator/#initialize-unit","text":"initialize registers a task cleanup and fetches shuffle blocks from remote and local xref:storage:BlockManager.adoc[BlockManagers]. Internally, initialize link:spark-TaskContext.adoc#addTaskCompletionListener[registers a TaskCompletionListener ] (that will < > right after the task finishes). initialize < >. initialize < fetchRequests internal registry)>>. As ShuffleBlockFetcherIterator is in initialization phase, initialize makes sure that < > and < > internal counters are both 0 . Otherwise, initialize throws an exception. initialize < > (from remote xref:storage:BlockManager.adoc[BlockManagers]). You should see the following INFO message in the logs: INFO ShuffleBlockFetcherIterator: Started [numFetches] remote fetches in [time] ms initialize < >. You should see the following DEBUG message in the logs: DEBUG ShuffleBlockFetcherIterator: Got local blocks in [time] ms NOTE: initialize is used exclusively when ShuffleBlockFetcherIterator is < >. == [[sendRequest]] Sending Remote Shuffle Block Fetch Request -- sendRequest Internal Method","title":"initialize(): Unit"},{"location":"storage/ShuffleBlockFetcherIterator/#source-scala_1","text":"","title":"[source, scala]"},{"location":"storage/ShuffleBlockFetcherIterator/#sendrequestreq-fetchrequest-unit","text":"Internally, when sendRequest runs, you should see the following DEBUG message in the logs: DEBUG ShuffleBlockFetcherIterator: Sending request for [blocks.size] blocks ([size] B) from [hostPort] sendRequest increments < > and < > internal counters. NOTE: The input FetchRequest contains the remote xref:storage:BlockManagerId.adoc[] address and the shuffle blocks to fetch (as a sequence of xref:storage:BlockId.adoc[] and their sizes). sendRequest xref:storage:ShuffleClient.adoc#fetchBlocks[requests ShuffleClient to fetch shuffle blocks] (from the host, the port, and the executor as defined in the input FetchRequest ). NOTE: ShuffleClient was defined when < >. sendRequest registers a BlockFetchingListener with ShuffleClient that: < > adds it as SuccessFetchResult to < > internal queue. < > adds it as FailureFetchResult to < > internal queue. NOTE: sendRequest is used exclusively when ShuffleBlockFetcherIterator is requested to < >. === [[sendRequest-BlockFetchingListener-onBlockFetchSuccess]] onBlockFetchSuccess Callback","title":"sendRequest(req: FetchRequest): Unit"},{"location":"storage/ShuffleBlockFetcherIterator/#source-scala_2","text":"","title":"[source, scala]"},{"location":"storage/ShuffleBlockFetcherIterator/#onblockfetchsuccessblockid-string-buf-managedbuffer-unit","text":"Internally, onBlockFetchSuccess checks if the < > and does the further processing if it is not. onBlockFetchSuccess marks the input blockId as received (i.e. removes it from all the blocks to fetch as requested in < >). onBlockFetchSuccess adds the managed buf (as SuccessFetchResult ) to < > internal queue. You should see the following DEBUG message in the logs: DEBUG ShuffleBlockFetcherIterator: remainingBlocks: [blocks] Regardless of zombie state of ShuffleBlockFetcherIterator, you should see the following TRACE message in the logs: TRACE ShuffleBlockFetcherIterator: Got remote block [blockId] after [time] ms === [[sendRequest-BlockFetchingListener-onBlockFetchFailure]] onBlockFetchFailure Callback","title":"onBlockFetchSuccess(blockId: String, buf: ManagedBuffer): Unit"},{"location":"storage/ShuffleBlockFetcherIterator/#source-scala_3","text":"","title":"[source, scala]"},{"location":"storage/ShuffleBlockFetcherIterator/#onblockfetchfailureblockid-string-e-throwable-unit","text":"When onBlockFetchFailure is called, you should see the following ERROR message in the logs: ERROR ShuffleBlockFetcherIterator: Failed to get block(s) from [hostPort] onBlockFetchFailure adds the block (as FailureFetchResult ) to < > internal queue. == [[throwFetchFailedException]] Throwing FetchFailedException (for ShuffleBlockId) -- throwFetchFailedException Internal Method","title":"onBlockFetchFailure(blockId: String, e: Throwable): Unit"},{"location":"storage/ShuffleBlockFetcherIterator/#source-scala_4","text":"throwFetchFailedException( blockId: BlockId, address: BlockManagerId, e: Throwable): Nothing throwFetchFailedException throws a xref:shuffle:FetchFailedException.adoc[FetchFailedException] when the input blockId is a ShuffleBlockId . NOTE: throwFetchFailedException creates a FetchFailedException passing on the root cause of a failure, i.e. the input e . Otherwise, throwFetchFailedException throws a SparkException : Failed to get block [blockId], which is not a shuffle block NOTE: throwFetchFailedException is used when ShuffleBlockFetcherIterator is requested for the < >. == [[cleanup]] Releasing Resources -- cleanup Internal Method","title":"[source, scala]"},{"location":"storage/ShuffleBlockFetcherIterator/#source-scala_5","text":"","title":"[source, scala]"},{"location":"storage/ShuffleBlockFetcherIterator/#cleanup-unit","text":"Internally, cleanup marks ShuffleBlockFetcherIterator a < >. cleanup < >. cleanup iterates over < > internal queue and for every SuccessFetchResult , increments remote bytes read and blocks fetched shuffle task metrics, and eventually releases the managed buffer. NOTE: cleanup is used when < >. == [[releaseCurrentResultBuffer]] Decrementing Reference Count Of and Releasing Result Buffer (for SuccessFetchResult) -- releaseCurrentResultBuffer Internal Method","title":"cleanup(): Unit"},{"location":"storage/ShuffleBlockFetcherIterator/#source-scala_6","text":"","title":"[source, scala]"},{"location":"storage/ShuffleBlockFetcherIterator/#releasecurrentresultbuffer-unit","text":"releaseCurrentResultBuffer decrements the < SuccessFetchResult reference>>'s buffer reference count if there is any. releaseCurrentResultBuffer releases < >. NOTE: releaseCurrentResultBuffer is used when < > and BufferReleasingInputStream closes. == [[fetchLocalBlocks]] fetchLocalBlocks Internal Method","title":"releaseCurrentResultBuffer(): Unit"},{"location":"storage/ShuffleBlockFetcherIterator/#source-scala_7","text":"","title":"[source, scala]"},{"location":"storage/ShuffleBlockFetcherIterator/#fetchlocalblocks-unit","text":"fetchLocalBlocks ...FIXME NOTE: fetchLocalBlocks is used when...FIXME == [[hasNext]] hasNext Method","title":"fetchLocalBlocks(): Unit"},{"location":"storage/ShuffleBlockFetcherIterator/#source-scala_8","text":"","title":"[source, scala]"},{"location":"storage/ShuffleBlockFetcherIterator/#hasnext-boolean","text":"NOTE: hasNext is part of Scala's link:++ https://www.scala-lang.org/api/current/scala/collection/Iterator.html#hasNext:Boolean++[Iterator Contract] to test whether this iterator can provide another element. hasNext is positive ( true ) when < > is less than < >. Otherwise, hasNext is negative ( false ). == [[splitLocalRemoteBlocks]] splitLocalRemoteBlocks Internal Method","title":"hasNext: Boolean"},{"location":"storage/ShuffleBlockFetcherIterator/#source-scala_9","text":"","title":"[source, scala]"},{"location":"storage/ShuffleBlockFetcherIterator/#splitlocalremoteblocks-arraybufferfetchrequest","text":"splitLocalRemoteBlocks ...FIXME NOTE: splitLocalRemoteBlocks is used exclusively when ShuffleBlockFetcherIterator is requested to < >. == [[next]] Retrieving Next Element -- next Method","title":"splitLocalRemoteBlocks(): ArrayBuffer[FetchRequest]"},{"location":"storage/ShuffleBlockFetcherIterator/#source-scala_10","text":"","title":"[source, scala]"},{"location":"storage/ShuffleBlockFetcherIterator/#next-blockid-inputstream","text":"NOTE: next is part of Scala's link:++ https://www.scala-lang.org/api/current/scala/collection/Iterator.html#next():A++[Iterator Contract] to produce the next element of this iterator. next ...FIXME","title":"next(): (BlockId, InputStream)"},{"location":"storage/ShuffleClient/","text":"= ShuffleClient ShuffleClient is the < > of < > that can < >. ShuffleClient can optionally be < > with an appId (that actually does nothing by default) ShuffleClient has < > that are used when BlockManager is requested for a xref:storage:BlockManager.adoc#shuffleMetricsSource[shuffle-related Spark metrics source] (only when Executor is xref:executor:Executor.adoc#creating-instance[created] for a non-local / cluster mode). [[contract]] [source, java] package org.apache.spark.network.shuffle; abstract class ShuffleClient implements Closeable { // only required methods that have no implementation // the others follow abstract void fetchBlocks( String host, int port, String execId, String[] blockIds, BlockFetchingListener listener, TempFileManager tempFileManager); } .(Subset of) ShuffleClient Contract [cols=\"1,2\",options=\"header\",width=\"100%\"] |=== | Method | Description | fetchBlocks | [[fetchBlocks]] Fetches a sequence of blocks from a remote block manager node asynchronously Used exclusively when ShuffleBlockFetcherIterator is requested to xref:storage:ShuffleBlockFetcherIterator.adoc#sendRequest[sendRequest] |=== [[implementations]] .ShuffleClients [cols=\"1,2\",options=\"header\",width=\"100%\"] |=== | ShuffleClient | Description | xref:storage:BlockTransferService.adoc[] | [[BlockTransferService]] | xref:storage:ExternalShuffleClient.adoc[] | [[ExternalShuffleClient]] |=== == [[init]] init Method [source, java] \u00b6 void init( String appId) init does nothing by default. [NOTE] \u00b6 init is used when: BlockManager is requested to xref:storage:BlockManager.adoc#initialize[initialize] * Spark on Mesos' MesosCoarseGrainedSchedulerBackend is requested to registered \u00b6 == [[shuffleMetrics]] Shuffle Metrics [source, java] \u00b6 MetricSet shuffleMetrics() \u00b6 shuffleMetrics returns an empty Dropwizard Metrics' https://metrics.dropwizard.io/3.1.0/apidocs/com/codahale/metrics/MetricSet.html[MetricSet ] by default. NOTE: shuffleMetrics is used exclusively when BlockManager is requested for a xref:storage:BlockManager.adoc#shuffleMetricsSource[shuffle-related Spark metrics source] (only when Executor is xref:executor:Executor.adoc#creating-instance[created] for a non-local / cluster mode).","title":"ShuffleClient"},{"location":"storage/ShuffleClient/#source-java","text":"void init( String appId) init does nothing by default.","title":"[source, java]"},{"location":"storage/ShuffleClient/#note","text":"init is used when: BlockManager is requested to xref:storage:BlockManager.adoc#initialize[initialize]","title":"[NOTE]"},{"location":"storage/ShuffleClient/#spark-on-mesos-mesoscoarsegrainedschedulerbackend-is-requested-to-registered","text":"== [[shuffleMetrics]] Shuffle Metrics","title":"* Spark on Mesos' MesosCoarseGrainedSchedulerBackend is requested to registered"},{"location":"storage/ShuffleClient/#source-java_1","text":"","title":"[source, java]"},{"location":"storage/ShuffleClient/#metricset-shufflemetrics","text":"shuffleMetrics returns an empty Dropwizard Metrics' https://metrics.dropwizard.io/3.1.0/apidocs/com/codahale/metrics/MetricSet.html[MetricSet ] by default. NOTE: shuffleMetrics is used exclusively when BlockManager is requested for a xref:storage:BlockManager.adoc#shuffleMetricsSource[shuffle-related Spark metrics source] (only when Executor is xref:executor:Executor.adoc#creating-instance[created] for a non-local / cluster mode).","title":"MetricSet shuffleMetrics()"},{"location":"storage/ShuffleMetricsSource/","text":"= ShuffleMetricsSource ShuffleMetricsSource is the xref:metrics:spark-metrics-Source.adoc[metrics source] of a xref:storage:BlockManager.adoc[] for < >. ShuffleMetricsSource lives on a Spark executor and is xref:executor:Executor.adoc#creating-instance-BlockManager-shuffleMetricsSource[registered only when a Spark application runs in a non-local / cluster mode]. .Registering ShuffleMetricsSource with \"executor\" MetricsSystem image::ShuffleMetricsSource.png[align=\"center\"] == [[creating-instance]] Creating Instance ShuffleMetricsSource takes the following to be created: < > < > ShuffleMetricsSource is created when BlockManager is requested for the xref:storage:BlockManager.adoc#shuffleMetricsSource[shuffle metrics source]. == [[metricSet]] MetricSet ShuffleMetricsSource is given a Dropwizard Metrics https://metrics.dropwizard.io/3.1.0/apidocs/com/codahale/metrics/MetricSet.html[MetricSet ] when < >. The MetricSet is requested from the xref:storage:ShuffleClient.adoc#shuffleMetrics[ShuffleClient] (of xref:storage:BlockManager.adoc#shuffleClient[BlockManager]). == [[sourceName]] Source Name ShuffleMetricsSource is given a name when < > that is one of the following: NettyBlockTransfer when xref:ROOT:configuration-properties.adoc#spark.shuffle.service.enabled[spark.shuffle.service.enabled] configuration property is off ( false ) ExternalShuffle when xref:ROOT:configuration-properties.adoc#spark.shuffle.service.enabled[spark.shuffle.service.enabled] configuration property is on ( true )","title":"ShuffleMetricsSource"},{"location":"storage/StorageLevel/","text":"= StorageLevel StorageLevel describes how an RDD is persisted (and addresses the following concerns): Does RDD use disk? Does RDD < > to store data? How much of RDD is in memory? Does RDD use off-heap memory? Should an RDD be serialized or < > (while storing the data)? How many replicas (default: 1 ) to use (can only be less than 40 )? There are the following StorageLevel (number _2 in the name denotes 2 replicas): [[NONE]] NONE (default) DISK_ONLY DISK_ONLY_2 [[MEMORY_ONLY]] MEMORY_ONLY (default for link:spark-rdd-caching.adoc#cache[ cache operation] for RDDs) MEMORY_ONLY_2 MEMORY_ONLY_SER MEMORY_ONLY_SER_2 [[MEMORY_AND_DISK]] MEMORY_AND_DISK MEMORY_AND_DISK_2 MEMORY_AND_DISK_SER MEMORY_AND_DISK_SER_2 OFF_HEAP You can check out the storage level using getStorageLevel() operation. val lines = sc.textFile(\"README.md\") scala> lines.getStorageLevel res0: org.apache.spark.storage.StorageLevel = StorageLevel(disk=false, memory=false, offheap=false, deserialized=false, replication=1) [[useMemory]] StorageLevel can indicate to use memory for data storage using useMemory flag. [source, scala] \u00b6 useMemory: Boolean \u00b6 [[useDisk]] StorageLevel can indicate to use disk for data storage using useDisk flag. [source, scala] \u00b6 useDisk: Boolean \u00b6 [[deserialized]] StorageLevel can indicate to store data in deserialized format using deserialized flag. [source, scala] \u00b6 deserialized: Boolean \u00b6 [[replication]] StorageLevel can indicate to replicate the data to other block managers using replication property. [source, scala] \u00b6 replication: Int \u00b6","title":"StorageLevel"},{"location":"storage/StorageLevel/#source-scala","text":"","title":"[source, scala]"},{"location":"storage/StorageLevel/#usememory-boolean","text":"[[useDisk]] StorageLevel can indicate to use disk for data storage using useDisk flag.","title":"useMemory: Boolean"},{"location":"storage/StorageLevel/#source-scala_1","text":"","title":"[source, scala]"},{"location":"storage/StorageLevel/#usedisk-boolean","text":"[[deserialized]] StorageLevel can indicate to store data in deserialized format using deserialized flag.","title":"useDisk: Boolean"},{"location":"storage/StorageLevel/#source-scala_2","text":"","title":"[source, scala]"},{"location":"storage/StorageLevel/#deserialized-boolean","text":"[[replication]] StorageLevel can indicate to replicate the data to other block managers using replication property.","title":"deserialized: Boolean"},{"location":"storage/StorageLevel/#source-scala_3","text":"","title":"[source, scala]"},{"location":"storage/StorageLevel/#replication-int","text":"","title":"replication: Int"},{"location":"storage/StorageStatus/","text":"== [[StorageStatus]] StorageStatus StorageStatus is a developer API that Spark uses to pass \"just enough\" information about registered xref:storage:BlockManager.adoc[BlockManagers] in a Spark application between Spark services (mostly for monitoring purposes like link:spark-webui.adoc[web UI] or xref:ROOT:SparkListener.adoc[]s). [NOTE] \u00b6 There are two ways to access StorageStatus about all the known BlockManagers in a Spark application: xref:ROOT:SparkContext.adoc#getExecutorStorageStatus[SparkContext.getExecutorStorageStatus] * Being a xref:ROOT:SparkListener.adoc[] and intercepting xref:ROOT:SparkListener.adoc#onBlockManagerAdded[onBlockManagerAdded] and xref:ROOT:SparkListener.adoc#onBlockManagerRemoved[onBlockManagerRemoved] events \u00b6 StorageStatus is < > when: BlockManagerMasterEndpoint xref:storage:BlockManagerMasterEndpoint.adoc#storageStatus[is requested for storage status] (of every xref:storage:BlockManager.adoc[BlockManager] in a Spark application) StorageStatusListener link:spark-webui-StorageStatusListener.adoc#onBlockManagerAdded[gets notified about a new BlockManager ] (in a Spark application) [[internal-registries]] .StorageStatus's Internal Registries and Counters [cols=\"1,2\",options=\"header\",width=\"100%\"] |=== | Name | Description | [[_nonRddBlocks]] _nonRddBlocks | Lookup table of BlockIds per BlockId . Used when...FIXME | [[_rddBlocks]] _rddBlocks | Lookup table of BlockIds with BlockStatus per RDD id. Used when...FIXME |=== === [[updateStorageInfo]] updateStorageInfo Internal Method [source, scala] \u00b6 updateStorageInfo( blockId: BlockId, newBlockStatus: BlockStatus): Unit updateStorageInfo ...FIXME NOTE: updateStorageInfo is used when...FIXME === [[creating-instance]] Creating StorageStatus Instance StorageStatus takes the following when created: [[blockManagerId]] xref:storage:BlockManagerId.adoc[] [[maxMem]] Maximum memory -- xref:storage:BlockManager.adoc#maxMemory[total available on-heap and off-heap memory for storage on the BlockManager ] StorageStatus initializes the < >. === [[rddBlocksById]] Getting RDD Blocks For RDD -- rddBlocksById Method [source, scala] \u00b6 rddBlocksById(rddId: Int): Map[BlockId, BlockStatus] \u00b6 rddBlocksById gives the blocks (as BlockId with their status as BlockStatus ) that belong to rddId RDD. [NOTE] \u00b6 rddBlocksById is used when: StorageStatusListener link:spark-webui-StorageStatusListener.adoc#updateStorageStatus-unpersistedRDD[removes the RDD blocks of an unpersisted RDD]. AllRDDResource does getRDDStorageInfo StorageUtils does getRddBlockLocations \u00b6 === [[removeBlock]] Removing Block (From Internal Registries) -- removeBlock Internal Method [source, scala] \u00b6 removeBlock(blockId: BlockId): Option[BlockStatus] \u00b6 removeBlock removes blockId from <<_rddBlocks, _rddBlocks>> registry and returns it. Internally, removeBlock < > of blockId (to be empty, i.e. removed). removeBlock branches off per the type of xref:storage:BlockId.adoc[], i.e. RDDBlockId or not. For a RDDBlockId , removeBlock finds the RDD in <<_rddBlocks, _rddBlocks>> and removes the blockId . removeBlock removes the RDD (from <<_rddBlocks, _rddBlocks>>) completely, if there are no more blocks registered. For a non- RDDBlockId , removeBlock removes blockId from <<_nonRddBlocks, _nonRddBlocks>> registry. NOTE: removeBlock is used when StorageStatusListener link:spark-webui-StorageStatusListener.adoc#updateStorageStatus-unpersistedRDD[removes RDD blocks for an unpersisted RDD] or link:spark-webui-StorageStatusListener.adoc#updateStorageStatus-executor[updates storage status for an executor]. === [[addBlock]] Registering Status of Data Block -- addBlock Method [source, scala] \u00b6 addBlock( blockId: BlockId, blockStatus: BlockStatus): Unit addBlock ...FIXME NOTE: addBlock is used when...FIXME === [[getBlock]] getBlock Method [source, scala] \u00b6 getBlock(blockId: BlockId): Option[BlockStatus] \u00b6 getBlock ...FIXME NOTE: getBlock is used when...FIXME","title":"StorageStatus"},{"location":"storage/StorageStatus/#note","text":"There are two ways to access StorageStatus about all the known BlockManagers in a Spark application: xref:ROOT:SparkContext.adoc#getExecutorStorageStatus[SparkContext.getExecutorStorageStatus]","title":"[NOTE]"},{"location":"storage/StorageStatus/#being-a-xrefrootsparklisteneradoc-and-intercepting-xrefrootsparklisteneradoconblockmanageraddedonblockmanageradded-and-xrefrootsparklisteneradoconblockmanagerremovedonblockmanagerremoved-events","text":"StorageStatus is < > when: BlockManagerMasterEndpoint xref:storage:BlockManagerMasterEndpoint.adoc#storageStatus[is requested for storage status] (of every xref:storage:BlockManager.adoc[BlockManager] in a Spark application) StorageStatusListener link:spark-webui-StorageStatusListener.adoc#onBlockManagerAdded[gets notified about a new BlockManager ] (in a Spark application) [[internal-registries]] .StorageStatus's Internal Registries and Counters [cols=\"1,2\",options=\"header\",width=\"100%\"] |=== | Name | Description | [[_nonRddBlocks]] _nonRddBlocks | Lookup table of BlockIds per BlockId . Used when...FIXME | [[_rddBlocks]] _rddBlocks | Lookup table of BlockIds with BlockStatus per RDD id. Used when...FIXME |=== === [[updateStorageInfo]] updateStorageInfo Internal Method","title":"* Being a xref:ROOT:SparkListener.adoc[] and intercepting xref:ROOT:SparkListener.adoc#onBlockManagerAdded[onBlockManagerAdded] and xref:ROOT:SparkListener.adoc#onBlockManagerRemoved[onBlockManagerRemoved] events"},{"location":"storage/StorageStatus/#source-scala","text":"updateStorageInfo( blockId: BlockId, newBlockStatus: BlockStatus): Unit updateStorageInfo ...FIXME NOTE: updateStorageInfo is used when...FIXME === [[creating-instance]] Creating StorageStatus Instance StorageStatus takes the following when created: [[blockManagerId]] xref:storage:BlockManagerId.adoc[] [[maxMem]] Maximum memory -- xref:storage:BlockManager.adoc#maxMemory[total available on-heap and off-heap memory for storage on the BlockManager ] StorageStatus initializes the < >. === [[rddBlocksById]] Getting RDD Blocks For RDD -- rddBlocksById Method","title":"[source, scala]"},{"location":"storage/StorageStatus/#source-scala_1","text":"","title":"[source, scala]"},{"location":"storage/StorageStatus/#rddblocksbyidrddid-int-mapblockid-blockstatus","text":"rddBlocksById gives the blocks (as BlockId with their status as BlockStatus ) that belong to rddId RDD.","title":"rddBlocksById(rddId: Int): Map[BlockId, BlockStatus]"},{"location":"storage/StorageStatus/#note_1","text":"rddBlocksById is used when: StorageStatusListener link:spark-webui-StorageStatusListener.adoc#updateStorageStatus-unpersistedRDD[removes the RDD blocks of an unpersisted RDD]. AllRDDResource does getRDDStorageInfo","title":"[NOTE]"},{"location":"storage/StorageStatus/#storageutils-does-getrddblocklocations","text":"=== [[removeBlock]] Removing Block (From Internal Registries) -- removeBlock Internal Method","title":"StorageUtils does getRddBlockLocations"},{"location":"storage/StorageStatus/#source-scala_2","text":"","title":"[source, scala]"},{"location":"storage/StorageStatus/#removeblockblockid-blockid-optionblockstatus","text":"removeBlock removes blockId from <<_rddBlocks, _rddBlocks>> registry and returns it. Internally, removeBlock < > of blockId (to be empty, i.e. removed). removeBlock branches off per the type of xref:storage:BlockId.adoc[], i.e. RDDBlockId or not. For a RDDBlockId , removeBlock finds the RDD in <<_rddBlocks, _rddBlocks>> and removes the blockId . removeBlock removes the RDD (from <<_rddBlocks, _rddBlocks>>) completely, if there are no more blocks registered. For a non- RDDBlockId , removeBlock removes blockId from <<_nonRddBlocks, _nonRddBlocks>> registry. NOTE: removeBlock is used when StorageStatusListener link:spark-webui-StorageStatusListener.adoc#updateStorageStatus-unpersistedRDD[removes RDD blocks for an unpersisted RDD] or link:spark-webui-StorageStatusListener.adoc#updateStorageStatus-executor[updates storage status for an executor]. === [[addBlock]] Registering Status of Data Block -- addBlock Method","title":"removeBlock(blockId: BlockId): Option[BlockStatus]"},{"location":"storage/StorageStatus/#source-scala_3","text":"addBlock( blockId: BlockId, blockStatus: BlockStatus): Unit addBlock ...FIXME NOTE: addBlock is used when...FIXME === [[getBlock]] getBlock Method","title":"[source, scala]"},{"location":"storage/StorageStatus/#source-scala_4","text":"","title":"[source, scala]"},{"location":"storage/StorageStatus/#getblockblockid-blockid-optionblockstatus","text":"getBlock ...FIXME NOTE: getBlock is used when...FIXME","title":"getBlock(blockId: BlockId): Option[BlockStatus]"},{"location":"tools/spark-shell/","text":"== [[spark-shell]] Spark Shell -- spark-shell shell script Spark shell is an interactive environment where you can learn how to make the most out of Apache Spark quickly and conveniently. TIP: Spark shell is particularly helpful for fast interactive prototyping. Under the covers, Spark shell is a standalone Spark application written in Scala that offers environment with auto-completion (using TAB key) where you can run ad-hoc queries and get familiar with the features of Spark (that help you in developing your own standalone Spark applications). It is a very convenient tool to explore the many things available in Spark with immediate feedback. It is one of the many reasons why link:spark-overview.adoc#why-spark[Spark is so helpful for tasks to process datasets of any size]. There are variants of Spark shell for different languages: spark-shell for Scala, pyspark for Python and sparkR for R. NOTE: This document (and the book in general) uses spark-shell for Scala only. You can start Spark shell using < spark-shell script>>. $ ./bin/spark-shell scala> spark-shell is an extension of Scala REPL with automatic instantiation of link:spark-sql-SparkSession.adoc[SparkSession] as spark (and xref:ROOT:SparkContext.adoc[] as sc ). [source, scala] \u00b6 scala> :type spark org.apache.spark.sql.SparkSession // Learn the current version of Spark in use scala> spark.version res0: String = 2.1.0-SNAPSHOT spark-shell also imports link:spark-sql-SparkSession.adoc#implicits[Scala SQL's implicits] and link:spark-sql-SparkSession.adoc#sql[ sql method]. [source, scala] \u00b6 scala> :imports 1) import spark.implicits._ (59 terms, 38 are implicit) 2) import spark.sql (1 terms) [NOTE] \u00b6 When you execute spark-shell you actually execute link:spark-submit.adoc[Spark submit] as follows: [options=\"wrap\"] \u00b6 org.apache.spark.deploy.SparkSubmit --class org.apache.spark.repl.Main --name Spark shell spark-shell \u00b6 Set SPARK_PRINT_LAUNCH_COMMAND to see the entire command to be executed. Refer to link:spark-tips-and-tricks.adoc#SPARK_PRINT_LAUNCH_COMMAND[Print Launch Command of Spark Scripts]. \u00b6 === [[using-spark-shell]] Using Spark shell You start Spark shell using spark-shell script (available in bin directory). $ ./bin/spark-shell Setting default log level to \"WARN\". To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel). WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable WARN ObjectStore: Failed to get database global_temp, returning NoSuchObjectException Spark context Web UI available at http://10.47.71.138:4040 Spark context available as 'sc' (master = local[*], app id = local-1477858597347). Spark session available as 'spark'. Welcome to ____ __ / __/__ ___ _____/ /__ _\\ \\/ _ \\/ _ `/ __/ '_/ /___/ .__/\\_,_/_/ /_/\\_\\ version 2.1.0-SNAPSHOT /_/ Using Scala version 2.11.8 (Java HotSpot(TM) 64-Bit Server VM, Java 1.8.0_112) Type in expressions to have them evaluated. Type :help for more information. scala> Spark shell creates an instance of link:spark-sql-SparkSession.adoc[SparkSession] under the name spark for you (so you don't have to know the details how to do it yourself on day 1). scala> :type spark org.apache.spark.sql.SparkSession Besides, there is also sc value created which is an instance of xref:ROOT:SparkContext.adoc[]. scala> :type sc org.apache.spark.SparkContext To close Spark shell, you press Ctrl+D or type in :q (or any subset of :quit ). scala> :q === [[settings]] Settings .Spark Properties [cols=\"1,1,2\",options=\"header\",width=\"100%\"] |=== | Spark Property | Default Value | Description | [[spark_repl_class_uri]] spark.repl.class.uri | null | Used in spark-shell to create REPL ClassLoader to load new classes defined in the Scala REPL as a user types code. Enable INFO logging level for xref:executor:Executor.adoc[org.apache.spark.executor.Executor] logger to have the value printed out to the logs: INFO Using REPL class URI: [classUri] |===","title":"spark-shell"},{"location":"tools/spark-shell/#source-scala","text":"scala> :type spark org.apache.spark.sql.SparkSession // Learn the current version of Spark in use scala> spark.version res0: String = 2.1.0-SNAPSHOT spark-shell also imports link:spark-sql-SparkSession.adoc#implicits[Scala SQL's implicits] and link:spark-sql-SparkSession.adoc#sql[ sql method].","title":"[source, scala]"},{"location":"tools/spark-shell/#source-scala_1","text":"scala> :imports 1) import spark.implicits._ (59 terms, 38 are implicit) 2) import spark.sql (1 terms)","title":"[source, scala]"},{"location":"tools/spark-shell/#note","text":"When you execute spark-shell you actually execute link:spark-submit.adoc[Spark submit] as follows:","title":"[NOTE]"},{"location":"tools/spark-shell/#optionswrap","text":"","title":"[options=\"wrap\"]"},{"location":"tools/spark-shell/#orgapachesparkdeploysparksubmit-class-orgapachesparkreplmain-name-spark-shell-spark-shell","text":"","title":"org.apache.spark.deploy.SparkSubmit --class org.apache.spark.repl.Main --name Spark shell spark-shell"},{"location":"tools/spark-shell/#set-spark_print_launch_command-to-see-the-entire-command-to-be-executed-refer-to-linkspark-tips-and-tricksadocspark_print_launch_commandprint-launch-command-of-spark-scripts","text":"=== [[using-spark-shell]] Using Spark shell You start Spark shell using spark-shell script (available in bin directory). $ ./bin/spark-shell Setting default log level to \"WARN\". To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel). WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable WARN ObjectStore: Failed to get database global_temp, returning NoSuchObjectException Spark context Web UI available at http://10.47.71.138:4040 Spark context available as 'sc' (master = local[*], app id = local-1477858597347). Spark session available as 'spark'. Welcome to ____ __ / __/__ ___ _____/ /__ _\\ \\/ _ \\/ _ `/ __/ '_/ /___/ .__/\\_,_/_/ /_/\\_\\ version 2.1.0-SNAPSHOT /_/ Using Scala version 2.11.8 (Java HotSpot(TM) 64-Bit Server VM, Java 1.8.0_112) Type in expressions to have them evaluated. Type :help for more information. scala> Spark shell creates an instance of link:spark-sql-SparkSession.adoc[SparkSession] under the name spark for you (so you don't have to know the details how to do it yourself on day 1). scala> :type spark org.apache.spark.sql.SparkSession Besides, there is also sc value created which is an instance of xref:ROOT:SparkContext.adoc[]. scala> :type sc org.apache.spark.SparkContext To close Spark shell, you press Ctrl+D or type in :q (or any subset of :quit ). scala> :q === [[settings]] Settings .Spark Properties [cols=\"1,1,2\",options=\"header\",width=\"100%\"] |=== | Spark Property | Default Value | Description | [[spark_repl_class_uri]] spark.repl.class.uri | null | Used in spark-shell to create REPL ClassLoader to load new classes defined in the Scala REPL as a user types code. Enable INFO logging level for xref:executor:Executor.adoc[org.apache.spark.executor.Executor] logger to have the value printed out to the logs: INFO Using REPL class URI: [classUri] |===","title":"Set SPARK_PRINT_LAUNCH_COMMAND to see the entire command to be executed. Refer to link:spark-tips-and-tricks.adoc#SPARK_PRINT_LAUNCH_COMMAND[Print Launch Command of Spark Scripts]."}]}