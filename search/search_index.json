{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"The Internals of Apache Spark 3.0.1 \u00b6 Welcome to The Internals of Apache Spark online book! I'm Jacek Laskowski , a Seasoned IT Professional specializing in Apache Spark , Delta Lake , Apache Kafka and Kafka Streams . I'm very excited to have you here and hope you will enjoy exploring the internals of Apache Spark as much as I have. Flannery O'Connor I write to discover what I know. \"The Internals Of\" series I'm also writing other online books in the \"The Internals Of\" series. Please visit \"The Internals Of\" Online Books home page. Expect text and code snippets from a variety of public sources. Attribution follows. Now, let me introduce you to Apache Spark .","title":"Welcome"},{"location":"#the-internals-of-apache-spark-301","text":"Welcome to The Internals of Apache Spark online book! I'm Jacek Laskowski , a Seasoned IT Professional specializing in Apache Spark , Delta Lake , Apache Kafka and Kafka Streams . I'm very excited to have you here and hope you will enjoy exploring the internals of Apache Spark as much as I have. Flannery O'Connor I write to discover what I know. \"The Internals Of\" series I'm also writing other online books in the \"The Internals Of\" series. Please visit \"The Internals Of\" Online Books home page. Expect text and code snippets from a variety of public sources. Attribution follows. Now, let me introduce you to Apache Spark .","title":"The Internals of Apache Spark 3.0.1"},{"location":"SparkConf/","text":"= SparkConf Every user program starts with creating an instance of SparkConf that holds the xref:ROOT:spark-deployment-environments.adoc#master-urls[master URL] to connect to ( spark.master ), the name for your Spark application (that is later displayed in xref:webui:index.adoc[web UI] and becomes spark.app.name ) and other Spark properties required for proper runs. The instance of SparkConf can be used to create xref:ROOT:SparkContext.adoc[SparkContext]. [TIP] \u00b6 Start xref spark-shell.adoc[Spark shell] with --conf spark.logConf=true to log the effective Spark configuration as INFO when SparkContext is started. $ ./bin/spark-shell --conf spark.logConf=true ... 15/10/19 17:13:49 INFO SparkContext: Running Spark version 1.6.0-SNAPSHOT 15/10/19 17:13:49 INFO SparkContext: Spark configuration: spark.app.name=Spark shell spark.home=/Users/jacek/dev/oss/spark spark.jars= spark.logConf=true spark.master=local[*] spark.repl.class.uri=http://10.5.10.20:64055 spark.submit.deployMode=client ... Use sc.getConf.toDebugString to have a richer output once SparkContext has finished initializing. \u00b6 You can query for the values of Spark properties in xref spark-shell.adoc[Spark shell] as follows: scala> sc.getConf.getOption(\"spark.local.dir\") res0: Option[String] = None scala> sc.getConf.getOption(\"spark.app.name\") res1: Option[String] = Some(Spark shell) scala> sc.getConf.get(\"spark.master\") res2: String = local[*] == [[setIfMissing]] setIfMissing Method CAUTION: FIXME == [[isExecutorStartupConf]] isExecutorStartupConf Method CAUTION: FIXME == [[set]] set Method CAUTION: FIXME == Setting up Spark Properties There are the following places where a Spark application looks for Spark properties (in the order of importance from the least important to the most important): conf/spark-defaults.conf - the configuration file with the default Spark properties. Read xref:ROOT:spark-properties.adoc#spark-defaults-conf[spark-defaults.conf]. --conf or -c - the command-line option used by xref spark-submit.adoc[spark-submit] (and other shell scripts that use spark-submit or spark-class under the covers, e.g. spark-shell ) SparkConf == [[default-configuration]] Default Configuration The default Spark configuration is created when you execute the following code: [source, scala] \u00b6 import org.apache.spark.SparkConf val conf = new SparkConf It simply loads spark.* system properties. You can use conf.toDebugString or conf.getAll to have the spark.* system properties loaded printed out. [source, scala] \u00b6 scala> conf.getAll res0: Array[(String, String)] = Array((spark.app.name,Spark shell), (spark.jars,\"\"), (spark.master,local[*]), (spark.submit.deployMode,client)) scala> conf.toDebugString res1: String = spark.app.name=Spark shell spark.jars= spark.master=local[*] spark.submit.deployMode=client scala> println(conf.toDebugString) spark.app.name=Spark shell spark.jars= spark.master=local[*] spark.submit.deployMode=client == [[getAppId]] Unique Identifier of Spark Application -- getAppId Method [source, scala] \u00b6 getAppId: String \u00b6 getAppId returns the value of xref:ROOT:configuration-properties.adoc#spark.app.id[spark.app.id] configuration property or throws a NoSuchElementException if not set. getAppId is used when: NettyBlockTransferService is requested to xref:storage:NettyBlockTransferService.adoc#init[init] (and creates a xref:storage:NettyBlockRpcServer.adoc#creating-instance[NettyBlockRpcServer] as well as xref:storage:NettyBlockTransferService.adoc#appId[saves the identifier for later use]). Executor xref:executor:Executor.adoc#creating-instance[is created] (in non-local mode and xref:storage:BlockManager.adoc#initialize[requests BlockManager to initialize]). == [[getAvroSchema]] getAvroSchema Method [source, scala] \u00b6 getAvroSchema: Map[Long, String] \u00b6 getAvroSchema takes all avro.schema -prefixed configuration properties from < > and...FIXME getAvroSchema is used when KryoSerializer is created (and initializes avroSchemas).","title":"SparkConf"},{"location":"SparkConf/#tip","text":"Start xref spark-shell.adoc[Spark shell] with --conf spark.logConf=true to log the effective Spark configuration as INFO when SparkContext is started. $ ./bin/spark-shell --conf spark.logConf=true ... 15/10/19 17:13:49 INFO SparkContext: Running Spark version 1.6.0-SNAPSHOT 15/10/19 17:13:49 INFO SparkContext: Spark configuration: spark.app.name=Spark shell spark.home=/Users/jacek/dev/oss/spark spark.jars= spark.logConf=true spark.master=local[*] spark.repl.class.uri=http://10.5.10.20:64055 spark.submit.deployMode=client ...","title":"[TIP]"},{"location":"SparkConf/#use-scgetconftodebugstring-to-have-a-richer-output-once-sparkcontext-has-finished-initializing","text":"You can query for the values of Spark properties in xref spark-shell.adoc[Spark shell] as follows: scala> sc.getConf.getOption(\"spark.local.dir\") res0: Option[String] = None scala> sc.getConf.getOption(\"spark.app.name\") res1: Option[String] = Some(Spark shell) scala> sc.getConf.get(\"spark.master\") res2: String = local[*] == [[setIfMissing]] setIfMissing Method CAUTION: FIXME == [[isExecutorStartupConf]] isExecutorStartupConf Method CAUTION: FIXME == [[set]] set Method CAUTION: FIXME == Setting up Spark Properties There are the following places where a Spark application looks for Spark properties (in the order of importance from the least important to the most important): conf/spark-defaults.conf - the configuration file with the default Spark properties. Read xref:ROOT:spark-properties.adoc#spark-defaults-conf[spark-defaults.conf]. --conf or -c - the command-line option used by xref spark-submit.adoc[spark-submit] (and other shell scripts that use spark-submit or spark-class under the covers, e.g. spark-shell ) SparkConf == [[default-configuration]] Default Configuration The default Spark configuration is created when you execute the following code:","title":"Use sc.getConf.toDebugString to have a richer output once SparkContext has finished initializing."},{"location":"SparkConf/#source-scala","text":"import org.apache.spark.SparkConf val conf = new SparkConf It simply loads spark.* system properties. You can use conf.toDebugString or conf.getAll to have the spark.* system properties loaded printed out.","title":"[source, scala]"},{"location":"SparkConf/#source-scala_1","text":"scala> conf.getAll res0: Array[(String, String)] = Array((spark.app.name,Spark shell), (spark.jars,\"\"), (spark.master,local[*]), (spark.submit.deployMode,client)) scala> conf.toDebugString res1: String = spark.app.name=Spark shell spark.jars= spark.master=local[*] spark.submit.deployMode=client scala> println(conf.toDebugString) spark.app.name=Spark shell spark.jars= spark.master=local[*] spark.submit.deployMode=client == [[getAppId]] Unique Identifier of Spark Application -- getAppId Method","title":"[source, scala]"},{"location":"SparkConf/#source-scala_2","text":"","title":"[source, scala]"},{"location":"SparkConf/#getappid-string","text":"getAppId returns the value of xref:ROOT:configuration-properties.adoc#spark.app.id[spark.app.id] configuration property or throws a NoSuchElementException if not set. getAppId is used when: NettyBlockTransferService is requested to xref:storage:NettyBlockTransferService.adoc#init[init] (and creates a xref:storage:NettyBlockRpcServer.adoc#creating-instance[NettyBlockRpcServer] as well as xref:storage:NettyBlockTransferService.adoc#appId[saves the identifier for later use]). Executor xref:executor:Executor.adoc#creating-instance[is created] (in non-local mode and xref:storage:BlockManager.adoc#initialize[requests BlockManager to initialize]). == [[getAvroSchema]] getAvroSchema Method","title":"getAppId: String"},{"location":"SparkConf/#source-scala_3","text":"","title":"[source, scala]"},{"location":"SparkConf/#getavroschema-maplong-string","text":"getAvroSchema takes all avro.schema -prefixed configuration properties from < > and...FIXME getAvroSchema is used when KryoSerializer is created (and initializes avroSchemas).","title":"getAvroSchema: Map[Long, String]"},{"location":"SparkContext/","text":"= SparkContext SparkContext is the entry point to all components of Apache Spark (execution engine) and so the heart of a Spark application. In fact, you can consider an application a Spark application only when it uses a SparkContext (directly or indirectly). [[methods]] .SparkContext's Developer API (Public Methods) [cols=\"1,3\",options=\"header\",width=\"100%\"] |=== | Method | Description | < > a| [[addJar]] [source, scala] \u00b6 addJar(path: String): Unit \u00b6 | a| More to be added soon |=== Spark context link:spark-SparkContext-creating-instance-internals.adoc[sets up internal services] and establishes a connection to a link:spark-deployment-environments.adoc[Spark execution environment]. Once a < > you can use it to < >, < > and < >, access Spark services and < > (until SparkContext is < >). A Spark context is essentially a client of Spark's execution environment and acts as the master of your Spark application (don't get confused with the other meaning of link:spark-master.adoc[Master] in Spark, though). .Spark context acts as the master of your Spark application image::diagrams/sparkcontext-services.png[align=\"center\"] SparkContext offers the following functions: Getting current status of a Spark application ** < > ** < > ** < > ** < > ** < > ** < > ** < > that specifies the number of link:spark-rdd-partitions.adoc[partitions] in RDDs when they are created without specifying the number explicitly by a user. ** < > ** < > ** < > ** < > ** < > Setting Configuration ** < > ** link:spark-sparkcontext-local-properties.adoc[Local Properties -- Creating Logical Job Groups] ** < > ** < > Creating Distributed Entities ** < > ** < > ** < > Accessing services, e.g. < >, < >, xref:scheduler:LiveListenerBus.adoc[], xref:storage:BlockManager.adoc[BlockManager], xref:scheduler:SchedulerBackend.adoc[SchedulerBackends], xref:shuffle:ShuffleManager.adoc[ShuffleManager] and the < >. < > < > < > < > < > < > < > < > < > < > TIP: Read the scaladoc of http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.SparkContext[org.apache.spark.SparkContext ]. == [[addFile]] addFile Method [source, scala] \u00b6 addFile( path: String): Unit // <1> addFile( path: String, recursive: Boolean): Unit <1> recursive flag is off addFile adds the path file to be downloaded...FIXME [NOTE] \u00b6 addFile is used when: SparkContext is link:spark-SparkContext-creating-instance-internals.adoc#files[initialized] (and files were defined) Spark SQL's AddFileCommand is executed * Spark SQL's SessionResourceLoader is requested to load a file resource \u00b6 == [[unpersistRDD]] Removing RDD Blocks from BlockManagerMaster -- unpersistRDD Internal Method [source, scala] \u00b6 unpersistRDD(rddId: Int, blocking: Boolean = true): Unit \u00b6 unpersistRDD requests BlockManagerMaster to xref:storage:BlockManagerMaster.adoc#removeRdd[remove the blocks for the RDD] (given rddId ). NOTE: unpersistRDD uses SparkEnv xref:core:SparkEnv.adoc#blockManager[to access the current BlockManager ] that is in turn used to xref:storage:BlockManager.adoc#master[access the current BlockManagerMaster ]. unpersistRDD removes rddId from < > registry. In the end, unpersistRDD posts a xref:ROOT:SparkListener.adoc#SparkListenerUnpersistRDD[SparkListenerUnpersistRDD] (with rddId ) to < >. [NOTE] \u00b6 unpersistRDD is used when: ContextCleaner does xref:core:ContextCleaner.adoc#doCleanupRDD[doCleanupRDD] SparkContext < > (i.e. marks an RDD as non-persistent) \u00b6 == [[applicationId]] Unique Identifier of Spark Application -- applicationId Method CAUTION: FIXME == [[postApplicationStart]] postApplicationStart Internal Method [source, scala] \u00b6 postApplicationStart(): Unit \u00b6 postApplicationStart ...FIXME NOTE: postApplicationStart is used exclusively while SparkContext is being < > == [[postApplicationEnd]] postApplicationEnd Method CAUTION: FIXME == [[clearActiveContext]] clearActiveContext Method CAUTION: FIXME == [[getPersistentRDDs]] Accessing persistent RDDs -- getPersistentRDDs Method [source, scala] \u00b6 getPersistentRDDs: Map[Int, RDD[_]] \u00b6 getPersistentRDDs returns the collection of RDDs that have marked themselves as persistent via link:spark-rdd-caching.adoc#cache[cache]. Internally, getPersistentRDDs returns < > internal registry. == [[cancelJob]] Cancelling Job -- cancelJob Method [source, scala] \u00b6 cancelJob(jobId: Int) \u00b6 cancelJob requests DAGScheduler xref:scheduler:DAGScheduler.adoc#cancelJob[to cancel a Spark job]. == [[cancelStage]] Cancelling Stage -- cancelStage Methods [source, scala] \u00b6 cancelStage(stageId: Int): Unit cancelStage(stageId: Int, reason: String): Unit cancelStage simply requests DAGScheduler xref:scheduler:DAGScheduler.adoc#cancelJob[to cancel a Spark stage] (with an optional reason ). NOTE: cancelStage is used when StagesTab link:spark-webui-StagesTab.adoc#handleKillRequest[handles a kill request] (from a user in web UI). == [[dynamic-allocation]] Programmable Dynamic Allocation SparkContext offers the following methods as the developer API for xref:ROOT:spark-dynamic-allocation.adoc[]: < > < > < > (private!) < > === [[requestExecutors]] Requesting New Executors -- requestExecutors Method [source, scala] \u00b6 requestExecutors(numAdditionalExecutors: Int): Boolean \u00b6 requestExecutors requests numAdditionalExecutors executors from xref:scheduler:CoarseGrainedSchedulerBackend.adoc[CoarseGrainedSchedulerBackend]. === [[killExecutors]] Requesting to Kill Executors -- killExecutors Method [source, scala] \u00b6 killExecutors(executorIds: Seq[String]): Boolean \u00b6 CAUTION: FIXME === [[requestTotalExecutors]] Requesting Total Executors -- requestTotalExecutors Method [source, scala] \u00b6 requestTotalExecutors( numExecutors: Int, localityAwareTasks: Int, hostToLocalTaskCount: Map[String, Int]): Boolean requestTotalExecutors is a private[spark] method that xref:scheduler:CoarseGrainedSchedulerBackend.adoc#requestTotalExecutors[requests the exact number of executors from a coarse-grained scheduler backend]. NOTE: It works for xref:scheduler:CoarseGrainedSchedulerBackend.adoc[coarse-grained scheduler backends] only. When called for other scheduler backends you should see the following WARN message in the logs: WARN Requesting executors is only supported in coarse-grained mode === [[getExecutorIds]] Getting Executor Ids -- getExecutorIds Method getExecutorIds is a private[spark] method that is part of link:spark-service-ExecutorAllocationClient.adoc[ExecutorAllocationClient contract]. It simply xref:scheduler:CoarseGrainedSchedulerBackend.adoc#getExecutorIds[passes the call on to the current coarse-grained scheduler backend, i.e. calls getExecutorIds ]. NOTE: It works for xref:scheduler:CoarseGrainedSchedulerBackend.adoc[coarse-grained scheduler backends] only. When called for other scheduler backends you should see the following WARN message in the logs: WARN Requesting executors is only supported in coarse-grained mode CAUTION: FIXME Why does SparkContext implement the method for coarse-grained scheduler backends? Why doesn't SparkContext throw an exception when the method is called? Nobody seems to be using it (!) == [[creating-instance]] Creating SparkContext Instance You can create a SparkContext instance with or without creating a xref:ROOT:SparkConf.adoc[SparkConf] object first. NOTE: You may want to read link:spark-SparkContext-creating-instance-internals.adoc[Inside Creating SparkContext] to learn what happens behind the scenes when SparkContext is created. === [[getOrCreate]] Getting Existing or Creating New SparkContext -- getOrCreate Methods [source, scala] \u00b6 getOrCreate(): SparkContext getOrCreate(conf: SparkConf): SparkContext getOrCreate methods allow you to get the existing SparkContext or create a new one. [source, scala] \u00b6 import org.apache.spark.SparkContext val sc = SparkContext.getOrCreate() // Using an explicit SparkConf object import org.apache.spark.SparkConf val conf = new SparkConf() .setMaster(\"local[*]\") .setAppName(\"SparkMe App\") val sc = SparkContext.getOrCreate(conf) The no-param getOrCreate method requires that the two mandatory Spark settings - < > and < > - are specified using link:spark-submit.adoc[spark-submit]. === [[constructors]] Constructors [source, scala] \u00b6 SparkContext() SparkContext(conf: SparkConf) SparkContext(master: String, appName: String, conf: SparkConf) SparkContext( master: String, appName: String, sparkHome: String = null, jars: Seq[String] = Nil, environment: Map[String, String] = Map()) You can create a SparkContext instance using the four constructors. [source, scala] \u00b6 import org.apache.spark.SparkConf val conf = new SparkConf() .setMaster(\"local[*]\") .setAppName(\"SparkMe App\") import org.apache.spark.SparkContext val sc = new SparkContext(conf) When a Spark context starts up you should see the following INFO in the logs (amongst the other messages that come from the Spark services): INFO SparkContext: Running Spark version 2.0.0-SNAPSHOT NOTE: Only one SparkContext may be running in a single JVM (check out https://issues.apache.org/jira/browse/SPARK-2243[SPARK-2243 Support multiple SparkContexts in the same JVM]). Sharing access to a SparkContext in the JVM is the solution to share data within Spark (without relying on other means of data sharing using external data stores). == [[env]] Accessing Current SparkEnv -- env Method CAUTION: FIXME == [[getConf]] Getting Current SparkConf -- getConf Method [source, scala] \u00b6 getConf: SparkConf \u00b6 getConf returns the current xref:ROOT:SparkConf.adoc[SparkConf]. NOTE: Changing the SparkConf object does not change the current configuration (as the method returns a copy). == [[master]][[master-url]] Deployment Environment -- master Method [source, scala] \u00b6 master: String \u00b6 master method returns the current value of xref:ROOT:configuration-properties.adoc#spark.master[spark.master] which is the link:spark-deployment-environments.adoc[deployment environment] in use. == [[appName]] Application Name -- appName Method [source, scala] \u00b6 appName: String \u00b6 appName gives the value of the mandatory xref:ROOT:SparkConf.adoc#spark.app.name[spark.app.name] setting. NOTE: appName is used when link:spark-standalone.adoc#SparkDeploySchedulerBackend[ SparkDeploySchedulerBackend starts], link:spark-webui-SparkUI.adoc#createLiveUI[ SparkUI creates a web UI], when postApplicationStart is executed, and for Mesos and checkpointing in Spark Streaming. == [[applicationAttemptId]] Unique Identifier of Execution Attempt -- applicationAttemptId Method [source, scala] \u00b6 applicationAttemptId: Option[String] \u00b6 applicationAttemptId gives the unique identifier of the execution attempt of a Spark application. [NOTE] \u00b6 applicationAttemptId is used when: xref:scheduler:ShuffleMapTask.adoc#creating-instance[ShuffleMapTask] and xref:scheduler:ResultTask.adoc#creating-instance[ResultTask] are created * SparkContext < > \u00b6 == [[getExecutorStorageStatus]] Storage Status (of All BlockManagers) -- getExecutorStorageStatus Method [source, scala] \u00b6 getExecutorStorageStatus: Array[StorageStatus] \u00b6 getExecutorStorageStatus xref:storage:BlockManagerMaster.adoc#getStorageStatus[requests BlockManagerMaster for storage status] (of all xref:storage:BlockManager.adoc[BlockManagers]). NOTE: getExecutorStorageStatus is a developer API. [NOTE] \u00b6 getExecutorStorageStatus is used when: SparkContext < > * SparkStatusTracker link:spark-sparkcontext-SparkStatusTracker.adoc#getExecutorInfos[is requested for information about all known executors] \u00b6 == [[deployMode]] Deploy Mode -- deployMode Method [source,scala] \u00b6 deployMode: String \u00b6 deployMode returns the current value of link:spark-deploy-mode.adoc[spark.submit.deployMode] setting or client if not set. == [[getSchedulingMode]] Scheduling Mode -- getSchedulingMode Method [source, scala] \u00b6 getSchedulingMode: SchedulingMode.SchedulingMode \u00b6 getSchedulingMode returns the current link:spark-scheduler-SchedulingMode.adoc[Scheduling Mode]. == [[getPoolForName]] Schedulable (Pool) by Name -- getPoolForName Method [source, scala] \u00b6 getPoolForName(pool: String): Option[Schedulable] \u00b6 getPoolForName returns a link:spark-scheduler-Schedulable.adoc[Schedulable] by the pool name, if one exists. NOTE: getPoolForName is part of the Developer's API and may change in the future. Internally, it requests the xref:scheduler:TaskScheduler.adoc#rootPool[TaskScheduler for the root pool] and link:spark-scheduler-Pool.adoc#schedulableNameToSchedulable[looks up the Schedulable by the pool name]. It is exclusively used to link:spark-webui-PoolPage.adoc[show pool details in web UI (for a stage)]. == [[getAllPools]] All Schedulable Pools -- getAllPools Method [source, scala] \u00b6 getAllPools: Seq[Schedulable] \u00b6 getAllPools collects the link:spark-scheduler-Pool.adoc[Pools] in xref:scheduler:TaskScheduler.adoc#contract[TaskScheduler.rootPool]. NOTE: TaskScheduler.rootPool is part of the xref:scheduler:TaskScheduler.adoc#contract[TaskScheduler Contract]. NOTE: getAllPools is part of the Developer's API. CAUTION: FIXME Where is the method used? NOTE: getAllPools is used to calculate pool names for link:spark-webui-AllStagesPage.adoc#pool-names[Stages tab in web UI] with FAIR scheduling mode used. == [[defaultParallelism]] Default Level of Parallelism [source, scala] \u00b6 defaultParallelism: Int \u00b6 defaultParallelism requests < > for the xref:scheduler:TaskScheduler.adoc#defaultParallelism[default level of parallelism]. NOTE: Default level of parallelism specifies the number of link:spark-rdd-partitions.adoc[partitions] in RDDs when created without specifying them explicitly by a user. [NOTE] \u00b6 defaultParallelism is used in < >, SparkContext.range and < > (as well as Spark Streaming's DStream.countByValue and DStream.countByValueAndWindow et al.). defaultParallelism is also used to instantiate xref:rdd:HashPartitioner.adoc[HashPartitioner] and for the minimum number of partitions in xref:rdd:spark-rdd-HadoopRDD.adoc[HadoopRDDs]. \u00b6 == [[taskScheduler]] Current Spark Scheduler (aka TaskScheduler) -- taskScheduler Property [source, scala] \u00b6 taskScheduler: TaskScheduler taskScheduler_=(ts: TaskScheduler): Unit taskScheduler manages (i.e. reads or writes) <<_taskScheduler, _taskScheduler>> internal property. == [[version]] Getting Spark Version -- version Property [source, scala] \u00b6 version: String \u00b6 version returns the Spark version this SparkContext uses. == [[makeRDD]] makeRDD Method CAUTION: FIXME == [[submitJob]] Submitting Jobs Asynchronously -- submitJob Method [source, scala] \u00b6 submitJob T, U, R : SimpleFutureAction[R] submitJob submits a job in an asynchronous, non-blocking way to xref:scheduler:DAGScheduler.adoc#submitJob[DAGScheduler]. It cleans the processPartition input function argument and returns an instance of link:spark-rdd-actions.adoc#FutureAction[SimpleFutureAction] that holds the xref:scheduler:spark-scheduler-JobWaiter.adoc[JobWaiter] instance. CAUTION: FIXME What are resultFunc ? It is used in: link:spark-rdd-actions.adoc#AsyncRDDActions[AsyncRDDActions] methods link:spark-streaming/spark-streaming.adoc[Spark Streaming] for link:spark-streaming/spark-streaming-receivertracker.adoc#ReceiverTrackerEndpoint-startReceiver[ReceiverTrackerEndpoint.startReceiver] == [[spark-configuration]] Spark Configuration CAUTION: FIXME == [[sparkcontext-and-rdd]] SparkContext and RDDs You use a Spark context to create RDDs (see < >). When an RDD is created, it belongs to and is completely owned by the Spark context it originated from. RDDs can't by design be shared between SparkContexts. .A Spark context creates a living space for RDDs. image::diagrams/sparkcontext-rdds.png[align=\"center\"] == [[creating-rdds]][[parallelize]] Creating RDD -- parallelize Method SparkContext allows you to create many different RDDs from input sources like: Scala's collections, i.e. sc.parallelize(0 to 100) local or remote filesystems, i.e. sc.textFile(\"README.md\") Any Hadoop InputSource using sc.newAPIHadoopFile Read xref:rdd:index.adoc#creating-rdds[Creating RDDs] in xref:rdd:index.adoc[RDD - Resilient Distributed Dataset]. == [[unpersist]] Unpersisting RDD (Marking RDD as Non-Persistent) -- unpersist Method CAUTION: FIXME unpersist removes an RDD from the master's xref:storage:BlockManager.adoc[Block Manager] (calls removeRdd(rddId: Int, blocking: Boolean) ) and the internal < > mapping. It finally posts xref:ROOT:SparkListener.adoc#SparkListenerUnpersistRDD[SparkListenerUnpersistRDD] message to listenerBus . == [[setCheckpointDir]] Setting Checkpoint Directory -- setCheckpointDir Method [source, scala] \u00b6 setCheckpointDir(directory: String) \u00b6 setCheckpointDir method is used to set up the checkpoint directory...FIXME CAUTION: FIXME == [[register]] Registering Accumulator -- register Methods [source, scala] \u00b6 register(acc: AccumulatorV2[ , _]): Unit register(acc: AccumulatorV2[ , _], name: String): Unit register registers the acc link:spark-accumulators.adoc[accumulator]. You can optionally give an accumulator a name . TIP: You can create built-in accumulators for longs, doubles, and collection types using < >. Internally, register link:spark-accumulators.adoc#register[registers acc accumulator] (with the current SparkContext). == [[creating-accumulators]][[longAccumulator]][[doubleAccumulator]][[collectionAccumulator]] Creating Built-In Accumulators [source, scala] \u00b6 longAccumulator: LongAccumulator longAccumulator(name: String): LongAccumulator doubleAccumulator: DoubleAccumulator doubleAccumulator(name: String): DoubleAccumulator collectionAccumulator[T]: CollectionAccumulator[T] collectionAccumulator T : CollectionAccumulator[T] You can use longAccumulator , doubleAccumulator or collectionAccumulator to create and register link:spark-accumulators.adoc[accumulators] for simple and collection values. longAccumulator returns link:spark-accumulators.adoc#LongAccumulator[LongAccumulator] with the zero value 0 . doubleAccumulator returns link:spark-accumulators.adoc#DoubleAccumulator[DoubleAccumulator] with the zero value 0.0 . collectionAccumulator returns link:spark-accumulators.adoc#CollectionAccumulator[CollectionAccumulator] with the zero value java.util.List[T] . [source, scala] \u00b6 scala> val acc = sc.longAccumulator acc: org.apache.spark.util.LongAccumulator = LongAccumulator(id: 0, name: None, value: 0) scala> val counter = sc.longAccumulator(\"counter\") counter: org.apache.spark.util.LongAccumulator = LongAccumulator(id: 1, name: Some(counter), value: 0) scala> counter.value res0: Long = 0 scala> sc.parallelize(0 to 9).foreach(n => counter.add(n)) scala> counter.value res3: Long = 45 The name input parameter allows you to give a name to an accumulator and have it displayed in link:spark-webui-StagePage.adoc#accumulators[Spark UI] (under Stages tab for a given stage). .Accumulators in the Spark UI image::spark-webui-accumulators.png[align=\"center\"] TIP: You can register custom accumulators using < > methods. == [[broadcast]] Creating Broadcast Variable -- broadcast Method [source, scala] \u00b6 broadcast T : Broadcast[T] broadcast method creates a xref:ROOT:Broadcast.adoc[]. It is a shared memory with value (as broadcast blocks) on the driver and later on all Spark executors. [source,plaintext] \u00b6 val sc: SparkContext = ??? scala> val hello = sc.broadcast(\"hello\") hello: org.apache.spark.broadcast.Broadcast[String] = Broadcast(0) Spark transfers the value to Spark executors once , and tasks can share it without incurring repetitive network transmissions when the broadcast variable is used multiple times. .Broadcasting a value to executors image::sparkcontext-broadcast-executors.png[align=\"center\"] Internally, broadcast requests BroadcastManager for a xref:core:BroadcastManager.adoc#newBroadcast[new broadcast variable]. NOTE: The current BroadcastManager is available using xref:core:SparkEnv.adoc#broadcastManager[ SparkEnv.broadcastManager ] attribute and is always xref:core:BroadcastManager.adoc[BroadcastManager] (with few internal configuration changes to reflect where it runs, i.e. inside the driver or executors). You should see the following INFO message in the logs: Created broadcast [id] from [callSite] If ContextCleaner is defined, the xref:core:ContextCleaner.adoc#[new broadcast variable is registered for cleanup]. [NOTE] \u00b6 Spark does not support broadcasting RDDs. scala> sc.broadcast(sc.range(0, 10)) java.lang.IllegalArgumentException: requirement failed: Can not directly broadcast RDDs; instead, call collect() and broadcast the result. at scala.Predef$.require(Predef.scala:224) at org.apache.spark.SparkContext.broadcast(SparkContext.scala:1392) ... 48 elided \u00b6 Once created, the broadcast variable (and other blocks) are displayed per executor and the driver in web UI (under link:spark-webui-executors.adoc[Executors tab]). .Broadcast Variables In web UI's Executors Tab image::spark-broadcast-webui-executors-rdd-blocks.png[align=\"center\"] == [[jars]] Distribute JARs to workers The jar you specify with SparkContext.addJar will be copied to all the worker nodes. The configuration setting spark.jars is a comma-separated list of jar paths to be included in all tasks executed from this SparkContext. A path can either be a local file, a file in HDFS (or other Hadoop-supported filesystems), an HTTP, HTTPS or FTP URI, or local:/path for a file on every worker node. scala> sc.addJar(\"build.sbt\") 15/11/11 21:54:54 INFO SparkContext: Added JAR build.sbt at http://192.168.1.4:49427/jars/build.sbt with timestamp 1447275294457 CAUTION: FIXME Why is HttpFileServer used for addJar? === SparkContext as Application-Wide Counter SparkContext keeps track of: [[nextShuffleId]] * shuffle ids using nextShuffleId internal counter for xref:scheduler:ShuffleMapStage.adoc[registering shuffle dependencies] to xref:shuffle:ShuffleManager.adoc[Shuffle Service]. == [[runJob]] Running Job Synchronously xref:rdd:index.adoc#actions[RDD actions] run link:spark-scheduler-ActiveJob.adoc[jobs] using one of runJob methods. [source, scala] \u00b6 runJob T, U : Unit runJob T, U : Array[U] runJob T, U : Array[U] runJob T, U : Array[U] runJob T, U : Array[U] runJob T, U runJob T, U: ClassTag runJob executes a function on one or many partitions of a RDD (in a SparkContext space) to produce a collection of values per partition. NOTE: runJob can only work when a SparkContext is not < >. Internally, runJob first makes sure that the SparkContext is not < >. If it is, you should see the following IllegalStateException exception in the logs: java.lang.IllegalStateException: SparkContext has been shutdown at org.apache.spark.SparkContext.runJob(SparkContext.scala:1893) at org.apache.spark.SparkContext.runJob(SparkContext.scala:1914) at org.apache.spark.SparkContext.runJob(SparkContext.scala:1934) ... 48 elided runJob then < > and < func closure>>. You should see the following INFO message in the logs: INFO SparkContext: Starting job: [callSite] With link:spark-rdd-lineage.adoc#spark_logLineage[spark.logLineage] enabled (which is not by default), you should see the following INFO message with link:spark-rdd-lineage.adoc#toDebugString[toDebugString] (executed on rdd ): INFO SparkContext: RDD's recursive dependencies: [toDebugString] runJob requests xref:scheduler:DAGScheduler.adoc#runJob[ DAGScheduler to run a job]. TIP: runJob just prepares input parameters for xref:scheduler:DAGScheduler.adoc#runJob[ DAGScheduler to run a job]. After DAGScheduler is done and the job has finished, runJob link:spark-sparkcontext-ConsoleProgressBar.adoc#finishAll[stops ConsoleProgressBar ] and xref:ROOT:rdd-checkpointing.adoc#doCheckpoint[performs RDD checkpointing of rdd ]. TIP: For some actions, e.g. first() and lookup() , there is no need to compute all the partitions of the RDD in a job. And Spark knows it. [source,scala] \u00b6 // RDD to work with val lines = sc.parallelize(Seq(\"hello world\", \"nice to see you\")) import org.apache.spark.TaskContext scala> sc.runJob(lines, (t: TaskContext, i: Iterator[String]) => 1) // <1> res0: Array[Int] = Array(1, 1) // <2> <1> Run a job using runJob on lines RDD with a function that returns 1 for every partition (of lines RDD). <2> What can you say about the number of partitions of the lines RDD? Is your result res0 different than mine? Why? TIP: Read link:spark-TaskContext.adoc[TaskContext]. Running a job is essentially executing a func function on all or a subset of partitions in an rdd RDD and returning the result as an array (with elements being the results per partition). .Executing action image::spark-runjob.png[align=\"center\"] == [[stop]][[stopping]] Stopping SparkContext -- stop Method [source, scala] \u00b6 stop(): Unit \u00b6 stop stops the SparkContext. Internally, stop enables stopped internal flag. If already stopped, you should see the following INFO message in the logs: INFO SparkContext: SparkContext already stopped. stop then does the following: Removes _shutdownHookRef from ShutdownHookManager < SparkListenerApplicationEnd >> (to < >) link:spark-webui-SparkUI.adoc#stop[Stops web UI] link:spark-metrics-MetricsSystem.adoc#report[Requests MetricSystem to report metrics] (from all registered sinks) xref:core:ContextCleaner.adoc#stop[Stops ContextCleaner ] link:spark-ExecutorAllocationManager.adoc#stop[Requests ExecutorAllocationManager to stop] If LiveListenerBus was started, xref:scheduler:LiveListenerBus.adoc#stop[requests LiveListenerBus to stop] Requests xref:spark-history-server:EventLoggingListener.adoc#stop[ EventLoggingListener to stop] Requests xref:scheduler:DAGScheduler.adoc#stop[ DAGScheduler to stop] Requests xref:rpc:index.adoc#stop[RpcEnv to stop HeartbeatReceiver endpoint] Requests link:spark-sparkcontext-ConsoleProgressBar.adoc#stop[ ConsoleProgressBar to stop] Clears the reference to TaskScheduler , i.e. _taskScheduler is null Requests xref:core:SparkEnv.adoc#stop[ SparkEnv to stop] and clears SparkEnv Clears link:yarn/spark-yarn-client.adoc#SPARK_YARN_MODE[ SPARK_YARN_MODE flag] < > Ultimately, you should see the following INFO message in the logs: INFO SparkContext: Successfully stopped SparkContext == [[addSparkListener]] Registering SparkListener -- addSparkListener Method [source, scala] \u00b6 addSparkListener(listener: SparkListenerInterface): Unit \u00b6 You can register a custom xref:ROOT:SparkListener.adoc#SparkListenerInterface[SparkListenerInterface] using addSparkListener method NOTE: You can also register custom listeners using xref:ROOT:configuration-properties.adoc#spark.extraListeners[spark.extraListeners] configuration property. == [[custom-schedulers]] Custom SchedulerBackend, TaskScheduler and DAGScheduler By default, SparkContext uses ( private[spark] class) org.apache.spark.scheduler.DAGScheduler , but you can develop your own custom DAGScheduler implementation, and use ( private[spark] ) SparkContext.dagScheduler_=(ds: DAGScheduler) method to assign yours. It is also applicable to SchedulerBackend and TaskScheduler using schedulerBackend_=(sb: SchedulerBackend) and taskScheduler_=(ts: TaskScheduler) methods, respectively. CAUTION: FIXME Make it an advanced exercise. == [[events]] Events When a Spark context starts, it triggers xref:ROOT:SparkListener.adoc#SparkListenerEnvironmentUpdate[SparkListenerEnvironmentUpdate] and xref:ROOT:SparkListener.adoc#SparkListenerApplicationStart[SparkListenerApplicationStart] messages. Refer to the section < >. == [[setLogLevel]][[setting-default-log-level]] Setting Default Logging Level -- setLogLevel Method [source, scala] \u00b6 setLogLevel(logLevel: String) \u00b6 setLogLevel allows you to set the root logging level in a Spark application, e.g. link:spark-shell.adoc[Spark shell]. Internally, setLogLevel calls link:++ http://logging.apache.org/log4j/2.x/log4j-api/apidocs/org/apache/logging/log4j/Level.html#toLevel(java.lang.String)++[org.apache.log4j.Level.toLevel(logLevel )] that it then uses to set using link:++ http://logging.apache.org/log4j/2.x/log4j-api/apidocs/org/apache/logging/log4j/LogManager.html#getRootLogger()++[org.apache.log4j.LogManager.getRootLogger().setLevel(level )]. [TIP] \u00b6 You can directly set the logging level using link:++ http://logging.apache.org/log4j/2.x/log4j-api/apidocs/org/apache/logging/log4j/LogManager.html#getLogger()++[org.apache.log4j.LogManager.getLogger ()]. [source, scala] \u00b6 LogManager.getLogger(\"org\").setLevel(Level.OFF) \u00b6 ==== == [[clean]][[closure-cleaning]] Closure Cleaning -- clean Method [source, scala] \u00b6 clean(f: F, checkSerializable: Boolean = true): F \u00b6 Every time an action is called, Spark cleans up the closure, i.e. the body of the action, before it is serialized and sent over the wire to executors. SparkContext comes with clean(f: F, checkSerializable: Boolean = true) method that does this. It in turn calls ClosureCleaner.clean method. Not only does ClosureCleaner.clean method clean the closure, but also does it transitively, i.e. referenced closures are cleaned transitively. A closure is considered serializable as long as it does not explicitly reference unserializable objects. It does so by traversing the hierarchy of enclosing closures and null out any references that are not actually used by the starting closure. [TIP] \u00b6 Enable DEBUG logging level for org.apache.spark.util.ClosureCleaner logger to see what happens inside the class. Add the following line to conf/log4j.properties : log4j.logger.org.apache.spark.util.ClosureCleaner=DEBUG Refer to link:spark-logging.adoc[Logging]. \u00b6 With DEBUG logging level you should see the following messages in the logs: +++ Cleaning closure [func] ([func.getClass.getName]) +++ + declared fields: [declaredFields.size] [field] ... +++ closure [func] ([func.getClass.getName]) is now cleaned +++ Serialization is verified using a new instance of Serializer (as xref:core:SparkEnv.adoc#closureSerializer[closure Serializer]). Refer to link:spark-serialization.adoc[Serialization]. CAUTION: FIXME an example, please. == [[hadoopConfiguration]] Hadoop Configuration While a < >, so is a Hadoop configuration (as an instance of https://hadoop.apache.org/docs/current/api/org/apache/hadoop/conf/Configuration.html[org.apache.hadoop.conf.Configuration ] that is available as _hadoopConfiguration ). NOTE: link:spark-SparkHadoopUtil.adoc#newConfiguration[SparkHadoopUtil.get.newConfiguration] is used. If a SparkConf is provided it is used to build the configuration as described. Otherwise, the default Configuration object is returned. If AWS_ACCESS_KEY_ID and AWS_SECRET_ACCESS_KEY are both available, the following settings are set for the Hadoop configuration: fs.s3.awsAccessKeyId , fs.s3n.awsAccessKeyId , fs.s3a.access.key are set to the value of AWS_ACCESS_KEY_ID fs.s3.awsSecretAccessKey , fs.s3n.awsSecretAccessKey , and fs.s3a.secret.key are set to the value of AWS_SECRET_ACCESS_KEY Every spark.hadoop. setting becomes a setting of the configuration with the prefix spark.hadoop. removed for the key. The value of spark.buffer.size (default: 65536 ) is used as the value of io.file.buffer.size . == [[listenerBus]] listenerBus -- LiveListenerBus Event Bus listenerBus is a xref:scheduler:LiveListenerBus.adoc[] object that acts as a mechanism to announce events to other services on the link:spark-driver.adoc[driver]. NOTE: It is created and started when link:spark-SparkContext-creating-instance-internals.adoc[SparkContext starts] and, since it is a single-JVM event bus, is exclusively used on the driver. NOTE: listenerBus is a private[spark] value in SparkContext. == [[startTime]] Time when SparkContext was Created -- startTime Property [source, scala] \u00b6 startTime: Long \u00b6 startTime is the time in milliseconds when < >. [source, scala] \u00b6 scala> sc.startTime res0: Long = 1464425605653 == [[sparkUser]] Spark User -- sparkUser Property [source, scala] \u00b6 sparkUser: String \u00b6 sparkUser is the user who started the SparkContext instance. NOTE: It is computed when link:spark-SparkContext-creating-instance-internals.adoc#sparkUser[SparkContext is created] using link:spark-SparkContext-creating-instance-internals.adoc#[Utils.getCurrentUserName]. == [[submitMapStage]] Submitting ShuffleDependency for Execution -- submitMapStage Internal Method [source, scala] \u00b6 submitMapStage K, V, C : SimpleFutureAction[MapOutputStatistics] submitMapStage xref:scheduler:DAGScheduler.adoc#submitMapStage[submits the input ShuffleDependency to DAGScheduler for execution] and returns a SimpleFutureAction . Internally, submitMapStage < > first and submits it with localProperties . NOTE: Interestingly, submitMapStage is used exclusively when Spark SQL's link:spark-sql-SparkPlan-ShuffleExchange.adoc[ShuffleExchange] physical operator is executed. NOTE: submitMapStage seems related to xref:scheduler:DAGScheduler.adoc#adaptive-query-planning[Adaptive Query Planning / Adaptive Scheduling]. == [[getCallSite]] Calculating Call Site -- getCallSite Method CAUTION: FIXME == [[cancelJobGroup]] Cancelling Job Group -- cancelJobGroup Method [source, scala] \u00b6 cancelJobGroup(groupId: String) \u00b6 cancelJobGroup requests DAGScheduler xref:scheduler:DAGScheduler.adoc#cancelJobGroup[to cancel a group of active Spark jobs]. NOTE: cancelJobGroup is used exclusively when SparkExecuteStatementOperation does cancel . == [[cancelAllJobs]] Cancelling All Running and Scheduled Jobs -- cancelAllJobs Method CAUTION: FIXME NOTE: cancelAllJobs is used when link:spark-shell.adoc[spark-shell] is terminated (e.g. using Ctrl+C, so it can in turn terminate all active Spark jobs) or SparkSQLCLIDriver is terminated. == [[setJobGroup]] Setting Local Properties to Group Spark Jobs -- setJobGroup Method [source, scala] \u00b6 setJobGroup( groupId: String, description: String, interruptOnCancel: Boolean = false): Unit setJobGroup link:spark-sparkcontext-local-properties.adoc#setLocalProperty[sets local properties]: spark.jobGroup.id as groupId spark.job.description as description spark.job.interruptOnCancel as interruptOnCancel [NOTE] \u00b6 setJobGroup is used when: Spark Thrift Server's SparkExecuteStatementOperation runs a query Structured Streaming's StreamExecution runs batches \u00b6 == [[cleaner]] ContextCleaner [source, scala] \u00b6 cleaner: Option[ContextCleaner] \u00b6 SparkContext may have a xref:core:ContextCleaner.adoc[ContextCleaner] defined. ContextCleaner is created when xref:ROOT:spark-SparkContext-creating-instance-internals.adoc#_cleaner[SparkContext is created] with xref:ROOT:configuration-properties.adoc#spark.cleaner.referenceTracking[spark.cleaner.referenceTracking] configuration property enabled. == [[getPreferredLocs]] Finding Preferred Locations (Placement Preferences) for RDD Partition [source, scala] \u00b6 getPreferredLocs( rdd: RDD[_], partition: Int): Seq[TaskLocation] getPreferredLocs simply xref:scheduler:DAGScheduler.adoc#getPreferredLocs[requests DAGScheduler for the preferred locations for partition ]. NOTE: Preferred locations of a partition of a RDD are also called placement preferences or locality preferences . getPreferredLocs is used in CoalescedRDDPartition, DefaultPartitionCoalescer and PartitionerAwareUnionRDD. == [[persistRDD]] Registering RDD in persistentRdds Internal Registry -- persistRDD Internal Method [source, scala] \u00b6 persistRDD(rdd: RDD[_]): Unit \u00b6 persistRDD registers rdd in < > internal registry. NOTE: persistRDD is used exclusively when RDD is xref:rdd:index.adoc#persist-internal[persisted or locally checkpointed]. == [[getRDDStorageInfo]] Getting Storage Status of Cached RDDs (as RDDInfos) -- getRDDStorageInfo Methods [source, scala] \u00b6 getRDDStorageInfo: Array[RDDInfo] // <1> getRDDStorageInfo(filter: RDD[_] => Boolean): Array[RDDInfo] // <2> <1> Part of Spark's Developer API that uses <2> filtering no RDDs getRDDStorageInfo takes all the RDDs (from < > registry) that match filter and creates a collection of xref:storage:RDDInfo.adoc[RDDInfo] instances. getRDDStorageInfo then link:spark-webui-StorageListener.adoc#StorageUtils.updateRddInfo[updates the RDDInfos] with the < > (in a Spark application). In the end, getRDDStorageInfo gives only the RDD that are cached (i.e. the sum of memory and disk sizes as well as the number of partitions cached are greater than 0 ). NOTE: getRDDStorageInfo is used when RDD link:spark-rdd-lineage.adoc#toDebugString[is requested for RDD lineage graph]. == [[settings]] Settings === [[spark.driver.allowMultipleContexts]] spark.driver.allowMultipleContexts Quoting the scaladoc of http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.SparkContext[org.apache.spark.SparkContext ]: Only one SparkContext may be active per JVM. You must stop() the active SparkContext before creating a new one. You can however control the behaviour using spark.driver.allowMultipleContexts flag. It is disabled, i.e. false , by default. If enabled (i.e. true ), Spark prints the following WARN message to the logs: WARN Multiple running SparkContexts detected in the same JVM! If disabled (default), it will throw an SparkException exception: Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at: [ctx.creationSite.longForm] When creating an instance of SparkContext, Spark marks the current thread as having it being created (very early in the instantiation process). CAUTION: It's not guaranteed that Spark will work properly with two or more SparkContexts. Consider the feature a work in progress. == [[statusStore]] Accessing AppStatusStore [source, scala] \u00b6 statusStore: AppStatusStore \u00b6 statusStore gives the current xref:core:AppStatusStore.adoc[]. statusStore is used when: SparkContext is requested to < > ConsoleProgressBar is requested to xref:ROOT:spark-sparkcontext-ConsoleProgressBar.adoc#refresh[refresh] SharedState (Spark SQL) is requested for a SQLAppStatusStore == [[uiWebUrl]] Requesting URL of web UI -- uiWebUrl Method [source, scala] \u00b6 uiWebUrl: Option[String] \u00b6 uiWebUrl requests the link:spark-SparkContext-creating-instance-internals.adoc#_ui[SparkUI] for link:spark-webui-WebUI.adoc#webUrl[webUrl]. == [[maxNumConcurrentTasks]] maxNumConcurrentTasks Method [source, scala] \u00b6 maxNumConcurrentTasks(): Int \u00b6 maxNumConcurrentTasks simply requests the < > for the xref:scheduler:SchedulerBackend.adoc#maxNumConcurrentTasks[maximum number of tasks that can be launched concurrently]. NOTE: maxNumConcurrentTasks is used exclusively when DAGScheduler is requested to xref:scheduler:DAGScheduler.adoc#checkBarrierStageWithNumSlots[checkBarrierStageWithNumSlots]. == [[createTaskScheduler]] Creating SchedulerBackend and TaskScheduler -- createTaskScheduler Internal Factory Method [source, scala] \u00b6 createTaskScheduler( sc: SparkContext, master: String, deployMode: String): (SchedulerBackend, TaskScheduler) createTaskScheduler creates the xref:scheduler:SchedulerBackend.adoc[SchedulerBackend] and the xref:scheduler:TaskScheduler.adoc[TaskScheduler] for the given master URL and deployment mode. .SparkContext creates Task Scheduler and Scheduler Backend image::diagrams/sparkcontext-createtaskscheduler.png[align=\"center\"] Internally, createTaskScheduler branches off per the given master URL (link:spark-deployment-environments.adoc#master-urls[master URL]) to select the requested implementations. createTaskScheduler understands the following master URLs: local - local mode with 1 thread only local[n] or local[*] - local mode with n threads local[n, m] or local[*, m] -- local mode with n threads and m number of failures spark://hostname:port for Spark Standalone local-cluster[n, m, z] -- local cluster with n workers, m cores per worker, and z memory per worker any other URL is passed to < getClusterManager to load an external cluster manager>>. CAUTION: FIXME == [[environment-variables]] Environment Variables .Environment Variables [cols=\"1,1,2\",options=\"header\",width=\"100%\"] |=== | Environment Variable | Default Value | Description | [[SPARK_EXECUTOR_MEMORY]] SPARK_EXECUTOR_MEMORY | 1024 | Amount of memory to allocate for a Spark executor in MB. See xref:executor:Executor.adoc#memory[Executor Memory]. [[SPARK_USER]] SPARK_USER The user who is running SparkContext. Available later as < >. === == [[postEnvironmentUpdate]] Posting SparkListenerEnvironmentUpdate Event [source, scala] \u00b6 postEnvironmentUpdate(): Unit \u00b6 postEnvironmentUpdate ...FIXME NOTE: postEnvironmentUpdate is used when SparkContext is < >, and requested to < > and < >. == [[addJar-internals]] addJar Method [source, scala] \u00b6 addJar(path: String): Unit \u00b6 addJar ...FIXME NOTE: addJar is used when...FIXME == [[runApproximateJob]] Running Approximate Job [source, scala] \u00b6 runApproximateJob T, U, R : PartialResult[R] runApproximateJob...FIXME runApproximateJob is used when: DoubleRDDFunctions is requested to meanApprox and sumApprox RDD is requested to countApprox and countByValueApprox == [[killTaskAttempt]] Killing Task [source, scala] \u00b6 killTaskAttempt( taskId: Long, interruptThread: Boolean = true, reason: String = \"killed via SparkContext.killTaskAttempt\"): Boolean killTaskAttempt requests the < > to xref:scheduler:DAGScheduler.adoc#killTaskAttempt[kill a task]. == [[checkpointFile]] checkpointFile Internal Method [source, scala] \u00b6 checkpointFile T: ClassTag : RDD[T] checkpointFile...FIXME == [[logging]] Logging Enable ALL logging level for org.apache.spark.SparkContext logger to see what happens inside. Add the following line to conf/log4j.properties : [source,plaintext] \u00b6 log4j.logger.org.apache.spark.SparkContext=ALL \u00b6 Refer to xref:ROOT:spark-logging.adoc[Logging]. == [[internal-properties]] Internal Properties === [[checkpointDir]] Checkpoint Directory [source,scala] \u00b6 checkpointDir: Option[String] = None \u00b6 checkpointDir is...FIXME === [[persistentRdds]] persistentRdds Lookup Table Lookup table of persistent/cached RDDs per their ids. Used when SparkContext is requested to: < > < > < > < > === [[stopped]] stopped Flag Flag that says whether...FIXME ( true ) or not ( false ) === [[_taskScheduler]] TaskScheduler xref:scheduler:TaskScheduler.adoc[TaskScheduler]","title":"SparkContext"},{"location":"SparkContext/#source-scala","text":"","title":"[source, scala]"},{"location":"SparkContext/#addjarpath-string-unit","text":"| a| More to be added soon |=== Spark context link:spark-SparkContext-creating-instance-internals.adoc[sets up internal services] and establishes a connection to a link:spark-deployment-environments.adoc[Spark execution environment]. Once a < > you can use it to < >, < > and < >, access Spark services and < > (until SparkContext is < >). A Spark context is essentially a client of Spark's execution environment and acts as the master of your Spark application (don't get confused with the other meaning of link:spark-master.adoc[Master] in Spark, though). .Spark context acts as the master of your Spark application image::diagrams/sparkcontext-services.png[align=\"center\"] SparkContext offers the following functions: Getting current status of a Spark application ** < > ** < > ** < > ** < > ** < > ** < > ** < > that specifies the number of link:spark-rdd-partitions.adoc[partitions] in RDDs when they are created without specifying the number explicitly by a user. ** < > ** < > ** < > ** < > ** < > Setting Configuration ** < > ** link:spark-sparkcontext-local-properties.adoc[Local Properties -- Creating Logical Job Groups] ** < > ** < > Creating Distributed Entities ** < > ** < > ** < > Accessing services, e.g. < >, < >, xref:scheduler:LiveListenerBus.adoc[], xref:storage:BlockManager.adoc[BlockManager], xref:scheduler:SchedulerBackend.adoc[SchedulerBackends], xref:shuffle:ShuffleManager.adoc[ShuffleManager] and the < >. < > < > < > < > < > < > < > < > < > < > TIP: Read the scaladoc of http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.SparkContext[org.apache.spark.SparkContext ]. == [[addFile]] addFile Method","title":"addJar(path: String): Unit"},{"location":"SparkContext/#source-scala_1","text":"addFile( path: String): Unit // <1> addFile( path: String, recursive: Boolean): Unit <1> recursive flag is off addFile adds the path file to be downloaded...FIXME","title":"[source, scala]"},{"location":"SparkContext/#note","text":"addFile is used when: SparkContext is link:spark-SparkContext-creating-instance-internals.adoc#files[initialized] (and files were defined) Spark SQL's AddFileCommand is executed","title":"[NOTE]"},{"location":"SparkContext/#spark-sqls-sessionresourceloader-is-requested-to-load-a-file-resource","text":"== [[unpersistRDD]] Removing RDD Blocks from BlockManagerMaster -- unpersistRDD Internal Method","title":"* Spark SQL's SessionResourceLoader is requested to load a file resource"},{"location":"SparkContext/#source-scala_2","text":"","title":"[source, scala]"},{"location":"SparkContext/#unpersistrddrddid-int-blocking-boolean-true-unit","text":"unpersistRDD requests BlockManagerMaster to xref:storage:BlockManagerMaster.adoc#removeRdd[remove the blocks for the RDD] (given rddId ). NOTE: unpersistRDD uses SparkEnv xref:core:SparkEnv.adoc#blockManager[to access the current BlockManager ] that is in turn used to xref:storage:BlockManager.adoc#master[access the current BlockManagerMaster ]. unpersistRDD removes rddId from < > registry. In the end, unpersistRDD posts a xref:ROOT:SparkListener.adoc#SparkListenerUnpersistRDD[SparkListenerUnpersistRDD] (with rddId ) to < >.","title":"unpersistRDD(rddId: Int, blocking: Boolean = true): Unit"},{"location":"SparkContext/#note_1","text":"unpersistRDD is used when: ContextCleaner does xref:core:ContextCleaner.adoc#doCleanupRDD[doCleanupRDD]","title":"[NOTE]"},{"location":"SparkContext/#sparkcontext-ie-marks-an-rdd-as-non-persistent","text":"== [[applicationId]] Unique Identifier of Spark Application -- applicationId Method CAUTION: FIXME == [[postApplicationStart]] postApplicationStart Internal Method","title":"SparkContext &lt;&gt; (i.e. marks an RDD as non-persistent)"},{"location":"SparkContext/#source-scala_3","text":"","title":"[source, scala]"},{"location":"SparkContext/#postapplicationstart-unit","text":"postApplicationStart ...FIXME NOTE: postApplicationStart is used exclusively while SparkContext is being < > == [[postApplicationEnd]] postApplicationEnd Method CAUTION: FIXME == [[clearActiveContext]] clearActiveContext Method CAUTION: FIXME == [[getPersistentRDDs]] Accessing persistent RDDs -- getPersistentRDDs Method","title":"postApplicationStart(): Unit"},{"location":"SparkContext/#source-scala_4","text":"","title":"[source, scala]"},{"location":"SparkContext/#getpersistentrdds-mapint-rdd_","text":"getPersistentRDDs returns the collection of RDDs that have marked themselves as persistent via link:spark-rdd-caching.adoc#cache[cache]. Internally, getPersistentRDDs returns < > internal registry. == [[cancelJob]] Cancelling Job -- cancelJob Method","title":"getPersistentRDDs: Map[Int, RDD[_]]"},{"location":"SparkContext/#source-scala_5","text":"","title":"[source, scala]"},{"location":"SparkContext/#canceljobjobid-int","text":"cancelJob requests DAGScheduler xref:scheduler:DAGScheduler.adoc#cancelJob[to cancel a Spark job]. == [[cancelStage]] Cancelling Stage -- cancelStage Methods","title":"cancelJob(jobId: Int)"},{"location":"SparkContext/#source-scala_6","text":"cancelStage(stageId: Int): Unit cancelStage(stageId: Int, reason: String): Unit cancelStage simply requests DAGScheduler xref:scheduler:DAGScheduler.adoc#cancelJob[to cancel a Spark stage] (with an optional reason ). NOTE: cancelStage is used when StagesTab link:spark-webui-StagesTab.adoc#handleKillRequest[handles a kill request] (from a user in web UI). == [[dynamic-allocation]] Programmable Dynamic Allocation SparkContext offers the following methods as the developer API for xref:ROOT:spark-dynamic-allocation.adoc[]: < > < > < > (private!) < > === [[requestExecutors]] Requesting New Executors -- requestExecutors Method","title":"[source, scala]"},{"location":"SparkContext/#source-scala_7","text":"","title":"[source, scala]"},{"location":"SparkContext/#requestexecutorsnumadditionalexecutors-int-boolean","text":"requestExecutors requests numAdditionalExecutors executors from xref:scheduler:CoarseGrainedSchedulerBackend.adoc[CoarseGrainedSchedulerBackend]. === [[killExecutors]] Requesting to Kill Executors -- killExecutors Method","title":"requestExecutors(numAdditionalExecutors: Int): Boolean"},{"location":"SparkContext/#source-scala_8","text":"","title":"[source, scala]"},{"location":"SparkContext/#killexecutorsexecutorids-seqstring-boolean","text":"CAUTION: FIXME === [[requestTotalExecutors]] Requesting Total Executors -- requestTotalExecutors Method","title":"killExecutors(executorIds: Seq[String]): Boolean"},{"location":"SparkContext/#source-scala_9","text":"requestTotalExecutors( numExecutors: Int, localityAwareTasks: Int, hostToLocalTaskCount: Map[String, Int]): Boolean requestTotalExecutors is a private[spark] method that xref:scheduler:CoarseGrainedSchedulerBackend.adoc#requestTotalExecutors[requests the exact number of executors from a coarse-grained scheduler backend]. NOTE: It works for xref:scheduler:CoarseGrainedSchedulerBackend.adoc[coarse-grained scheduler backends] only. When called for other scheduler backends you should see the following WARN message in the logs: WARN Requesting executors is only supported in coarse-grained mode === [[getExecutorIds]] Getting Executor Ids -- getExecutorIds Method getExecutorIds is a private[spark] method that is part of link:spark-service-ExecutorAllocationClient.adoc[ExecutorAllocationClient contract]. It simply xref:scheduler:CoarseGrainedSchedulerBackend.adoc#getExecutorIds[passes the call on to the current coarse-grained scheduler backend, i.e. calls getExecutorIds ]. NOTE: It works for xref:scheduler:CoarseGrainedSchedulerBackend.adoc[coarse-grained scheduler backends] only. When called for other scheduler backends you should see the following WARN message in the logs: WARN Requesting executors is only supported in coarse-grained mode CAUTION: FIXME Why does SparkContext implement the method for coarse-grained scheduler backends? Why doesn't SparkContext throw an exception when the method is called? Nobody seems to be using it (!) == [[creating-instance]] Creating SparkContext Instance You can create a SparkContext instance with or without creating a xref:ROOT:SparkConf.adoc[SparkConf] object first. NOTE: You may want to read link:spark-SparkContext-creating-instance-internals.adoc[Inside Creating SparkContext] to learn what happens behind the scenes when SparkContext is created. === [[getOrCreate]] Getting Existing or Creating New SparkContext -- getOrCreate Methods","title":"[source, scala]"},{"location":"SparkContext/#source-scala_10","text":"getOrCreate(): SparkContext getOrCreate(conf: SparkConf): SparkContext getOrCreate methods allow you to get the existing SparkContext or create a new one.","title":"[source, scala]"},{"location":"SparkContext/#source-scala_11","text":"import org.apache.spark.SparkContext val sc = SparkContext.getOrCreate() // Using an explicit SparkConf object import org.apache.spark.SparkConf val conf = new SparkConf() .setMaster(\"local[*]\") .setAppName(\"SparkMe App\") val sc = SparkContext.getOrCreate(conf) The no-param getOrCreate method requires that the two mandatory Spark settings - < > and < > - are specified using link:spark-submit.adoc[spark-submit]. === [[constructors]] Constructors","title":"[source, scala]"},{"location":"SparkContext/#source-scala_12","text":"SparkContext() SparkContext(conf: SparkConf) SparkContext(master: String, appName: String, conf: SparkConf) SparkContext( master: String, appName: String, sparkHome: String = null, jars: Seq[String] = Nil, environment: Map[String, String] = Map()) You can create a SparkContext instance using the four constructors.","title":"[source, scala]"},{"location":"SparkContext/#source-scala_13","text":"import org.apache.spark.SparkConf val conf = new SparkConf() .setMaster(\"local[*]\") .setAppName(\"SparkMe App\") import org.apache.spark.SparkContext val sc = new SparkContext(conf) When a Spark context starts up you should see the following INFO in the logs (amongst the other messages that come from the Spark services): INFO SparkContext: Running Spark version 2.0.0-SNAPSHOT NOTE: Only one SparkContext may be running in a single JVM (check out https://issues.apache.org/jira/browse/SPARK-2243[SPARK-2243 Support multiple SparkContexts in the same JVM]). Sharing access to a SparkContext in the JVM is the solution to share data within Spark (without relying on other means of data sharing using external data stores). == [[env]] Accessing Current SparkEnv -- env Method CAUTION: FIXME == [[getConf]] Getting Current SparkConf -- getConf Method","title":"[source, scala]"},{"location":"SparkContext/#source-scala_14","text":"","title":"[source, scala]"},{"location":"SparkContext/#getconf-sparkconf","text":"getConf returns the current xref:ROOT:SparkConf.adoc[SparkConf]. NOTE: Changing the SparkConf object does not change the current configuration (as the method returns a copy). == [[master]][[master-url]] Deployment Environment -- master Method","title":"getConf: SparkConf"},{"location":"SparkContext/#source-scala_15","text":"","title":"[source, scala]"},{"location":"SparkContext/#master-string","text":"master method returns the current value of xref:ROOT:configuration-properties.adoc#spark.master[spark.master] which is the link:spark-deployment-environments.adoc[deployment environment] in use. == [[appName]] Application Name -- appName Method","title":"master: String"},{"location":"SparkContext/#source-scala_16","text":"","title":"[source, scala]"},{"location":"SparkContext/#appname-string","text":"appName gives the value of the mandatory xref:ROOT:SparkConf.adoc#spark.app.name[spark.app.name] setting. NOTE: appName is used when link:spark-standalone.adoc#SparkDeploySchedulerBackend[ SparkDeploySchedulerBackend starts], link:spark-webui-SparkUI.adoc#createLiveUI[ SparkUI creates a web UI], when postApplicationStart is executed, and for Mesos and checkpointing in Spark Streaming. == [[applicationAttemptId]] Unique Identifier of Execution Attempt -- applicationAttemptId Method","title":"appName: String"},{"location":"SparkContext/#source-scala_17","text":"","title":"[source, scala]"},{"location":"SparkContext/#applicationattemptid-optionstring","text":"applicationAttemptId gives the unique identifier of the execution attempt of a Spark application.","title":"applicationAttemptId: Option[String]"},{"location":"SparkContext/#note_2","text":"applicationAttemptId is used when: xref:scheduler:ShuffleMapTask.adoc#creating-instance[ShuffleMapTask] and xref:scheduler:ResultTask.adoc#creating-instance[ResultTask] are created","title":"[NOTE]"},{"location":"SparkContext/#sparkcontext","text":"== [[getExecutorStorageStatus]] Storage Status (of All BlockManagers) -- getExecutorStorageStatus Method","title":"* SparkContext &lt;&gt;"},{"location":"SparkContext/#source-scala_18","text":"","title":"[source, scala]"},{"location":"SparkContext/#getexecutorstoragestatus-arraystoragestatus","text":"getExecutorStorageStatus xref:storage:BlockManagerMaster.adoc#getStorageStatus[requests BlockManagerMaster for storage status] (of all xref:storage:BlockManager.adoc[BlockManagers]). NOTE: getExecutorStorageStatus is a developer API.","title":"getExecutorStorageStatus: Array[StorageStatus]"},{"location":"SparkContext/#note_3","text":"getExecutorStorageStatus is used when: SparkContext < >","title":"[NOTE]"},{"location":"SparkContext/#sparkstatustracker-linkspark-sparkcontext-sparkstatustrackeradocgetexecutorinfosis-requested-for-information-about-all-known-executors","text":"== [[deployMode]] Deploy Mode -- deployMode Method","title":"* SparkStatusTracker link:spark-sparkcontext-SparkStatusTracker.adoc#getExecutorInfos[is requested for information about all known executors]"},{"location":"SparkContext/#sourcescala","text":"","title":"[source,scala]"},{"location":"SparkContext/#deploymode-string","text":"deployMode returns the current value of link:spark-deploy-mode.adoc[spark.submit.deployMode] setting or client if not set. == [[getSchedulingMode]] Scheduling Mode -- getSchedulingMode Method","title":"deployMode: String"},{"location":"SparkContext/#source-scala_19","text":"","title":"[source, scala]"},{"location":"SparkContext/#getschedulingmode-schedulingmodeschedulingmode","text":"getSchedulingMode returns the current link:spark-scheduler-SchedulingMode.adoc[Scheduling Mode]. == [[getPoolForName]] Schedulable (Pool) by Name -- getPoolForName Method","title":"getSchedulingMode: SchedulingMode.SchedulingMode"},{"location":"SparkContext/#source-scala_20","text":"","title":"[source, scala]"},{"location":"SparkContext/#getpoolfornamepool-string-optionschedulable","text":"getPoolForName returns a link:spark-scheduler-Schedulable.adoc[Schedulable] by the pool name, if one exists. NOTE: getPoolForName is part of the Developer's API and may change in the future. Internally, it requests the xref:scheduler:TaskScheduler.adoc#rootPool[TaskScheduler for the root pool] and link:spark-scheduler-Pool.adoc#schedulableNameToSchedulable[looks up the Schedulable by the pool name]. It is exclusively used to link:spark-webui-PoolPage.adoc[show pool details in web UI (for a stage)]. == [[getAllPools]] All Schedulable Pools -- getAllPools Method","title":"getPoolForName(pool: String): Option[Schedulable]"},{"location":"SparkContext/#source-scala_21","text":"","title":"[source, scala]"},{"location":"SparkContext/#getallpools-seqschedulable","text":"getAllPools collects the link:spark-scheduler-Pool.adoc[Pools] in xref:scheduler:TaskScheduler.adoc#contract[TaskScheduler.rootPool]. NOTE: TaskScheduler.rootPool is part of the xref:scheduler:TaskScheduler.adoc#contract[TaskScheduler Contract]. NOTE: getAllPools is part of the Developer's API. CAUTION: FIXME Where is the method used? NOTE: getAllPools is used to calculate pool names for link:spark-webui-AllStagesPage.adoc#pool-names[Stages tab in web UI] with FAIR scheduling mode used. == [[defaultParallelism]] Default Level of Parallelism","title":"getAllPools: Seq[Schedulable]"},{"location":"SparkContext/#source-scala_22","text":"","title":"[source, scala]"},{"location":"SparkContext/#defaultparallelism-int","text":"defaultParallelism requests < > for the xref:scheduler:TaskScheduler.adoc#defaultParallelism[default level of parallelism]. NOTE: Default level of parallelism specifies the number of link:spark-rdd-partitions.adoc[partitions] in RDDs when created without specifying them explicitly by a user.","title":"defaultParallelism: Int"},{"location":"SparkContext/#note_4","text":"defaultParallelism is used in < >, SparkContext.range and < > (as well as Spark Streaming's DStream.countByValue and DStream.countByValueAndWindow et al.).","title":"[NOTE]"},{"location":"SparkContext/#defaultparallelism-is-also-used-to-instantiate-xrefrddhashpartitioneradochashpartitioner-and-for-the-minimum-number-of-partitions-in-xrefrddspark-rdd-hadooprddadochadooprdds","text":"== [[taskScheduler]] Current Spark Scheduler (aka TaskScheduler) -- taskScheduler Property","title":"defaultParallelism is also used to instantiate xref:rdd:HashPartitioner.adoc[HashPartitioner] and for the minimum number of partitions in xref:rdd:spark-rdd-HadoopRDD.adoc[HadoopRDDs]."},{"location":"SparkContext/#source-scala_23","text":"taskScheduler: TaskScheduler taskScheduler_=(ts: TaskScheduler): Unit taskScheduler manages (i.e. reads or writes) <<_taskScheduler, _taskScheduler>> internal property. == [[version]] Getting Spark Version -- version Property","title":"[source, scala]"},{"location":"SparkContext/#source-scala_24","text":"","title":"[source, scala]"},{"location":"SparkContext/#version-string","text":"version returns the Spark version this SparkContext uses. == [[makeRDD]] makeRDD Method CAUTION: FIXME == [[submitJob]] Submitting Jobs Asynchronously -- submitJob Method","title":"version: String"},{"location":"SparkContext/#source-scala_25","text":"submitJob T, U, R : SimpleFutureAction[R] submitJob submits a job in an asynchronous, non-blocking way to xref:scheduler:DAGScheduler.adoc#submitJob[DAGScheduler]. It cleans the processPartition input function argument and returns an instance of link:spark-rdd-actions.adoc#FutureAction[SimpleFutureAction] that holds the xref:scheduler:spark-scheduler-JobWaiter.adoc[JobWaiter] instance. CAUTION: FIXME What are resultFunc ? It is used in: link:spark-rdd-actions.adoc#AsyncRDDActions[AsyncRDDActions] methods link:spark-streaming/spark-streaming.adoc[Spark Streaming] for link:spark-streaming/spark-streaming-receivertracker.adoc#ReceiverTrackerEndpoint-startReceiver[ReceiverTrackerEndpoint.startReceiver] == [[spark-configuration]] Spark Configuration CAUTION: FIXME == [[sparkcontext-and-rdd]] SparkContext and RDDs You use a Spark context to create RDDs (see < >). When an RDD is created, it belongs to and is completely owned by the Spark context it originated from. RDDs can't by design be shared between SparkContexts. .A Spark context creates a living space for RDDs. image::diagrams/sparkcontext-rdds.png[align=\"center\"] == [[creating-rdds]][[parallelize]] Creating RDD -- parallelize Method SparkContext allows you to create many different RDDs from input sources like: Scala's collections, i.e. sc.parallelize(0 to 100) local or remote filesystems, i.e. sc.textFile(\"README.md\") Any Hadoop InputSource using sc.newAPIHadoopFile Read xref:rdd:index.adoc#creating-rdds[Creating RDDs] in xref:rdd:index.adoc[RDD - Resilient Distributed Dataset]. == [[unpersist]] Unpersisting RDD (Marking RDD as Non-Persistent) -- unpersist Method CAUTION: FIXME unpersist removes an RDD from the master's xref:storage:BlockManager.adoc[Block Manager] (calls removeRdd(rddId: Int, blocking: Boolean) ) and the internal < > mapping. It finally posts xref:ROOT:SparkListener.adoc#SparkListenerUnpersistRDD[SparkListenerUnpersistRDD] message to listenerBus . == [[setCheckpointDir]] Setting Checkpoint Directory -- setCheckpointDir Method","title":"[source, scala]"},{"location":"SparkContext/#source-scala_26","text":"","title":"[source, scala]"},{"location":"SparkContext/#setcheckpointdirdirectory-string","text":"setCheckpointDir method is used to set up the checkpoint directory...FIXME CAUTION: FIXME == [[register]] Registering Accumulator -- register Methods","title":"setCheckpointDir(directory: String)"},{"location":"SparkContext/#source-scala_27","text":"register(acc: AccumulatorV2[ , _]): Unit register(acc: AccumulatorV2[ , _], name: String): Unit register registers the acc link:spark-accumulators.adoc[accumulator]. You can optionally give an accumulator a name . TIP: You can create built-in accumulators for longs, doubles, and collection types using < >. Internally, register link:spark-accumulators.adoc#register[registers acc accumulator] (with the current SparkContext). == [[creating-accumulators]][[longAccumulator]][[doubleAccumulator]][[collectionAccumulator]] Creating Built-In Accumulators","title":"[source, scala]"},{"location":"SparkContext/#source-scala_28","text":"longAccumulator: LongAccumulator longAccumulator(name: String): LongAccumulator doubleAccumulator: DoubleAccumulator doubleAccumulator(name: String): DoubleAccumulator collectionAccumulator[T]: CollectionAccumulator[T] collectionAccumulator T : CollectionAccumulator[T] You can use longAccumulator , doubleAccumulator or collectionAccumulator to create and register link:spark-accumulators.adoc[accumulators] for simple and collection values. longAccumulator returns link:spark-accumulators.adoc#LongAccumulator[LongAccumulator] with the zero value 0 . doubleAccumulator returns link:spark-accumulators.adoc#DoubleAccumulator[DoubleAccumulator] with the zero value 0.0 . collectionAccumulator returns link:spark-accumulators.adoc#CollectionAccumulator[CollectionAccumulator] with the zero value java.util.List[T] .","title":"[source, scala]"},{"location":"SparkContext/#source-scala_29","text":"scala> val acc = sc.longAccumulator acc: org.apache.spark.util.LongAccumulator = LongAccumulator(id: 0, name: None, value: 0) scala> val counter = sc.longAccumulator(\"counter\") counter: org.apache.spark.util.LongAccumulator = LongAccumulator(id: 1, name: Some(counter), value: 0) scala> counter.value res0: Long = 0 scala> sc.parallelize(0 to 9).foreach(n => counter.add(n)) scala> counter.value res3: Long = 45 The name input parameter allows you to give a name to an accumulator and have it displayed in link:spark-webui-StagePage.adoc#accumulators[Spark UI] (under Stages tab for a given stage). .Accumulators in the Spark UI image::spark-webui-accumulators.png[align=\"center\"] TIP: You can register custom accumulators using < > methods. == [[broadcast]] Creating Broadcast Variable -- broadcast Method","title":"[source, scala]"},{"location":"SparkContext/#source-scala_30","text":"broadcast T : Broadcast[T] broadcast method creates a xref:ROOT:Broadcast.adoc[]. It is a shared memory with value (as broadcast blocks) on the driver and later on all Spark executors.","title":"[source, scala]"},{"location":"SparkContext/#sourceplaintext","text":"val sc: SparkContext = ??? scala> val hello = sc.broadcast(\"hello\") hello: org.apache.spark.broadcast.Broadcast[String] = Broadcast(0) Spark transfers the value to Spark executors once , and tasks can share it without incurring repetitive network transmissions when the broadcast variable is used multiple times. .Broadcasting a value to executors image::sparkcontext-broadcast-executors.png[align=\"center\"] Internally, broadcast requests BroadcastManager for a xref:core:BroadcastManager.adoc#newBroadcast[new broadcast variable]. NOTE: The current BroadcastManager is available using xref:core:SparkEnv.adoc#broadcastManager[ SparkEnv.broadcastManager ] attribute and is always xref:core:BroadcastManager.adoc[BroadcastManager] (with few internal configuration changes to reflect where it runs, i.e. inside the driver or executors). You should see the following INFO message in the logs: Created broadcast [id] from [callSite] If ContextCleaner is defined, the xref:core:ContextCleaner.adoc#[new broadcast variable is registered for cleanup].","title":"[source,plaintext]"},{"location":"SparkContext/#note_5","text":"Spark does not support broadcasting RDDs.","title":"[NOTE]"},{"location":"SparkContext/#scala-scbroadcastscrange0-10-javalangillegalargumentexception-requirement-failed-can-not-directly-broadcast-rdds-instead-call-collect-and-broadcast-the-result-at-scalapredefrequirepredefscala224-at-orgapachesparksparkcontextbroadcastsparkcontextscala1392-48-elided","text":"Once created, the broadcast variable (and other blocks) are displayed per executor and the driver in web UI (under link:spark-webui-executors.adoc[Executors tab]). .Broadcast Variables In web UI's Executors Tab image::spark-broadcast-webui-executors-rdd-blocks.png[align=\"center\"] == [[jars]] Distribute JARs to workers The jar you specify with SparkContext.addJar will be copied to all the worker nodes. The configuration setting spark.jars is a comma-separated list of jar paths to be included in all tasks executed from this SparkContext. A path can either be a local file, a file in HDFS (or other Hadoop-supported filesystems), an HTTP, HTTPS or FTP URI, or local:/path for a file on every worker node. scala> sc.addJar(\"build.sbt\") 15/11/11 21:54:54 INFO SparkContext: Added JAR build.sbt at http://192.168.1.4:49427/jars/build.sbt with timestamp 1447275294457 CAUTION: FIXME Why is HttpFileServer used for addJar? === SparkContext as Application-Wide Counter SparkContext keeps track of: [[nextShuffleId]] * shuffle ids using nextShuffleId internal counter for xref:scheduler:ShuffleMapStage.adoc[registering shuffle dependencies] to xref:shuffle:ShuffleManager.adoc[Shuffle Service]. == [[runJob]] Running Job Synchronously xref:rdd:index.adoc#actions[RDD actions] run link:spark-scheduler-ActiveJob.adoc[jobs] using one of runJob methods.","title":"scala&gt; sc.broadcast(sc.range(0, 10))\njava.lang.IllegalArgumentException: requirement failed: Can not directly broadcast RDDs; instead, call collect() and broadcast the result.\n  at scala.Predef$.require(Predef.scala:224)\n  at org.apache.spark.SparkContext.broadcast(SparkContext.scala:1392)\n  ... 48 elided\n"},{"location":"SparkContext/#source-scala_31","text":"runJob T, U : Unit runJob T, U : Array[U] runJob T, U : Array[U] runJob T, U : Array[U] runJob T, U : Array[U] runJob T, U runJob T, U: ClassTag runJob executes a function on one or many partitions of a RDD (in a SparkContext space) to produce a collection of values per partition. NOTE: runJob can only work when a SparkContext is not < >. Internally, runJob first makes sure that the SparkContext is not < >. If it is, you should see the following IllegalStateException exception in the logs: java.lang.IllegalStateException: SparkContext has been shutdown at org.apache.spark.SparkContext.runJob(SparkContext.scala:1893) at org.apache.spark.SparkContext.runJob(SparkContext.scala:1914) at org.apache.spark.SparkContext.runJob(SparkContext.scala:1934) ... 48 elided runJob then < > and < func closure>>. You should see the following INFO message in the logs: INFO SparkContext: Starting job: [callSite] With link:spark-rdd-lineage.adoc#spark_logLineage[spark.logLineage] enabled (which is not by default), you should see the following INFO message with link:spark-rdd-lineage.adoc#toDebugString[toDebugString] (executed on rdd ): INFO SparkContext: RDD's recursive dependencies: [toDebugString] runJob requests xref:scheduler:DAGScheduler.adoc#runJob[ DAGScheduler to run a job]. TIP: runJob just prepares input parameters for xref:scheduler:DAGScheduler.adoc#runJob[ DAGScheduler to run a job]. After DAGScheduler is done and the job has finished, runJob link:spark-sparkcontext-ConsoleProgressBar.adoc#finishAll[stops ConsoleProgressBar ] and xref:ROOT:rdd-checkpointing.adoc#doCheckpoint[performs RDD checkpointing of rdd ]. TIP: For some actions, e.g. first() and lookup() , there is no need to compute all the partitions of the RDD in a job. And Spark knows it.","title":"[source, scala]"},{"location":"SparkContext/#sourcescala_1","text":"// RDD to work with val lines = sc.parallelize(Seq(\"hello world\", \"nice to see you\")) import org.apache.spark.TaskContext scala> sc.runJob(lines, (t: TaskContext, i: Iterator[String]) => 1) // <1> res0: Array[Int] = Array(1, 1) // <2> <1> Run a job using runJob on lines RDD with a function that returns 1 for every partition (of lines RDD). <2> What can you say about the number of partitions of the lines RDD? Is your result res0 different than mine? Why? TIP: Read link:spark-TaskContext.adoc[TaskContext]. Running a job is essentially executing a func function on all or a subset of partitions in an rdd RDD and returning the result as an array (with elements being the results per partition). .Executing action image::spark-runjob.png[align=\"center\"] == [[stop]][[stopping]] Stopping SparkContext -- stop Method","title":"[source,scala]"},{"location":"SparkContext/#source-scala_32","text":"","title":"[source, scala]"},{"location":"SparkContext/#stop-unit","text":"stop stops the SparkContext. Internally, stop enables stopped internal flag. If already stopped, you should see the following INFO message in the logs: INFO SparkContext: SparkContext already stopped. stop then does the following: Removes _shutdownHookRef from ShutdownHookManager < SparkListenerApplicationEnd >> (to < >) link:spark-webui-SparkUI.adoc#stop[Stops web UI] link:spark-metrics-MetricsSystem.adoc#report[Requests MetricSystem to report metrics] (from all registered sinks) xref:core:ContextCleaner.adoc#stop[Stops ContextCleaner ] link:spark-ExecutorAllocationManager.adoc#stop[Requests ExecutorAllocationManager to stop] If LiveListenerBus was started, xref:scheduler:LiveListenerBus.adoc#stop[requests LiveListenerBus to stop] Requests xref:spark-history-server:EventLoggingListener.adoc#stop[ EventLoggingListener to stop] Requests xref:scheduler:DAGScheduler.adoc#stop[ DAGScheduler to stop] Requests xref:rpc:index.adoc#stop[RpcEnv to stop HeartbeatReceiver endpoint] Requests link:spark-sparkcontext-ConsoleProgressBar.adoc#stop[ ConsoleProgressBar to stop] Clears the reference to TaskScheduler , i.e. _taskScheduler is null Requests xref:core:SparkEnv.adoc#stop[ SparkEnv to stop] and clears SparkEnv Clears link:yarn/spark-yarn-client.adoc#SPARK_YARN_MODE[ SPARK_YARN_MODE flag] < > Ultimately, you should see the following INFO message in the logs: INFO SparkContext: Successfully stopped SparkContext == [[addSparkListener]] Registering SparkListener -- addSparkListener Method","title":"stop(): Unit"},{"location":"SparkContext/#source-scala_33","text":"","title":"[source, scala]"},{"location":"SparkContext/#addsparklistenerlistener-sparklistenerinterface-unit","text":"You can register a custom xref:ROOT:SparkListener.adoc#SparkListenerInterface[SparkListenerInterface] using addSparkListener method NOTE: You can also register custom listeners using xref:ROOT:configuration-properties.adoc#spark.extraListeners[spark.extraListeners] configuration property. == [[custom-schedulers]] Custom SchedulerBackend, TaskScheduler and DAGScheduler By default, SparkContext uses ( private[spark] class) org.apache.spark.scheduler.DAGScheduler , but you can develop your own custom DAGScheduler implementation, and use ( private[spark] ) SparkContext.dagScheduler_=(ds: DAGScheduler) method to assign yours. It is also applicable to SchedulerBackend and TaskScheduler using schedulerBackend_=(sb: SchedulerBackend) and taskScheduler_=(ts: TaskScheduler) methods, respectively. CAUTION: FIXME Make it an advanced exercise. == [[events]] Events When a Spark context starts, it triggers xref:ROOT:SparkListener.adoc#SparkListenerEnvironmentUpdate[SparkListenerEnvironmentUpdate] and xref:ROOT:SparkListener.adoc#SparkListenerApplicationStart[SparkListenerApplicationStart] messages. Refer to the section < >. == [[setLogLevel]][[setting-default-log-level]] Setting Default Logging Level -- setLogLevel Method","title":"addSparkListener(listener: SparkListenerInterface): Unit"},{"location":"SparkContext/#source-scala_34","text":"","title":"[source, scala]"},{"location":"SparkContext/#setloglevelloglevel-string","text":"setLogLevel allows you to set the root logging level in a Spark application, e.g. link:spark-shell.adoc[Spark shell]. Internally, setLogLevel calls link:++ http://logging.apache.org/log4j/2.x/log4j-api/apidocs/org/apache/logging/log4j/Level.html#toLevel(java.lang.String)++[org.apache.log4j.Level.toLevel(logLevel )] that it then uses to set using link:++ http://logging.apache.org/log4j/2.x/log4j-api/apidocs/org/apache/logging/log4j/LogManager.html#getRootLogger()++[org.apache.log4j.LogManager.getRootLogger().setLevel(level )].","title":"setLogLevel(logLevel: String)"},{"location":"SparkContext/#tip","text":"You can directly set the logging level using link:++ http://logging.apache.org/log4j/2.x/log4j-api/apidocs/org/apache/logging/log4j/LogManager.html#getLogger()++[org.apache.log4j.LogManager.getLogger ()].","title":"[TIP]"},{"location":"SparkContext/#source-scala_35","text":"","title":"[source, scala]"},{"location":"SparkContext/#logmanagergetloggerorgsetlevelleveloff","text":"==== == [[clean]][[closure-cleaning]] Closure Cleaning -- clean Method","title":"LogManager.getLogger(\"org\").setLevel(Level.OFF)"},{"location":"SparkContext/#source-scala_36","text":"","title":"[source, scala]"},{"location":"SparkContext/#cleanf-f-checkserializable-boolean-true-f","text":"Every time an action is called, Spark cleans up the closure, i.e. the body of the action, before it is serialized and sent over the wire to executors. SparkContext comes with clean(f: F, checkSerializable: Boolean = true) method that does this. It in turn calls ClosureCleaner.clean method. Not only does ClosureCleaner.clean method clean the closure, but also does it transitively, i.e. referenced closures are cleaned transitively. A closure is considered serializable as long as it does not explicitly reference unserializable objects. It does so by traversing the hierarchy of enclosing closures and null out any references that are not actually used by the starting closure.","title":"clean(f: F, checkSerializable: Boolean = true): F"},{"location":"SparkContext/#tip_1","text":"Enable DEBUG logging level for org.apache.spark.util.ClosureCleaner logger to see what happens inside the class. Add the following line to conf/log4j.properties : log4j.logger.org.apache.spark.util.ClosureCleaner=DEBUG","title":"[TIP]"},{"location":"SparkContext/#refer-to-linkspark-loggingadoclogging","text":"With DEBUG logging level you should see the following messages in the logs: +++ Cleaning closure [func] ([func.getClass.getName]) +++ + declared fields: [declaredFields.size] [field] ... +++ closure [func] ([func.getClass.getName]) is now cleaned +++ Serialization is verified using a new instance of Serializer (as xref:core:SparkEnv.adoc#closureSerializer[closure Serializer]). Refer to link:spark-serialization.adoc[Serialization]. CAUTION: FIXME an example, please. == [[hadoopConfiguration]] Hadoop Configuration While a < >, so is a Hadoop configuration (as an instance of https://hadoop.apache.org/docs/current/api/org/apache/hadoop/conf/Configuration.html[org.apache.hadoop.conf.Configuration ] that is available as _hadoopConfiguration ). NOTE: link:spark-SparkHadoopUtil.adoc#newConfiguration[SparkHadoopUtil.get.newConfiguration] is used. If a SparkConf is provided it is used to build the configuration as described. Otherwise, the default Configuration object is returned. If AWS_ACCESS_KEY_ID and AWS_SECRET_ACCESS_KEY are both available, the following settings are set for the Hadoop configuration: fs.s3.awsAccessKeyId , fs.s3n.awsAccessKeyId , fs.s3a.access.key are set to the value of AWS_ACCESS_KEY_ID fs.s3.awsSecretAccessKey , fs.s3n.awsSecretAccessKey , and fs.s3a.secret.key are set to the value of AWS_SECRET_ACCESS_KEY Every spark.hadoop. setting becomes a setting of the configuration with the prefix spark.hadoop. removed for the key. The value of spark.buffer.size (default: 65536 ) is used as the value of io.file.buffer.size . == [[listenerBus]] listenerBus -- LiveListenerBus Event Bus listenerBus is a xref:scheduler:LiveListenerBus.adoc[] object that acts as a mechanism to announce events to other services on the link:spark-driver.adoc[driver]. NOTE: It is created and started when link:spark-SparkContext-creating-instance-internals.adoc[SparkContext starts] and, since it is a single-JVM event bus, is exclusively used on the driver. NOTE: listenerBus is a private[spark] value in SparkContext. == [[startTime]] Time when SparkContext was Created -- startTime Property","title":"Refer to link:spark-logging.adoc[Logging]."},{"location":"SparkContext/#source-scala_37","text":"","title":"[source, scala]"},{"location":"SparkContext/#starttime-long","text":"startTime is the time in milliseconds when < >.","title":"startTime: Long"},{"location":"SparkContext/#source-scala_38","text":"scala> sc.startTime res0: Long = 1464425605653 == [[sparkUser]] Spark User -- sparkUser Property","title":"[source, scala]"},{"location":"SparkContext/#source-scala_39","text":"","title":"[source, scala]"},{"location":"SparkContext/#sparkuser-string","text":"sparkUser is the user who started the SparkContext instance. NOTE: It is computed when link:spark-SparkContext-creating-instance-internals.adoc#sparkUser[SparkContext is created] using link:spark-SparkContext-creating-instance-internals.adoc#[Utils.getCurrentUserName]. == [[submitMapStage]] Submitting ShuffleDependency for Execution -- submitMapStage Internal Method","title":"sparkUser: String"},{"location":"SparkContext/#source-scala_40","text":"submitMapStage K, V, C : SimpleFutureAction[MapOutputStatistics] submitMapStage xref:scheduler:DAGScheduler.adoc#submitMapStage[submits the input ShuffleDependency to DAGScheduler for execution] and returns a SimpleFutureAction . Internally, submitMapStage < > first and submits it with localProperties . NOTE: Interestingly, submitMapStage is used exclusively when Spark SQL's link:spark-sql-SparkPlan-ShuffleExchange.adoc[ShuffleExchange] physical operator is executed. NOTE: submitMapStage seems related to xref:scheduler:DAGScheduler.adoc#adaptive-query-planning[Adaptive Query Planning / Adaptive Scheduling]. == [[getCallSite]] Calculating Call Site -- getCallSite Method CAUTION: FIXME == [[cancelJobGroup]] Cancelling Job Group -- cancelJobGroup Method","title":"[source, scala]"},{"location":"SparkContext/#source-scala_41","text":"","title":"[source, scala]"},{"location":"SparkContext/#canceljobgroupgroupid-string","text":"cancelJobGroup requests DAGScheduler xref:scheduler:DAGScheduler.adoc#cancelJobGroup[to cancel a group of active Spark jobs]. NOTE: cancelJobGroup is used exclusively when SparkExecuteStatementOperation does cancel . == [[cancelAllJobs]] Cancelling All Running and Scheduled Jobs -- cancelAllJobs Method CAUTION: FIXME NOTE: cancelAllJobs is used when link:spark-shell.adoc[spark-shell] is terminated (e.g. using Ctrl+C, so it can in turn terminate all active Spark jobs) or SparkSQLCLIDriver is terminated. == [[setJobGroup]] Setting Local Properties to Group Spark Jobs -- setJobGroup Method","title":"cancelJobGroup(groupId: String)"},{"location":"SparkContext/#source-scala_42","text":"setJobGroup( groupId: String, description: String, interruptOnCancel: Boolean = false): Unit setJobGroup link:spark-sparkcontext-local-properties.adoc#setLocalProperty[sets local properties]: spark.jobGroup.id as groupId spark.job.description as description spark.job.interruptOnCancel as interruptOnCancel","title":"[source, scala]"},{"location":"SparkContext/#note_6","text":"setJobGroup is used when: Spark Thrift Server's SparkExecuteStatementOperation runs a query","title":"[NOTE]"},{"location":"SparkContext/#structured-streamings-streamexecution-runs-batches","text":"== [[cleaner]] ContextCleaner","title":"Structured Streaming's StreamExecution runs batches"},{"location":"SparkContext/#source-scala_43","text":"","title":"[source, scala]"},{"location":"SparkContext/#cleaner-optioncontextcleaner","text":"SparkContext may have a xref:core:ContextCleaner.adoc[ContextCleaner] defined. ContextCleaner is created when xref:ROOT:spark-SparkContext-creating-instance-internals.adoc#_cleaner[SparkContext is created] with xref:ROOT:configuration-properties.adoc#spark.cleaner.referenceTracking[spark.cleaner.referenceTracking] configuration property enabled. == [[getPreferredLocs]] Finding Preferred Locations (Placement Preferences) for RDD Partition","title":"cleaner: Option[ContextCleaner]"},{"location":"SparkContext/#source-scala_44","text":"getPreferredLocs( rdd: RDD[_], partition: Int): Seq[TaskLocation] getPreferredLocs simply xref:scheduler:DAGScheduler.adoc#getPreferredLocs[requests DAGScheduler for the preferred locations for partition ]. NOTE: Preferred locations of a partition of a RDD are also called placement preferences or locality preferences . getPreferredLocs is used in CoalescedRDDPartition, DefaultPartitionCoalescer and PartitionerAwareUnionRDD. == [[persistRDD]] Registering RDD in persistentRdds Internal Registry -- persistRDD Internal Method","title":"[source, scala]"},{"location":"SparkContext/#source-scala_45","text":"","title":"[source, scala]"},{"location":"SparkContext/#persistrddrdd-rdd_-unit","text":"persistRDD registers rdd in < > internal registry. NOTE: persistRDD is used exclusively when RDD is xref:rdd:index.adoc#persist-internal[persisted or locally checkpointed]. == [[getRDDStorageInfo]] Getting Storage Status of Cached RDDs (as RDDInfos) -- getRDDStorageInfo Methods","title":"persistRDD(rdd: RDD[_]): Unit"},{"location":"SparkContext/#source-scala_46","text":"getRDDStorageInfo: Array[RDDInfo] // <1> getRDDStorageInfo(filter: RDD[_] => Boolean): Array[RDDInfo] // <2> <1> Part of Spark's Developer API that uses <2> filtering no RDDs getRDDStorageInfo takes all the RDDs (from < > registry) that match filter and creates a collection of xref:storage:RDDInfo.adoc[RDDInfo] instances. getRDDStorageInfo then link:spark-webui-StorageListener.adoc#StorageUtils.updateRddInfo[updates the RDDInfos] with the < > (in a Spark application). In the end, getRDDStorageInfo gives only the RDD that are cached (i.e. the sum of memory and disk sizes as well as the number of partitions cached are greater than 0 ). NOTE: getRDDStorageInfo is used when RDD link:spark-rdd-lineage.adoc#toDebugString[is requested for RDD lineage graph]. == [[settings]] Settings === [[spark.driver.allowMultipleContexts]] spark.driver.allowMultipleContexts Quoting the scaladoc of http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.SparkContext[org.apache.spark.SparkContext ]: Only one SparkContext may be active per JVM. You must stop() the active SparkContext before creating a new one. You can however control the behaviour using spark.driver.allowMultipleContexts flag. It is disabled, i.e. false , by default. If enabled (i.e. true ), Spark prints the following WARN message to the logs: WARN Multiple running SparkContexts detected in the same JVM! If disabled (default), it will throw an SparkException exception: Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at: [ctx.creationSite.longForm] When creating an instance of SparkContext, Spark marks the current thread as having it being created (very early in the instantiation process). CAUTION: It's not guaranteed that Spark will work properly with two or more SparkContexts. Consider the feature a work in progress. == [[statusStore]] Accessing AppStatusStore","title":"[source, scala]"},{"location":"SparkContext/#source-scala_47","text":"","title":"[source, scala]"},{"location":"SparkContext/#statusstore-appstatusstore","text":"statusStore gives the current xref:core:AppStatusStore.adoc[]. statusStore is used when: SparkContext is requested to < > ConsoleProgressBar is requested to xref:ROOT:spark-sparkcontext-ConsoleProgressBar.adoc#refresh[refresh] SharedState (Spark SQL) is requested for a SQLAppStatusStore == [[uiWebUrl]] Requesting URL of web UI -- uiWebUrl Method","title":"statusStore: AppStatusStore"},{"location":"SparkContext/#source-scala_48","text":"","title":"[source, scala]"},{"location":"SparkContext/#uiweburl-optionstring","text":"uiWebUrl requests the link:spark-SparkContext-creating-instance-internals.adoc#_ui[SparkUI] for link:spark-webui-WebUI.adoc#webUrl[webUrl]. == [[maxNumConcurrentTasks]] maxNumConcurrentTasks Method","title":"uiWebUrl: Option[String]"},{"location":"SparkContext/#source-scala_49","text":"","title":"[source, scala]"},{"location":"SparkContext/#maxnumconcurrenttasks-int","text":"maxNumConcurrentTasks simply requests the < > for the xref:scheduler:SchedulerBackend.adoc#maxNumConcurrentTasks[maximum number of tasks that can be launched concurrently]. NOTE: maxNumConcurrentTasks is used exclusively when DAGScheduler is requested to xref:scheduler:DAGScheduler.adoc#checkBarrierStageWithNumSlots[checkBarrierStageWithNumSlots]. == [[createTaskScheduler]] Creating SchedulerBackend and TaskScheduler -- createTaskScheduler Internal Factory Method","title":"maxNumConcurrentTasks(): Int"},{"location":"SparkContext/#source-scala_50","text":"createTaskScheduler( sc: SparkContext, master: String, deployMode: String): (SchedulerBackend, TaskScheduler) createTaskScheduler creates the xref:scheduler:SchedulerBackend.adoc[SchedulerBackend] and the xref:scheduler:TaskScheduler.adoc[TaskScheduler] for the given master URL and deployment mode. .SparkContext creates Task Scheduler and Scheduler Backend image::diagrams/sparkcontext-createtaskscheduler.png[align=\"center\"] Internally, createTaskScheduler branches off per the given master URL (link:spark-deployment-environments.adoc#master-urls[master URL]) to select the requested implementations. createTaskScheduler understands the following master URLs: local - local mode with 1 thread only local[n] or local[*] - local mode with n threads local[n, m] or local[*, m] -- local mode with n threads and m number of failures spark://hostname:port for Spark Standalone local-cluster[n, m, z] -- local cluster with n workers, m cores per worker, and z memory per worker any other URL is passed to < getClusterManager to load an external cluster manager>>. CAUTION: FIXME == [[environment-variables]] Environment Variables .Environment Variables [cols=\"1,1,2\",options=\"header\",width=\"100%\"] |=== | Environment Variable | Default Value | Description | [[SPARK_EXECUTOR_MEMORY]] SPARK_EXECUTOR_MEMORY | 1024 | Amount of memory to allocate for a Spark executor in MB. See xref:executor:Executor.adoc#memory[Executor Memory]. [[SPARK_USER]] SPARK_USER The user who is running SparkContext. Available later as < >. === == [[postEnvironmentUpdate]] Posting SparkListenerEnvironmentUpdate Event","title":"[source, scala]"},{"location":"SparkContext/#source-scala_51","text":"","title":"[source, scala]"},{"location":"SparkContext/#postenvironmentupdate-unit","text":"postEnvironmentUpdate ...FIXME NOTE: postEnvironmentUpdate is used when SparkContext is < >, and requested to < > and < >. == [[addJar-internals]] addJar Method","title":"postEnvironmentUpdate(): Unit"},{"location":"SparkContext/#source-scala_52","text":"","title":"[source, scala]"},{"location":"SparkContext/#addjarpath-string-unit_1","text":"addJar ...FIXME NOTE: addJar is used when...FIXME == [[runApproximateJob]] Running Approximate Job","title":"addJar(path: String): Unit"},{"location":"SparkContext/#source-scala_53","text":"runApproximateJob T, U, R : PartialResult[R] runApproximateJob...FIXME runApproximateJob is used when: DoubleRDDFunctions is requested to meanApprox and sumApprox RDD is requested to countApprox and countByValueApprox == [[killTaskAttempt]] Killing Task","title":"[source, scala]"},{"location":"SparkContext/#source-scala_54","text":"killTaskAttempt( taskId: Long, interruptThread: Boolean = true, reason: String = \"killed via SparkContext.killTaskAttempt\"): Boolean killTaskAttempt requests the < > to xref:scheduler:DAGScheduler.adoc#killTaskAttempt[kill a task]. == [[checkpointFile]] checkpointFile Internal Method","title":"[source, scala]"},{"location":"SparkContext/#source-scala_55","text":"checkpointFile T: ClassTag : RDD[T] checkpointFile...FIXME == [[logging]] Logging Enable ALL logging level for org.apache.spark.SparkContext logger to see what happens inside. Add the following line to conf/log4j.properties :","title":"[source, scala]"},{"location":"SparkContext/#sourceplaintext_1","text":"","title":"[source,plaintext]"},{"location":"SparkContext/#log4jloggerorgapachesparksparkcontextall","text":"Refer to xref:ROOT:spark-logging.adoc[Logging]. == [[internal-properties]] Internal Properties === [[checkpointDir]] Checkpoint Directory","title":"log4j.logger.org.apache.spark.SparkContext=ALL"},{"location":"SparkContext/#sourcescala_2","text":"","title":"[source,scala]"},{"location":"SparkContext/#checkpointdir-optionstring-none","text":"checkpointDir is...FIXME === [[persistentRdds]] persistentRdds Lookup Table Lookup table of persistent/cached RDDs per their ids. Used when SparkContext is requested to: < > < > < > < > === [[stopped]] stopped Flag Flag that says whether...FIXME ( true ) or not ( false ) === [[_taskScheduler]] TaskScheduler xref:scheduler:TaskScheduler.adoc[TaskScheduler]","title":"checkpointDir: Option[String] = None"},{"location":"SparkEnv/","text":"= SparkEnv SparkEnv is the Spark Execution Environment with the < > of Apache Spark (that interact with each other to establish a distributed computing platform for a Spark application). SparkEnv are two separate execution environments for the < > and < >. == [[services]] Core Services [cols=\"40m,60\",options=\"header\",width=\"100%\"] |=== | Property | Service | [[blockManager]] blockManager | xref:storage:BlockManager.adoc[BlockManager] | [[broadcastManager]] broadcastManager | xref:core:BroadcastManager.adoc[] | [[closureSerializer]] closureSerializer | xref:serializer:Serializer.adoc[Serializer] | [[conf]] conf | xref:ROOT:SparkConf.adoc[SparkConf] | [[mapOutputTracker]] mapOutputTracker | xref:scheduler:MapOutputTracker.adoc[MapOutputTracker] | [[memoryManager]] memoryManager | xref:memory:MemoryManager.adoc[MemoryManager] | [[metricsSystem]] metricsSystem | xref:metrics:spark-metrics-MetricsSystem.adoc[MetricsSystem] | [[outputCommitCoordinator]] outputCommitCoordinator | xref:scheduler:OutputCommitCoordinator.adoc[OutputCommitCoordinator] | [[rpcEnv]] rpcEnv | xref:rpc:RpcEnv.adoc[] | [[securityManager]] securityManager | SecurityManager | [[serializer]] serializer | xref:serializer:Serializer.adoc[Serializer] | [[serializerManager]] serializerManager | xref:serializer:SerializerManager.adoc[SerializerManager] | [[shuffleManager]] shuffleManager | xref:shuffle:ShuffleManager.adoc[ShuffleManager] |=== == [[creating-instance]] Creating Instance SparkEnv takes the following to be created: [[executorId]] String < > < > < > < > < > < > < > < > < > < > < > < > < > SparkEnv is created when: xref:ROOT:spark-SparkContext-creating-instance-internals.adoc[SparkContext] is created (for the < >) CoarseGrainedExecutorBackend is requested to xref:executor:CoarseGrainedExecutorBackend.adoc#run[run] (for < >) MesosExecutorBackend is requested to xref:spark-on-mesos:spark-executor-backends-MesosExecutorBackend.adoc#registered[registered] (for < >) == [[get]] Accessing SparkEnv [source, scala] \u00b6 get: SparkEnv \u00b6 get returns the SparkEnv on the driver and executors. [source, scala] \u00b6 import org.apache.spark.SparkEnv assert(SparkEnv.get.isInstanceOf[SparkEnv]) == [[create]] Creating \"Base\" SparkEnv (for Driver and Executors) [source, scala] \u00b6 create( conf: SparkConf, executorId: String, bindAddress: String, advertiseAddress: String, port: Option[Int], isLocal: Boolean, numUsableCores: Int, ioEncryptionKey: Option[Array[Byte]], listenerBus: LiveListenerBus = null, mockOutputCommitCoordinator: Option[OutputCommitCoordinator] = None): SparkEnv create is an utility to create the \"base\" SparkEnv (that is \"enhanced\" for the driver and executors later on). .create's Input Arguments and Their Usage [cols=\"1,2\",options=\"header\",width=\"100%\"] |=== | Input Argument | Usage | bindAddress | Used to create xref:rpc:index.adoc[RpcEnv] and xref:storage:NettyBlockTransferService.adoc#creating-instance[NettyBlockTransferService]. | advertiseAddress | Used to create xref:rpc:index.adoc[RpcEnv] and xref:storage:NettyBlockTransferService.adoc#creating-instance[NettyBlockTransferService]. | numUsableCores | Used to create xref:memory:MemoryManager.adoc[MemoryManager], xref:storage:NettyBlockTransferService.adoc#creating-instance[NettyBlockTransferService] and xref:storage:BlockManager.adoc#creating-instance[BlockManager]. |=== [[create-Serializer]] create creates a Serializer (based on < > setting). You should see the following DEBUG message in the logs: DEBUG SparkEnv: Using serializer: [serializer] [[create-closure-Serializer]] create creates a closure Serializer (based on < >). [[ShuffleManager]][[create-ShuffleManager]] create creates a xref:shuffle:ShuffleManager.adoc[ShuffleManager] given the value of xref:ROOT:configuration-properties.adoc#spark.shuffle.manager[spark.shuffle.manager] configuration property. [[MemoryManager]][[create-MemoryManager]] create creates a xref:memory:MemoryManager.adoc[MemoryManager] based on xref:ROOT:configuration-properties.adoc#spark.memory.useLegacyMode[spark.memory.useLegacyMode] setting (with xref:memory:UnifiedMemoryManager.adoc[UnifiedMemoryManager] being the default and numCores the input numUsableCores ). [[NettyBlockTransferService]][[create-NettyBlockTransferService]] create creates a xref:storage:NettyBlockTransferService.adoc#creating-instance[NettyBlockTransferService] with the following ports: link:spark-driver.adoc#spark_driver_blockManager_port[spark.driver.blockManager.port] for the driver (default: 0 ) xref:storage:BlockManager.adoc#spark_blockManager_port[spark.blockManager.port] for an executor (default: 0 ) NOTE: create uses the NettyBlockTransferService to < >. CAUTION: FIXME A picture with SparkEnv, NettyBlockTransferService and the ports \"armed\". [[BlockManagerMaster]][[create-BlockManagerMaster]] create creates a xref:storage:BlockManagerMaster.adoc#creating-instance[BlockManagerMaster] object with the BlockManagerMaster RPC endpoint reference (by < > and xref:storage:BlockManagerMasterEndpoint.adoc[]), the input xref:ROOT:SparkConf.adoc[SparkConf], and the input isDriver flag. .Creating BlockManager for the Driver image::sparkenv-driver-blockmanager.png[align=\"center\"] NOTE: create registers the BlockManagerMaster RPC endpoint for the driver and looks it up for executors. .Creating BlockManager for Executor image::sparkenv-executor-blockmanager.png[align=\"center\"] [[BlockManager]][[create-BlockManager]] create creates a xref:storage:BlockManager.adoc#creating-instance[BlockManager] (using the above < >, < > and other services). create creates a xref:core:BroadcastManager.adoc[]. [[MapOutputTracker]][[create-MapOutputTracker]] create creates a xref:scheduler:MapOutputTrackerMaster.adoc[MapOutputTrackerMaster] or xref:scheduler:MapOutputTrackerWorker.adoc[MapOutputTrackerWorker] for the driver and executors, respectively. NOTE: The choice of the real implementation of xref:scheduler:MapOutputTracker.adoc[MapOutputTracker] is based on whether the input executorId is driver or not. [[MapOutputTrackerMasterEndpoint]][[create-MapOutputTrackerMasterEndpoint]] create < RpcEndpoint >> as MapOutputTracker . It registers xref:scheduler:MapOutputTrackerMasterEndpoint.adoc[MapOutputTrackerMasterEndpoint] on the driver and creates a RPC endpoint reference on executors. The RPC endpoint reference gets assigned as the xref:scheduler:MapOutputTracker.adoc#trackerEndpoint[MapOutputTracker RPC endpoint]. CAUTION: FIXME [[create-CacheManager]] It creates a CacheManager. [[create-MetricsSystem]] It creates a MetricsSystem for a driver and a worker separately. It initializes userFiles temporary directory used for downloading dependencies for a driver while this is the executor's current working directory for an executor. [[create-OutputCommitCoordinator]] An OutputCommitCoordinator is created. create is used when SparkEnv is requested for the SparkEnv for the < > and < >. == [[registerOrLookupEndpoint]] Registering or Looking up RPC Endpoint by Name [source, scala] \u00b6 registerOrLookupEndpoint( name: String, endpointCreator: => RpcEndpoint) registerOrLookupEndpoint registers or looks up a RPC endpoint by name . If called from the driver, you should see the following INFO message in the logs: Registering [name] And the RPC endpoint is registered in the RPC environment. Otherwise, it obtains a RPC endpoint reference by name . == [[createDriverEnv]] Creating SparkEnv for Driver [source, scala] \u00b6 createDriverEnv( conf: SparkConf, isLocal: Boolean, listenerBus: LiveListenerBus, numCores: Int, mockOutputCommitCoordinator: Option[OutputCommitCoordinator] = None): SparkEnv createDriverEnv creates a SparkEnv execution environment for the driver. .Spark Environment for driver image::sparkenv-driver.png[align=\"center\"] createDriverEnv accepts an instance of xref:ROOT:SparkConf.adoc[SparkConf], link:spark-deployment-environments.adoc[whether it runs in local mode or not], xref:scheduler:LiveListenerBus.adoc[], the number of cores to use for execution in local mode or 0 otherwise, and a xref:scheduler:OutputCommitCoordinator.adoc[OutputCommitCoordinator] (default: none). createDriverEnv ensures that link:spark-driver.adoc#spark_driver_host[spark.driver.host] and link:spark-driver.adoc#spark_driver_port[spark.driver.port] settings are defined. It then passes the call straight on to the < > (with driver executor id, isDriver enabled, and the input parameters). NOTE: createDriverEnv is exclusively used by link:spark-SparkContext-creating-instance-internals.adoc#createSparkEnv[SparkContext to create a SparkEnv] (while a xref:ROOT:SparkContext.adoc#creating-instance[SparkContext is being created for the driver]). == [[createExecutorEnv]] Creating SparkEnv for Executor [source, scala] \u00b6 createExecutorEnv( conf: SparkConf, executorId: String, hostname: String, numCores: Int, ioEncryptionKey: Option[Array[Byte]], isLocal: Boolean): SparkEnv createExecutorEnv creates an executor's (execution) environment that is the Spark execution environment for an executor. .Spark Environment for executor image::sparkenv-executor.png[align=\"center\"] NOTE: createExecutorEnv is a private[spark] method. createExecutorEnv simply < > (passing in all the input parameters) and < >. NOTE: The number of cores numCores is configured using --cores command-line option of CoarseGrainedExecutorBackend and is specific to a cluster manager. NOTE: createExecutorEnv is used when xref:executor:CoarseGrainedExecutorBackend.adoc#run[ CoarseGrainedExecutorBackend runs] and link:spark-executor-backends-MesosExecutorBackend.adoc#registered[ MesosExecutorBackend registers a Spark executor]. == [[stop]] Stopping SparkEnv [source, scala] \u00b6 stop(): Unit \u00b6 stop checks < > internal flag and does nothing when enabled already. Otherwise, stop turns isStopped flag on, stops all pythonWorkers and requests the following services to stop: xref:scheduler:MapOutputTracker.adoc#stop[MapOutputTracker] xref:shuffle:ShuffleManager.adoc#stop[ShuffleManager] xref:core:BroadcastManager.adoc#stop[BroadcastManager] xref:storage:BlockManager.adoc#stop[BlockManager] xref:storage:BlockManagerMaster.adoc#stop[BlockManagerMaster] link:spark-metrics-MetricsSystem.adoc#stop[MetricsSystem] xref:scheduler:OutputCommitCoordinator.adoc#stop[OutputCommitCoordinator] stop xref:rpc:index.adoc#shutdown[requests RpcEnv to shut down] and xref:rpc:index.adoc#awaitTermination[waits till it terminates]. Only on the driver, stop deletes the < >. You can see the following WARN message in the logs if the deletion fails. Exception while deleting Spark temp dir: [path] NOTE: stop is used when xref:ROOT:SparkContext.adoc#stop[ SparkContext stops] (on the driver) and xref:executor:Executor.adoc#stop[ Executor stops]. == [[set]] set Method [source, scala] \u00b6 set(e: SparkEnv): Unit \u00b6 set saves the input SparkEnv to < > internal registry (as the default SparkEnv). NOTE: set is used when...FIXME == [[environmentDetails]] environmentDetails Utility [source, scala] \u00b6 environmentDetails( conf: SparkConf, schedulingMode: String, addedJars: Seq[String], addedFiles: Seq[String]): Map[String, Seq[(String, String)]] environmentDetails...FIXME environmentDetails is used when SparkContext is requested to xref:ROOT:SparkContext.adoc#postEnvironmentUpdate[post a SparkListenerEnvironmentUpdate event]. == [[logging]] Logging Enable ALL logging level for org.apache.spark.SparkEnv logger to see what happens inside. Add the following line to conf/log4j.properties : [source] \u00b6 log4j.logger.org.apache.spark.SparkEnv=ALL \u00b6 Refer to xref:ROOT:spark-logging.adoc[Logging]. == [[internal-properties]] Internal Properties [cols=\"30m,70\",options=\"header\",width=\"100%\"] |=== | Name | Description | isStopped | [[isStopped]] Used to mark SparkEnv stopped Default: false | driverTmpDir | [[driverTmpDir]] |===","title":"SparkEnv"},{"location":"SparkEnv/#source-scala","text":"","title":"[source, scala]"},{"location":"SparkEnv/#get-sparkenv","text":"get returns the SparkEnv on the driver and executors.","title":"get: SparkEnv"},{"location":"SparkEnv/#source-scala_1","text":"import org.apache.spark.SparkEnv assert(SparkEnv.get.isInstanceOf[SparkEnv]) == [[create]] Creating \"Base\" SparkEnv (for Driver and Executors)","title":"[source, scala]"},{"location":"SparkEnv/#source-scala_2","text":"create( conf: SparkConf, executorId: String, bindAddress: String, advertiseAddress: String, port: Option[Int], isLocal: Boolean, numUsableCores: Int, ioEncryptionKey: Option[Array[Byte]], listenerBus: LiveListenerBus = null, mockOutputCommitCoordinator: Option[OutputCommitCoordinator] = None): SparkEnv create is an utility to create the \"base\" SparkEnv (that is \"enhanced\" for the driver and executors later on). .create's Input Arguments and Their Usage [cols=\"1,2\",options=\"header\",width=\"100%\"] |=== | Input Argument | Usage | bindAddress | Used to create xref:rpc:index.adoc[RpcEnv] and xref:storage:NettyBlockTransferService.adoc#creating-instance[NettyBlockTransferService]. | advertiseAddress | Used to create xref:rpc:index.adoc[RpcEnv] and xref:storage:NettyBlockTransferService.adoc#creating-instance[NettyBlockTransferService]. | numUsableCores | Used to create xref:memory:MemoryManager.adoc[MemoryManager], xref:storage:NettyBlockTransferService.adoc#creating-instance[NettyBlockTransferService] and xref:storage:BlockManager.adoc#creating-instance[BlockManager]. |=== [[create-Serializer]] create creates a Serializer (based on < > setting). You should see the following DEBUG message in the logs: DEBUG SparkEnv: Using serializer: [serializer] [[create-closure-Serializer]] create creates a closure Serializer (based on < >). [[ShuffleManager]][[create-ShuffleManager]] create creates a xref:shuffle:ShuffleManager.adoc[ShuffleManager] given the value of xref:ROOT:configuration-properties.adoc#spark.shuffle.manager[spark.shuffle.manager] configuration property. [[MemoryManager]][[create-MemoryManager]] create creates a xref:memory:MemoryManager.adoc[MemoryManager] based on xref:ROOT:configuration-properties.adoc#spark.memory.useLegacyMode[spark.memory.useLegacyMode] setting (with xref:memory:UnifiedMemoryManager.adoc[UnifiedMemoryManager] being the default and numCores the input numUsableCores ). [[NettyBlockTransferService]][[create-NettyBlockTransferService]] create creates a xref:storage:NettyBlockTransferService.adoc#creating-instance[NettyBlockTransferService] with the following ports: link:spark-driver.adoc#spark_driver_blockManager_port[spark.driver.blockManager.port] for the driver (default: 0 ) xref:storage:BlockManager.adoc#spark_blockManager_port[spark.blockManager.port] for an executor (default: 0 ) NOTE: create uses the NettyBlockTransferService to < >. CAUTION: FIXME A picture with SparkEnv, NettyBlockTransferService and the ports \"armed\". [[BlockManagerMaster]][[create-BlockManagerMaster]] create creates a xref:storage:BlockManagerMaster.adoc#creating-instance[BlockManagerMaster] object with the BlockManagerMaster RPC endpoint reference (by < > and xref:storage:BlockManagerMasterEndpoint.adoc[]), the input xref:ROOT:SparkConf.adoc[SparkConf], and the input isDriver flag. .Creating BlockManager for the Driver image::sparkenv-driver-blockmanager.png[align=\"center\"] NOTE: create registers the BlockManagerMaster RPC endpoint for the driver and looks it up for executors. .Creating BlockManager for Executor image::sparkenv-executor-blockmanager.png[align=\"center\"] [[BlockManager]][[create-BlockManager]] create creates a xref:storage:BlockManager.adoc#creating-instance[BlockManager] (using the above < >, < > and other services). create creates a xref:core:BroadcastManager.adoc[]. [[MapOutputTracker]][[create-MapOutputTracker]] create creates a xref:scheduler:MapOutputTrackerMaster.adoc[MapOutputTrackerMaster] or xref:scheduler:MapOutputTrackerWorker.adoc[MapOutputTrackerWorker] for the driver and executors, respectively. NOTE: The choice of the real implementation of xref:scheduler:MapOutputTracker.adoc[MapOutputTracker] is based on whether the input executorId is driver or not. [[MapOutputTrackerMasterEndpoint]][[create-MapOutputTrackerMasterEndpoint]] create < RpcEndpoint >> as MapOutputTracker . It registers xref:scheduler:MapOutputTrackerMasterEndpoint.adoc[MapOutputTrackerMasterEndpoint] on the driver and creates a RPC endpoint reference on executors. The RPC endpoint reference gets assigned as the xref:scheduler:MapOutputTracker.adoc#trackerEndpoint[MapOutputTracker RPC endpoint]. CAUTION: FIXME [[create-CacheManager]] It creates a CacheManager. [[create-MetricsSystem]] It creates a MetricsSystem for a driver and a worker separately. It initializes userFiles temporary directory used for downloading dependencies for a driver while this is the executor's current working directory for an executor. [[create-OutputCommitCoordinator]] An OutputCommitCoordinator is created. create is used when SparkEnv is requested for the SparkEnv for the < > and < >. == [[registerOrLookupEndpoint]] Registering or Looking up RPC Endpoint by Name","title":"[source, scala]"},{"location":"SparkEnv/#source-scala_3","text":"registerOrLookupEndpoint( name: String, endpointCreator: => RpcEndpoint) registerOrLookupEndpoint registers or looks up a RPC endpoint by name . If called from the driver, you should see the following INFO message in the logs: Registering [name] And the RPC endpoint is registered in the RPC environment. Otherwise, it obtains a RPC endpoint reference by name . == [[createDriverEnv]] Creating SparkEnv for Driver","title":"[source, scala]"},{"location":"SparkEnv/#source-scala_4","text":"createDriverEnv( conf: SparkConf, isLocal: Boolean, listenerBus: LiveListenerBus, numCores: Int, mockOutputCommitCoordinator: Option[OutputCommitCoordinator] = None): SparkEnv createDriverEnv creates a SparkEnv execution environment for the driver. .Spark Environment for driver image::sparkenv-driver.png[align=\"center\"] createDriverEnv accepts an instance of xref:ROOT:SparkConf.adoc[SparkConf], link:spark-deployment-environments.adoc[whether it runs in local mode or not], xref:scheduler:LiveListenerBus.adoc[], the number of cores to use for execution in local mode or 0 otherwise, and a xref:scheduler:OutputCommitCoordinator.adoc[OutputCommitCoordinator] (default: none). createDriverEnv ensures that link:spark-driver.adoc#spark_driver_host[spark.driver.host] and link:spark-driver.adoc#spark_driver_port[spark.driver.port] settings are defined. It then passes the call straight on to the < > (with driver executor id, isDriver enabled, and the input parameters). NOTE: createDriverEnv is exclusively used by link:spark-SparkContext-creating-instance-internals.adoc#createSparkEnv[SparkContext to create a SparkEnv] (while a xref:ROOT:SparkContext.adoc#creating-instance[SparkContext is being created for the driver]). == [[createExecutorEnv]] Creating SparkEnv for Executor","title":"[source, scala]"},{"location":"SparkEnv/#source-scala_5","text":"createExecutorEnv( conf: SparkConf, executorId: String, hostname: String, numCores: Int, ioEncryptionKey: Option[Array[Byte]], isLocal: Boolean): SparkEnv createExecutorEnv creates an executor's (execution) environment that is the Spark execution environment for an executor. .Spark Environment for executor image::sparkenv-executor.png[align=\"center\"] NOTE: createExecutorEnv is a private[spark] method. createExecutorEnv simply < > (passing in all the input parameters) and < >. NOTE: The number of cores numCores is configured using --cores command-line option of CoarseGrainedExecutorBackend and is specific to a cluster manager. NOTE: createExecutorEnv is used when xref:executor:CoarseGrainedExecutorBackend.adoc#run[ CoarseGrainedExecutorBackend runs] and link:spark-executor-backends-MesosExecutorBackend.adoc#registered[ MesosExecutorBackend registers a Spark executor]. == [[stop]] Stopping SparkEnv","title":"[source, scala]"},{"location":"SparkEnv/#source-scala_6","text":"","title":"[source, scala]"},{"location":"SparkEnv/#stop-unit","text":"stop checks < > internal flag and does nothing when enabled already. Otherwise, stop turns isStopped flag on, stops all pythonWorkers and requests the following services to stop: xref:scheduler:MapOutputTracker.adoc#stop[MapOutputTracker] xref:shuffle:ShuffleManager.adoc#stop[ShuffleManager] xref:core:BroadcastManager.adoc#stop[BroadcastManager] xref:storage:BlockManager.adoc#stop[BlockManager] xref:storage:BlockManagerMaster.adoc#stop[BlockManagerMaster] link:spark-metrics-MetricsSystem.adoc#stop[MetricsSystem] xref:scheduler:OutputCommitCoordinator.adoc#stop[OutputCommitCoordinator] stop xref:rpc:index.adoc#shutdown[requests RpcEnv to shut down] and xref:rpc:index.adoc#awaitTermination[waits till it terminates]. Only on the driver, stop deletes the < >. You can see the following WARN message in the logs if the deletion fails. Exception while deleting Spark temp dir: [path] NOTE: stop is used when xref:ROOT:SparkContext.adoc#stop[ SparkContext stops] (on the driver) and xref:executor:Executor.adoc#stop[ Executor stops]. == [[set]] set Method","title":"stop(): Unit"},{"location":"SparkEnv/#source-scala_7","text":"","title":"[source, scala]"},{"location":"SparkEnv/#sete-sparkenv-unit","text":"set saves the input SparkEnv to < > internal registry (as the default SparkEnv). NOTE: set is used when...FIXME == [[environmentDetails]] environmentDetails Utility","title":"set(e: SparkEnv): Unit"},{"location":"SparkEnv/#source-scala_8","text":"environmentDetails( conf: SparkConf, schedulingMode: String, addedJars: Seq[String], addedFiles: Seq[String]): Map[String, Seq[(String, String)]] environmentDetails...FIXME environmentDetails is used when SparkContext is requested to xref:ROOT:SparkContext.adoc#postEnvironmentUpdate[post a SparkListenerEnvironmentUpdate event]. == [[logging]] Logging Enable ALL logging level for org.apache.spark.SparkEnv logger to see what happens inside. Add the following line to conf/log4j.properties :","title":"[source, scala]"},{"location":"SparkEnv/#source","text":"","title":"[source]"},{"location":"SparkEnv/#log4jloggerorgapachesparksparkenvall","text":"Refer to xref:ROOT:spark-logging.adoc[Logging]. == [[internal-properties]] Internal Properties [cols=\"30m,70\",options=\"header\",width=\"100%\"] |=== | Name | Description | isStopped | [[isStopped]] Used to mark SparkEnv stopped Default: false | driverTmpDir | [[driverTmpDir]] |===","title":"log4j.logger.org.apache.spark.SparkEnv=ALL"},{"location":"overview/","text":"Apache Spark \u00b6 Apache Spark is an open-source distributed general-purpose cluster computing framework with (mostly) in-memory data processing engine that can do ETL, analytics, machine learning and graph processing on large volumes of data at rest (batch processing) or in motion (streaming processing) with rich concise high-level APIs for the programming languages: Scala, Python, Java, R, and SQL. You could also describe Spark as a distributed, data processing engine for batch and streaming modes featuring SQL queries, graph processing, and machine learning. In contrast to Hadoop\u2019s two-stage disk-based MapReduce computation engine, Spark's multi-stage (mostly) in-memory computing engine allows for running most computations in memory, and hence most of the time provides better performance for certain applications, e.g. iterative algorithms or interactive data mining (read Spark officially sets a new record in large-scale sorting ). Spark aims at speed, ease of use, extensibility and interactive analytics. Spark is a distributed platform for executing complex multi-stage applications , like machine learning algorithms , and interactive ad hoc queries . Spark provides an efficient abstraction for in-memory cluster computing called Resilient Distributed Dataset . Using Spark Application Frameworks, Spark simplifies access to machine learning and predictive analytics at scale. Spark is mainly written in http://scala-lang.org/[Scala ], but provides developer API for languages like Java, Python, and R. If you have large amounts of data that requires low latency processing that a typical MapReduce program cannot provide, Spark is a viable alternative. Access any data type across any data source. Huge demand for storage and data processing. The Apache Spark project is an umbrella for https://jaceklaskowski.gitbooks.io/mastering-spark-sql/[SQL ] (with Datasets), https://jaceklaskowski.gitbooks.io/spark-structured-streaming/[streaming ], http://spark.apache.org/mllib/[machine learning] (pipelines) and http://spark.apache.org/graphx/[graph ] processing engines built on top of the Spark Core. You can run them all in a single application using a consistent API. Spark runs locally as well as in clusters, on-premises or in cloud. It runs on top of Hadoop YARN, Apache Mesos, standalone or in the cloud (Amazon EC2 or IBM Bluemix). Apache Spark's https://jaceklaskowski.gitbooks.io/spark-structured-streaming/[Structured Streaming] and https://jaceklaskowski.gitbooks.io/mastering-spark-sql/[SQL ] programming models with MLlib and GraphX make it easier for developers and data scientists to build applications that exploit machine learning and graph analytics. At a high level, any Spark application creates RDDs out of some input, run xref:rdd:index.adoc[(lazy) transformations] of these RDDs to some other form (shape), and finally perform xref:rdd:index.adoc[actions] to collect or store data. Not much, huh? You can look at Spark from programmer's, data engineer's and administrator's point of view. And to be honest, all three types of people will spend quite a lot of their time with Spark to finally reach the point where they exploit all the available features. Programmers use language-specific APIs (and work at the level of RDDs using transformations and actions), data engineers use higher-level abstractions like DataFrames or Pipelines APIs or external tools (that connect to Spark), and finally it all can only be possible to run because administrators set up Spark clusters to deploy Spark applications to. It is Spark's goal to be a general-purpose computing platform with various specialized applications frameworks on top of a single unified engine. NOTE: When you hear \"Apache Spark\" it can be two things -- the Spark engine aka Spark Core or the Apache Spark open source project which is an \"umbrella\" term for Spark Core and the accompanying Spark Application Frameworks, i.e. Spark SQL, link:spark-streaming/spark-streaming.adoc[Spark Streaming], link:spark-mllib/spark-mllib.adoc[Spark MLlib] and link:spark-graphx.adoc[Spark GraphX] that sit on top of Spark Core and the main data abstraction in Spark called xref:rdd:index.adoc[RDD - Resilient Distributed Dataset]. == [[why-spark]] Why Spark Let's list a few of the many reasons for Spark. We are doing it first, and then comes the overview that lends a more technical helping hand. === Easy to Get Started Spark offers link:spark-shell.adoc[spark-shell] that makes for a very easy head start to writing and running Spark applications on the command line on your laptop. You could then use link:spark-standalone.adoc[Spark Standalone] built-in cluster manager to deploy your Spark applications to a production-grade cluster to run on a full dataset. === Unified Engine for Diverse Workloads As said by Matei Zaharia - the author of Apache Spark - in https://youtu.be/49Hr5xZyTEA[Introduction to AmpLab Spark Internals video] (quoting with few changes): One of the Spark project goals was to deliver a platform that supports a very wide array of diverse workflows - not only MapReduce batch jobs (there were available in Hadoop already at that time), but also iterative computations like graph algorithms or Machine Learning. And also different scales of workloads from sub-second interactive jobs to jobs that run for many hours. Spark combines batch, interactive, and streaming workloads under one rich concise API. Spark supports near real-time streaming workloads via link:spark-streaming/spark-streaming.adoc[Spark Streaming] application framework. ETL workloads and Analytics workloads are different, however Spark attempts to offer a unified platform for a wide variety of workloads. Graph and Machine Learning algorithms are iterative by nature and less saves to disk or transfers over network means better performance. There is also support for interactive workloads using Spark shell. You should watch the video https://youtu.be/SxAxAhn-BDU[What is Apache Spark?] by Mike Olson, Chief Strategy Officer and Co-Founder at Cloudera, who provides a very exceptional overview of Apache Spark, its rise in popularity in the open source community, and how Spark is primed to replace MapReduce as the general processing engine in Hadoop. === Leverages the Best in distributed batch data processing When you think about distributed batch data processing , link:varia/spark-hadoop.adoc[Hadoop] naturally comes to mind as a viable solution. Spark draws many ideas out of Hadoop MapReduce. They work together well - Spark on YARN and HDFS - while improving on the performance and simplicity of the distributed computing engine. For many, Spark is Hadoop++, i.e. MapReduce done in a better way. And it should not come as a surprise, without Hadoop MapReduce (its advances and deficiencies), Spark would not have been born at all. === RDD - Distributed Parallel Scala Collections As a Scala developer, you may find Spark's RDD API very similar (if not identical) to http://www.scala-lang.org/docu/files/collections-api/collections.html[Scala's Collections API]. It is also exposed in Java, Python and R (as well as SQL, i.e. SparkSQL, in a sense). So, when you have a need for distributed Collections API in Scala, Spark with RDD API should be a serious contender. === [[rich-standard-library]] Rich Standard Library Not only can you use map and reduce (as in Hadoop MapReduce jobs) in Spark, but also a vast array of other higher-level operators to ease your Spark queries and application development. It expanded on the available computation styles beyond the only map-and-reduce available in Hadoop MapReduce. === Unified development and deployment environment for all Regardless of the Spark tools you use - the Spark API for the many programming languages supported - Scala, Java, Python, R, or link:spark-shell.adoc[the Spark shell], or the many Spark Application Frameworks leveraging the concept of xref:rdd:index.adoc[RDD], i.e. Spark SQL, link:spark-streaming/spark-streaming.adoc[Spark Streaming], link:spark-mllib/spark-mllib.adoc[Spark MLlib] and link:spark-graphx.adoc[Spark GraphX], you still use the same development and deployment environment to for large data sets to yield a result, be it a prediction (link:spark-mllib/spark-mllib.adoc[Spark MLlib]), a structured data queries (Spark SQL) or just a large distributed batch (Spark Core) or streaming (Spark Streaming) computation. It's also very productive of Spark that teams can exploit the different skills the team members have acquired so far. Data analysts, data scientists, Python programmers, or Java, or Scala, or R, can all use the same Spark platform using tailor-made API. It makes for bringing skilled people with their expertise in different programming languages together to a Spark project. === Interactive Exploration / Exploratory Analytics It is also called ad hoc queries . Using link:spark-shell.adoc[the Spark shell] you can execute computations to process large amount of data ( The Big Data ). It's all interactive and very useful to explore the data before final production release. Also, using the Spark shell you can access any link:spark-cluster.adoc[Spark cluster] as if it was your local machine. Just point the Spark shell to a 20-node of 10TB RAM memory in total (using --master ) and use all the components (and their abstractions) like Spark SQL, Spark MLlib, link:spark-streaming/spark-streaming.adoc[Spark Streaming], and Spark GraphX. Depending on your needs and skills, you may see a better fit for SQL vs programming APIs or apply machine learning algorithms (Spark MLlib) from data in graph data structures (Spark GraphX). === Single Environment Regardless of which programming language you are good at, be it Scala, Java, Python, R or SQL, you can use the same single clustered runtime environment for prototyping, ad hoc queries, and deploying your applications leveraging the many ingestion data points offered by the Spark platform. You can be as low-level as using RDD API directly or leverage higher-level APIs of Spark SQL (Datasets), Spark MLlib (ML Pipelines), Spark GraphX (Graphs) or link:spark-streaming/spark-streaming.adoc[Spark Streaming] (DStreams). Or use them all in a single application. The single programming model and execution engine for different kinds of workloads simplify development and deployment architectures. === Data Integration Toolkit with Rich Set of Supported Data Sources Spark can read from many types of data sources -- relational, NoSQL, file systems, etc. -- using many types of data formats - Parquet, Avro, CSV, JSON. Both, input and output data sources, allow programmers and data engineers use Spark as the platform with the large amount of data that is read from or saved to for processing, interactively (using Spark shell) or in applications. === Tools unavailable then, at your fingertips now As much and often as it's recommended http://c2.com/cgi/wiki?PickTheRightToolForTheJob[to pick the right tool for the job], it's not always feasible. Time, personal preference, operating system you work on are all factors to decide what is right at a time (and using a hammer can be a reasonable choice). Spark embraces many concepts in a single unified development and runtime environment. Machine learning that is so tool- and feature-rich in Python, e.g. SciKit library, can now be used by Scala developers (as Pipeline API in Spark MLlib or calling pipe() ). DataFrames from R are available in Scala, Java, Python, R APIs. Single node computations in machine learning algorithms are migrated to their distributed versions in Spark MLlib. This single platform gives plenty of opportunities for Python, Scala, Java, and R programmers as well as data engineers (SparkR) and scientists (using proprietary enterprise data warehouses with link:spark-sql-thrift-server.adoc[Thrift JDBC/ODBC Server] in Spark SQL). Mind the proverb https://en.wiktionary.org/wiki/if_all_you_have_is_a_hammer,_everything_looks_like_a_nail[if all you have is a hammer, everything looks like a nail], too. === Low-level Optimizations Apache Spark uses a xref:scheduler:DAGScheduler.adoc[directed acyclic graph (DAG) of computation stages] (aka execution DAG ). It postpones any processing until really required for actions. Spark's lazy evaluation gives plenty of opportunities to induce low-level optimizations (so users have to know less to do more). Mind the proverb https://en.wiktionary.org/wiki/less_is_more[less is more]. === Excels at low-latency iterative workloads Spark supports diverse workloads, but successfully targets low-latency iterative ones. They are often used in Machine Learning and graph algorithms. Many Machine Learning algorithms require plenty of iterations before the result models get optimal, like logistic regression. The same applies to graph algorithms to traverse all the nodes and edges when needed. Such computations can increase their performance when the interim partial results are stored in memory or at very fast solid state drives. Spark can link:spark-rdd-caching.adoc[cache intermediate data in memory for faster model building and training]. Once the data is loaded to memory (as an initial step), reusing it multiple times incurs no performance slowdowns. Also, graph algorithms can traverse graphs one connection per iteration with the partial result in memory. Less disk access and network can make a huge difference when you need to process lots of data, esp. when it is a BIG Data. === ETL done easier Spark gives Extract, Transform and Load (ETL) a new look with the many programming languages supported - Scala, Java, Python (less likely R). You can use them all or pick the best for a problem. Scala in Spark, especially, makes for a much less boiler-plate code (comparing to other languages and approaches like MapReduce in Java). === [[unified-api]] Unified Concise High-Level API Spark offers a unified, concise, high-level APIs for batch analytics (RDD API), SQL queries (Dataset API), real-time analysis (DStream API), machine learning (ML Pipeline API) and graph processing (Graph API). Developers no longer have to learn many different processing engines and platforms, and let the time be spent on mastering framework APIs per use case (atop a single computation engine Spark). === Different kinds of data processing using unified API Spark offers three kinds of data processing using batch , interactive , and stream processing with the unified API and data structures. === Little to no disk use for better performance In the no-so-long-ago times, when the most prevalent distributed computing framework was link:varia/spark-hadoop.adoc[Hadoop MapReduce], you could reuse a data between computation (even partial ones!) only after you've written it to an external storage like link:varia/spark-hadoop.adoc[Hadoop Distributed Filesystem (HDFS)]. It can cost you a lot of time to compute even very basic multi-stage computations. It simply suffers from IO (and perhaps network) overhead. One of the many motivations to build Spark was to have a framework that is good at data reuse. Spark cuts it out in a way to keep as much data as possible in memory and keep it there until a job is finished. It doesn't matter how many stages belong to a job. What does matter is the available memory and how effective you are in using Spark API (so xref:rdd:index.adoc[no shuffle occur]). The less network and disk IO, the better performance, and Spark tries hard to find ways to minimize both. === Fault Tolerance included Faults are not considered a special case in Spark, but obvious consequence of being a parallel and distributed system. Spark handles and recovers from faults by default without particularly complex logic to deal with them. === Small Codebase Invites Contributors Spark's design is fairly simple and the code that comes out of it is not huge comparing to the features it offers. The reasonably small codebase of Spark invites project contributors - programmers who extend the platform and fix bugs in a more steady pace. == [[i-want-more]] Further reading or watching (video) https://youtu.be/L029ZNBG7bk[Keynote : Spark 2.0 - Matei Zaharia, Apache Spark Creator and CTO of Databricks]","title":"Overview"},{"location":"overview/#apache-spark","text":"Apache Spark is an open-source distributed general-purpose cluster computing framework with (mostly) in-memory data processing engine that can do ETL, analytics, machine learning and graph processing on large volumes of data at rest (batch processing) or in motion (streaming processing) with rich concise high-level APIs for the programming languages: Scala, Python, Java, R, and SQL. You could also describe Spark as a distributed, data processing engine for batch and streaming modes featuring SQL queries, graph processing, and machine learning. In contrast to Hadoop\u2019s two-stage disk-based MapReduce computation engine, Spark's multi-stage (mostly) in-memory computing engine allows for running most computations in memory, and hence most of the time provides better performance for certain applications, e.g. iterative algorithms or interactive data mining (read Spark officially sets a new record in large-scale sorting ). Spark aims at speed, ease of use, extensibility and interactive analytics. Spark is a distributed platform for executing complex multi-stage applications , like machine learning algorithms , and interactive ad hoc queries . Spark provides an efficient abstraction for in-memory cluster computing called Resilient Distributed Dataset . Using Spark Application Frameworks, Spark simplifies access to machine learning and predictive analytics at scale. Spark is mainly written in http://scala-lang.org/[Scala ], but provides developer API for languages like Java, Python, and R. If you have large amounts of data that requires low latency processing that a typical MapReduce program cannot provide, Spark is a viable alternative. Access any data type across any data source. Huge demand for storage and data processing. The Apache Spark project is an umbrella for https://jaceklaskowski.gitbooks.io/mastering-spark-sql/[SQL ] (with Datasets), https://jaceklaskowski.gitbooks.io/spark-structured-streaming/[streaming ], http://spark.apache.org/mllib/[machine learning] (pipelines) and http://spark.apache.org/graphx/[graph ] processing engines built on top of the Spark Core. You can run them all in a single application using a consistent API. Spark runs locally as well as in clusters, on-premises or in cloud. It runs on top of Hadoop YARN, Apache Mesos, standalone or in the cloud (Amazon EC2 or IBM Bluemix). Apache Spark's https://jaceklaskowski.gitbooks.io/spark-structured-streaming/[Structured Streaming] and https://jaceklaskowski.gitbooks.io/mastering-spark-sql/[SQL ] programming models with MLlib and GraphX make it easier for developers and data scientists to build applications that exploit machine learning and graph analytics. At a high level, any Spark application creates RDDs out of some input, run xref:rdd:index.adoc[(lazy) transformations] of these RDDs to some other form (shape), and finally perform xref:rdd:index.adoc[actions] to collect or store data. Not much, huh? You can look at Spark from programmer's, data engineer's and administrator's point of view. And to be honest, all three types of people will spend quite a lot of their time with Spark to finally reach the point where they exploit all the available features. Programmers use language-specific APIs (and work at the level of RDDs using transformations and actions), data engineers use higher-level abstractions like DataFrames or Pipelines APIs or external tools (that connect to Spark), and finally it all can only be possible to run because administrators set up Spark clusters to deploy Spark applications to. It is Spark's goal to be a general-purpose computing platform with various specialized applications frameworks on top of a single unified engine. NOTE: When you hear \"Apache Spark\" it can be two things -- the Spark engine aka Spark Core or the Apache Spark open source project which is an \"umbrella\" term for Spark Core and the accompanying Spark Application Frameworks, i.e. Spark SQL, link:spark-streaming/spark-streaming.adoc[Spark Streaming], link:spark-mllib/spark-mllib.adoc[Spark MLlib] and link:spark-graphx.adoc[Spark GraphX] that sit on top of Spark Core and the main data abstraction in Spark called xref:rdd:index.adoc[RDD - Resilient Distributed Dataset]. == [[why-spark]] Why Spark Let's list a few of the many reasons for Spark. We are doing it first, and then comes the overview that lends a more technical helping hand. === Easy to Get Started Spark offers link:spark-shell.adoc[spark-shell] that makes for a very easy head start to writing and running Spark applications on the command line on your laptop. You could then use link:spark-standalone.adoc[Spark Standalone] built-in cluster manager to deploy your Spark applications to a production-grade cluster to run on a full dataset. === Unified Engine for Diverse Workloads As said by Matei Zaharia - the author of Apache Spark - in https://youtu.be/49Hr5xZyTEA[Introduction to AmpLab Spark Internals video] (quoting with few changes): One of the Spark project goals was to deliver a platform that supports a very wide array of diverse workflows - not only MapReduce batch jobs (there were available in Hadoop already at that time), but also iterative computations like graph algorithms or Machine Learning. And also different scales of workloads from sub-second interactive jobs to jobs that run for many hours. Spark combines batch, interactive, and streaming workloads under one rich concise API. Spark supports near real-time streaming workloads via link:spark-streaming/spark-streaming.adoc[Spark Streaming] application framework. ETL workloads and Analytics workloads are different, however Spark attempts to offer a unified platform for a wide variety of workloads. Graph and Machine Learning algorithms are iterative by nature and less saves to disk or transfers over network means better performance. There is also support for interactive workloads using Spark shell. You should watch the video https://youtu.be/SxAxAhn-BDU[What is Apache Spark?] by Mike Olson, Chief Strategy Officer and Co-Founder at Cloudera, who provides a very exceptional overview of Apache Spark, its rise in popularity in the open source community, and how Spark is primed to replace MapReduce as the general processing engine in Hadoop. === Leverages the Best in distributed batch data processing When you think about distributed batch data processing , link:varia/spark-hadoop.adoc[Hadoop] naturally comes to mind as a viable solution. Spark draws many ideas out of Hadoop MapReduce. They work together well - Spark on YARN and HDFS - while improving on the performance and simplicity of the distributed computing engine. For many, Spark is Hadoop++, i.e. MapReduce done in a better way. And it should not come as a surprise, without Hadoop MapReduce (its advances and deficiencies), Spark would not have been born at all. === RDD - Distributed Parallel Scala Collections As a Scala developer, you may find Spark's RDD API very similar (if not identical) to http://www.scala-lang.org/docu/files/collections-api/collections.html[Scala's Collections API]. It is also exposed in Java, Python and R (as well as SQL, i.e. SparkSQL, in a sense). So, when you have a need for distributed Collections API in Scala, Spark with RDD API should be a serious contender. === [[rich-standard-library]] Rich Standard Library Not only can you use map and reduce (as in Hadoop MapReduce jobs) in Spark, but also a vast array of other higher-level operators to ease your Spark queries and application development. It expanded on the available computation styles beyond the only map-and-reduce available in Hadoop MapReduce. === Unified development and deployment environment for all Regardless of the Spark tools you use - the Spark API for the many programming languages supported - Scala, Java, Python, R, or link:spark-shell.adoc[the Spark shell], or the many Spark Application Frameworks leveraging the concept of xref:rdd:index.adoc[RDD], i.e. Spark SQL, link:spark-streaming/spark-streaming.adoc[Spark Streaming], link:spark-mllib/spark-mllib.adoc[Spark MLlib] and link:spark-graphx.adoc[Spark GraphX], you still use the same development and deployment environment to for large data sets to yield a result, be it a prediction (link:spark-mllib/spark-mllib.adoc[Spark MLlib]), a structured data queries (Spark SQL) or just a large distributed batch (Spark Core) or streaming (Spark Streaming) computation. It's also very productive of Spark that teams can exploit the different skills the team members have acquired so far. Data analysts, data scientists, Python programmers, or Java, or Scala, or R, can all use the same Spark platform using tailor-made API. It makes for bringing skilled people with their expertise in different programming languages together to a Spark project. === Interactive Exploration / Exploratory Analytics It is also called ad hoc queries . Using link:spark-shell.adoc[the Spark shell] you can execute computations to process large amount of data ( The Big Data ). It's all interactive and very useful to explore the data before final production release. Also, using the Spark shell you can access any link:spark-cluster.adoc[Spark cluster] as if it was your local machine. Just point the Spark shell to a 20-node of 10TB RAM memory in total (using --master ) and use all the components (and their abstractions) like Spark SQL, Spark MLlib, link:spark-streaming/spark-streaming.adoc[Spark Streaming], and Spark GraphX. Depending on your needs and skills, you may see a better fit for SQL vs programming APIs or apply machine learning algorithms (Spark MLlib) from data in graph data structures (Spark GraphX). === Single Environment Regardless of which programming language you are good at, be it Scala, Java, Python, R or SQL, you can use the same single clustered runtime environment for prototyping, ad hoc queries, and deploying your applications leveraging the many ingestion data points offered by the Spark platform. You can be as low-level as using RDD API directly or leverage higher-level APIs of Spark SQL (Datasets), Spark MLlib (ML Pipelines), Spark GraphX (Graphs) or link:spark-streaming/spark-streaming.adoc[Spark Streaming] (DStreams). Or use them all in a single application. The single programming model and execution engine for different kinds of workloads simplify development and deployment architectures. === Data Integration Toolkit with Rich Set of Supported Data Sources Spark can read from many types of data sources -- relational, NoSQL, file systems, etc. -- using many types of data formats - Parquet, Avro, CSV, JSON. Both, input and output data sources, allow programmers and data engineers use Spark as the platform with the large amount of data that is read from or saved to for processing, interactively (using Spark shell) or in applications. === Tools unavailable then, at your fingertips now As much and often as it's recommended http://c2.com/cgi/wiki?PickTheRightToolForTheJob[to pick the right tool for the job], it's not always feasible. Time, personal preference, operating system you work on are all factors to decide what is right at a time (and using a hammer can be a reasonable choice). Spark embraces many concepts in a single unified development and runtime environment. Machine learning that is so tool- and feature-rich in Python, e.g. SciKit library, can now be used by Scala developers (as Pipeline API in Spark MLlib or calling pipe() ). DataFrames from R are available in Scala, Java, Python, R APIs. Single node computations in machine learning algorithms are migrated to their distributed versions in Spark MLlib. This single platform gives plenty of opportunities for Python, Scala, Java, and R programmers as well as data engineers (SparkR) and scientists (using proprietary enterprise data warehouses with link:spark-sql-thrift-server.adoc[Thrift JDBC/ODBC Server] in Spark SQL). Mind the proverb https://en.wiktionary.org/wiki/if_all_you_have_is_a_hammer,_everything_looks_like_a_nail[if all you have is a hammer, everything looks like a nail], too. === Low-level Optimizations Apache Spark uses a xref:scheduler:DAGScheduler.adoc[directed acyclic graph (DAG) of computation stages] (aka execution DAG ). It postpones any processing until really required for actions. Spark's lazy evaluation gives plenty of opportunities to induce low-level optimizations (so users have to know less to do more). Mind the proverb https://en.wiktionary.org/wiki/less_is_more[less is more]. === Excels at low-latency iterative workloads Spark supports diverse workloads, but successfully targets low-latency iterative ones. They are often used in Machine Learning and graph algorithms. Many Machine Learning algorithms require plenty of iterations before the result models get optimal, like logistic regression. The same applies to graph algorithms to traverse all the nodes and edges when needed. Such computations can increase their performance when the interim partial results are stored in memory or at very fast solid state drives. Spark can link:spark-rdd-caching.adoc[cache intermediate data in memory for faster model building and training]. Once the data is loaded to memory (as an initial step), reusing it multiple times incurs no performance slowdowns. Also, graph algorithms can traverse graphs one connection per iteration with the partial result in memory. Less disk access and network can make a huge difference when you need to process lots of data, esp. when it is a BIG Data. === ETL done easier Spark gives Extract, Transform and Load (ETL) a new look with the many programming languages supported - Scala, Java, Python (less likely R). You can use them all or pick the best for a problem. Scala in Spark, especially, makes for a much less boiler-plate code (comparing to other languages and approaches like MapReduce in Java). === [[unified-api]] Unified Concise High-Level API Spark offers a unified, concise, high-level APIs for batch analytics (RDD API), SQL queries (Dataset API), real-time analysis (DStream API), machine learning (ML Pipeline API) and graph processing (Graph API). Developers no longer have to learn many different processing engines and platforms, and let the time be spent on mastering framework APIs per use case (atop a single computation engine Spark). === Different kinds of data processing using unified API Spark offers three kinds of data processing using batch , interactive , and stream processing with the unified API and data structures. === Little to no disk use for better performance In the no-so-long-ago times, when the most prevalent distributed computing framework was link:varia/spark-hadoop.adoc[Hadoop MapReduce], you could reuse a data between computation (even partial ones!) only after you've written it to an external storage like link:varia/spark-hadoop.adoc[Hadoop Distributed Filesystem (HDFS)]. It can cost you a lot of time to compute even very basic multi-stage computations. It simply suffers from IO (and perhaps network) overhead. One of the many motivations to build Spark was to have a framework that is good at data reuse. Spark cuts it out in a way to keep as much data as possible in memory and keep it there until a job is finished. It doesn't matter how many stages belong to a job. What does matter is the available memory and how effective you are in using Spark API (so xref:rdd:index.adoc[no shuffle occur]). The less network and disk IO, the better performance, and Spark tries hard to find ways to minimize both. === Fault Tolerance included Faults are not considered a special case in Spark, but obvious consequence of being a parallel and distributed system. Spark handles and recovers from faults by default without particularly complex logic to deal with them. === Small Codebase Invites Contributors Spark's design is fairly simple and the code that comes out of it is not huge comparing to the features it offers. The reasonably small codebase of Spark invites project contributors - programmers who extend the platform and fix bugs in a more steady pace. == [[i-want-more]] Further reading or watching (video) https://youtu.be/L029ZNBG7bk[Keynote : Spark 2.0 - Matei Zaharia, Apache Spark Creator and CTO of Databricks]","title":"Apache Spark"},{"location":"spark-SparkContext-creating-instance-internals/","text":"== Inside Creating SparkContext This document describes what happens when you xref:ROOT:SparkContext.adoc#creating-instance[create a new SparkContext]. [source, scala] \u00b6 import org.apache.spark.{SparkConf, SparkContext} // 1. Create Spark configuration val conf = new SparkConf() .setAppName(\"SparkMe Application\") .setMaster(\"local[*]\") // local mode // 2. Create Spark context val sc = new SparkContext(conf) NOTE: The example uses Spark in link:local/spark-local.adoc[local mode], but the initialization with link:spark-cluster.adoc[the other cluster modes] would follow similar steps. Creating SparkContext instance starts by setting the internal allowMultipleContexts field with the value of xref:ROOT:SparkContext.adoc#spark.driver.allowMultipleContexts[spark.driver.allowMultipleContexts] and marking this SparkContext instance as partially constructed. It makes sure that no other thread is creating a SparkContext instance in this JVM. It does so by synchronizing on SPARK_CONTEXT_CONSTRUCTOR_LOCK and using the internal atomic reference activeContext (that eventually has a fully-created SparkContext instance). [NOTE] \u00b6 The entire code of SparkContext that creates a fully-working SparkContext instance is between two statements: [source, scala] \u00b6 SparkContext.markPartiallyConstructed(this, allowMultipleContexts) // the SparkContext code goes here SparkContext.setActiveContext(this, allowMultipleContexts) \u00b6 ==== xref:ROOT:SparkContext.adoc#startTime[startTime] is set to the current time in milliseconds. < > internal flag is set to false . The very first information printed out is the version of Spark as an INFO message: INFO SparkContext: Running Spark version 2.0.0-SNAPSHOT TIP: You can use xref:ROOT:SparkContext.adoc#version[version] method to learn about the current Spark version or org.apache.spark.SPARK_VERSION value. A xref:scheduler:LiveListenerBus.adoc#creating-instance[LiveListenerBus instance is created] (as listenerBus ). [[sparkUser]] The xref:ROOT:SparkContext.adoc#sparkUser[current user name] is computed. CAUTION: FIXME Where is sparkUser used? It saves the input SparkConf (as _conf ). CAUTION: FIXME Review _conf.validateSettings() It ensures that the first mandatory setting - spark.master is defined. SparkException is thrown if not. A master URL must be set in your configuration It ensures that the other mandatory setting - spark.app.name is defined. SparkException is thrown if not. An application name must be set in your configuration For link:yarn/spark-yarn-cluster-yarnclusterschedulerbackend.adoc[Spark on YARN in cluster deploy mode], it checks existence of spark.yarn.app.id . SparkException is thrown if it does not exist. Detected yarn cluster mode, but isn't running on a cluster. Deployment to YARN is not supported directly by SparkContext. Please use spark-submit. CAUTION: FIXME How to \"trigger\" the exception? What are the steps? When spark.logConf is enabled xref:ROOT:SparkConf.adoc[SparkConf.toDebugString] is called. NOTE: SparkConf.toDebugString is called very early in the initialization process and other settings configured afterwards are not included. Use sc.getConf.toDebugString once SparkContext is initialized. The driver's host and port are set if missing. link:spark-driver.adoc#spark_driver_host[spark.driver.host] becomes the value of < > (or an exception is thrown) while link:spark-driver.adoc#spark_driver_port[spark.driver.port] is set to 0 . NOTE: link:spark-driver.adoc#spark_driver_host[spark.driver.host] and link:spark-driver.adoc#spark_driver_port[spark.driver.port] are expected to be set on the driver. It is later asserted by xref:core:SparkEnv.adoc#createDriverEnv[SparkEnv]. xref:executor:Executor.adoc#spark.executor.id[spark.executor.id] setting is set to driver . TIP: Use sc.getConf.get(\"spark.executor.id\") to know where the code is executed -- xref:core:SparkEnv.adoc[driver or executors]. It sets the jars and files based on spark.jars and spark.files , respectively. These are files that are required for proper task execution on executors. If xref:spark-history-server:EventLoggingListener.adoc[event logging] is enabled, i.e. link:EventLoggingListener.adoc#spark_eventLog_enabled[spark.eventLog.enabled] flag is true , the internal field _eventLogDir is set to the value of link:EventLoggingListener.adoc#spark_eventLog_dir[spark.eventLog.dir] setting or the default value /tmp/spark-events . [[_eventLogCodec]] Also, if xref:spark-history-server:EventLoggingListener.adoc#spark_eventLog_compress[spark.eventLog.compress] is enabled (it is not by default), the short name of the xref CompressionCodec.adoc[CompressionCodec] is assigned to _eventLogCodec . The config key is xref:core:BroadcastManager.adoc#spark_io_compression_codec[spark.io.compression.codec] (default: lz4 ). TIP: Read about compression codecs in xref:core:BroadcastManager.adoc#compression[Compression]. === [[_listenerBus]] Creating LiveListenerBus SparkContext creates a xref:scheduler:LiveListenerBus.adoc#creating-instance[LiveListenerBus]. === [[_statusStore]] Creating Live AppStatusStore SparkContext requests AppStatusStore to create a xref:core:AppStatusStore.adoc#createLiveStore[live store] (i.e. the AppStatusStore for a live Spark application) and requests < > to add the xref:core:AppStatusStore.adoc#listener[AppStatusListener] to the xref:scheduler:LiveListenerBus.adoc#addToStatusQueue[status queue]. NOTE: The current AppStatusStore is available as xref:ROOT:SparkContext.adoc#statusStore[statusStore] property of the SparkContext . === [[_env]] Creating SparkEnv SparkContext creates a < > and requests SparkEnv to xref:core:SparkEnv.adoc#set[use the instance as the default SparkEnv]. CAUTION: FIXME Describe the following steps. MetadataCleaner is created. CAUTION: FIXME What's MetadataCleaner? === [[_statusTracker]] Creating SparkStatusTracker SparkContext creates a link:spark-sparkcontext-SparkStatusTracker.adoc#creating-instance[SparkStatusTracker] (with itself and the <<_statusStore, AppStatusStore>>). === [[_progressBar]] Creating ConsoleProgressBar SparkContext creates the optional link:spark-sparkcontext-ConsoleProgressBar.adoc#creating-instance[ConsoleProgressBar] when link:spark-webui-properties.adoc#spark.ui.showConsoleProgress[spark.ui.showConsoleProgress] property is enabled and the INFO logging level for SparkContext is disabled. === [[_ui]][[ui]] Creating SparkUI SparkContext creates a link:spark-webui-SparkUI.adoc#create[SparkUI] when link:spark-webui-properties.adoc#spark.ui.enabled[spark.ui.enabled] configuration property is enabled (i.e. true ) with the following: <<_statusStore, AppStatusStore>> Name of the Spark application that is exactly the value of xref:ROOT:SparkConf.adoc#spark.app.name[spark.app.name] configuration property Empty base path NOTE: link:spark-webui-properties.adoc#spark.ui.enabled[spark.ui.enabled] Spark property is assumed enabled when undefined. CAUTION: FIXME Where's _ui used? A Hadoop configuration is created. See xref:ROOT:SparkContext.adoc#hadoopConfiguration[Hadoop Configuration]. [[jars]] If there are jars given through the SparkContext constructor, they are added using addJar . [[files]] If there were files specified, they are added using xref:ROOT:SparkContext.adoc#addFile[addFile]. At this point in time, the amount of memory to allocate to each executor (as _executorMemory ) is calculated. It is the value of xref:executor:Executor.adoc#spark.executor.memory[spark.executor.memory] setting, or xref:ROOT:SparkContext.adoc#environment-variables[SPARK_EXECUTOR_MEMORY] environment variable (or currently-deprecated SPARK_MEM ), or defaults to 1024 . _executorMemory is later available as sc.executorMemory and used for LOCAL_CLUSTER_REGEX, link:spark-standalone.adoc#SparkDeploySchedulerBackend[Spark Standalone's SparkDeploySchedulerBackend], to set executorEnvs(\"SPARK_EXECUTOR_MEMORY\") , MesosSchedulerBackend, CoarseMesosSchedulerBackend. The value of SPARK_PREPEND_CLASSES environment variable is included in executorEnvs . [CAUTION] \u00b6 FIXME What's _executorMemory ? What's the unit of the value of _executorMemory exactly? What are \"SPARK_TESTING\", \"spark.testing\"? How do they contribute to executorEnvs ? What's executorEnvs ? \u00b6 The Mesos scheduler backend's configuration is included in executorEnvs , i.e. xref:ROOT:SparkContext.adoc#environment-variables[SPARK_EXECUTOR_MEMORY], _conf.getExecutorEnv , and SPARK_USER . [[_heartbeatReceiver]] SparkContext registers link:spark-HeartbeatReceiver.adoc[HeartbeatReceiver RPC endpoint]. SparkContext object is requested to xref:ROOT:SparkContext.adoc#createTaskScheduler[create the SchedulerBackend with the TaskScheduler] (for the given master URL) and the result becomes the internal _schedulerBackend and _taskScheduler . NOTE: The internal _schedulerBackend and _taskScheduler are used by schedulerBackend and taskScheduler methods, respectively. xref:scheduler:DAGScheduler.adoc#creating-instance[DAGScheduler is created] (as _dagScheduler ). [[TaskSchedulerIsSet]] SparkContext sends a blocking link:spark-HeartbeatReceiver.adoc#TaskSchedulerIsSet[ TaskSchedulerIsSet message to HeartbeatReceiver RPC endpoint] (to inform that the TaskScheduler is now available). === [[taskScheduler-start]] Starting TaskScheduler SparkContext xref:scheduler:TaskScheduler.adoc#start[starts TaskScheduler ]. === [[_applicationId]][[_applicationAttemptId]] Setting Spark Application's and Execution Attempt's IDs -- _applicationId and _applicationAttemptId SparkContext sets the internal fields -- _applicationId and _applicationAttemptId -- (using applicationId and applicationAttemptId methods from the xref:scheduler:TaskScheduler.adoc#contract[TaskScheduler Contract]). NOTE: SparkContext requests TaskScheduler for the xref:scheduler:TaskScheduler.adoc#applicationId[unique identifier of a Spark application] (that is currently only implemented by xref:scheduler:TaskSchedulerImpl.adoc#applicationId[TaskSchedulerImpl] that uses SchedulerBackend to xref:scheduler:SchedulerBackend.adoc#applicationId[request the identifier]). NOTE: The unique identifier of a Spark application is used to initialize link:spark-webui-SparkUI.adoc#setAppId[SparkUI] and xref:storage:BlockManager.adoc#initialize[BlockManager]. NOTE: _applicationAttemptId is used when SparkContext is requested for the xref:ROOT:SparkContext.adoc#applicationAttemptId[unique identifier of execution attempt of a Spark application] and when EventLoggingListener xref:spark-history-server:EventLoggingListener.adoc#creating-instance[is created]. === [[spark.app.id]] Setting spark.app.id Spark Property in SparkConf SparkContext sets xref:ROOT:SparkConf.adoc#spark.app.id[spark.app.id] property to be the <<_applicationId, unique identifier of a Spark application>> and, if enabled, link:spark-webui-SparkUI.adoc#setAppId[passes it on to SparkUI ]. === [[BlockManager-initialization]] Initializing BlockManager The xref:storage:BlockManager.adoc#initialize[BlockManager (for the driver) is initialized] (with _applicationId ). === [[MetricsSystem-start]] Starting MetricsSystem SparkContext requests the MetricsSystem to link:spark-metrics-MetricsSystem.adoc#start[start]. NOTE: SparkContext starts MetricsSystem after < > as MetricsSystem uses it to link:spark-metrics-MetricsSystem.adoc#buildRegistryName[build unique identifiers fo metrics sources]. === [[MetricsSystem-getServletHandlers]] Requesting JSON Servlet Handler SparkContext requests the MetricsSystem for a link:spark-metrics-MetricsSystem.adoc#getServletHandlers[JSON servlet handler] and requests the <<_ui, SparkUI>> to link:spark-webui-WebUI.adoc#attachHandler[attach it]. [[_eventLogger]] _eventLogger is created and started if isEventLogEnabled . It uses xref:spark-history-server:EventLoggingListener.adoc[EventLoggingListener] that gets registered to xref:scheduler:LiveListenerBus.adoc[]. CAUTION: FIXME Why is _eventLogger required to be the internal field of SparkContext? Where is this used? [[ExecutorAllocationManager]] For xref:ROOT:spark-dynamic-allocation.adoc[], link:spark-ExecutorAllocationManager.adoc#creating-instance[ ExecutorAllocationManager is created] (as _executorAllocationManager ) and immediately link:spark-ExecutorAllocationManager.adoc#start[started]. NOTE: _executorAllocationManager is exposed (as a method) to link:yarn/spark-yarn-yarnschedulerbackend.adoc#reset[YARN scheduler backends to reset their state to the initial state]. [[_cleaner]][[ContextCleaner]] With xref:ROOT:configuration-properties.adoc#spark.cleaner.referenceTracking[spark.cleaner.referenceTracking] configuration property enabled, SparkContext xref:core:ContextCleaner.adoc#creating-instance[creates ContextCleaner ] (as _cleaner ) and xref:core:ContextCleaner.adoc#start[started] immediately. Otherwise, _cleaner is empty. CAUTION: FIXME It'd be quite useful to have all the properties with their default values in sc.getConf.toDebugString , so when a configuration is not included but does change Spark runtime configuration, it should be added to _conf . [[registering_SparkListeners]] It < SparkListenerEvent event delivery to the listeners>>. [[postEnvironmentUpdate]] postEnvironmentUpdate is called that posts xref:ROOT:SparkListener.adoc#SparkListenerEnvironmentUpdate[SparkListenerEnvironmentUpdate] message on xref:scheduler:LiveListenerBus.adoc[] with information about Task Scheduler's scheduling mode, added jar and file paths, and other environmental details. They are displayed in web UI's link:spark-webui-environment.adoc[Environment tab]. [[postApplicationStart]] xref:ROOT:SparkListener.adoc#SparkListenerApplicationStart[SparkListenerApplicationStart] message is posted to xref:scheduler:LiveListenerBus.adoc[] (using the internal postApplicationStart method). [[postStartHook]] TaskScheduler xref:scheduler:TaskScheduler.adoc#postStartHook[is notified that SparkContext is almost fully initialized]. NOTE: xref:scheduler:TaskScheduler.adoc#postStartHook[TaskScheduler.postStartHook] does nothing by default, but custom implementations offer more advanced features, i.e. TaskSchedulerImpl xref:scheduler:TaskSchedulerImpl.adoc#postStartHook[blocks the current thread until SchedulerBackend is ready]. There is also YarnClusterScheduler for Spark on YARN in cluster deploy mode. === [[registerSource]] Registering Metrics Sources SparkContext requests MetricsSystem to link:spark-metrics-MetricsSystem.adoc#registerSource[register metrics sources] for the following services: . xref:scheduler:DAGScheduler.adoc#metricsSource[DAGScheduler] . link:spark-BlockManager-BlockManagerSource.adoc[BlockManager] . link:spark-ExecutorAllocationManager.adoc#executorAllocationManagerSource[ExecutorAllocationManager] (for xref:ROOT:spark-dynamic-allocation.adoc[]) === [[addShutdownHook]] Adding Shutdown Hook SparkContext adds a shutdown hook (using ShutdownHookManager.addShutdownHook() ). You should see the following DEBUG message in the logs: DEBUG Adding shutdown hook CAUTION: FIXME ShutdownHookManager.addShutdownHook() Any non-fatal Exception leads to termination of the Spark context instance. CAUTION: FIXME What does NonFatal represent in Scala? CAUTION: FIXME Finish me === [[nextShuffleId]][[nextRddId]] Initializing nextShuffleId and nextRddId Internal Counters nextShuffleId and nextRddId start with 0 . CAUTION: FIXME Where are nextShuffleId and nextRddId used? A new instance of Spark context is created and ready for operation. === [[getClusterManager]] Loading External Cluster Manager for URL (getClusterManager method) [source, scala] \u00b6 getClusterManager(url: String): Option[ExternalClusterManager] \u00b6 getClusterManager loads xref:scheduler:ExternalClusterManager.adoc[] that xref:scheduler:ExternalClusterManager.adoc#canCreate[can handle the input url ]. If there are two or more external cluster managers that could handle url , a SparkException is thrown: Multiple Cluster Managers ([serviceLoaders]) registered for the url [url]. NOTE: getClusterManager uses Java's link:++ https://docs.oracle.com/javase/8/docs/api/java/util/ServiceLoader.html#load-java.lang.Class-java.lang.ClassLoader-++[ServiceLoader.load ] method. NOTE: getClusterManager is used to find a cluster manager for a master URL when xref:ROOT:SparkContext.adoc#createTaskScheduler[creating a SchedulerBackend and a TaskScheduler for the driver]. === [[setupAndStartListenerBus]] setupAndStartListenerBus [source, scala] \u00b6 setupAndStartListenerBus(): Unit \u00b6 setupAndStartListenerBus is an internal method that reads xref:ROOT:configuration-properties.adoc#spark.extraListeners[spark.extraListeners] configuration property from the current xref:ROOT:SparkConf.adoc[SparkConf] to create and register xref:ROOT:SparkListener.adoc#SparkListenerInterface[SparkListenerInterface] listeners. It expects that the class name represents a SparkListenerInterface listener with one of the following constructors (in this order): a single-argument constructor that accepts xref:ROOT:SparkConf.adoc[SparkConf] a zero-argument constructor setupAndStartListenerBus xref:scheduler:LiveListenerBus.adoc#ListenerBus-addListener[registers every listener class]. You should see the following INFO message in the logs: INFO Registered listener [className] It xref:scheduler:LiveListenerBus.adoc#start[starts LiveListenerBus] and records it in the internal _listenerBusStarted . When no single- SparkConf or zero-argument constructor could be found for a class name in xref:ROOT:configuration-properties.adoc#spark.extraListeners[spark.extraListeners] configuration property, a SparkException is thrown with the message: [className] did not have a zero-argument constructor or a single-argument constructor that accepts SparkConf. Note: if the class is defined inside of another Scala class, then its constructors may accept an implicit parameter that references the enclosing class; in this case, you must define the listener as a top-level class in order to prevent this extra parameter from breaking Spark's ability to find a valid constructor. Any exception while registering a xref:ROOT:SparkListener.adoc#SparkListenerInterface[SparkListenerInterface] listener xref:ROOT:SparkContext.adoc#stop[stops the SparkContext] and a SparkException is thrown and the source exception's message. Exception when registering SparkListener [TIP] \u00b6 Set INFO on org.apache.spark.SparkContext logger to see the extra listeners being registered. INFO SparkContext: Registered listener pl.japila.spark.CustomSparkListener \u00b6 === [[createSparkEnv]] Creating SparkEnv for Driver -- createSparkEnv Method [source, scala] \u00b6 createSparkEnv( conf: SparkConf, isLocal: Boolean, listenerBus: LiveListenerBus): SparkEnv createSparkEnv simply delegates the call to xref:core:SparkEnv.adoc#createDriverEnv[SparkEnv to create a SparkEnv for the driver]. It calculates the number of cores to 1 for local master URL, the number of processors available for JVM for * or the exact number in the master URL, or 0 for the cluster master URLs. === [[getCurrentUserName]] Utils.getCurrentUserName Method [source, scala] \u00b6 getCurrentUserName(): String \u00b6 getCurrentUserName computes the user name who has started the xref:ROOT:SparkContext.adoc[SparkContext] instance. NOTE: It is later available as xref:ROOT:SparkContext.adoc#sparkUser[SparkContext.sparkUser]. Internally, it reads xref:ROOT:SparkContext.adoc#SPARK_USER[SPARK_USER] environment variable and, if not set, reverts to Hadoop Security API's UserGroupInformation.getCurrentUser().getShortUserName() . NOTE: It is another place where Spark relies on Hadoop API for its operation. === [[localHostName]] Utils.localHostName Method localHostName computes the local host name. It starts by checking SPARK_LOCAL_HOSTNAME environment variable for the value. If it is not defined, it uses SPARK_LOCAL_IP to find the name (using InetAddress.getByName ). If it is not defined either, it calls InetAddress.getLocalHost for the name. NOTE: Utils.localHostName is executed while xref:ROOT:SparkContext.adoc#creating-instance[ SparkContext is created] and also to compute the default value of link:spark-driver.adoc#spark_driver_host[spark.driver.host Spark property]. CAUTION: FIXME Review the rest. === [[stopped]] stopped Flag CAUTION: FIXME Where is this used?","title":"Creating SparkContext"},{"location":"spark-SparkContext-creating-instance-internals/#source-scala","text":"import org.apache.spark.{SparkConf, SparkContext} // 1. Create Spark configuration val conf = new SparkConf() .setAppName(\"SparkMe Application\") .setMaster(\"local[*]\") // local mode // 2. Create Spark context val sc = new SparkContext(conf) NOTE: The example uses Spark in link:local/spark-local.adoc[local mode], but the initialization with link:spark-cluster.adoc[the other cluster modes] would follow similar steps. Creating SparkContext instance starts by setting the internal allowMultipleContexts field with the value of xref:ROOT:SparkContext.adoc#spark.driver.allowMultipleContexts[spark.driver.allowMultipleContexts] and marking this SparkContext instance as partially constructed. It makes sure that no other thread is creating a SparkContext instance in this JVM. It does so by synchronizing on SPARK_CONTEXT_CONSTRUCTOR_LOCK and using the internal atomic reference activeContext (that eventually has a fully-created SparkContext instance).","title":"[source, scala]"},{"location":"spark-SparkContext-creating-instance-internals/#note","text":"The entire code of SparkContext that creates a fully-working SparkContext instance is between two statements:","title":"[NOTE]"},{"location":"spark-SparkContext-creating-instance-internals/#source-scala_1","text":"SparkContext.markPartiallyConstructed(this, allowMultipleContexts) // the SparkContext code goes here","title":"[source, scala]"},{"location":"spark-SparkContext-creating-instance-internals/#sparkcontextsetactivecontextthis-allowmultiplecontexts","text":"==== xref:ROOT:SparkContext.adoc#startTime[startTime] is set to the current time in milliseconds. < > internal flag is set to false . The very first information printed out is the version of Spark as an INFO message: INFO SparkContext: Running Spark version 2.0.0-SNAPSHOT TIP: You can use xref:ROOT:SparkContext.adoc#version[version] method to learn about the current Spark version or org.apache.spark.SPARK_VERSION value. A xref:scheduler:LiveListenerBus.adoc#creating-instance[LiveListenerBus instance is created] (as listenerBus ). [[sparkUser]] The xref:ROOT:SparkContext.adoc#sparkUser[current user name] is computed. CAUTION: FIXME Where is sparkUser used? It saves the input SparkConf (as _conf ). CAUTION: FIXME Review _conf.validateSettings() It ensures that the first mandatory setting - spark.master is defined. SparkException is thrown if not. A master URL must be set in your configuration It ensures that the other mandatory setting - spark.app.name is defined. SparkException is thrown if not. An application name must be set in your configuration For link:yarn/spark-yarn-cluster-yarnclusterschedulerbackend.adoc[Spark on YARN in cluster deploy mode], it checks existence of spark.yarn.app.id . SparkException is thrown if it does not exist. Detected yarn cluster mode, but isn't running on a cluster. Deployment to YARN is not supported directly by SparkContext. Please use spark-submit. CAUTION: FIXME How to \"trigger\" the exception? What are the steps? When spark.logConf is enabled xref:ROOT:SparkConf.adoc[SparkConf.toDebugString] is called. NOTE: SparkConf.toDebugString is called very early in the initialization process and other settings configured afterwards are not included. Use sc.getConf.toDebugString once SparkContext is initialized. The driver's host and port are set if missing. link:spark-driver.adoc#spark_driver_host[spark.driver.host] becomes the value of < > (or an exception is thrown) while link:spark-driver.adoc#spark_driver_port[spark.driver.port] is set to 0 . NOTE: link:spark-driver.adoc#spark_driver_host[spark.driver.host] and link:spark-driver.adoc#spark_driver_port[spark.driver.port] are expected to be set on the driver. It is later asserted by xref:core:SparkEnv.adoc#createDriverEnv[SparkEnv]. xref:executor:Executor.adoc#spark.executor.id[spark.executor.id] setting is set to driver . TIP: Use sc.getConf.get(\"spark.executor.id\") to know where the code is executed -- xref:core:SparkEnv.adoc[driver or executors]. It sets the jars and files based on spark.jars and spark.files , respectively. These are files that are required for proper task execution on executors. If xref:spark-history-server:EventLoggingListener.adoc[event logging] is enabled, i.e. link:EventLoggingListener.adoc#spark_eventLog_enabled[spark.eventLog.enabled] flag is true , the internal field _eventLogDir is set to the value of link:EventLoggingListener.adoc#spark_eventLog_dir[spark.eventLog.dir] setting or the default value /tmp/spark-events . [[_eventLogCodec]] Also, if xref:spark-history-server:EventLoggingListener.adoc#spark_eventLog_compress[spark.eventLog.compress] is enabled (it is not by default), the short name of the xref CompressionCodec.adoc[CompressionCodec] is assigned to _eventLogCodec . The config key is xref:core:BroadcastManager.adoc#spark_io_compression_codec[spark.io.compression.codec] (default: lz4 ). TIP: Read about compression codecs in xref:core:BroadcastManager.adoc#compression[Compression]. === [[_listenerBus]] Creating LiveListenerBus SparkContext creates a xref:scheduler:LiveListenerBus.adoc#creating-instance[LiveListenerBus]. === [[_statusStore]] Creating Live AppStatusStore SparkContext requests AppStatusStore to create a xref:core:AppStatusStore.adoc#createLiveStore[live store] (i.e. the AppStatusStore for a live Spark application) and requests < > to add the xref:core:AppStatusStore.adoc#listener[AppStatusListener] to the xref:scheduler:LiveListenerBus.adoc#addToStatusQueue[status queue]. NOTE: The current AppStatusStore is available as xref:ROOT:SparkContext.adoc#statusStore[statusStore] property of the SparkContext . === [[_env]] Creating SparkEnv SparkContext creates a < > and requests SparkEnv to xref:core:SparkEnv.adoc#set[use the instance as the default SparkEnv]. CAUTION: FIXME Describe the following steps. MetadataCleaner is created. CAUTION: FIXME What's MetadataCleaner? === [[_statusTracker]] Creating SparkStatusTracker SparkContext creates a link:spark-sparkcontext-SparkStatusTracker.adoc#creating-instance[SparkStatusTracker] (with itself and the <<_statusStore, AppStatusStore>>). === [[_progressBar]] Creating ConsoleProgressBar SparkContext creates the optional link:spark-sparkcontext-ConsoleProgressBar.adoc#creating-instance[ConsoleProgressBar] when link:spark-webui-properties.adoc#spark.ui.showConsoleProgress[spark.ui.showConsoleProgress] property is enabled and the INFO logging level for SparkContext is disabled. === [[_ui]][[ui]] Creating SparkUI SparkContext creates a link:spark-webui-SparkUI.adoc#create[SparkUI] when link:spark-webui-properties.adoc#spark.ui.enabled[spark.ui.enabled] configuration property is enabled (i.e. true ) with the following: <<_statusStore, AppStatusStore>> Name of the Spark application that is exactly the value of xref:ROOT:SparkConf.adoc#spark.app.name[spark.app.name] configuration property Empty base path NOTE: link:spark-webui-properties.adoc#spark.ui.enabled[spark.ui.enabled] Spark property is assumed enabled when undefined. CAUTION: FIXME Where's _ui used? A Hadoop configuration is created. See xref:ROOT:SparkContext.adoc#hadoopConfiguration[Hadoop Configuration]. [[jars]] If there are jars given through the SparkContext constructor, they are added using addJar . [[files]] If there were files specified, they are added using xref:ROOT:SparkContext.adoc#addFile[addFile]. At this point in time, the amount of memory to allocate to each executor (as _executorMemory ) is calculated. It is the value of xref:executor:Executor.adoc#spark.executor.memory[spark.executor.memory] setting, or xref:ROOT:SparkContext.adoc#environment-variables[SPARK_EXECUTOR_MEMORY] environment variable (or currently-deprecated SPARK_MEM ), or defaults to 1024 . _executorMemory is later available as sc.executorMemory and used for LOCAL_CLUSTER_REGEX, link:spark-standalone.adoc#SparkDeploySchedulerBackend[Spark Standalone's SparkDeploySchedulerBackend], to set executorEnvs(\"SPARK_EXECUTOR_MEMORY\") , MesosSchedulerBackend, CoarseMesosSchedulerBackend. The value of SPARK_PREPEND_CLASSES environment variable is included in executorEnvs .","title":"SparkContext.setActiveContext(this, allowMultipleContexts)"},{"location":"spark-SparkContext-creating-instance-internals/#caution","text":"FIXME What's _executorMemory ? What's the unit of the value of _executorMemory exactly? What are \"SPARK_TESTING\", \"spark.testing\"? How do they contribute to executorEnvs ?","title":"[CAUTION]"},{"location":"spark-SparkContext-creating-instance-internals/#whats-executorenvs","text":"The Mesos scheduler backend's configuration is included in executorEnvs , i.e. xref:ROOT:SparkContext.adoc#environment-variables[SPARK_EXECUTOR_MEMORY], _conf.getExecutorEnv , and SPARK_USER . [[_heartbeatReceiver]] SparkContext registers link:spark-HeartbeatReceiver.adoc[HeartbeatReceiver RPC endpoint]. SparkContext object is requested to xref:ROOT:SparkContext.adoc#createTaskScheduler[create the SchedulerBackend with the TaskScheduler] (for the given master URL) and the result becomes the internal _schedulerBackend and _taskScheduler . NOTE: The internal _schedulerBackend and _taskScheduler are used by schedulerBackend and taskScheduler methods, respectively. xref:scheduler:DAGScheduler.adoc#creating-instance[DAGScheduler is created] (as _dagScheduler ). [[TaskSchedulerIsSet]] SparkContext sends a blocking link:spark-HeartbeatReceiver.adoc#TaskSchedulerIsSet[ TaskSchedulerIsSet message to HeartbeatReceiver RPC endpoint] (to inform that the TaskScheduler is now available). === [[taskScheduler-start]] Starting TaskScheduler SparkContext xref:scheduler:TaskScheduler.adoc#start[starts TaskScheduler ]. === [[_applicationId]][[_applicationAttemptId]] Setting Spark Application's and Execution Attempt's IDs -- _applicationId and _applicationAttemptId SparkContext sets the internal fields -- _applicationId and _applicationAttemptId -- (using applicationId and applicationAttemptId methods from the xref:scheduler:TaskScheduler.adoc#contract[TaskScheduler Contract]). NOTE: SparkContext requests TaskScheduler for the xref:scheduler:TaskScheduler.adoc#applicationId[unique identifier of a Spark application] (that is currently only implemented by xref:scheduler:TaskSchedulerImpl.adoc#applicationId[TaskSchedulerImpl] that uses SchedulerBackend to xref:scheduler:SchedulerBackend.adoc#applicationId[request the identifier]). NOTE: The unique identifier of a Spark application is used to initialize link:spark-webui-SparkUI.adoc#setAppId[SparkUI] and xref:storage:BlockManager.adoc#initialize[BlockManager]. NOTE: _applicationAttemptId is used when SparkContext is requested for the xref:ROOT:SparkContext.adoc#applicationAttemptId[unique identifier of execution attempt of a Spark application] and when EventLoggingListener xref:spark-history-server:EventLoggingListener.adoc#creating-instance[is created]. === [[spark.app.id]] Setting spark.app.id Spark Property in SparkConf SparkContext sets xref:ROOT:SparkConf.adoc#spark.app.id[spark.app.id] property to be the <<_applicationId, unique identifier of a Spark application>> and, if enabled, link:spark-webui-SparkUI.adoc#setAppId[passes it on to SparkUI ]. === [[BlockManager-initialization]] Initializing BlockManager The xref:storage:BlockManager.adoc#initialize[BlockManager (for the driver) is initialized] (with _applicationId ). === [[MetricsSystem-start]] Starting MetricsSystem SparkContext requests the MetricsSystem to link:spark-metrics-MetricsSystem.adoc#start[start]. NOTE: SparkContext starts MetricsSystem after < > as MetricsSystem uses it to link:spark-metrics-MetricsSystem.adoc#buildRegistryName[build unique identifiers fo metrics sources]. === [[MetricsSystem-getServletHandlers]] Requesting JSON Servlet Handler SparkContext requests the MetricsSystem for a link:spark-metrics-MetricsSystem.adoc#getServletHandlers[JSON servlet handler] and requests the <<_ui, SparkUI>> to link:spark-webui-WebUI.adoc#attachHandler[attach it]. [[_eventLogger]] _eventLogger is created and started if isEventLogEnabled . It uses xref:spark-history-server:EventLoggingListener.adoc[EventLoggingListener] that gets registered to xref:scheduler:LiveListenerBus.adoc[]. CAUTION: FIXME Why is _eventLogger required to be the internal field of SparkContext? Where is this used? [[ExecutorAllocationManager]] For xref:ROOT:spark-dynamic-allocation.adoc[], link:spark-ExecutorAllocationManager.adoc#creating-instance[ ExecutorAllocationManager is created] (as _executorAllocationManager ) and immediately link:spark-ExecutorAllocationManager.adoc#start[started]. NOTE: _executorAllocationManager is exposed (as a method) to link:yarn/spark-yarn-yarnschedulerbackend.adoc#reset[YARN scheduler backends to reset their state to the initial state]. [[_cleaner]][[ContextCleaner]] With xref:ROOT:configuration-properties.adoc#spark.cleaner.referenceTracking[spark.cleaner.referenceTracking] configuration property enabled, SparkContext xref:core:ContextCleaner.adoc#creating-instance[creates ContextCleaner ] (as _cleaner ) and xref:core:ContextCleaner.adoc#start[started] immediately. Otherwise, _cleaner is empty. CAUTION: FIXME It'd be quite useful to have all the properties with their default values in sc.getConf.toDebugString , so when a configuration is not included but does change Spark runtime configuration, it should be added to _conf . [[registering_SparkListeners]] It < SparkListenerEvent event delivery to the listeners>>. [[postEnvironmentUpdate]] postEnvironmentUpdate is called that posts xref:ROOT:SparkListener.adoc#SparkListenerEnvironmentUpdate[SparkListenerEnvironmentUpdate] message on xref:scheduler:LiveListenerBus.adoc[] with information about Task Scheduler's scheduling mode, added jar and file paths, and other environmental details. They are displayed in web UI's link:spark-webui-environment.adoc[Environment tab]. [[postApplicationStart]] xref:ROOT:SparkListener.adoc#SparkListenerApplicationStart[SparkListenerApplicationStart] message is posted to xref:scheduler:LiveListenerBus.adoc[] (using the internal postApplicationStart method). [[postStartHook]] TaskScheduler xref:scheduler:TaskScheduler.adoc#postStartHook[is notified that SparkContext is almost fully initialized]. NOTE: xref:scheduler:TaskScheduler.adoc#postStartHook[TaskScheduler.postStartHook] does nothing by default, but custom implementations offer more advanced features, i.e. TaskSchedulerImpl xref:scheduler:TaskSchedulerImpl.adoc#postStartHook[blocks the current thread until SchedulerBackend is ready]. There is also YarnClusterScheduler for Spark on YARN in cluster deploy mode. === [[registerSource]] Registering Metrics Sources SparkContext requests MetricsSystem to link:spark-metrics-MetricsSystem.adoc#registerSource[register metrics sources] for the following services: . xref:scheduler:DAGScheduler.adoc#metricsSource[DAGScheduler] . link:spark-BlockManager-BlockManagerSource.adoc[BlockManager] . link:spark-ExecutorAllocationManager.adoc#executorAllocationManagerSource[ExecutorAllocationManager] (for xref:ROOT:spark-dynamic-allocation.adoc[]) === [[addShutdownHook]] Adding Shutdown Hook SparkContext adds a shutdown hook (using ShutdownHookManager.addShutdownHook() ). You should see the following DEBUG message in the logs: DEBUG Adding shutdown hook CAUTION: FIXME ShutdownHookManager.addShutdownHook() Any non-fatal Exception leads to termination of the Spark context instance. CAUTION: FIXME What does NonFatal represent in Scala? CAUTION: FIXME Finish me === [[nextShuffleId]][[nextRddId]] Initializing nextShuffleId and nextRddId Internal Counters nextShuffleId and nextRddId start with 0 . CAUTION: FIXME Where are nextShuffleId and nextRddId used? A new instance of Spark context is created and ready for operation. === [[getClusterManager]] Loading External Cluster Manager for URL (getClusterManager method)","title":"What's executorEnvs?"},{"location":"spark-SparkContext-creating-instance-internals/#source-scala_2","text":"","title":"[source, scala]"},{"location":"spark-SparkContext-creating-instance-internals/#getclustermanagerurl-string-optionexternalclustermanager","text":"getClusterManager loads xref:scheduler:ExternalClusterManager.adoc[] that xref:scheduler:ExternalClusterManager.adoc#canCreate[can handle the input url ]. If there are two or more external cluster managers that could handle url , a SparkException is thrown: Multiple Cluster Managers ([serviceLoaders]) registered for the url [url]. NOTE: getClusterManager uses Java's link:++ https://docs.oracle.com/javase/8/docs/api/java/util/ServiceLoader.html#load-java.lang.Class-java.lang.ClassLoader-++[ServiceLoader.load ] method. NOTE: getClusterManager is used to find a cluster manager for a master URL when xref:ROOT:SparkContext.adoc#createTaskScheduler[creating a SchedulerBackend and a TaskScheduler for the driver]. === [[setupAndStartListenerBus]] setupAndStartListenerBus","title":"getClusterManager(url: String): Option[ExternalClusterManager]"},{"location":"spark-SparkContext-creating-instance-internals/#source-scala_3","text":"","title":"[source, scala]"},{"location":"spark-SparkContext-creating-instance-internals/#setupandstartlistenerbus-unit","text":"setupAndStartListenerBus is an internal method that reads xref:ROOT:configuration-properties.adoc#spark.extraListeners[spark.extraListeners] configuration property from the current xref:ROOT:SparkConf.adoc[SparkConf] to create and register xref:ROOT:SparkListener.adoc#SparkListenerInterface[SparkListenerInterface] listeners. It expects that the class name represents a SparkListenerInterface listener with one of the following constructors (in this order): a single-argument constructor that accepts xref:ROOT:SparkConf.adoc[SparkConf] a zero-argument constructor setupAndStartListenerBus xref:scheduler:LiveListenerBus.adoc#ListenerBus-addListener[registers every listener class]. You should see the following INFO message in the logs: INFO Registered listener [className] It xref:scheduler:LiveListenerBus.adoc#start[starts LiveListenerBus] and records it in the internal _listenerBusStarted . When no single- SparkConf or zero-argument constructor could be found for a class name in xref:ROOT:configuration-properties.adoc#spark.extraListeners[spark.extraListeners] configuration property, a SparkException is thrown with the message: [className] did not have a zero-argument constructor or a single-argument constructor that accepts SparkConf. Note: if the class is defined inside of another Scala class, then its constructors may accept an implicit parameter that references the enclosing class; in this case, you must define the listener as a top-level class in order to prevent this extra parameter from breaking Spark's ability to find a valid constructor. Any exception while registering a xref:ROOT:SparkListener.adoc#SparkListenerInterface[SparkListenerInterface] listener xref:ROOT:SparkContext.adoc#stop[stops the SparkContext] and a SparkException is thrown and the source exception's message. Exception when registering SparkListener","title":"setupAndStartListenerBus(): Unit"},{"location":"spark-SparkContext-creating-instance-internals/#tip","text":"Set INFO on org.apache.spark.SparkContext logger to see the extra listeners being registered.","title":"[TIP]"},{"location":"spark-SparkContext-creating-instance-internals/#info-sparkcontext-registered-listener-pljapilasparkcustomsparklistener","text":"=== [[createSparkEnv]] Creating SparkEnv for Driver -- createSparkEnv Method","title":"INFO SparkContext: Registered listener pl.japila.spark.CustomSparkListener\n"},{"location":"spark-SparkContext-creating-instance-internals/#source-scala_4","text":"createSparkEnv( conf: SparkConf, isLocal: Boolean, listenerBus: LiveListenerBus): SparkEnv createSparkEnv simply delegates the call to xref:core:SparkEnv.adoc#createDriverEnv[SparkEnv to create a SparkEnv for the driver]. It calculates the number of cores to 1 for local master URL, the number of processors available for JVM for * or the exact number in the master URL, or 0 for the cluster master URLs. === [[getCurrentUserName]] Utils.getCurrentUserName Method","title":"[source, scala]"},{"location":"spark-SparkContext-creating-instance-internals/#source-scala_5","text":"","title":"[source, scala]"},{"location":"spark-SparkContext-creating-instance-internals/#getcurrentusername-string","text":"getCurrentUserName computes the user name who has started the xref:ROOT:SparkContext.adoc[SparkContext] instance. NOTE: It is later available as xref:ROOT:SparkContext.adoc#sparkUser[SparkContext.sparkUser]. Internally, it reads xref:ROOT:SparkContext.adoc#SPARK_USER[SPARK_USER] environment variable and, if not set, reverts to Hadoop Security API's UserGroupInformation.getCurrentUser().getShortUserName() . NOTE: It is another place where Spark relies on Hadoop API for its operation. === [[localHostName]] Utils.localHostName Method localHostName computes the local host name. It starts by checking SPARK_LOCAL_HOSTNAME environment variable for the value. If it is not defined, it uses SPARK_LOCAL_IP to find the name (using InetAddress.getByName ). If it is not defined either, it calls InetAddress.getLocalHost for the name. NOTE: Utils.localHostName is executed while xref:ROOT:SparkContext.adoc#creating-instance[ SparkContext is created] and also to compute the default value of link:spark-driver.adoc#spark_driver_host[spark.driver.host Spark property]. CAUTION: FIXME Review the rest. === [[stopped]] stopped Flag CAUTION: FIXME Where is this used?","title":"getCurrentUserName(): String"},{"location":"spark-logging/","text":"Logging \u00b6 Spark uses log4j for logging. Logging Levels \u00b6 The valid logging levels are log4j's Levels (from most specific to least): OFF (most specific, no logging) FATAL (most specific, little data) ERROR WARN INFO DEBUG TRACE (least specific, a lot of data) ALL (least specific, all data) conf/log4j.properties \u00b6 You can set up the default logging for Spark shell in conf/log4j.properties . Use conf/log4j.properties.template as a starting point. Setting Default Log Level Programatically \u00b6 Refer to Setting Default Log Level Programatically in SparkContext -- Entry Point to Spark Core . Setting Log Levels in Spark Applications \u00b6 In standalone Spark applications or while in Spark Shell session, use the following: import org.apache.log4j.{Level, Logger} Logger.getLogger(classOf[RackResolver]).getLevel Logger.getLogger(\"org\").setLevel(Level.OFF) Logger.getLogger(\"akka\").setLevel(Level.OFF) sbt \u00b6 When running a Spark application from within sbt using run task, you can use the following build.sbt to configure logging levels: fork in run := true javaOptions in run ++= Seq( \"-Dlog4j.debug=true\", \"-Dlog4j.configuration=log4j.properties\") outputStrategy := Some(StdoutOutput) With the above configuration log4j.properties file should be on CLASSPATH which can be in src/main/resources directory (that is included in CLASSPATH by default). When run starts, you should see the following output in sbt: [spark-activator]> run [info] Running StreamingApp log4j: Trying to find [log4j.properties] using context classloader sun.misc.Launcher$AppClassLoader@1b6d3586. log4j: Using URL [file:/Users/jacek/dev/oss/spark-activator/target/scala-2.11/classes/log4j.properties] for automatic log4j configuration. log4j: Reading configuration from URL file:/Users/jacek/dev/oss/spark-activator/target/scala-2.11/classes/log4j.properties Disabling Logging \u00b6 Use the following conf/log4j.properties to disable logging completely: log4j.logger.org=OFF","title":"Logging"},{"location":"spark-logging/#logging","text":"Spark uses log4j for logging.","title":"Logging"},{"location":"spark-logging/#logging-levels","text":"The valid logging levels are log4j's Levels (from most specific to least): OFF (most specific, no logging) FATAL (most specific, little data) ERROR WARN INFO DEBUG TRACE (least specific, a lot of data) ALL (least specific, all data)","title":" Logging Levels"},{"location":"spark-logging/#conflog4jproperties","text":"You can set up the default logging for Spark shell in conf/log4j.properties . Use conf/log4j.properties.template as a starting point.","title":"conf/log4j.properties"},{"location":"spark-logging/#setting-default-log-level-programatically","text":"Refer to Setting Default Log Level Programatically in SparkContext -- Entry Point to Spark Core .","title":" Setting Default Log Level Programatically"},{"location":"spark-logging/#setting-log-levels-in-spark-applications","text":"In standalone Spark applications or while in Spark Shell session, use the following: import org.apache.log4j.{Level, Logger} Logger.getLogger(classOf[RackResolver]).getLevel Logger.getLogger(\"org\").setLevel(Level.OFF) Logger.getLogger(\"akka\").setLevel(Level.OFF)","title":" Setting Log Levels in Spark Applications"},{"location":"spark-logging/#sbt","text":"When running a Spark application from within sbt using run task, you can use the following build.sbt to configure logging levels: fork in run := true javaOptions in run ++= Seq( \"-Dlog4j.debug=true\", \"-Dlog4j.configuration=log4j.properties\") outputStrategy := Some(StdoutOutput) With the above configuration log4j.properties file should be on CLASSPATH which can be in src/main/resources directory (that is included in CLASSPATH by default). When run starts, you should see the following output in sbt: [spark-activator]> run [info] Running StreamingApp log4j: Trying to find [log4j.properties] using context classloader sun.misc.Launcher$AppClassLoader@1b6d3586. log4j: Using URL [file:/Users/jacek/dev/oss/spark-activator/target/scala-2.11/classes/log4j.properties] for automatic log4j configuration. log4j: Reading configuration from URL file:/Users/jacek/dev/oss/spark-activator/target/scala-2.11/classes/log4j.properties","title":"sbt"},{"location":"spark-logging/#disabling-logging","text":"Use the following conf/log4j.properties to disable logging completely: log4j.logger.org=OFF","title":"Disabling Logging"},{"location":"metrics/","text":"Spark Metrics \u00b6 Spark Metrics gives you execution metrics of Spark subsystems ( metrics instances , e.g. the driver of a Spark application or the master of a Spark Standalone cluster). Spark Metrics uses Dropwizard Metrics 3.1.0 Java library for the metrics infrastructure. Metrics is a Java library which gives you unparalleled insight into what your code does in production. Metrics provides a powerful toolkit of ways to measure the behavior of critical components in your production environment . Metrics Systems \u00b6 applicationMaster \u00b6 Registered when ApplicationMaster (Hadoop YARN) is requested to createAllocator applications \u00b6 Registered when Master (Spark Standalone) is created driver \u00b6 Registered when SparkEnv is created for the driver executor \u00b6 Registered when SparkEnv is created for an executor master \u00b6 Registered when Master (Spark Standalone) is created mesos_cluster \u00b6 Registered when MesosClusterScheduler (Apache Mesos) is created shuffleService \u00b6 Registered when ExternalShuffleService is created worker \u00b6 Registered when Worker (Spark Standalone) is created MetricsSystem \u00b6 Spark Metrics uses MetricsSystem . MetricsSystem uses Dropwizard Metrics' link:spark-metrics-MetricsSystem.md#registry[MetricRegistry] that acts as the integration point between Spark and the metrics library. A Spark subsystem can access the MetricsSystem through the SparkEnv.metricsSystem property. val metricsSystem = SparkEnv.get.metricsSystem MetricsConfig \u00b6 MetricsConfig is the configuration of the link:spark-metrics-MetricsSystem.md[MetricsSystem] (i.e. metrics link:spark-metrics-Source.md[sources] and link:spark-metrics-Sink.md[sinks]). metrics.properties is the default metrics configuration file. It is configured using link:spark-metrics-properties.md#spark.metrics.conf[spark.metrics.conf] configuration property. The file is first loaded from the path directly before using Spark's CLASSPATH. MetricsConfig also accepts a metrics configuration using spark.metrics.conf. -prefixed configuration properties. Spark comes with conf/metrics.properties.template file that is a template of metrics configuration. MetricsServlet Metrics Sink \u00b6 Among the metrics sinks is link:spark-metrics-MetricsServlet.md[MetricsServlet] that is used when sink.servlet metrics sink is configured in link:spark-metrics-MetricsConfig.md[metrics configuration]. CAUTION: FIXME Describe configuration files and properties JmxSink Metrics Sink \u00b6 Enable org.apache.spark.metrics.sink.JmxSink in link:spark-metrics-MetricsConfig.md[metrics configuration]. You can then use jconsole to access Spark metrics through JMX. *.sink.jmx.class=org.apache.spark.metrics.sink.JmxSink JSON URI Path \u00b6 Metrics System is available at http://localhost:4040/metrics/json (for the default setup of a Spark application). $ http --follow http://localhost:4040/metrics/json HTTP/1.1 200 OK Cache-Control: no-cache, no-store, must-revalidate Content-Length: 2200 Content-Type: text/json;charset=utf-8 Date: Sat, 25 Feb 2017 14:14:16 GMT Server: Jetty(9.2.z-SNAPSHOT) X-Frame-Options: SAMEORIGIN { \"counters\": { \"app-20170225151406-0000.driver.HiveExternalCatalog.fileCacheHits\": { \"count\": 0 }, \"app-20170225151406-0000.driver.HiveExternalCatalog.filesDiscovered\": { \"count\": 0 }, \"app-20170225151406-0000.driver.HiveExternalCatalog.hiveClientCalls\": { \"count\": 2 }, \"app-20170225151406-0000.driver.HiveExternalCatalog.parallelListingJobCount\": { \"count\": 0 }, \"app-20170225151406-0000.driver.HiveExternalCatalog.partitionsFetched\": { \"count\": 0 } }, \"gauges\": { ... \"timers\": { \"app-20170225151406-0000.driver.DAGScheduler.messageProcessingTime\": { \"count\": 0, \"duration_units\": \"milliseconds\", \"m15_rate\": 0.0, \"m1_rate\": 0.0, \"m5_rate\": 0.0, \"max\": 0.0, \"mean\": 0.0, \"mean_rate\": 0.0, \"min\": 0.0, \"p50\": 0.0, \"p75\": 0.0, \"p95\": 0.0, \"p98\": 0.0, \"p99\": 0.0, \"p999\": 0.0, \"rate_units\": \"calls/second\", \"stddev\": 0.0 } }, \"version\": \"3.0.0\" } NOTE: You can access a Spark subsystem's MetricsSystem using its corresponding \"leading\" port, e.g. 4040 for the driver , 8080 for Spark Standalone's master and applications . NOTE: You have to use the trailing slash ( / ) to have the output. Spark Standalone Master \u00b6 $ http http://192.168.1.4:8080/metrics/master/json/path HTTP/1.1 200 OK Cache-Control: no-cache, no-store, must-revalidate Content-Length: 207 Content-Type: text/json;charset=UTF-8 Server: Jetty(8.y.z-SNAPSHOT) X-Frame-Options: SAMEORIGIN { \"counters\": {}, \"gauges\": { \"master.aliveWorkers\": { \"value\": 0 }, \"master.apps\": { \"value\": 0 }, \"master.waitingApps\": { \"value\": 0 }, \"master.workers\": { \"value\": 0 } }, \"histograms\": {}, \"meters\": {}, \"timers\": {}, \"version\": \"3.0.0\" }","title":"Spark Metrics"},{"location":"metrics/#spark-metrics","text":"Spark Metrics gives you execution metrics of Spark subsystems ( metrics instances , e.g. the driver of a Spark application or the master of a Spark Standalone cluster). Spark Metrics uses Dropwizard Metrics 3.1.0 Java library for the metrics infrastructure. Metrics is a Java library which gives you unparalleled insight into what your code does in production. Metrics provides a powerful toolkit of ways to measure the behavior of critical components in your production environment .","title":"Spark Metrics"},{"location":"metrics/#metrics-systems","text":"","title":"Metrics Systems"},{"location":"metrics/#applicationmaster","text":"Registered when ApplicationMaster (Hadoop YARN) is requested to createAllocator","title":"applicationMaster"},{"location":"metrics/#applications","text":"Registered when Master (Spark Standalone) is created","title":"applications"},{"location":"metrics/#driver","text":"Registered when SparkEnv is created for the driver","title":"driver"},{"location":"metrics/#executor","text":"Registered when SparkEnv is created for an executor","title":"executor"},{"location":"metrics/#master","text":"Registered when Master (Spark Standalone) is created","title":"master"},{"location":"metrics/#mesos_cluster","text":"Registered when MesosClusterScheduler (Apache Mesos) is created","title":"mesos_cluster"},{"location":"metrics/#shuffleservice","text":"Registered when ExternalShuffleService is created","title":"shuffleService"},{"location":"metrics/#worker","text":"Registered when Worker (Spark Standalone) is created","title":"worker"},{"location":"metrics/#metricssystem","text":"Spark Metrics uses MetricsSystem . MetricsSystem uses Dropwizard Metrics' link:spark-metrics-MetricsSystem.md#registry[MetricRegistry] that acts as the integration point between Spark and the metrics library. A Spark subsystem can access the MetricsSystem through the SparkEnv.metricsSystem property. val metricsSystem = SparkEnv.get.metricsSystem","title":" MetricsSystem"},{"location":"metrics/#metricsconfig","text":"MetricsConfig is the configuration of the link:spark-metrics-MetricsSystem.md[MetricsSystem] (i.e. metrics link:spark-metrics-Source.md[sources] and link:spark-metrics-Sink.md[sinks]). metrics.properties is the default metrics configuration file. It is configured using link:spark-metrics-properties.md#spark.metrics.conf[spark.metrics.conf] configuration property. The file is first loaded from the path directly before using Spark's CLASSPATH. MetricsConfig also accepts a metrics configuration using spark.metrics.conf. -prefixed configuration properties. Spark comes with conf/metrics.properties.template file that is a template of metrics configuration.","title":" MetricsConfig"},{"location":"metrics/#metricsservlet-metrics-sink","text":"Among the metrics sinks is link:spark-metrics-MetricsServlet.md[MetricsServlet] that is used when sink.servlet metrics sink is configured in link:spark-metrics-MetricsConfig.md[metrics configuration]. CAUTION: FIXME Describe configuration files and properties","title":" MetricsServlet Metrics Sink"},{"location":"metrics/#jmxsink-metrics-sink","text":"Enable org.apache.spark.metrics.sink.JmxSink in link:spark-metrics-MetricsConfig.md[metrics configuration]. You can then use jconsole to access Spark metrics through JMX. *.sink.jmx.class=org.apache.spark.metrics.sink.JmxSink","title":" JmxSink Metrics Sink"},{"location":"metrics/#json-uri-path","text":"Metrics System is available at http://localhost:4040/metrics/json (for the default setup of a Spark application). $ http --follow http://localhost:4040/metrics/json HTTP/1.1 200 OK Cache-Control: no-cache, no-store, must-revalidate Content-Length: 2200 Content-Type: text/json;charset=utf-8 Date: Sat, 25 Feb 2017 14:14:16 GMT Server: Jetty(9.2.z-SNAPSHOT) X-Frame-Options: SAMEORIGIN { \"counters\": { \"app-20170225151406-0000.driver.HiveExternalCatalog.fileCacheHits\": { \"count\": 0 }, \"app-20170225151406-0000.driver.HiveExternalCatalog.filesDiscovered\": { \"count\": 0 }, \"app-20170225151406-0000.driver.HiveExternalCatalog.hiveClientCalls\": { \"count\": 2 }, \"app-20170225151406-0000.driver.HiveExternalCatalog.parallelListingJobCount\": { \"count\": 0 }, \"app-20170225151406-0000.driver.HiveExternalCatalog.partitionsFetched\": { \"count\": 0 } }, \"gauges\": { ... \"timers\": { \"app-20170225151406-0000.driver.DAGScheduler.messageProcessingTime\": { \"count\": 0, \"duration_units\": \"milliseconds\", \"m15_rate\": 0.0, \"m1_rate\": 0.0, \"m5_rate\": 0.0, \"max\": 0.0, \"mean\": 0.0, \"mean_rate\": 0.0, \"min\": 0.0, \"p50\": 0.0, \"p75\": 0.0, \"p95\": 0.0, \"p98\": 0.0, \"p99\": 0.0, \"p999\": 0.0, \"rate_units\": \"calls/second\", \"stddev\": 0.0 } }, \"version\": \"3.0.0\" } NOTE: You can access a Spark subsystem's MetricsSystem using its corresponding \"leading\" port, e.g. 4040 for the driver , 8080 for Spark Standalone's master and applications . NOTE: You have to use the trailing slash ( / ) to have the output.","title":"JSON URI Path"},{"location":"metrics/#spark-standalone-master","text":"$ http http://192.168.1.4:8080/metrics/master/json/path HTTP/1.1 200 OK Cache-Control: no-cache, no-store, must-revalidate Content-Length: 207 Content-Type: text/json;charset=UTF-8 Server: Jetty(8.y.z-SNAPSHOT) X-Frame-Options: SAMEORIGIN { \"counters\": {}, \"gauges\": { \"master.aliveWorkers\": { \"value\": 0 }, \"master.apps\": { \"value\": 0 }, \"master.waitingApps\": { \"value\": 0 }, \"master.workers\": { \"value\": 0 } }, \"histograms\": {}, \"meters\": {}, \"timers\": {}, \"version\": \"3.0.0\" }","title":"Spark Standalone Master"},{"location":"metrics/DAGSchedulerSource/","text":"DAGSchedulerSource \u00b6 DAGSchedulerSource is the metrics source of DAGScheduler . DAGScheduler uses Spark Metrics System to report metrics about internal status. The name of the source is DAGScheduler . DAGSchedulerSource emits the following metrics: stage.failedStages - the number of failed stages stage.runningStages - the number of running stages stage.waitingStages - the number of waiting stages job.allJobs - the number of all jobs job.activeJobs - the number of active jobs","title":"DAGSchedulerSource"},{"location":"metrics/DAGSchedulerSource/#dagschedulersource","text":"DAGSchedulerSource is the metrics source of DAGScheduler . DAGScheduler uses Spark Metrics System to report metrics about internal status. The name of the source is DAGScheduler . DAGSchedulerSource emits the following metrics: stage.failedStages - the number of failed stages stage.runningStages - the number of running stages stage.waitingStages - the number of waiting stages job.allJobs - the number of all jobs job.activeJobs - the number of active jobs","title":"DAGSchedulerSource"},{"location":"metrics/JvmSource/","text":"JvmSource \u00b6 JvmSource is a metrics source . The name of the source is jvm . JvmSource registers the build-in Codehale metrics: GarbageCollectorMetricSet MemoryUsageGaugeSet BufferPoolMetricSet Among the metrics is total.committed (from MemoryUsageGaugeSet ) that describes the current usage of the heap and non-heap memories.","title":"JvmSource"},{"location":"metrics/JvmSource/#jvmsource","text":"JvmSource is a metrics source . The name of the source is jvm . JvmSource registers the build-in Codehale metrics: GarbageCollectorMetricSet MemoryUsageGaugeSet BufferPoolMetricSet Among the metrics is total.committed (from MemoryUsageGaugeSet ) that describes the current usage of the heap and non-heap memories.","title":"JvmSource"},{"location":"metrics/MetricsConfig/","text":"MetricsConfig \u00b6 MetricsConfig is the configuration of the MetricsSystem (i.e. metrics sources and sinks ). MetricsConfig is < > when link:spark-metrics-MetricsSystem.adoc#creating-instance[MetricsSystem] is. MetricsConfig uses metrics.properties as the default metrics configuration file. It is configured using link:spark-metrics-properties.adoc#spark.metrics.conf[spark.metrics.conf] configuration property. The file is first loaded from the path directly before using Spark's CLASSPATH. MetricsConfig accepts a metrics configuration using spark.metrics.conf. -prefixed configuration properties. Spark comes with conf/metrics.properties.template file that is a template of metrics configuration. MetricsConfig < > that the < > are always defined. [[default-properties]] .MetricsConfig's Default Metrics Properties [cols=\"1,2\",options=\"header\",width=\"100%\"] |=== | Name | Description | *.sink.servlet.class | org.apache.spark.metrics.sink.MetricsServlet | *.sink.servlet.path | /metrics/json | master.sink.servlet.path | /metrics/master/json | applications.sink.servlet.path | /metrics/applications/json |=== [NOTE] \u00b6 The order of precedence of metrics configuration settings is as follows: . < > . link:spark-metrics-properties.adoc#spark.metrics.conf[spark.metrics.conf] configuration property or metrics.properties configuration file . spark.metrics.conf. -prefixed Spark properties ==== [[creating-instance]] [[conf]] MetricsConfig takes a xref:ROOT:SparkConf.adoc[SparkConf] when created. [[internal-registries]] .MetricsConfig's Internal Registries and Counters [cols=\"1,2\",options=\"header\",width=\"100%\"] |=== | Name | Description | [[properties]] properties | https://docs.oracle.com/javase/8/docs/api/java/util/Properties.html[java.util.Properties ] with metrics properties Used to < > per-subsystem's < >. | [[perInstanceSubProperties]] perInstanceSubProperties | Lookup table of metrics properties per subsystem |=== === [[initialize]] Initializing MetricsConfig -- initialize Method [source, scala] \u00b6 initialize(): Unit \u00b6 initialize < > and < > (that is defined using link:spark-metrics-properties.adoc#spark.metrics.conf[spark.metrics.conf] configuration property). initialize takes all Spark properties that start with spark.metrics.conf. prefix from < > and adds them to < > (without the prefix). In the end, initialize splits < > with the default configuration (denoted as * ) assigned to all subsystems afterwards. NOTE: initialize accepts * (star) for the default configuration or any combination of lower- and upper-case letters for Spark subsystem names. NOTE: initialize is used exclusively when MetricsSystem is link:spark-metrics-MetricsSystem.adoc#creating-instance[created]. === [[setDefaultProperties]] setDefaultProperties Internal Method [source, scala] \u00b6 setDefaultProperties(prop: Properties): Unit \u00b6 setDefaultProperties sets the < > (in the input prop ). NOTE: setDefaultProperties is used exclusively when MetricsConfig < >. === [[loadPropertiesFromFile]] Loading Custom Metrics Configuration File or metrics.properties -- loadPropertiesFromFile Method [source, scala] \u00b6 loadPropertiesFromFile(path: Option[String]): Unit \u00b6 loadPropertiesFromFile tries to open the input path file (if defined) or the default metrics configuration file metrics.properties (on CLASSPATH). If either file is available, loadPropertiesFromFile loads the properties (to < > registry). In case of exceptions, you should see the following ERROR message in the logs followed by the exception. ERROR Error loading configuration file [file] NOTE: loadPropertiesFromFile is used exclusively when MetricsConfig < >. === [[subProperties]] Grouping Properties Per Subsystem -- subProperties Method [source, scala] \u00b6 subProperties(prop: Properties, regex: Regex): mutable.HashMap[String, Properties] \u00b6 subProperties takes prop properties and destructures keys given regex . subProperties takes the matching prefix (of a key per regex ) and uses it as a new key with the value(s) being the matching suffix(es). [source, scala] \u00b6 driver.hello.world => (driver, (hello.world)) \u00b6 NOTE: subProperties is used when MetricsConfig < > (to apply the default metrics configuration) and when MetricsSystem link:spark-metrics-MetricsSystem.adoc#registerSources[registers metrics sources] and link:spark-metrics-MetricsSystem.adoc#registerSinks[sinks]. === [[getInstance]] getInstance Method [source, scala] \u00b6 getInstance(inst: String): Properties \u00b6 getInstance ...FIXME NOTE: getInstance is used when...FIXME","title":"MetricsConfig"},{"location":"metrics/MetricsConfig/#metricsconfig","text":"MetricsConfig is the configuration of the MetricsSystem (i.e. metrics sources and sinks ). MetricsConfig is < > when link:spark-metrics-MetricsSystem.adoc#creating-instance[MetricsSystem] is. MetricsConfig uses metrics.properties as the default metrics configuration file. It is configured using link:spark-metrics-properties.adoc#spark.metrics.conf[spark.metrics.conf] configuration property. The file is first loaded from the path directly before using Spark's CLASSPATH. MetricsConfig accepts a metrics configuration using spark.metrics.conf. -prefixed configuration properties. Spark comes with conf/metrics.properties.template file that is a template of metrics configuration. MetricsConfig < > that the < > are always defined. [[default-properties]] .MetricsConfig's Default Metrics Properties [cols=\"1,2\",options=\"header\",width=\"100%\"] |=== | Name | Description | *.sink.servlet.class | org.apache.spark.metrics.sink.MetricsServlet | *.sink.servlet.path | /metrics/json | master.sink.servlet.path | /metrics/master/json | applications.sink.servlet.path | /metrics/applications/json |===","title":"MetricsConfig"},{"location":"metrics/MetricsConfig/#note","text":"The order of precedence of metrics configuration settings is as follows: . < > . link:spark-metrics-properties.adoc#spark.metrics.conf[spark.metrics.conf] configuration property or metrics.properties configuration file . spark.metrics.conf. -prefixed Spark properties ==== [[creating-instance]] [[conf]] MetricsConfig takes a xref:ROOT:SparkConf.adoc[SparkConf] when created. [[internal-registries]] .MetricsConfig's Internal Registries and Counters [cols=\"1,2\",options=\"header\",width=\"100%\"] |=== | Name | Description | [[properties]] properties | https://docs.oracle.com/javase/8/docs/api/java/util/Properties.html[java.util.Properties ] with metrics properties Used to < > per-subsystem's < >. | [[perInstanceSubProperties]] perInstanceSubProperties | Lookup table of metrics properties per subsystem |=== === [[initialize]] Initializing MetricsConfig -- initialize Method","title":"[NOTE]"},{"location":"metrics/MetricsConfig/#source-scala","text":"","title":"[source, scala]"},{"location":"metrics/MetricsConfig/#initialize-unit","text":"initialize < > and < > (that is defined using link:spark-metrics-properties.adoc#spark.metrics.conf[spark.metrics.conf] configuration property). initialize takes all Spark properties that start with spark.metrics.conf. prefix from < > and adds them to < > (without the prefix). In the end, initialize splits < > with the default configuration (denoted as * ) assigned to all subsystems afterwards. NOTE: initialize accepts * (star) for the default configuration or any combination of lower- and upper-case letters for Spark subsystem names. NOTE: initialize is used exclusively when MetricsSystem is link:spark-metrics-MetricsSystem.adoc#creating-instance[created]. === [[setDefaultProperties]] setDefaultProperties Internal Method","title":"initialize(): Unit"},{"location":"metrics/MetricsConfig/#source-scala_1","text":"","title":"[source, scala]"},{"location":"metrics/MetricsConfig/#setdefaultpropertiesprop-properties-unit","text":"setDefaultProperties sets the < > (in the input prop ). NOTE: setDefaultProperties is used exclusively when MetricsConfig < >. === [[loadPropertiesFromFile]] Loading Custom Metrics Configuration File or metrics.properties -- loadPropertiesFromFile Method","title":"setDefaultProperties(prop: Properties): Unit"},{"location":"metrics/MetricsConfig/#source-scala_2","text":"","title":"[source, scala]"},{"location":"metrics/MetricsConfig/#loadpropertiesfromfilepath-optionstring-unit","text":"loadPropertiesFromFile tries to open the input path file (if defined) or the default metrics configuration file metrics.properties (on CLASSPATH). If either file is available, loadPropertiesFromFile loads the properties (to < > registry). In case of exceptions, you should see the following ERROR message in the logs followed by the exception. ERROR Error loading configuration file [file] NOTE: loadPropertiesFromFile is used exclusively when MetricsConfig < >. === [[subProperties]] Grouping Properties Per Subsystem -- subProperties Method","title":"loadPropertiesFromFile(path: Option[String]): Unit"},{"location":"metrics/MetricsConfig/#source-scala_3","text":"","title":"[source, scala]"},{"location":"metrics/MetricsConfig/#subpropertiesprop-properties-regex-regex-mutablehashmapstring-properties","text":"subProperties takes prop properties and destructures keys given regex . subProperties takes the matching prefix (of a key per regex ) and uses it as a new key with the value(s) being the matching suffix(es).","title":"subProperties(prop: Properties, regex: Regex): mutable.HashMap[String, Properties]"},{"location":"metrics/MetricsConfig/#source-scala_4","text":"","title":"[source, scala]"},{"location":"metrics/MetricsConfig/#driverhelloworld-driver-helloworld","text":"NOTE: subProperties is used when MetricsConfig < > (to apply the default metrics configuration) and when MetricsSystem link:spark-metrics-MetricsSystem.adoc#registerSources[registers metrics sources] and link:spark-metrics-MetricsSystem.adoc#registerSinks[sinks]. === [[getInstance]] getInstance Method","title":"driver.hello.world =&gt; (driver, (hello.world))"},{"location":"metrics/MetricsConfig/#source-scala_5","text":"","title":"[source, scala]"},{"location":"metrics/MetricsConfig/#getinstanceinst-string-properties","text":"getInstance ...FIXME NOTE: getInstance is used when...FIXME","title":"getInstance(inst: String): Properties"},{"location":"metrics/MetricsServlet/","text":"MetricsServlet JSON Metrics Sink \u00b6 MetricsServlet is a metrics sink that gives metrics snapshots in JSON format. MetricsServlet is a \"special\" sink as it is only available to the metrics instances with a web UI: Driver of a Spark application Spark Standalone's Master and Worker You can access the metrics from MetricsServlet at /metrics/json URI by default. The entire URL depends on a metrics instance, e.g. http://localhost:4040/metrics/json/ for a running Spark application. $ http http://localhost:4040/metrics/json/ HTTP/1.1 200 OK Cache-Control: no-cache, no-store, must-revalidate Content-Length: 5005 Content-Type: text/json;charset=utf-8 Date: Mon, 11 Jun 2018 06:29:03 GMT Server: Jetty(9.3.z-SNAPSHOT) X-Content-Type-Options: nosniff X-Frame-Options: SAMEORIGIN X-XSS-Protection: 1; mode=block { \"counters\": { \"local-1528698499919.driver.HiveExternalCatalog.fileCacheHits\": { \"count\": 0 }, \"local-1528698499919.driver.HiveExternalCatalog.filesDiscovered\": { \"count\": 0 }, \"local-1528698499919.driver.HiveExternalCatalog.hiveClientCalls\": { \"count\": 0 }, \"local-1528698499919.driver.HiveExternalCatalog.parallelListingJobCount\": { \"count\": 0 }, \"local-1528698499919.driver.HiveExternalCatalog.partitionsFetched\": { \"count\": 0 }, \"local-1528698499919.driver.LiveListenerBus.numEventsPosted\": { \"count\": 7 }, \"local-1528698499919.driver.LiveListenerBus.queue.appStatus.numDroppedEvents\": { \"count\": 0 }, \"local-1528698499919.driver.LiveListenerBus.queue.executorManagement.numDroppedEvents\": { \"count\": 0 } }, ... MetricsServlet is < > exclusively when MetricsSystem is started (and requested to register metrics sinks ). MetricsServlet can be configured using configuration properties with sink.servlet prefix (in link:spark-metrics-MetricsConfig.adoc[metrics configuration]). That is not required since MetricsConfig link:spark-metrics-MetricsConfig.adoc#setDefaultProperties[makes sure] that MetricsServlet is always configured. MetricsServlet uses https://fasterxml.github.io/jackson-databind/[jackson-databind ], the general data-binding package for Jackson (as < >) with https://metrics.dropwizard.io/3.1.0/[Dropwizard Metrics] library (i.e. registering a Coda Hale MetricsModule ). [[properties]] .MetricsServlet's Configuration Properties [cols=\"1,1,2\",options=\"header\",width=\"100%\"] |=== | Name | Default | Description | path | /metrics/json/ | [[path]] Path URI prefix to bind to | sample | false | [[sample]] Whether to show entire set of samples for histograms |=== [[internal-registries]] .MetricsServlet's Internal Properties (e.g. Registries, Counters and Flags) [cols=\"1,2\",options=\"header\",width=\"100%\"] |=== | Name | Description | mapper | [[mapper]] Jaxson's https://fasterxml.github.io/jackson-databind/javadoc/2.6/com/fasterxml/jackson/databind/ObjectMapper.html[com.fasterxml.jackson.databind.ObjectMapper ] that \"provides functionality for reading and writing JSON, either to and from basic POJOs (Plain Old Java Objects), or to and from a general-purpose JSON Tree Model (JsonNode), as well as related functionality for performing conversions.\" When created, mapper is requested to register a Coda Hale https://metrics.dropwizard.io/3.1.0/apidocs/com/codahale/metrics/json/MetricsModule.html[com.codahale.metrics.json.MetricsModule ]. Used exclusively when MetricsServlet is requested to < >. | servletPath | [[servletPath]] Value of < > configuration property | servletShowSample | [[servletShowSample]] Flag to control whether to show samples ( true ) or not ( false ). servletShowSample is the value of < > configuration property (if defined) or false . Used when < > is requested to register a Coda Hale https://metrics.dropwizard.io/3.1.0/apidocs/com/codahale/metrics/json/MetricsModule.html[com.codahale.metrics.json.MetricsModule ]. |=== === [[creating-instance]] Creating MetricsServlet Instance MetricsServlet takes the following when created: [[property]] Configuration Properties (as Java Properties ) [[registry]] Dropwizard Metrics' https://metrics.dropwizard.io/3.1.0/apidocs/com/codahale/metrics/MetricRegistry.html[MetricRegistry ] [[securityMgr]] SecurityManager MetricsServlet initializes the < >. === [[getMetricsSnapshot]] Requesting Metrics Snapshot -- getMetricsSnapshot Method [source, scala] \u00b6 getMetricsSnapshot(request: HttpServletRequest): String \u00b6 getMetricsSnapshot simply requests the < > to serialize the < > to a JSON string (using link:++ https://fasterxml.github.io/jackson-databind/javadoc/2.6/com/fasterxml/jackson/databind/ObjectMapper.html#writeValueAsString-java.lang.Object-++[ObjectMapper.writeValueAsString ]). NOTE: getMetricsSnapshot is used exclusively when MetricsServlet is requested to < >. === [[getHandlers]] Requesting JSON Servlet Handler -- getHandlers Method [source, scala] \u00b6 getHandlers(conf: SparkConf): Array[ServletContextHandler] \u00b6 getHandlers returns just a single ServletContextHandler (in a collection) that gives < > in JSON format at every request at < > URI path. NOTE: getHandlers is used exclusively when MetricsSystem is requested for link:MetricsSystem.md#getServletHandlers[metrics ServletContextHandlers].","title":"MetricsServlet"},{"location":"metrics/MetricsServlet/#metricsservlet-json-metrics-sink","text":"MetricsServlet is a metrics sink that gives metrics snapshots in JSON format. MetricsServlet is a \"special\" sink as it is only available to the metrics instances with a web UI: Driver of a Spark application Spark Standalone's Master and Worker You can access the metrics from MetricsServlet at /metrics/json URI by default. The entire URL depends on a metrics instance, e.g. http://localhost:4040/metrics/json/ for a running Spark application. $ http http://localhost:4040/metrics/json/ HTTP/1.1 200 OK Cache-Control: no-cache, no-store, must-revalidate Content-Length: 5005 Content-Type: text/json;charset=utf-8 Date: Mon, 11 Jun 2018 06:29:03 GMT Server: Jetty(9.3.z-SNAPSHOT) X-Content-Type-Options: nosniff X-Frame-Options: SAMEORIGIN X-XSS-Protection: 1; mode=block { \"counters\": { \"local-1528698499919.driver.HiveExternalCatalog.fileCacheHits\": { \"count\": 0 }, \"local-1528698499919.driver.HiveExternalCatalog.filesDiscovered\": { \"count\": 0 }, \"local-1528698499919.driver.HiveExternalCatalog.hiveClientCalls\": { \"count\": 0 }, \"local-1528698499919.driver.HiveExternalCatalog.parallelListingJobCount\": { \"count\": 0 }, \"local-1528698499919.driver.HiveExternalCatalog.partitionsFetched\": { \"count\": 0 }, \"local-1528698499919.driver.LiveListenerBus.numEventsPosted\": { \"count\": 7 }, \"local-1528698499919.driver.LiveListenerBus.queue.appStatus.numDroppedEvents\": { \"count\": 0 }, \"local-1528698499919.driver.LiveListenerBus.queue.executorManagement.numDroppedEvents\": { \"count\": 0 } }, ... MetricsServlet is < > exclusively when MetricsSystem is started (and requested to register metrics sinks ). MetricsServlet can be configured using configuration properties with sink.servlet prefix (in link:spark-metrics-MetricsConfig.adoc[metrics configuration]). That is not required since MetricsConfig link:spark-metrics-MetricsConfig.adoc#setDefaultProperties[makes sure] that MetricsServlet is always configured. MetricsServlet uses https://fasterxml.github.io/jackson-databind/[jackson-databind ], the general data-binding package for Jackson (as < >) with https://metrics.dropwizard.io/3.1.0/[Dropwizard Metrics] library (i.e. registering a Coda Hale MetricsModule ). [[properties]] .MetricsServlet's Configuration Properties [cols=\"1,1,2\",options=\"header\",width=\"100%\"] |=== | Name | Default | Description | path | /metrics/json/ | [[path]] Path URI prefix to bind to | sample | false | [[sample]] Whether to show entire set of samples for histograms |=== [[internal-registries]] .MetricsServlet's Internal Properties (e.g. Registries, Counters and Flags) [cols=\"1,2\",options=\"header\",width=\"100%\"] |=== | Name | Description | mapper | [[mapper]] Jaxson's https://fasterxml.github.io/jackson-databind/javadoc/2.6/com/fasterxml/jackson/databind/ObjectMapper.html[com.fasterxml.jackson.databind.ObjectMapper ] that \"provides functionality for reading and writing JSON, either to and from basic POJOs (Plain Old Java Objects), or to and from a general-purpose JSON Tree Model (JsonNode), as well as related functionality for performing conversions.\" When created, mapper is requested to register a Coda Hale https://metrics.dropwizard.io/3.1.0/apidocs/com/codahale/metrics/json/MetricsModule.html[com.codahale.metrics.json.MetricsModule ]. Used exclusively when MetricsServlet is requested to < >. | servletPath | [[servletPath]] Value of < > configuration property | servletShowSample | [[servletShowSample]] Flag to control whether to show samples ( true ) or not ( false ). servletShowSample is the value of < > configuration property (if defined) or false . Used when < > is requested to register a Coda Hale https://metrics.dropwizard.io/3.1.0/apidocs/com/codahale/metrics/json/MetricsModule.html[com.codahale.metrics.json.MetricsModule ]. |=== === [[creating-instance]] Creating MetricsServlet Instance MetricsServlet takes the following when created: [[property]] Configuration Properties (as Java Properties ) [[registry]] Dropwizard Metrics' https://metrics.dropwizard.io/3.1.0/apidocs/com/codahale/metrics/MetricRegistry.html[MetricRegistry ] [[securityMgr]] SecurityManager MetricsServlet initializes the < >. === [[getMetricsSnapshot]] Requesting Metrics Snapshot -- getMetricsSnapshot Method","title":"MetricsServlet JSON Metrics Sink"},{"location":"metrics/MetricsServlet/#source-scala","text":"","title":"[source, scala]"},{"location":"metrics/MetricsServlet/#getmetricssnapshotrequest-httpservletrequest-string","text":"getMetricsSnapshot simply requests the < > to serialize the < > to a JSON string (using link:++ https://fasterxml.github.io/jackson-databind/javadoc/2.6/com/fasterxml/jackson/databind/ObjectMapper.html#writeValueAsString-java.lang.Object-++[ObjectMapper.writeValueAsString ]). NOTE: getMetricsSnapshot is used exclusively when MetricsServlet is requested to < >. === [[getHandlers]] Requesting JSON Servlet Handler -- getHandlers Method","title":"getMetricsSnapshot(request: HttpServletRequest): String"},{"location":"metrics/MetricsServlet/#source-scala_1","text":"","title":"[source, scala]"},{"location":"metrics/MetricsServlet/#gethandlersconf-sparkconf-arrayservletcontexthandler","text":"getHandlers returns just a single ServletContextHandler (in a collection) that gives < > in JSON format at every request at < > URI path. NOTE: getHandlers is used exclusively when MetricsSystem is requested for link:MetricsSystem.md#getServletHandlers[metrics ServletContextHandlers].","title":"getHandlers(conf: SparkConf): Array[ServletContextHandler]"},{"location":"metrics/MetricsSystem/","text":"MetricsSystem \u00b6 MetricsSystem is a registry of metrics sources and sinks of a Spark subsystem . Creating Instance \u00b6 MetricsSystem takes the following to be created: Instance Name SparkConf SecurityManager While being created, MetricsSystem requests the MetricsConfig to initialize . MetricsSystem is created (using createMetricsSystem utility) for the Metrics Systems . Creating MetricsSystem \u00b6 createMetricsSystem ( instance : String conf : SparkConf securityMgr : SecurityManager ) : MetricsSystem createMetricsSystem creates a new MetricsSystem (for the given parameters). createMetricsSystem is used to create metrics systems . Metrics Sources for Spark SQL \u00b6 CodegenMetrics HiveCatalogMetrics Registering Metrics Source \u00b6 registerSource ( source : Source ) : Unit registerSource adds source to the sources internal registry. registerSource creates an identifier for the metrics source and registers it with the MetricRegistry . registerSource uses Metrics' MetricRegistry.register to register a metrics source under a given name. registerSource prints out the following INFO message to the logs when registering a name more than once: Metrics already registered Building Metrics Source Identifier \u00b6 buildRegistryName ( source : Source ) : String buildRegistryName uses spark-metrics-properties.md#spark.metrics.namespace[spark.metrics.namespace] and xref:executor:Executor.md#spark.executor.id[spark.executor.id] Spark properties to differentiate between a Spark application's driver and executors, and the other Spark framework's components. (only when < > is driver or executor ) buildRegistryName builds metrics source name that is made up of link:spark-metrics-properties.md#spark.metrics.namespace[spark.metrics.namespace], xref:executor:Executor.md#spark.executor.id[spark.executor.id] and the name of the source . Note buildRegistryName uses Dropwizard Metrics' MetricRegistry to build metrics source identifiers. FIXME Finish for the other components. buildRegistryName is used when MetricsSystem is requested to register or remove a metrics source. Registering Metrics Sources for Spark Instance \u00b6 registerSources () : Unit registerSources finds < > configuration for the < >. NOTE: instance is defined when MetricsSystem < >. registerSources finds the configuration of all the link:spark-metrics-Source.md[metrics sources] for the subsystem (as described with source. prefix). For every metrics source, registerSources finds class property, creates an instance, and in the end < >. When registerSources fails, you should see the following ERROR message in the logs followed by the exception. Source class [classPath] cannot be instantiated registerSources is used when MetricsSystem is requested to start . Requesting JSON Servlet Handler \u00b6 getServletHandlers : Array [ ServletContextHandler ] If the MetricsSystem is < > and the < > is defined for the metrics system, getServletHandlers simply requests the < > for the link:spark-metrics-MetricsServlet.md#getHandlers[JSON servlet handler]. When MetricsSystem is not < > getServletHandlers throws an IllegalArgumentException . Can only call getServletHandlers on a running MetricsSystem getServletHandlers is used when: SparkContext is created (Spark Standalone) Master and Worker are requested to start Registering Metrics Sinks \u00b6 registerSinks () : Unit registerSinks requests the < > for the link:spark-metrics-MetricsConfig.md#getInstance[configuration] of the < >. registerSinks requests the < > for the link:spark-metrics-MetricsConfig.md#subProperties[configuration] of all metrics sinks (i.e. configuration entries that match ^sink\\\\.(.+)\\\\.(.+) regular expression). For every metrics sink configuration, registerSinks takes class property and (if defined) creates an instance of the metric sink using an constructor that takes the configuration, < > and < >. For a single servlet metrics sink, registerSinks converts the sink to a link:spark-metrics-MetricsServlet.md[MetricsServlet] and sets the < > internal registry. For all other metrics sinks, registerSinks adds the sink to the < > internal registry. In case of an Exception , registerSinks prints out the following ERROR message to the logs: Sink class [classPath] cannot be instantiated registerSinks is used when MetricsSystem is requested to start . Stopping \u00b6 stop () : Unit stop ...FIXME Reporting Metrics \u00b6 report () : Unit report simply requests the registered metrics sinks to report metrics . Starting \u00b6 start () : Unit start turns < > flag on. NOTE: start can only be called once and < > an IllegalArgumentException when called multiple times. start < > the < > for Spark SQL, i.e. CodegenMetrics and HiveCatalogMetrics . start then registers the configured metrics < > and < > for the < >. In the end, start requests the registered < > to link:spark-metrics-Sink.md#start[start]. [[start-IllegalArgumentException]] start throws an IllegalArgumentException when < > flag is on. requirement failed: Attempting to start a MetricsSystem that is already running Logging \u00b6 Enable ALL logging level for org.apache.spark.metrics.MetricsSystem logger to see what happens inside. Add the following line to conf/log4j.properties : log4j.logger.org.apache.spark.metrics.MetricsSystem=ALL Refer to Logging . Internal Registries \u00b6 MetricRegistry \u00b6 Integration point to Dropwizard Metrics' MetricRegistry Used when MetricsSystem is requested to: Register or remove a metrics source Start (that in turn registers metrics sinks ) MetricsConfig \u00b6 MetricsConfig Initialized when MetricsSystem is < >. Used when MetricsSystem registers < > and < >. MetricsServlet \u00b6 MetricsServlet JSON metrics sink that is only available for the < > with a web UI (i.e. the driver of a Spark application and Spark Standalone's Master ). MetricsSystem may have at most one MetricsServlet JSON metrics sink (which is registered by default ). Initialized when MetricsSystem registers < > (and finds a configuration entry with servlet sink name). Used when MetricsSystem is requested for a < >. running Flag \u00b6 Indicates whether MetricsSystem has been started ( true ) or not ( false ) Default: false sinks \u00b6 Metrics sinks Used when MetricsSystem < > and < >. sources \u00b6 Metrics sources Used when MetricsSystem < >.","title":"MetricsSystem"},{"location":"metrics/MetricsSystem/#metricssystem","text":"MetricsSystem is a registry of metrics sources and sinks of a Spark subsystem .","title":"MetricsSystem"},{"location":"metrics/MetricsSystem/#creating-instance","text":"MetricsSystem takes the following to be created: Instance Name SparkConf SecurityManager While being created, MetricsSystem requests the MetricsConfig to initialize . MetricsSystem is created (using createMetricsSystem utility) for the Metrics Systems .","title":"Creating Instance"},{"location":"metrics/MetricsSystem/#creating-metricssystem","text":"createMetricsSystem ( instance : String conf : SparkConf securityMgr : SecurityManager ) : MetricsSystem createMetricsSystem creates a new MetricsSystem (for the given parameters). createMetricsSystem is used to create metrics systems .","title":" Creating MetricsSystem"},{"location":"metrics/MetricsSystem/#metrics-sources-for-spark-sql","text":"CodegenMetrics HiveCatalogMetrics","title":" Metrics Sources for Spark SQL"},{"location":"metrics/MetricsSystem/#registering-metrics-source","text":"registerSource ( source : Source ) : Unit registerSource adds source to the sources internal registry. registerSource creates an identifier for the metrics source and registers it with the MetricRegistry . registerSource uses Metrics' MetricRegistry.register to register a metrics source under a given name. registerSource prints out the following INFO message to the logs when registering a name more than once: Metrics already registered","title":" Registering Metrics Source"},{"location":"metrics/MetricsSystem/#building-metrics-source-identifier","text":"buildRegistryName ( source : Source ) : String buildRegistryName uses spark-metrics-properties.md#spark.metrics.namespace[spark.metrics.namespace] and xref:executor:Executor.md#spark.executor.id[spark.executor.id] Spark properties to differentiate between a Spark application's driver and executors, and the other Spark framework's components. (only when < > is driver or executor ) buildRegistryName builds metrics source name that is made up of link:spark-metrics-properties.md#spark.metrics.namespace[spark.metrics.namespace], xref:executor:Executor.md#spark.executor.id[spark.executor.id] and the name of the source . Note buildRegistryName uses Dropwizard Metrics' MetricRegistry to build metrics source identifiers. FIXME Finish for the other components. buildRegistryName is used when MetricsSystem is requested to register or remove a metrics source.","title":" Building Metrics Source Identifier"},{"location":"metrics/MetricsSystem/#registering-metrics-sources-for-spark-instance","text":"registerSources () : Unit registerSources finds < > configuration for the < >. NOTE: instance is defined when MetricsSystem < >. registerSources finds the configuration of all the link:spark-metrics-Source.md[metrics sources] for the subsystem (as described with source. prefix). For every metrics source, registerSources finds class property, creates an instance, and in the end < >. When registerSources fails, you should see the following ERROR message in the logs followed by the exception. Source class [classPath] cannot be instantiated registerSources is used when MetricsSystem is requested to start .","title":" Registering Metrics Sources for Spark Instance"},{"location":"metrics/MetricsSystem/#requesting-json-servlet-handler","text":"getServletHandlers : Array [ ServletContextHandler ] If the MetricsSystem is < > and the < > is defined for the metrics system, getServletHandlers simply requests the < > for the link:spark-metrics-MetricsServlet.md#getHandlers[JSON servlet handler]. When MetricsSystem is not < > getServletHandlers throws an IllegalArgumentException . Can only call getServletHandlers on a running MetricsSystem getServletHandlers is used when: SparkContext is created (Spark Standalone) Master and Worker are requested to start","title":" Requesting JSON Servlet Handler"},{"location":"metrics/MetricsSystem/#registering-metrics-sinks","text":"registerSinks () : Unit registerSinks requests the < > for the link:spark-metrics-MetricsConfig.md#getInstance[configuration] of the < >. registerSinks requests the < > for the link:spark-metrics-MetricsConfig.md#subProperties[configuration] of all metrics sinks (i.e. configuration entries that match ^sink\\\\.(.+)\\\\.(.+) regular expression). For every metrics sink configuration, registerSinks takes class property and (if defined) creates an instance of the metric sink using an constructor that takes the configuration, < > and < >. For a single servlet metrics sink, registerSinks converts the sink to a link:spark-metrics-MetricsServlet.md[MetricsServlet] and sets the < > internal registry. For all other metrics sinks, registerSinks adds the sink to the < > internal registry. In case of an Exception , registerSinks prints out the following ERROR message to the logs: Sink class [classPath] cannot be instantiated registerSinks is used when MetricsSystem is requested to start .","title":" Registering Metrics Sinks"},{"location":"metrics/MetricsSystem/#stopping","text":"stop () : Unit stop ...FIXME","title":" Stopping"},{"location":"metrics/MetricsSystem/#reporting-metrics","text":"report () : Unit report simply requests the registered metrics sinks to report metrics .","title":" Reporting Metrics"},{"location":"metrics/MetricsSystem/#starting","text":"start () : Unit start turns < > flag on. NOTE: start can only be called once and < > an IllegalArgumentException when called multiple times. start < > the < > for Spark SQL, i.e. CodegenMetrics and HiveCatalogMetrics . start then registers the configured metrics < > and < > for the < >. In the end, start requests the registered < > to link:spark-metrics-Sink.md#start[start]. [[start-IllegalArgumentException]] start throws an IllegalArgumentException when < > flag is on. requirement failed: Attempting to start a MetricsSystem that is already running","title":" Starting"},{"location":"metrics/MetricsSystem/#logging","text":"Enable ALL logging level for org.apache.spark.metrics.MetricsSystem logger to see what happens inside. Add the following line to conf/log4j.properties : log4j.logger.org.apache.spark.metrics.MetricsSystem=ALL Refer to Logging .","title":"Logging"},{"location":"metrics/MetricsSystem/#internal-registries","text":"","title":"Internal Registries"},{"location":"metrics/MetricsSystem/#metricregistry","text":"Integration point to Dropwizard Metrics' MetricRegistry Used when MetricsSystem is requested to: Register or remove a metrics source Start (that in turn registers metrics sinks )","title":" MetricRegistry"},{"location":"metrics/MetricsSystem/#metricsconfig","text":"MetricsConfig Initialized when MetricsSystem is < >. Used when MetricsSystem registers < > and < >.","title":" MetricsConfig"},{"location":"metrics/MetricsSystem/#metricsservlet","text":"MetricsServlet JSON metrics sink that is only available for the < > with a web UI (i.e. the driver of a Spark application and Spark Standalone's Master ). MetricsSystem may have at most one MetricsServlet JSON metrics sink (which is registered by default ). Initialized when MetricsSystem registers < > (and finds a configuration entry with servlet sink name). Used when MetricsSystem is requested for a < >.","title":" MetricsServlet"},{"location":"metrics/MetricsSystem/#running-flag","text":"Indicates whether MetricsSystem has been started ( true ) or not ( false ) Default: false","title":" running Flag"},{"location":"metrics/MetricsSystem/#sinks","text":"Metrics sinks Used when MetricsSystem < > and < >.","title":" sinks"},{"location":"metrics/MetricsSystem/#sources","text":"Metrics sources Used when MetricsSystem < >.","title":" sources"},{"location":"metrics/Sink/","text":"Sink \u00b6 Sink is a < > of metrics sinks . [[contract]] [source, scala] package org.apache.spark.metrics.sink trait Sink { def start(): Unit def stop(): Unit def report(): Unit } NOTE: Sink is a private[spark] contract. .Sink Contract [cols=\"1,2\",options=\"header\",width=\"100%\"] |=== | Method | Description | start | [[start]] Used when...FIXME | stop | [[stop]] Used when...FIXME | report | [[report]] Used when...FIXME |=== [[implementations]] .Sinks [cols=\"1,2\",options=\"header\",width=\"100%\"] |=== | Sink | Description | ConsoleSink | [[ConsoleSink]] | CsvSink | [[CsvSink]] | GraphiteSink | [[GraphiteSink]] | JmxSink | [[JmxSink]] | link:spark-metrics-MetricsServlet.adoc[MetricsServlet] | [[MetricsServlet]] | Slf4jSink | [[Slf4jSink]] | StatsdSink | [[StatsdSink]] |=== NOTE: All known < > in Spark 2.3 are in org.apache.spark.metrics.sink Scala package.","title":"Sink"},{"location":"metrics/Sink/#sink","text":"Sink is a < > of metrics sinks . [[contract]] [source, scala] package org.apache.spark.metrics.sink trait Sink { def start(): Unit def stop(): Unit def report(): Unit } NOTE: Sink is a private[spark] contract. .Sink Contract [cols=\"1,2\",options=\"header\",width=\"100%\"] |=== | Method | Description | start | [[start]] Used when...FIXME | stop | [[stop]] Used when...FIXME | report | [[report]] Used when...FIXME |=== [[implementations]] .Sinks [cols=\"1,2\",options=\"header\",width=\"100%\"] |=== | Sink | Description | ConsoleSink | [[ConsoleSink]] | CsvSink | [[CsvSink]] | GraphiteSink | [[GraphiteSink]] | JmxSink | [[JmxSink]] | link:spark-metrics-MetricsServlet.adoc[MetricsServlet] | [[MetricsServlet]] | Slf4jSink | [[Slf4jSink]] | StatsdSink | [[StatsdSink]] |=== NOTE: All known < > in Spark 2.3 are in org.apache.spark.metrics.sink Scala package.","title":"Sink"},{"location":"metrics/Source/","text":"== [[Source]] Source -- Contract of Metrics Sources Source is a < > of metrics sources . [[contract]] [source, scala] package org.apache.spark.metrics.source trait Source { def sourceName: String def metricRegistry: MetricRegistry } NOTE: Source is a private[spark] contract. .Source Contract [cols=\"1,2\",options=\"header\",width=\"100%\"] |=== | Method | Description | sourceName | [[sourceName]] Used when...FIXME | metricRegistry | [[metricRegistry]] Dropwizard Metrics' https://metrics.dropwizard.io/3.1.0/apidocs/com/codahale/metrics/MetricRegistry.html[MetricRegistry ] Used when...FIXME |=== [[implementations]] .Sources [cols=\"1,2\",options=\"header\",width=\"100%\"] |=== | Source | Description | ApplicationSource | [[ApplicationSource]] | xref:storage:spark-BlockManager-BlockManagerSource.adoc[BlockManagerSource] | [[BlockManagerSource]] | CacheMetrics | [[CacheMetrics]] | CodegenMetrics | [[CodegenMetrics]] | xref:metrics:spark-scheduler-DAGSchedulerSource.adoc[DAGSchedulerSource] | [[DAGSchedulerSource]] | xref:ROOT:spark-service-ExecutorAllocationManagerSource.adoc[ExecutorAllocationManagerSource] | [[ExecutorAllocationManagerSource]] | xref:executor:ExecutorSource.adoc[] | [[ExecutorSource]] | ExternalShuffleServiceSource | [[ExternalShuffleServiceSource]] | HiveCatalogMetrics | [[HiveCatalogMetrics]] | xref:metrics:JvmSource.adoc[JvmSource] | [[JvmSource]] | LiveListenerBusMetrics | [[LiveListenerBusMetrics]] | MasterSource | [[MasterSource]] | MesosClusterSchedulerSource | [[MesosClusterSchedulerSource]] | xref:storage:ShuffleMetricsSource.adoc[] | [[ShuffleMetricsSource]] | StreamingSource | [[StreamingSource]] | WorkerSource | [[WorkerSource]] |===","title":"Source"},{"location":"metrics/configuration-properties/","text":"Configuration Properties \u00b6 spark.metrics.conf \u00b6 The metrics configuration file Default: metrics.properties spark.metrics.namespace \u00b6 Root namespace for metrics reporting Default: Spark Application ID (i.e. spark.app.id configuration property) Since a Spark application's ID changes with every execution of a Spark application, a custom namespace can be specified for an easier metrics reporting. Used when MetricsSystem is requested for a metrics source identifier ( metrics namespace )","title":"Configuration Properties"},{"location":"metrics/configuration-properties/#configuration-properties","text":"","title":"Configuration Properties"},{"location":"metrics/configuration-properties/#sparkmetricsconf","text":"The metrics configuration file Default: metrics.properties","title":" spark.metrics.conf"},{"location":"metrics/configuration-properties/#sparkmetricsnamespace","text":"Root namespace for metrics reporting Default: Spark Application ID (i.e. spark.app.id configuration property) Since a Spark application's ID changes with every execution of a Spark application, a custom namespace can be specified for an easier metrics reporting. Used when MetricsSystem is requested for a metrics source identifier ( metrics namespace )","title":" spark.metrics.namespace"},{"location":"rdd/","text":"Resilient Distributed Dataset (RDD) \u00b6 Resilient Distributed Dataset (aka RDD ) is the primary data abstraction in Apache Spark and the core of Spark (that I often refer to as \"Spark Core\"). .The origins of RDD The original paper that gave birth to the concept of RDD is https://cs.stanford.edu/~matei/papers/2012/nsdi_spark.pdf[Resilient Distributed Datasets: A Fault-Tolerant Abstraction for In-Memory Cluster Computing] by Matei Zaharia, et al. An RDD is a description of a fault-tolerant and resilient computation over a distributed collection of records (spread over < >). NOTE: One could compare RDDs to collections in Scala, i.e. a RDD is computed on many JVMs while a Scala collection lives on a single JVM. Using RDD Spark hides data partitioning and so distribution that in turn allowed them to design parallel computational framework with a higher-level programming interface (API) for four mainstream programming languages. The features of RDDs (decomposing the name): Resilient , i.e. fault-tolerant with the help of < > and so able to recompute missing or damaged partitions due to node failures. Distributed with data residing on multiple nodes in a link:spark-cluster.adoc[cluster]. Dataset is a collection of link:spark-rdd-partitions.adoc[partitioned data] with primitive values or values of values, e.g. tuples or other objects (that represent records of the data you work with). .RDDs image::spark-rdds.png[align=\"center\"] From the scaladoc of http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.rdd.RDD[org.apache.spark.rdd.RDD ]: A Resilient Distributed Dataset (RDD), the basic abstraction in Spark. Represents an immutable, partitioned collection of elements that can be operated on in parallel. From the original paper about RDD - https://cs.stanford.edu/~matei/papers/2012/nsdi_spark.pdf[Resilient Distributed Datasets: A Fault-Tolerant Abstraction for In-Memory Cluster Computing]: Resilient Distributed Datasets (RDDs) are a distributed memory abstraction that lets programmers perform in-memory computations on large clusters in a fault-tolerant manner. Beside the above traits (that are directly embedded in the name of the data abstraction - RDD) it has the following additional traits: In-Memory , i.e. data inside RDD is stored in memory as much (size) and long (time) as possible. Immutable or Read-Only , i.e. it does not change once created and can only be transformed using transformations to new RDDs. Lazy evaluated , i.e. the data inside RDD is not available or transformed until an action is executed that triggers the execution. Cacheable , i.e. you can hold all the data in a persistent \"storage\" like memory (default and the most preferred) or disk (the least preferred due to access speed). Parallel , i.e. process data in parallel. Typed -- RDD records have types, e.g. Long in RDD[Long] or (Int, String) in RDD[(Int, String)] . Partitioned -- records are partitioned (split into logical partitions) and distributed across nodes in a cluster. Location-Stickiness -- RDD can define < > to compute partitions (as close to the records as possible). NOTE: Preferred location (aka locality preferences or placement preferences or locality info ) is information about the locations of RDD records (that Spark's xref:scheduler:DAGScheduler.adoc#preferred-locations[DAGScheduler] uses to place computing partitions on to have the tasks as close to the data as possible). Computing partitions in a RDD is a distributed process by design and to achieve even data distribution as well as leverage link:spark-data-locality.adoc[data locality] (in distributed systems like HDFS or Cassandra in which data is partitioned by default), they are partitioned to a fixed number of link:spark-rdd-partitions.adoc[partitions] - logical chunks (parts) of data. The logical division is for processing only and internally it is not divided whatsoever. Each partition comprises of records . .RDDs image::spark-rdd-partitioned-distributed.png[align=\"center\"] link:spark-rdd-partitions.adoc[Partitions are the units of parallelism]. You can control the number of partitions of a RDD using link:spark-rdd-partitions.adoc#repartition[repartition] or link:spark-rdd-partitions.adoc#coalesce[coalesce] transformations. Spark tries to be as close to data as possible without wasting time to send data across network by means of link:spark-rdd-shuffle.adoc[RDD shuffling], and creates as many partitions as required to follow the storage layout and thus optimize data access. It leads to a one-to-one mapping between (physical) data in distributed data storage, e.g. HDFS or Cassandra, and partitions. RDDs support two kinds of operations: < > - lazy operations that return another RDD. < > - operations that trigger computation and return values. The motivation to create RDD were ( https://www.cs.berkeley.edu/~matei/papers/2012/nsdi_spark.pdf[after the authors]) two types of applications that current computing frameworks handle inefficiently: iterative algorithms in machine learning and graph computations. interactive data mining tools as ad-hoc queries on the same dataset. The goal is to reuse intermediate in-memory results across multiple data-intensive workloads with no need for copying large amounts of data over the network. Technically, RDDs follow the < > defined by the five main intrinsic properties: [[dependencies]] Parent RDDs (aka xref:rdd:RDD.adoc#dependencies[RDD dependencies]) An array of link:spark-rdd-partitions.adoc[partitions] that a dataset is divided to. A xref:rdd:RDD.adoc#compute[compute] function to do a computation on partitions. An optional xref:rdd:Partitioner.adoc[Partitioner] that defines how keys are hashed, and the pairs partitioned (for key-value RDDs) Optional < > (aka locality info ), i.e. hosts for a partition where the records live or are the closest to read from. This RDD abstraction supports an expressive set of operations without having to modify scheduler for each one. [[context]] An RDD is a named (by name ) and uniquely identified (by id ) entity in a xref:ROOT:SparkContext.adoc[] (available as context property). RDDs live in one and only one xref:ROOT:SparkContext.adoc[] that creates a logical boundary. NOTE: RDDs cannot be shared between SparkContexts (see xref:ROOT:SparkContext.adoc#sparkcontext-and-rdd[SparkContext and RDDs]). An RDD can optionally have a friendly name accessible using name that can be changed using = : scala> val ns = sc.parallelize(0 to 10) ns: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[2] at parallelize at <console>:24 scala> ns.id res0: Int = 2 scala> ns.name res1: String = null scala> ns.name = \"Friendly name\" ns.name: String = Friendly name scala> ns.name res2: String = Friendly name scala> ns.toDebugString res3: String = (8) Friendly name ParallelCollectionRDD[2] at parallelize at <console>:24 [] RDDs are a container of instructions on how to materialize big (arrays of) distributed data, and how to split it into partitions so Spark (using xref:executor:Executor.adoc[executors]) can hold some of them. In general data distribution can help executing processing in parallel so a task processes a chunk of data that it could eventually keep in memory. Spark does jobs in parallel, and RDDs are split into partitions to be processed and written in parallel. Inside a partition, data is processed sequentially. Saving partitions results in part-files instead of one single file (unless there is a single partition). == [[transformations]] Transformations A transformation is a lazy operation on a RDD that returns another RDD, e.g. map , flatMap , filter , reduceByKey , join , cogroup , etc. Find out more in xref:rdd:spark-rdd-transformations.adoc[Transformations]. == [[actions]] Actions An action is an operation that triggers execution of < > and returns a value (to a Spark driver - the user program). TIP: Go in-depth in the section link:spark-rdd-actions.adoc[Actions]. == [[creating-rdds]] Creating RDDs === SparkContext.parallelize One way to create a RDD is with SparkContext.parallelize method. It accepts a collection of elements as shown below ( sc is a SparkContext instance): scala> val rdd = sc.parallelize(1 to 1000) rdd: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[0] at parallelize at <console>:25 You may also want to randomize the sample data: scala> val data = Seq.fill(10)(util.Random.nextInt) data: Seq[Int] = List(-964985204, 1662791, -1820544313, -383666422, -111039198, 310967683, 1114081267, 1244509086, 1797452433, 124035586) scala> val rdd = sc.parallelize(data) rdd: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[0] at parallelize at <console>:29 Given the reason to use Spark to process more data than your own laptop could handle, SparkContext.parallelize is mainly used to learn Spark in the Spark shell. SparkContext.parallelize requires all the data to be available on a single machine - the Spark driver - that eventually hits the limits of your laptop. === SparkContext.makeRDD CAUTION: FIXME What's the use case for makeRDD ? scala> sc.makeRDD(0 to 1000) res0: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[1] at makeRDD at <console>:25 === SparkContext.textFile One of the easiest ways to create an RDD is to use SparkContext.textFile to read files. You can use the local README.md file (and then flatMap over the lines inside to have an RDD of words): scala> val words = sc.textFile(\"README.md\").flatMap(_.split(\"\\\\W+\")).cache words: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[27] at flatMap at <console>:24 NOTE: You link:spark-rdd-caching.adoc[cache] it so the computation is not performed every time you work with words . == [[creating-rdds-from-input]] Creating RDDs from Input Refer to link:spark-io.adoc[Using Input and Output (I/O)] to learn about the IO API to create RDDs. === Transformations RDD transformations by definition transform an RDD into another RDD and hence are the way to create new ones. Refer to < > section to learn more. == RDDs in Web UI It is quite informative to look at RDDs in the Web UI that is at http://localhost:4040 for link:spark-shell.adoc[Spark shell]. Execute the following Spark application (type all the lines in spark-shell ): [source,scala] \u00b6 val ints = sc.parallelize(1 to 100) // <1> ints.setName(\"Hundred ints\") // <2> ints.cache // <3> ints.count // <4> <1> Creates an RDD with hundred of numbers (with as many partitions as possible) <2> Sets the name of the RDD <3> Caches the RDD for performance reasons that also makes it visible in Storage tab in the web UI <4> Executes action (and materializes the RDD) With the above executed, you should see the following in the Web UI: .RDD with custom name image::spark-ui-rdd-name.png[align=\"center\"] Click the name of the RDD (under RDD Name ) and you will get the details of how the RDD is cached. .RDD Storage Info image::spark-ui-storage-hundred-ints.png[align=\"center\"] Execute the following Spark job and you will see how the number of partitions decreases. ints.repartition(2).count .Number of tasks after repartition image::spark-ui-repartition-2.png[align=\"center\"]","title":"Resilient Distributed Dataset"},{"location":"rdd/#resilient-distributed-dataset-rdd","text":"Resilient Distributed Dataset (aka RDD ) is the primary data abstraction in Apache Spark and the core of Spark (that I often refer to as \"Spark Core\"). .The origins of RDD The original paper that gave birth to the concept of RDD is https://cs.stanford.edu/~matei/papers/2012/nsdi_spark.pdf[Resilient Distributed Datasets: A Fault-Tolerant Abstraction for In-Memory Cluster Computing] by Matei Zaharia, et al. An RDD is a description of a fault-tolerant and resilient computation over a distributed collection of records (spread over < >). NOTE: One could compare RDDs to collections in Scala, i.e. a RDD is computed on many JVMs while a Scala collection lives on a single JVM. Using RDD Spark hides data partitioning and so distribution that in turn allowed them to design parallel computational framework with a higher-level programming interface (API) for four mainstream programming languages. The features of RDDs (decomposing the name): Resilient , i.e. fault-tolerant with the help of < > and so able to recompute missing or damaged partitions due to node failures. Distributed with data residing on multiple nodes in a link:spark-cluster.adoc[cluster]. Dataset is a collection of link:spark-rdd-partitions.adoc[partitioned data] with primitive values or values of values, e.g. tuples or other objects (that represent records of the data you work with). .RDDs image::spark-rdds.png[align=\"center\"] From the scaladoc of http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.rdd.RDD[org.apache.spark.rdd.RDD ]: A Resilient Distributed Dataset (RDD), the basic abstraction in Spark. Represents an immutable, partitioned collection of elements that can be operated on in parallel. From the original paper about RDD - https://cs.stanford.edu/~matei/papers/2012/nsdi_spark.pdf[Resilient Distributed Datasets: A Fault-Tolerant Abstraction for In-Memory Cluster Computing]: Resilient Distributed Datasets (RDDs) are a distributed memory abstraction that lets programmers perform in-memory computations on large clusters in a fault-tolerant manner. Beside the above traits (that are directly embedded in the name of the data abstraction - RDD) it has the following additional traits: In-Memory , i.e. data inside RDD is stored in memory as much (size) and long (time) as possible. Immutable or Read-Only , i.e. it does not change once created and can only be transformed using transformations to new RDDs. Lazy evaluated , i.e. the data inside RDD is not available or transformed until an action is executed that triggers the execution. Cacheable , i.e. you can hold all the data in a persistent \"storage\" like memory (default and the most preferred) or disk (the least preferred due to access speed). Parallel , i.e. process data in parallel. Typed -- RDD records have types, e.g. Long in RDD[Long] or (Int, String) in RDD[(Int, String)] . Partitioned -- records are partitioned (split into logical partitions) and distributed across nodes in a cluster. Location-Stickiness -- RDD can define < > to compute partitions (as close to the records as possible). NOTE: Preferred location (aka locality preferences or placement preferences or locality info ) is information about the locations of RDD records (that Spark's xref:scheduler:DAGScheduler.adoc#preferred-locations[DAGScheduler] uses to place computing partitions on to have the tasks as close to the data as possible). Computing partitions in a RDD is a distributed process by design and to achieve even data distribution as well as leverage link:spark-data-locality.adoc[data locality] (in distributed systems like HDFS or Cassandra in which data is partitioned by default), they are partitioned to a fixed number of link:spark-rdd-partitions.adoc[partitions] - logical chunks (parts) of data. The logical division is for processing only and internally it is not divided whatsoever. Each partition comprises of records . .RDDs image::spark-rdd-partitioned-distributed.png[align=\"center\"] link:spark-rdd-partitions.adoc[Partitions are the units of parallelism]. You can control the number of partitions of a RDD using link:spark-rdd-partitions.adoc#repartition[repartition] or link:spark-rdd-partitions.adoc#coalesce[coalesce] transformations. Spark tries to be as close to data as possible without wasting time to send data across network by means of link:spark-rdd-shuffle.adoc[RDD shuffling], and creates as many partitions as required to follow the storage layout and thus optimize data access. It leads to a one-to-one mapping between (physical) data in distributed data storage, e.g. HDFS or Cassandra, and partitions. RDDs support two kinds of operations: < > - lazy operations that return another RDD. < > - operations that trigger computation and return values. The motivation to create RDD were ( https://www.cs.berkeley.edu/~matei/papers/2012/nsdi_spark.pdf[after the authors]) two types of applications that current computing frameworks handle inefficiently: iterative algorithms in machine learning and graph computations. interactive data mining tools as ad-hoc queries on the same dataset. The goal is to reuse intermediate in-memory results across multiple data-intensive workloads with no need for copying large amounts of data over the network. Technically, RDDs follow the < > defined by the five main intrinsic properties: [[dependencies]] Parent RDDs (aka xref:rdd:RDD.adoc#dependencies[RDD dependencies]) An array of link:spark-rdd-partitions.adoc[partitions] that a dataset is divided to. A xref:rdd:RDD.adoc#compute[compute] function to do a computation on partitions. An optional xref:rdd:Partitioner.adoc[Partitioner] that defines how keys are hashed, and the pairs partitioned (for key-value RDDs) Optional < > (aka locality info ), i.e. hosts for a partition where the records live or are the closest to read from. This RDD abstraction supports an expressive set of operations without having to modify scheduler for each one. [[context]] An RDD is a named (by name ) and uniquely identified (by id ) entity in a xref:ROOT:SparkContext.adoc[] (available as context property). RDDs live in one and only one xref:ROOT:SparkContext.adoc[] that creates a logical boundary. NOTE: RDDs cannot be shared between SparkContexts (see xref:ROOT:SparkContext.adoc#sparkcontext-and-rdd[SparkContext and RDDs]). An RDD can optionally have a friendly name accessible using name that can be changed using = : scala> val ns = sc.parallelize(0 to 10) ns: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[2] at parallelize at <console>:24 scala> ns.id res0: Int = 2 scala> ns.name res1: String = null scala> ns.name = \"Friendly name\" ns.name: String = Friendly name scala> ns.name res2: String = Friendly name scala> ns.toDebugString res3: String = (8) Friendly name ParallelCollectionRDD[2] at parallelize at <console>:24 [] RDDs are a container of instructions on how to materialize big (arrays of) distributed data, and how to split it into partitions so Spark (using xref:executor:Executor.adoc[executors]) can hold some of them. In general data distribution can help executing processing in parallel so a task processes a chunk of data that it could eventually keep in memory. Spark does jobs in parallel, and RDDs are split into partitions to be processed and written in parallel. Inside a partition, data is processed sequentially. Saving partitions results in part-files instead of one single file (unless there is a single partition). == [[transformations]] Transformations A transformation is a lazy operation on a RDD that returns another RDD, e.g. map , flatMap , filter , reduceByKey , join , cogroup , etc. Find out more in xref:rdd:spark-rdd-transformations.adoc[Transformations]. == [[actions]] Actions An action is an operation that triggers execution of < > and returns a value (to a Spark driver - the user program). TIP: Go in-depth in the section link:spark-rdd-actions.adoc[Actions]. == [[creating-rdds]] Creating RDDs === SparkContext.parallelize One way to create a RDD is with SparkContext.parallelize method. It accepts a collection of elements as shown below ( sc is a SparkContext instance): scala> val rdd = sc.parallelize(1 to 1000) rdd: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[0] at parallelize at <console>:25 You may also want to randomize the sample data: scala> val data = Seq.fill(10)(util.Random.nextInt) data: Seq[Int] = List(-964985204, 1662791, -1820544313, -383666422, -111039198, 310967683, 1114081267, 1244509086, 1797452433, 124035586) scala> val rdd = sc.parallelize(data) rdd: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[0] at parallelize at <console>:29 Given the reason to use Spark to process more data than your own laptop could handle, SparkContext.parallelize is mainly used to learn Spark in the Spark shell. SparkContext.parallelize requires all the data to be available on a single machine - the Spark driver - that eventually hits the limits of your laptop. === SparkContext.makeRDD CAUTION: FIXME What's the use case for makeRDD ? scala> sc.makeRDD(0 to 1000) res0: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[1] at makeRDD at <console>:25 === SparkContext.textFile One of the easiest ways to create an RDD is to use SparkContext.textFile to read files. You can use the local README.md file (and then flatMap over the lines inside to have an RDD of words): scala> val words = sc.textFile(\"README.md\").flatMap(_.split(\"\\\\W+\")).cache words: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[27] at flatMap at <console>:24 NOTE: You link:spark-rdd-caching.adoc[cache] it so the computation is not performed every time you work with words . == [[creating-rdds-from-input]] Creating RDDs from Input Refer to link:spark-io.adoc[Using Input and Output (I/O)] to learn about the IO API to create RDDs. === Transformations RDD transformations by definition transform an RDD into another RDD and hence are the way to create new ones. Refer to < > section to learn more. == RDDs in Web UI It is quite informative to look at RDDs in the Web UI that is at http://localhost:4040 for link:spark-shell.adoc[Spark shell]. Execute the following Spark application (type all the lines in spark-shell ):","title":"Resilient Distributed Dataset (RDD)"},{"location":"rdd/#sourcescala","text":"val ints = sc.parallelize(1 to 100) // <1> ints.setName(\"Hundred ints\") // <2> ints.cache // <3> ints.count // <4> <1> Creates an RDD with hundred of numbers (with as many partitions as possible) <2> Sets the name of the RDD <3> Caches the RDD for performance reasons that also makes it visible in Storage tab in the web UI <4> Executes action (and materializes the RDD) With the above executed, you should see the following in the Web UI: .RDD with custom name image::spark-ui-rdd-name.png[align=\"center\"] Click the name of the RDD (under RDD Name ) and you will get the details of how the RDD is cached. .RDD Storage Info image::spark-ui-storage-hundred-ints.png[align=\"center\"] Execute the following Spark job and you will see how the number of partitions decreases. ints.repartition(2).count .Number of tasks after repartition image::spark-ui-repartition-2.png[align=\"center\"]","title":"[source,scala]"},{"location":"rdd/Aggregator/","text":"= [[Aggregator]] Aggregator Aggregator is a set of < > used to aggregate data using xref:rdd:PairRDDFunctions.adoc#combineByKeyWithClassTag[PairRDDFunctions.combineByKeyWithClassTag] transformation. Aggregator[K, V, C] is a parameterized type of K keys, V values, and C combiner (partial) values. [[creating-instance]][[aggregation-functions]] Aggregator transforms an RDD[(K, V)] into an RDD[(K, C)] (for a \"combined type\" C) using the functions: [[createCombiner]] createCombiner: V => C [[mergeValue]] mergeValue: (C, V) => C [[mergeCombiners]] mergeCombiners: (C, C) => C Aggregator is used to create a xref:rdd:ShuffleDependency.adoc[ShuffleDependency] and xref:shuffle:ExternalSorter.adoc[ExternalSorter]. == [[combineValuesByKey]] combineValuesByKey Method [source, scala] \u00b6 combineValuesByKey( iter: Iterator[_ <: Product2[K, V]], context: TaskContext): Iterator[(K, C)] combineValuesByKey creates a new xref:shuffle:ExternalAppendOnlyMap.adoc[ExternalAppendOnlyMap] (with the < >). combineValuesByKey requests the ExternalAppendOnlyMap to xref:shuffle:ExternalAppendOnlyMap.adoc#insertAll[insert all key-value pairs] from the given iterator (that is the values of a partition). combineValuesByKey < >. In the end, combineValuesByKey requests the ExternalAppendOnlyMap for an xref:shuffle:ExternalAppendOnlyMap.adoc#iterator[iterator of \"combined\" pairs]. combineValuesByKey is used when: xref:rdd:PairRDDFunctions.adoc#combineByKeyWithClassTag[PairRDDFunctions.combineByKeyWithClassTag] transformation is used (with the same Partitioner as the RDD's) BlockStoreShuffleReader is requested to xref:shuffle:BlockStoreShuffleReader.adoc#read[read combined records for a reduce task] (with the xref:rdd:ShuffleDependency.adoc#mapSideCombine[Map-Size Partial Aggregation Flag] off) == [[combineCombinersByKey]] combineCombinersByKey Method [source, scala] \u00b6 combineCombinersByKey( iter: Iterator[_ <: Product2[K, C]], context: TaskContext): Iterator[(K, C)] combineCombinersByKey...FIXME combineCombinersByKey is used when BlockStoreShuffleReader is requested to xref:shuffle:BlockStoreShuffleReader.adoc#read[read combined records for a reduce task] (with the xref:rdd:ShuffleDependency.adoc#mapSideCombine[Map-Size Partial Aggregation Flag] on). == [[updateMetrics]] Updating Task Metrics [source, scala] \u00b6 updateMetrics( context: TaskContext, map: ExternalAppendOnlyMap[_, _, _]): Unit updateMetrics requests the input xref:scheduler:spark-TaskContext.adoc[TaskContext] for the xref:scheduler:spark-TaskContext.adoc#taskMetrics[TaskMetrics] to update the metrics based on the metrics of the input xref:shuffle:ExternalAppendOnlyMap.adoc[ExternalAppendOnlyMap]: xref:executor:TaskMetrics.adoc#incMemoryBytesSpilled[Increment memory bytes spilled] xref:executor:TaskMetrics.adoc#incDiskBytesSpilled[Increment disk bytes spilled] xref:executor:TaskMetrics.adoc#incPeakExecutionMemory[Increment peak execution memory] updateMetrics is used when Aggregator is requested to < > and < >.","title":"Aggregator"},{"location":"rdd/Aggregator/#source-scala","text":"combineValuesByKey( iter: Iterator[_ <: Product2[K, V]], context: TaskContext): Iterator[(K, C)] combineValuesByKey creates a new xref:shuffle:ExternalAppendOnlyMap.adoc[ExternalAppendOnlyMap] (with the < >). combineValuesByKey requests the ExternalAppendOnlyMap to xref:shuffle:ExternalAppendOnlyMap.adoc#insertAll[insert all key-value pairs] from the given iterator (that is the values of a partition). combineValuesByKey < >. In the end, combineValuesByKey requests the ExternalAppendOnlyMap for an xref:shuffle:ExternalAppendOnlyMap.adoc#iterator[iterator of \"combined\" pairs]. combineValuesByKey is used when: xref:rdd:PairRDDFunctions.adoc#combineByKeyWithClassTag[PairRDDFunctions.combineByKeyWithClassTag] transformation is used (with the same Partitioner as the RDD's) BlockStoreShuffleReader is requested to xref:shuffle:BlockStoreShuffleReader.adoc#read[read combined records for a reduce task] (with the xref:rdd:ShuffleDependency.adoc#mapSideCombine[Map-Size Partial Aggregation Flag] off) == [[combineCombinersByKey]] combineCombinersByKey Method","title":"[source, scala]"},{"location":"rdd/Aggregator/#source-scala_1","text":"combineCombinersByKey( iter: Iterator[_ <: Product2[K, C]], context: TaskContext): Iterator[(K, C)] combineCombinersByKey...FIXME combineCombinersByKey is used when BlockStoreShuffleReader is requested to xref:shuffle:BlockStoreShuffleReader.adoc#read[read combined records for a reduce task] (with the xref:rdd:ShuffleDependency.adoc#mapSideCombine[Map-Size Partial Aggregation Flag] on). == [[updateMetrics]] Updating Task Metrics","title":"[source, scala]"},{"location":"rdd/Aggregator/#source-scala_2","text":"updateMetrics( context: TaskContext, map: ExternalAppendOnlyMap[_, _, _]): Unit updateMetrics requests the input xref:scheduler:spark-TaskContext.adoc[TaskContext] for the xref:scheduler:spark-TaskContext.adoc#taskMetrics[TaskMetrics] to update the metrics based on the metrics of the input xref:shuffle:ExternalAppendOnlyMap.adoc[ExternalAppendOnlyMap]: xref:executor:TaskMetrics.adoc#incMemoryBytesSpilled[Increment memory bytes spilled] xref:executor:TaskMetrics.adoc#incDiskBytesSpilled[Increment disk bytes spilled] xref:executor:TaskMetrics.adoc#incPeakExecutionMemory[Increment peak execution memory] updateMetrics is used when Aggregator is requested to < > and < >.","title":"[source, scala]"},{"location":"rdd/CheckpointRDD/","text":"= CheckpointRDD CheckpointRDD is...FIXME","title":"CheckpointRDD"},{"location":"rdd/HashPartitioner/","text":"= HashPartitioner HashPartitioner is a xref:rdd:Partitioner.adoc[Partitioner] for hash-based partitioning. HashPartitioner is used as the default Partitioner. == [[partitions]][[numPartitions]] Number of Partitions HashPartitioner takes a number of partitions to be created. HashPartitioner uses the number of partitions to find the < > (of a key-value record). == [[getPartition]] Finding Partition ID for Key [source, scala] \u00b6 getPartition(key: Any): Int \u00b6 getPartition returns 0 as the partition ID for null keys. For non- null keys, getPartition uses the key's {java-javadoc-url}/java/lang/Object.html#++hashCode--++[Object.hashCode] modulo the configured < >. For a negative result, getPartition adds the < > (used for the modulo operator) to make it positive. getPartition is part of the xref:rdd:Partitioner.adoc#getPartition[Partitioner] abstraction. == [[equals]] equals Method [source, scala] \u00b6 equals(other: Any): Boolean \u00b6 Two HashPartitioners are considered equal when the < > are the same. == [[hashCode]] hashCode Method [source, scala] \u00b6 hashCode: Int \u00b6 hashCode is the < >.","title":"HashPartitioner"},{"location":"rdd/HashPartitioner/#source-scala","text":"","title":"[source, scala]"},{"location":"rdd/HashPartitioner/#getpartitionkey-any-int","text":"getPartition returns 0 as the partition ID for null keys. For non- null keys, getPartition uses the key's {java-javadoc-url}/java/lang/Object.html#++hashCode--++[Object.hashCode] modulo the configured < >. For a negative result, getPartition adds the < > (used for the modulo operator) to make it positive. getPartition is part of the xref:rdd:Partitioner.adoc#getPartition[Partitioner] abstraction. == [[equals]] equals Method","title":"getPartition(key: Any): Int"},{"location":"rdd/HashPartitioner/#source-scala_1","text":"","title":"[source, scala]"},{"location":"rdd/HashPartitioner/#equalsother-any-boolean","text":"Two HashPartitioners are considered equal when the < > are the same. == [[hashCode]] hashCode Method","title":"equals(other: Any): Boolean"},{"location":"rdd/HashPartitioner/#source-scala_2","text":"","title":"[source, scala]"},{"location":"rdd/HashPartitioner/#hashcode-int","text":"hashCode is the < >.","title":"hashCode: Int"},{"location":"rdd/LocalRDDCheckpointData/","text":"= LocalRDDCheckpointData LocalRDDCheckpointData is...FIXME","title":"LocalRDDCheckpointData"},{"location":"rdd/PairRDDFunctions/","text":"= [[PairRDDFunctions]] PairRDDFunctions :page-toctitle: Transformations PairRDDFunctions is an extension of RDD API to provide additional < > for RDDs of key-value pairs ( RDD[(K, V)] ). PairRDDFunctions is available in RDDs of key-value pairs via Scala implicit conversion. [[transformations]] .PairRDDFunctions' Transformations [cols=\"30m,70\",options=\"header\",width=\"100%\"] |=== | Method | Description | aggregateByKey a| [[aggregateByKey]] [source, scala] \u00b6 aggregateByKey U: ClassTag ( seqOp: (U, V) => U, combOp: (U, U) => U): RDD[(K, U)] aggregateByKey U: ClassTag ( seqOp: (U, V) => U, combOp: (U, U) => U): RDD[(K, U)] aggregateByKey U: ClassTag ( seqOp: (U, V) => U, combOp: (U, U) => U): RDD[(K, U)] | combineByKey a| [[combineByKey]] [source, scala] \u00b6 combineByKey C : RDD[(K, C)] combineByKey C : RDD[(K, C)] combineByKey C : RDD[(K, C)] | countApproxDistinctByKey a| [[countApproxDistinctByKey]] [source, scala] \u00b6 countApproxDistinctByKey( relativeSD: Double = 0.05): RDD[(K, Long)] countApproxDistinctByKey( relativeSD: Double, numPartitions: Int): RDD[(K, Long)] countApproxDistinctByKey( relativeSD: Double, partitioner: Partitioner): RDD[(K, Long)] countApproxDistinctByKey( p: Int, sp: Int, partitioner: Partitioner): RDD[(K, Long)] | flatMapValues a| [[flatMapValues]] [source, scala] \u00b6 flatMapValues U : RDD[(K, U)] | foldByKey a| [[foldByKey]] [source, scala] \u00b6 foldByKey( zeroValue: V)( func: (V, V) => V): RDD[(K, V)] foldByKey( zeroValue: V, numPartitions: Int)( func: (V, V) => V): RDD[(K, V)] foldByKey( zeroValue: V, partitioner: Partitioner)( func: (V, V) => V): RDD[(K, V)] | mapValues a| [[mapValues]] [source, scala] \u00b6 mapValues U : RDD[(K, U)] | partitionBy a| [[partitionBy]] [source, scala] \u00b6 partitionBy( partitioner: Partitioner): RDD[(K, V)] | saveAsHadoopDataset a| [[saveAsHadoopDataset]] [source, scala] \u00b6 saveAsHadoopDataset( conf: JobConf): Unit saveAsHadoopDataset uses the SparkHadoopWriter utility to < > with a < > (for the given Hadoop https://hadoop.apache.org/docs/r2.7.3/api/org/apache/hadoop/mapred/JobConf.html[JobConf ]) | saveAsHadoopFile a| [[saveAsHadoopFile]] [source, scala] \u00b6 saveAsHadoopFile( path: String, keyClass: Class[ ], valueClass: Class[ ], outputFormatClass: Class[_ <: OutputFormat[ , _]], codec: Class[ <: CompressionCodec]): Unit saveAsHadoopFile( path: String, keyClass: Class[ ], valueClass: Class[ ], outputFormatClass: Class[_ <: OutputFormat[ , _]], conf: JobConf = new JobConf(self.context.hadoopConfiguration), codec: Option[Class[ <: CompressionCodec]] = None): Unit saveAsHadoopFile F <: OutputFormat[K, V] (implicit fm: ClassTag[F]): Unit saveAsHadoopFile F <: OutputFormat[K, V] (implicit fm: ClassTag[F]): Unit | saveAsNewAPIHadoopDataset a| [[saveAsNewAPIHadoopDataset]] [source, scala] \u00b6 saveAsNewAPIHadoopDataset( conf: Configuration): Unit Saves this RDD of key-value pairs ( RDD[K,V] ) to any Hadoop-supported storage system with new Hadoop API (using a Hadoop https://hadoop.apache.org/docs/r2.7.3/api/org/apache/hadoop/conf/Configuration.html[Configuration ] object for that storage system). The configuration should set relevant output params (an https://hadoop.apache.org/docs/r2.7.3/api/org/apache/hadoop/mapreduce/OutputFormat.html[output format], output paths, e.g. a table name to write to) in the same way as it would be configured for a Hadoop MapReduce job. saveAsNewAPIHadoopDataset uses the SparkHadoopWriter utility to < > with a < > (for the given Hadoop https://hadoop.apache.org/docs/r2.7.3/api/org/apache/hadoop/conf/Configuration.html[Configuration ]) | saveAsNewAPIHadoopFile a| [[saveAsNewAPIHadoopFile]] [source, scala] \u00b6 saveAsNewAPIHadoopFile( path: String, keyClass: Class[ ], valueClass: Class[ ], outputFormatClass: Class[_ <: NewOutputFormat[_, _]], conf: Configuration = self.context.hadoopConfiguration): Unit saveAsNewAPIHadoopFile F <: NewOutputFormat[K, V] (implicit fm: ClassTag[F]): Unit |=== == [[reduceByKey]][[groupByKey]] groupByKey and reduceByKey reduceByKey is sort of a particular case of < >. You may want to look at the number of partitions from another angle. It may often not be important to have a given number of partitions upfront (at RDD creation time upon link:spark-data-sources.adoc[loading data from data sources]), so only \"regrouping\" the data by key after it is an RDD might be...the key ( pun not intended ). You can use groupByKey or another PairRDDFunctions method to have a key in one processing flow. You could use partitionBy that is available for RDDs to be RDDs of tuples, i.e. PairRDD : rdd.keyBy(_.kind) .partitionBy(new HashPartitioner(PARTITIONS)) .foreachPartition(...) Think of situations where kind has low cardinality or highly skewed distribution and using the technique for partitioning might be not an optimal solution. You could do as follows: rdd.keyBy(_.kind).reduceByKey(....) or mapValues or plenty of other solutions. FIXME, man . == [[combineByKeyWithClassTag]] combineByKeyWithClassTag [source, scala] \u00b6 combineByKeyWithClassTag C (implicit ct: ClassTag[C]): RDD[(K, C)] // <1> combineByKeyWithClassTag C (implicit ct: ClassTag[C]): RDD[(K, C)] // <2> combineByKeyWithClassTag C (implicit ct: ClassTag[C]): RDD[(K, C)] <1> Uses the xref:rdd:Partitioner.adoc#defaultPartitioner[default partitioner] <2> Uses a xref:rdd:HashPartitioner.adoc[HashPartitioner] with the given number of partitions combineByKeyWithClassTag creates an xref:rdd:Aggregator.adoc[Aggregator] for the given aggregation functions. combineByKeyWithClassTag branches off per the given xref:rdd:Partitioner.adoc[Partitioner]. If the input partitioner and the RDD's are the same, combineByKeyWithClassTag simply xref:rdd:spark-rdd-transformations.adoc#mapPartitions[mapPartitions] on the RDD with the following arguments: Iterator of the xref:rdd:Aggregator.adoc#combineValuesByKey[Aggregator] preservesPartitioning flag turned on If the input partitioner is different than the RDD's, combineByKeyWithClassTag creates a xref:rdd:ShuffledRDD.adoc[ShuffledRDD] (with the Serializer, the Aggregator, and the mapSideCombine flag). === [[combineByKeyWithClassTag-usage]] Usage combineByKeyWithClassTag lays the foundation for the following transformations: < > < > < > < > < > < > === [[combineByKeyWithClassTag-requirements]] Requirements combineByKeyWithClassTag requires that the mergeCombiners is defined (not- null ) or throws an IllegalArgumentException: [source,plaintext] \u00b6 mergeCombiners must be defined \u00b6 combineByKeyWithClassTag throws a SparkException for the keys being of type array with the mapSideCombine flag enabled: [source,plaintext] \u00b6 Cannot use map-side combining with array keys. \u00b6 combineByKeyWithClassTag throws a SparkException for the keys being of type array with the partitioner being a xref:rdd:HashPartitioner.adoc[HashPartitioner]: [source,plaintext] \u00b6 HashPartitioner cannot partition array keys. \u00b6 === [[combineByKeyWithClassTag-example]] Example [source,scala] \u00b6 val nums = sc.parallelize(0 to 9, numSlices = 4) val groups = nums.keyBy(_ % 2) def createCombiner(n: Int) = { println(s\"createCombiner( n)\") n } def mergeValue(n1: Int, n2: Int) = { println(s\"mergeValue( n)\") n } def mergeValue(n1: Int, n2: Int) = { println(s\"mergeValue( n1, n2)\") n1 + n2 } def mergeCombiners(c1: Int, c2: Int) = { println(s\"mergeCombiners( n2)\") n1 + n2 } def mergeCombiners(c1: Int, c2: Int) = { println(s\"mergeCombiners( c1, $c2)\") c1 + c2 } val countByGroup = groups.combineByKeyWithClassTag( createCombiner, mergeValue, mergeCombiners) println(countByGroup.toDebugString) /* (4) ShuffledRDD[3] at combineByKeyWithClassTag at :31 [] +-(4) MapPartitionsRDD[1] at keyBy at :25 [] | ParallelCollectionRDD[0] at parallelize at :24 [] */","title":"PairRDDFunctions"},{"location":"rdd/PairRDDFunctions/#source-scala","text":"aggregateByKey U: ClassTag ( seqOp: (U, V) => U, combOp: (U, U) => U): RDD[(K, U)] aggregateByKey U: ClassTag ( seqOp: (U, V) => U, combOp: (U, U) => U): RDD[(K, U)] aggregateByKey U: ClassTag ( seqOp: (U, V) => U, combOp: (U, U) => U): RDD[(K, U)] | combineByKey a| [[combineByKey]]","title":"[source, scala]"},{"location":"rdd/PairRDDFunctions/#source-scala_1","text":"combineByKey C : RDD[(K, C)] combineByKey C : RDD[(K, C)] combineByKey C : RDD[(K, C)] | countApproxDistinctByKey a| [[countApproxDistinctByKey]]","title":"[source, scala]"},{"location":"rdd/PairRDDFunctions/#source-scala_2","text":"countApproxDistinctByKey( relativeSD: Double = 0.05): RDD[(K, Long)] countApproxDistinctByKey( relativeSD: Double, numPartitions: Int): RDD[(K, Long)] countApproxDistinctByKey( relativeSD: Double, partitioner: Partitioner): RDD[(K, Long)] countApproxDistinctByKey( p: Int, sp: Int, partitioner: Partitioner): RDD[(K, Long)] | flatMapValues a| [[flatMapValues]]","title":"[source, scala]"},{"location":"rdd/PairRDDFunctions/#source-scala_3","text":"flatMapValues U : RDD[(K, U)] | foldByKey a| [[foldByKey]]","title":"[source, scala]"},{"location":"rdd/PairRDDFunctions/#source-scala_4","text":"foldByKey( zeroValue: V)( func: (V, V) => V): RDD[(K, V)] foldByKey( zeroValue: V, numPartitions: Int)( func: (V, V) => V): RDD[(K, V)] foldByKey( zeroValue: V, partitioner: Partitioner)( func: (V, V) => V): RDD[(K, V)] | mapValues a| [[mapValues]]","title":"[source, scala]"},{"location":"rdd/PairRDDFunctions/#source-scala_5","text":"mapValues U : RDD[(K, U)] | partitionBy a| [[partitionBy]]","title":"[source, scala]"},{"location":"rdd/PairRDDFunctions/#source-scala_6","text":"partitionBy( partitioner: Partitioner): RDD[(K, V)] | saveAsHadoopDataset a| [[saveAsHadoopDataset]]","title":"[source, scala]"},{"location":"rdd/PairRDDFunctions/#source-scala_7","text":"saveAsHadoopDataset( conf: JobConf): Unit saveAsHadoopDataset uses the SparkHadoopWriter utility to < > with a < > (for the given Hadoop https://hadoop.apache.org/docs/r2.7.3/api/org/apache/hadoop/mapred/JobConf.html[JobConf ]) | saveAsHadoopFile a| [[saveAsHadoopFile]]","title":"[source, scala]"},{"location":"rdd/PairRDDFunctions/#source-scala_8","text":"saveAsHadoopFile( path: String, keyClass: Class[ ], valueClass: Class[ ], outputFormatClass: Class[_ <: OutputFormat[ , _]], codec: Class[ <: CompressionCodec]): Unit saveAsHadoopFile( path: String, keyClass: Class[ ], valueClass: Class[ ], outputFormatClass: Class[_ <: OutputFormat[ , _]], conf: JobConf = new JobConf(self.context.hadoopConfiguration), codec: Option[Class[ <: CompressionCodec]] = None): Unit saveAsHadoopFile F <: OutputFormat[K, V] (implicit fm: ClassTag[F]): Unit saveAsHadoopFile F <: OutputFormat[K, V] (implicit fm: ClassTag[F]): Unit | saveAsNewAPIHadoopDataset a| [[saveAsNewAPIHadoopDataset]]","title":"[source, scala]"},{"location":"rdd/PairRDDFunctions/#source-scala_9","text":"saveAsNewAPIHadoopDataset( conf: Configuration): Unit Saves this RDD of key-value pairs ( RDD[K,V] ) to any Hadoop-supported storage system with new Hadoop API (using a Hadoop https://hadoop.apache.org/docs/r2.7.3/api/org/apache/hadoop/conf/Configuration.html[Configuration ] object for that storage system). The configuration should set relevant output params (an https://hadoop.apache.org/docs/r2.7.3/api/org/apache/hadoop/mapreduce/OutputFormat.html[output format], output paths, e.g. a table name to write to) in the same way as it would be configured for a Hadoop MapReduce job. saveAsNewAPIHadoopDataset uses the SparkHadoopWriter utility to < > with a < > (for the given Hadoop https://hadoop.apache.org/docs/r2.7.3/api/org/apache/hadoop/conf/Configuration.html[Configuration ]) | saveAsNewAPIHadoopFile a| [[saveAsNewAPIHadoopFile]]","title":"[source, scala]"},{"location":"rdd/PairRDDFunctions/#source-scala_10","text":"saveAsNewAPIHadoopFile( path: String, keyClass: Class[ ], valueClass: Class[ ], outputFormatClass: Class[_ <: NewOutputFormat[_, _]], conf: Configuration = self.context.hadoopConfiguration): Unit saveAsNewAPIHadoopFile F <: NewOutputFormat[K, V] (implicit fm: ClassTag[F]): Unit |=== == [[reduceByKey]][[groupByKey]] groupByKey and reduceByKey reduceByKey is sort of a particular case of < >. You may want to look at the number of partitions from another angle. It may often not be important to have a given number of partitions upfront (at RDD creation time upon link:spark-data-sources.adoc[loading data from data sources]), so only \"regrouping\" the data by key after it is an RDD might be...the key ( pun not intended ). You can use groupByKey or another PairRDDFunctions method to have a key in one processing flow. You could use partitionBy that is available for RDDs to be RDDs of tuples, i.e. PairRDD : rdd.keyBy(_.kind) .partitionBy(new HashPartitioner(PARTITIONS)) .foreachPartition(...) Think of situations where kind has low cardinality or highly skewed distribution and using the technique for partitioning might be not an optimal solution. You could do as follows: rdd.keyBy(_.kind).reduceByKey(....) or mapValues or plenty of other solutions. FIXME, man . == [[combineByKeyWithClassTag]] combineByKeyWithClassTag","title":"[source, scala]"},{"location":"rdd/PairRDDFunctions/#source-scala_11","text":"combineByKeyWithClassTag C (implicit ct: ClassTag[C]): RDD[(K, C)] // <1> combineByKeyWithClassTag C (implicit ct: ClassTag[C]): RDD[(K, C)] // <2> combineByKeyWithClassTag C (implicit ct: ClassTag[C]): RDD[(K, C)] <1> Uses the xref:rdd:Partitioner.adoc#defaultPartitioner[default partitioner] <2> Uses a xref:rdd:HashPartitioner.adoc[HashPartitioner] with the given number of partitions combineByKeyWithClassTag creates an xref:rdd:Aggregator.adoc[Aggregator] for the given aggregation functions. combineByKeyWithClassTag branches off per the given xref:rdd:Partitioner.adoc[Partitioner]. If the input partitioner and the RDD's are the same, combineByKeyWithClassTag simply xref:rdd:spark-rdd-transformations.adoc#mapPartitions[mapPartitions] on the RDD with the following arguments: Iterator of the xref:rdd:Aggregator.adoc#combineValuesByKey[Aggregator] preservesPartitioning flag turned on If the input partitioner is different than the RDD's, combineByKeyWithClassTag creates a xref:rdd:ShuffledRDD.adoc[ShuffledRDD] (with the Serializer, the Aggregator, and the mapSideCombine flag). === [[combineByKeyWithClassTag-usage]] Usage combineByKeyWithClassTag lays the foundation for the following transformations: < > < > < > < > < > < > === [[combineByKeyWithClassTag-requirements]] Requirements combineByKeyWithClassTag requires that the mergeCombiners is defined (not- null ) or throws an IllegalArgumentException:","title":"[source, scala]"},{"location":"rdd/PairRDDFunctions/#sourceplaintext","text":"","title":"[source,plaintext]"},{"location":"rdd/PairRDDFunctions/#mergecombiners-must-be-defined","text":"combineByKeyWithClassTag throws a SparkException for the keys being of type array with the mapSideCombine flag enabled:","title":"mergeCombiners must be defined"},{"location":"rdd/PairRDDFunctions/#sourceplaintext_1","text":"","title":"[source,plaintext]"},{"location":"rdd/PairRDDFunctions/#cannot-use-map-side-combining-with-array-keys","text":"combineByKeyWithClassTag throws a SparkException for the keys being of type array with the partitioner being a xref:rdd:HashPartitioner.adoc[HashPartitioner]:","title":"Cannot use map-side combining with array keys."},{"location":"rdd/PairRDDFunctions/#sourceplaintext_2","text":"","title":"[source,plaintext]"},{"location":"rdd/PairRDDFunctions/#hashpartitioner-cannot-partition-array-keys","text":"=== [[combineByKeyWithClassTag-example]] Example","title":"HashPartitioner cannot partition array keys."},{"location":"rdd/PairRDDFunctions/#sourcescala","text":"val nums = sc.parallelize(0 to 9, numSlices = 4) val groups = nums.keyBy(_ % 2) def createCombiner(n: Int) = { println(s\"createCombiner( n)\") n } def mergeValue(n1: Int, n2: Int) = { println(s\"mergeValue( n)\") n } def mergeValue(n1: Int, n2: Int) = { println(s\"mergeValue( n1, n2)\") n1 + n2 } def mergeCombiners(c1: Int, c2: Int) = { println(s\"mergeCombiners( n2)\") n1 + n2 } def mergeCombiners(c1: Int, c2: Int) = { println(s\"mergeCombiners( c1, $c2)\") c1 + c2 } val countByGroup = groups.combineByKeyWithClassTag( createCombiner, mergeValue, mergeCombiners) println(countByGroup.toDebugString) /* (4) ShuffledRDD[3] at combineByKeyWithClassTag at :31 [] +-(4) MapPartitionsRDD[1] at keyBy at :25 [] | ParallelCollectionRDD[0] at parallelize at :24 [] */","title":"[source,scala]"},{"location":"rdd/Partitioner/","text":"= Partitioner Partitioner is an abstraction to define how the elements in a key-value pair RDD are partitioned by key. Partitioner < > (from 0 to < > - 1). Partitioner is used to ensure that records for a given key have to reside on a single partition. == [[implementations]] Available Partitioners [cols=\"30,70\",options=\"header\",width=\"100%\"] |=== | Partitioner | Description | xref:rdd:HashPartitioner.adoc[HashPartitioner] | [[HashPartitioner]] Hash-based partitioning | xref:rdd:RangePartitioner.adoc[RangePartitioner] | [[RangePartitioner]] |=== == [[numPartitions]] numPartitions Method [source, scala] \u00b6 numPartitions: Int \u00b6 numPartitions is the number of partition to use for < >. numPartitions is used when...FIXME == [[getPartition]] getPartition Method [source, scala] \u00b6 getPartition(key: Any): Int \u00b6 getPartition maps a given key to a partition ID (from 0 to < > - 1) getPartition is used when...FIXME == [[defaultPartitioner]] defaultPartitioner Method [source, scala] \u00b6 defaultPartitioner( rdd: RDD[ ], others: RDD[ ]*): Partitioner defaultPartitioner...FIXME defaultPartitioner is used when...FIXME","title":"Partitioner"},{"location":"rdd/Partitioner/#source-scala","text":"","title":"[source, scala]"},{"location":"rdd/Partitioner/#numpartitions-int","text":"numPartitions is the number of partition to use for < >. numPartitions is used when...FIXME == [[getPartition]] getPartition Method","title":"numPartitions: Int"},{"location":"rdd/Partitioner/#source-scala_1","text":"","title":"[source, scala]"},{"location":"rdd/Partitioner/#getpartitionkey-any-int","text":"getPartition maps a given key to a partition ID (from 0 to < > - 1) getPartition is used when...FIXME == [[defaultPartitioner]] defaultPartitioner Method","title":"getPartition(key: Any): Int"},{"location":"rdd/Partitioner/#source-scala_2","text":"defaultPartitioner( rdd: RDD[ ], others: RDD[ ]*): Partitioner defaultPartitioner...FIXME defaultPartitioner is used when...FIXME","title":"[source, scala]"},{"location":"rdd/RDD/","text":"= [[RDD]] RDD -- Description of Distributed Computation :navtitle: RDD [[T]] RDD is a description of a fault-tolerant and resilient computation over a possibly distributed collection of records (of type T ). == [[contract]] RDD Contract === [[compute]] Computing Partition (in TaskContext) [source, scala] \u00b6 compute( split: Partition, context: TaskContext): Iterator[T] compute computes the input split xref:rdd:spark-rdd-partitions.adoc[partition] in the xref:scheduler:spark-TaskContext.adoc[TaskContext] to produce a collection of values (of type T ). compute is implemented by any type of RDD in Spark and is called every time the records are requested unless RDD is xref:rdd:spark-rdd-caching.adoc[cached] or xref:ROOT:rdd-checkpointing.adoc[checkpointed] (and the records can be read from an external storage, but this time closer to the compute node). When an RDD is xref:rdd:spark-rdd-caching.adoc[cached], for specified xref:storage:StorageLevel.adoc[storage levels] (i.e. all but NONE )...FIXME compute runs on the xref:ROOT:spark-driver.adoc[driver]. compute is used when RDD is requested to < >. === [[getPartitions]] Partitions [source, scala] \u00b6 getPartitions: Array[Partition] \u00b6 getPartitions is used when RDD is requested for the < > (called only once as the value is cached afterwards). === [[getDependencies]] Dependencies [source, scala] \u00b6 getDependencies: Seq[Dependency[_]] \u00b6 getDependencies is used when RDD is requested for the < > (called only once as the value is cached afterwards). === [[getPreferredLocations]] Preferred Locations (Placement Preferences) [source, scala] \u00b6 getPreferredLocations( split: Partition): Seq[String] = Nil getPreferredLocations is used when RDD is requested for the < > of a given xref:rdd:spark-rdd-Partition.adoc[partition]. === [[partitioner]] Partitioner [source, scala] \u00b6 partitioner: Option[Partitioner] = None \u00b6 RDD can have a xref:rdd:Partitioner.adoc[Partitioner] defined. == [[extensions]][[implementations]] (Subset of) Available RDDs [cols=\"30,70\",options=\"header\",width=\"100%\"] |=== | RDD | Description | xref:rdd:spark-rdd-CoGroupedRDD.adoc[CoGroupedRDD] | [[CoGroupedRDD]] | CoalescedRDD | [[CoalescedRDD]] Result of xref:rdd:spark-rdd-partitions.adoc#repartition[repartition] or xref:rdd:spark-rdd-partitions.adoc#coalesce[coalesce] transformations | xref:rdd:spark-rdd-HadoopRDD.adoc[HadoopRDD] | [[HadoopRDD]] Allows for reading data stored in HDFS using the older MapReduce API. The most notable use case is the return RDD of SparkContext.textFile . | xref:rdd:spark-rdd-MapPartitionsRDD.adoc[MapPartitionsRDD] | [[MapPartitionsRDD]] Result of calling map-like operations (e.g. map , flatMap , filter , xref:rdd:spark-rdd-transformations.adoc#mapPartitions[mapPartitions]) | xref:rdd:spark-rdd-ParallelCollectionRDD.adoc[ParallelCollectionRDD] | [[ParallelCollectionRDD]] | xref:rdd:ShuffledRDD.adoc[ShuffledRDD] | [[ShuffledRDD]] Result of \"shuffle\" operators (e.g. xref:rdd:spark-rdd-partitions.adoc#repartition[repartition] or xref:rdd:spark-rdd-partitions.adoc#coalesce[coalesce]) |=== == [[creating-instance]] Creating Instance RDD takes the following to be created: [[_sc]] xref:ROOT:SparkContext.adoc[] [[deps]] Parent RDDs , i.e. xref:rdd:spark-rdd-Dependency.adoc[Dependencies] (that have to be all computed successfully before this RDD) RDD is an abstract class and cannot be created directly. It is created indirectly for the < >. == [[storageLevel]][[getStorageLevel]] StorageLevel RDD can have a xref:storage:StorageLevel.adoc[StorageLevel] specified. The default StorageLevel is xref:storage:StorageLevel.adoc#NONE[NONE]. storageLevel can be specified using < > method. storageLevel becomes NONE again after < >. The current StorageLevel is available using getStorageLevel method. [source, scala] \u00b6 getStorageLevel: StorageLevel \u00b6 == [[id]] Unique Identifier [source, scala] \u00b6 id: Int \u00b6 id is an unique identifier (aka RDD ID ) in the given <<_sc, SparkContext>>. id requests the < > for xref:ROOT:SparkContext.adoc#newRddId[newRddId] right when RDD is created. == [[isBarrier_]][[isBarrier]] Barrier Stage An RDD can be part of a xref:ROOT:spark-barrier-execution-mode.adoc#barrier-stage[barrier stage]. By default, isBarrier flag is enabled ( true ) when: . There are no xref:rdd:ShuffleDependency.adoc[ShuffleDependencies] among the < > . There is at least one xref:rdd:spark-rdd-Dependency.adoc#rdd[parent RDD] that has the flag enabled xref:rdd:ShuffledRDD.adoc[ShuffledRDD] has the flag always disabled. xref:rdd:spark-rdd-MapPartitionsRDD.adoc[MapPartitionsRDD] is the only one RDD that can have the flag enabled. == [[getOrCompute]] Getting Or Computing RDD Partition [source, scala] \u00b6 getOrCompute( partition: Partition, context: TaskContext): Iterator[T] getOrCompute creates a xref:storage:BlockId.adoc#RDDBlockId[RDDBlockId] for the < > and the link:spark-rdd-Partition.adoc#index[partition index]. getOrCompute requests the BlockManager to xref:storage:BlockManager.adoc#getOrElseUpdate[getOrElseUpdate] for the block ID (with the < > and the makeIterator function). NOTE: getOrCompute uses xref:core:SparkEnv.adoc#get[SparkEnv] to access the current xref:core:SparkEnv.adoc#blockManager[BlockManager]. [[getOrCompute-readCachedBlock]] getOrCompute records whether...FIXME (readCachedBlock) getOrCompute branches off per the response from the xref:storage:BlockManager.adoc#getOrElseUpdate[BlockManager] and whether the internal readCachedBlock flag is now on or still off. In either case, getOrCompute creates an link:spark-InterruptibleIterator.adoc[InterruptibleIterator]. NOTE: link:spark-InterruptibleIterator.adoc[InterruptibleIterator] simply delegates to a wrapped internal Iterator , but allows for link:spark-TaskContext.adoc#isInterrupted[task killing functionality]. For a BlockResult available and readCachedBlock flag on, getOrCompute ...FIXME For a BlockResult available and readCachedBlock flag off, getOrCompute ...FIXME NOTE: The BlockResult could be found in a local block manager or fetched from a remote block manager. It may also have been stored (persisted) just now. In either case, the BlockResult is available (and xref:storage:BlockManager.adoc#getOrElseUpdate[BlockManager.getOrElseUpdate] gives a Left value with the BlockResult ). For Right(iter) (regardless of the value of readCachedBlock flag since...FIXME), getOrCompute ...FIXME NOTE: xref:storage:BlockManager.adoc#getOrElseUpdate[BlockManager.getOrElseUpdate] gives a Right(iter) value to indicate an error with a block. NOTE: getOrCompute is used on Spark executors. NOTE: getOrCompute is used exclusively when RDD is requested for the < >. == [[dependencies]] RDD Dependencies [source, scala] \u00b6 dependencies: Seq[Dependency[_]] \u00b6 dependencies returns the link:spark-rdd-Dependency.adoc[dependencies of a RDD]. NOTE: dependencies is a final method that no class in Spark can ever override. Internally, dependencies checks out whether the RDD is xref:ROOT:rdd-checkpointing.adoc[checkpointed] and acts accordingly. For a RDD being checkpointed, dependencies returns a single-element collection with a link:spark-rdd-NarrowDependency.adoc#OneToOneDependency[OneToOneDependency]. For a non-checkpointed RDD, dependencies collection is computed using < getDependencies method>>. NOTE: getDependencies method is an abstract method that custom RDDs are required to provide. == [[iterator]] Accessing Records For Partition Lazily [source, scala] \u00b6 iterator( split: Partition, context: TaskContext): Iterator[T] iterator < split partition>> when xref:rdd:spark-rdd-caching.adoc[cached] or < >. == [[checkpointRDD]] Getting CheckpointRDD [source, scala] \u00b6 checkpointRDD: Option[CheckpointRDD[T]] \u00b6 checkpointRDD gives the CheckpointRDD from the < > internal registry if available (if the RDD was checkpointed). checkpointRDD is used when RDD is requested for the < >, < > and < >. == [[isCheckpointedAndMaterialized]] isCheckpointedAndMaterialized Method [source, scala] \u00b6 isCheckpointedAndMaterialized: Boolean \u00b6 isCheckpointedAndMaterialized...FIXME isCheckpointedAndMaterialized is used when RDD is requested to < >, < > and < >. == [[getNarrowAncestors]] getNarrowAncestors Method [source, scala] \u00b6 getNarrowAncestors: Seq[RDD[_]] \u00b6 getNarrowAncestors...FIXME getNarrowAncestors is used when StageInfo is requested to xref:scheduler:spark-scheduler-StageInfo.adoc#fromStage[fromStage]. == [[toLocalIterator]] toLocalIterator Method [source, scala] \u00b6 toLocalIterator: Iterator[T] \u00b6 toLocalIterator...FIXME == [[persist]] Persisting RDD [source, scala] \u00b6 persist(): this.type persist( newLevel: StorageLevel): this.type Refer to xref:rdd:spark-rdd-caching.adoc#persist[Persisting RDD]. == [[persist-internal]] persist Internal Method [source, scala] \u00b6 persist( newLevel: StorageLevel, allowOverride: Boolean): this.type persist...FIXME persist (private) is used when RDD is requested to < > and < >. == [[unpersist]] unpersist Method [source, scala] \u00b6 unpersist(blocking: Boolean = true): this.type \u00b6 unpersist...FIXME == [[localCheckpoint]] localCheckpoint Method [source, scala] \u00b6 localCheckpoint(): this.type \u00b6 localCheckpoint marks this RDD for xref:ROOT:rdd-checkpointing.adoc[local checkpointing] using Spark's caching layer. == [[computeOrReadCheckpoint]] Computing Partition or Reading From Checkpoint [source, scala] \u00b6 computeOrReadCheckpoint( split: Partition, context: TaskContext): Iterator[T] computeOrReadCheckpoint reads split partition from a checkpoint (< >) or < > yourself. computeOrReadCheckpoint is used when RDD is requested to < > or < >. == [[getNumPartitions]] Getting Number of Partitions [source, scala] \u00b6 getNumPartitions: Int \u00b6 getNumPartitions gives the number of partitions of a RDD. [source, scala] \u00b6 scala> sc.textFile(\"README.md\").getNumPartitions res0: Int = 2 scala> sc.textFile(\"README.md\", 5).getNumPartitions res1: Int = 5 == [[preferredLocations]] Defining Placement Preferences of RDD Partition [source, scala] \u00b6 preferredLocations( split: Partition): Seq[String] preferredLocations requests the CheckpointRDD for < > (if the RDD is checkpointed) or < >. preferredLocations is a template method that uses < > that custom RDDs can override to specify placement preferences for a partition. getPreferredLocations defines no placement preferences by default. preferredLocations is mainly used when DAGScheduler is requested to xref:scheduler:DAGScheduler.adoc#getPreferredLocs[compute the preferred locations for missing partitions]. == [[partitions]] Accessing RDD Partitions [source, scala] \u00b6 partitions: Array[Partition] \u00b6 partitions returns the xref:rdd:spark-rdd-partitions.adoc[Partitions] of a RDD . partitions requests CheckpointRDD for the < > (if the RDD is checkpointed) or < > and cache (in < > internal registry that is used next time). Partitions have the property that their internal index should be equal to their position in the owning RDD. == [[markCheckpointed]] markCheckpointed Method [source, scala] \u00b6 markCheckpointed(): Unit \u00b6 markCheckpointed...FIXME markCheckpointed is used when...FIXME == [[doCheckpoint]] doCheckpoint Method [source, scala] \u00b6 doCheckpoint(): Unit \u00b6 doCheckpoint...FIXME doCheckpoint is used when SparkContext is requested to xref:ROOT:SparkContext.adoc#runJob[run a job (synchronously)]. == [[checkpoint]] Reliable Checkpointing -- checkpoint Method [source, scala] \u00b6 checkpoint(): Unit \u00b6 checkpoint...FIXME checkpoint is used when...FIXME == [[isReliablyCheckpointed]] isReliablyCheckpointed Method [source, scala] \u00b6 isReliablyCheckpointed: Boolean \u00b6 isReliablyCheckpointed...FIXME isReliablyCheckpointed is used when...FIXME == [[getCheckpointFile]] getCheckpointFile Method [source, scala] \u00b6 getCheckpointFile: Option[String] \u00b6 getCheckpointFile...FIXME getCheckpointFile is used when...FIXME","title":"RDD"},{"location":"rdd/RDD/#source-scala","text":"compute( split: Partition, context: TaskContext): Iterator[T] compute computes the input split xref:rdd:spark-rdd-partitions.adoc[partition] in the xref:scheduler:spark-TaskContext.adoc[TaskContext] to produce a collection of values (of type T ). compute is implemented by any type of RDD in Spark and is called every time the records are requested unless RDD is xref:rdd:spark-rdd-caching.adoc[cached] or xref:ROOT:rdd-checkpointing.adoc[checkpointed] (and the records can be read from an external storage, but this time closer to the compute node). When an RDD is xref:rdd:spark-rdd-caching.adoc[cached], for specified xref:storage:StorageLevel.adoc[storage levels] (i.e. all but NONE )...FIXME compute runs on the xref:ROOT:spark-driver.adoc[driver]. compute is used when RDD is requested to < >. === [[getPartitions]] Partitions","title":"[source, scala]"},{"location":"rdd/RDD/#source-scala_1","text":"","title":"[source, scala]"},{"location":"rdd/RDD/#getpartitions-arraypartition","text":"getPartitions is used when RDD is requested for the < > (called only once as the value is cached afterwards). === [[getDependencies]] Dependencies","title":"getPartitions: Array[Partition]"},{"location":"rdd/RDD/#source-scala_2","text":"","title":"[source, scala]"},{"location":"rdd/RDD/#getdependencies-seqdependency_","text":"getDependencies is used when RDD is requested for the < > (called only once as the value is cached afterwards). === [[getPreferredLocations]] Preferred Locations (Placement Preferences)","title":"getDependencies: Seq[Dependency[_]]"},{"location":"rdd/RDD/#source-scala_3","text":"getPreferredLocations( split: Partition): Seq[String] = Nil getPreferredLocations is used when RDD is requested for the < > of a given xref:rdd:spark-rdd-Partition.adoc[partition]. === [[partitioner]] Partitioner","title":"[source, scala]"},{"location":"rdd/RDD/#source-scala_4","text":"","title":"[source, scala]"},{"location":"rdd/RDD/#partitioner-optionpartitioner-none","text":"RDD can have a xref:rdd:Partitioner.adoc[Partitioner] defined. == [[extensions]][[implementations]] (Subset of) Available RDDs [cols=\"30,70\",options=\"header\",width=\"100%\"] |=== | RDD | Description | xref:rdd:spark-rdd-CoGroupedRDD.adoc[CoGroupedRDD] | [[CoGroupedRDD]] | CoalescedRDD | [[CoalescedRDD]] Result of xref:rdd:spark-rdd-partitions.adoc#repartition[repartition] or xref:rdd:spark-rdd-partitions.adoc#coalesce[coalesce] transformations | xref:rdd:spark-rdd-HadoopRDD.adoc[HadoopRDD] | [[HadoopRDD]] Allows for reading data stored in HDFS using the older MapReduce API. The most notable use case is the return RDD of SparkContext.textFile . | xref:rdd:spark-rdd-MapPartitionsRDD.adoc[MapPartitionsRDD] | [[MapPartitionsRDD]] Result of calling map-like operations (e.g. map , flatMap , filter , xref:rdd:spark-rdd-transformations.adoc#mapPartitions[mapPartitions]) | xref:rdd:spark-rdd-ParallelCollectionRDD.adoc[ParallelCollectionRDD] | [[ParallelCollectionRDD]] | xref:rdd:ShuffledRDD.adoc[ShuffledRDD] | [[ShuffledRDD]] Result of \"shuffle\" operators (e.g. xref:rdd:spark-rdd-partitions.adoc#repartition[repartition] or xref:rdd:spark-rdd-partitions.adoc#coalesce[coalesce]) |=== == [[creating-instance]] Creating Instance RDD takes the following to be created: [[_sc]] xref:ROOT:SparkContext.adoc[] [[deps]] Parent RDDs , i.e. xref:rdd:spark-rdd-Dependency.adoc[Dependencies] (that have to be all computed successfully before this RDD) RDD is an abstract class and cannot be created directly. It is created indirectly for the < >. == [[storageLevel]][[getStorageLevel]] StorageLevel RDD can have a xref:storage:StorageLevel.adoc[StorageLevel] specified. The default StorageLevel is xref:storage:StorageLevel.adoc#NONE[NONE]. storageLevel can be specified using < > method. storageLevel becomes NONE again after < >. The current StorageLevel is available using getStorageLevel method.","title":"partitioner: Option[Partitioner] = None"},{"location":"rdd/RDD/#source-scala_5","text":"","title":"[source, scala]"},{"location":"rdd/RDD/#getstoragelevel-storagelevel","text":"== [[id]] Unique Identifier","title":"getStorageLevel: StorageLevel"},{"location":"rdd/RDD/#source-scala_6","text":"","title":"[source, scala]"},{"location":"rdd/RDD/#id-int","text":"id is an unique identifier (aka RDD ID ) in the given <<_sc, SparkContext>>. id requests the < > for xref:ROOT:SparkContext.adoc#newRddId[newRddId] right when RDD is created. == [[isBarrier_]][[isBarrier]] Barrier Stage An RDD can be part of a xref:ROOT:spark-barrier-execution-mode.adoc#barrier-stage[barrier stage]. By default, isBarrier flag is enabled ( true ) when: . There are no xref:rdd:ShuffleDependency.adoc[ShuffleDependencies] among the < > . There is at least one xref:rdd:spark-rdd-Dependency.adoc#rdd[parent RDD] that has the flag enabled xref:rdd:ShuffledRDD.adoc[ShuffledRDD] has the flag always disabled. xref:rdd:spark-rdd-MapPartitionsRDD.adoc[MapPartitionsRDD] is the only one RDD that can have the flag enabled. == [[getOrCompute]] Getting Or Computing RDD Partition","title":"id: Int"},{"location":"rdd/RDD/#source-scala_7","text":"getOrCompute( partition: Partition, context: TaskContext): Iterator[T] getOrCompute creates a xref:storage:BlockId.adoc#RDDBlockId[RDDBlockId] for the < > and the link:spark-rdd-Partition.adoc#index[partition index]. getOrCompute requests the BlockManager to xref:storage:BlockManager.adoc#getOrElseUpdate[getOrElseUpdate] for the block ID (with the < > and the makeIterator function). NOTE: getOrCompute uses xref:core:SparkEnv.adoc#get[SparkEnv] to access the current xref:core:SparkEnv.adoc#blockManager[BlockManager]. [[getOrCompute-readCachedBlock]] getOrCompute records whether...FIXME (readCachedBlock) getOrCompute branches off per the response from the xref:storage:BlockManager.adoc#getOrElseUpdate[BlockManager] and whether the internal readCachedBlock flag is now on or still off. In either case, getOrCompute creates an link:spark-InterruptibleIterator.adoc[InterruptibleIterator]. NOTE: link:spark-InterruptibleIterator.adoc[InterruptibleIterator] simply delegates to a wrapped internal Iterator , but allows for link:spark-TaskContext.adoc#isInterrupted[task killing functionality]. For a BlockResult available and readCachedBlock flag on, getOrCompute ...FIXME For a BlockResult available and readCachedBlock flag off, getOrCompute ...FIXME NOTE: The BlockResult could be found in a local block manager or fetched from a remote block manager. It may also have been stored (persisted) just now. In either case, the BlockResult is available (and xref:storage:BlockManager.adoc#getOrElseUpdate[BlockManager.getOrElseUpdate] gives a Left value with the BlockResult ). For Right(iter) (regardless of the value of readCachedBlock flag since...FIXME), getOrCompute ...FIXME NOTE: xref:storage:BlockManager.adoc#getOrElseUpdate[BlockManager.getOrElseUpdate] gives a Right(iter) value to indicate an error with a block. NOTE: getOrCompute is used on Spark executors. NOTE: getOrCompute is used exclusively when RDD is requested for the < >. == [[dependencies]] RDD Dependencies","title":"[source, scala]"},{"location":"rdd/RDD/#source-scala_8","text":"","title":"[source, scala]"},{"location":"rdd/RDD/#dependencies-seqdependency_","text":"dependencies returns the link:spark-rdd-Dependency.adoc[dependencies of a RDD]. NOTE: dependencies is a final method that no class in Spark can ever override. Internally, dependencies checks out whether the RDD is xref:ROOT:rdd-checkpointing.adoc[checkpointed] and acts accordingly. For a RDD being checkpointed, dependencies returns a single-element collection with a link:spark-rdd-NarrowDependency.adoc#OneToOneDependency[OneToOneDependency]. For a non-checkpointed RDD, dependencies collection is computed using < getDependencies method>>. NOTE: getDependencies method is an abstract method that custom RDDs are required to provide. == [[iterator]] Accessing Records For Partition Lazily","title":"dependencies: Seq[Dependency[_]]"},{"location":"rdd/RDD/#source-scala_9","text":"iterator( split: Partition, context: TaskContext): Iterator[T] iterator < split partition>> when xref:rdd:spark-rdd-caching.adoc[cached] or < >. == [[checkpointRDD]] Getting CheckpointRDD","title":"[source, scala]"},{"location":"rdd/RDD/#source-scala_10","text":"","title":"[source, scala]"},{"location":"rdd/RDD/#checkpointrdd-optioncheckpointrddt","text":"checkpointRDD gives the CheckpointRDD from the < > internal registry if available (if the RDD was checkpointed). checkpointRDD is used when RDD is requested for the < >, < > and < >. == [[isCheckpointedAndMaterialized]] isCheckpointedAndMaterialized Method","title":"checkpointRDD: Option[CheckpointRDD[T]]"},{"location":"rdd/RDD/#source-scala_11","text":"","title":"[source, scala]"},{"location":"rdd/RDD/#ischeckpointedandmaterialized-boolean","text":"isCheckpointedAndMaterialized...FIXME isCheckpointedAndMaterialized is used when RDD is requested to < >, < > and < >. == [[getNarrowAncestors]] getNarrowAncestors Method","title":"isCheckpointedAndMaterialized: Boolean"},{"location":"rdd/RDD/#source-scala_12","text":"","title":"[source, scala]"},{"location":"rdd/RDD/#getnarrowancestors-seqrdd_","text":"getNarrowAncestors...FIXME getNarrowAncestors is used when StageInfo is requested to xref:scheduler:spark-scheduler-StageInfo.adoc#fromStage[fromStage]. == [[toLocalIterator]] toLocalIterator Method","title":"getNarrowAncestors: Seq[RDD[_]]"},{"location":"rdd/RDD/#source-scala_13","text":"","title":"[source, scala]"},{"location":"rdd/RDD/#tolocaliterator-iteratort","text":"toLocalIterator...FIXME == [[persist]] Persisting RDD","title":"toLocalIterator: Iterator[T]"},{"location":"rdd/RDD/#source-scala_14","text":"persist(): this.type persist( newLevel: StorageLevel): this.type Refer to xref:rdd:spark-rdd-caching.adoc#persist[Persisting RDD]. == [[persist-internal]] persist Internal Method","title":"[source, scala]"},{"location":"rdd/RDD/#source-scala_15","text":"persist( newLevel: StorageLevel, allowOverride: Boolean): this.type persist...FIXME persist (private) is used when RDD is requested to < > and < >. == [[unpersist]] unpersist Method","title":"[source, scala]"},{"location":"rdd/RDD/#source-scala_16","text":"","title":"[source, scala]"},{"location":"rdd/RDD/#unpersistblocking-boolean-true-thistype","text":"unpersist...FIXME == [[localCheckpoint]] localCheckpoint Method","title":"unpersist(blocking: Boolean = true): this.type"},{"location":"rdd/RDD/#source-scala_17","text":"","title":"[source, scala]"},{"location":"rdd/RDD/#localcheckpoint-thistype","text":"localCheckpoint marks this RDD for xref:ROOT:rdd-checkpointing.adoc[local checkpointing] using Spark's caching layer. == [[computeOrReadCheckpoint]] Computing Partition or Reading From Checkpoint","title":"localCheckpoint(): this.type"},{"location":"rdd/RDD/#source-scala_18","text":"computeOrReadCheckpoint( split: Partition, context: TaskContext): Iterator[T] computeOrReadCheckpoint reads split partition from a checkpoint (< >) or < > yourself. computeOrReadCheckpoint is used when RDD is requested to < > or < >. == [[getNumPartitions]] Getting Number of Partitions","title":"[source, scala]"},{"location":"rdd/RDD/#source-scala_19","text":"","title":"[source, scala]"},{"location":"rdd/RDD/#getnumpartitions-int","text":"getNumPartitions gives the number of partitions of a RDD.","title":"getNumPartitions: Int"},{"location":"rdd/RDD/#source-scala_20","text":"scala> sc.textFile(\"README.md\").getNumPartitions res0: Int = 2 scala> sc.textFile(\"README.md\", 5).getNumPartitions res1: Int = 5 == [[preferredLocations]] Defining Placement Preferences of RDD Partition","title":"[source, scala]"},{"location":"rdd/RDD/#source-scala_21","text":"preferredLocations( split: Partition): Seq[String] preferredLocations requests the CheckpointRDD for < > (if the RDD is checkpointed) or < >. preferredLocations is a template method that uses < > that custom RDDs can override to specify placement preferences for a partition. getPreferredLocations defines no placement preferences by default. preferredLocations is mainly used when DAGScheduler is requested to xref:scheduler:DAGScheduler.adoc#getPreferredLocs[compute the preferred locations for missing partitions]. == [[partitions]] Accessing RDD Partitions","title":"[source, scala]"},{"location":"rdd/RDD/#source-scala_22","text":"","title":"[source, scala]"},{"location":"rdd/RDD/#partitions-arraypartition","text":"partitions returns the xref:rdd:spark-rdd-partitions.adoc[Partitions] of a RDD . partitions requests CheckpointRDD for the < > (if the RDD is checkpointed) or < > and cache (in < > internal registry that is used next time). Partitions have the property that their internal index should be equal to their position in the owning RDD. == [[markCheckpointed]] markCheckpointed Method","title":"partitions: Array[Partition]"},{"location":"rdd/RDD/#source-scala_23","text":"","title":"[source, scala]"},{"location":"rdd/RDD/#markcheckpointed-unit","text":"markCheckpointed...FIXME markCheckpointed is used when...FIXME == [[doCheckpoint]] doCheckpoint Method","title":"markCheckpointed(): Unit"},{"location":"rdd/RDD/#source-scala_24","text":"","title":"[source, scala]"},{"location":"rdd/RDD/#docheckpoint-unit","text":"doCheckpoint...FIXME doCheckpoint is used when SparkContext is requested to xref:ROOT:SparkContext.adoc#runJob[run a job (synchronously)]. == [[checkpoint]] Reliable Checkpointing -- checkpoint Method","title":"doCheckpoint(): Unit"},{"location":"rdd/RDD/#source-scala_25","text":"","title":"[source, scala]"},{"location":"rdd/RDD/#checkpoint-unit","text":"checkpoint...FIXME checkpoint is used when...FIXME == [[isReliablyCheckpointed]] isReliablyCheckpointed Method","title":"checkpoint(): Unit"},{"location":"rdd/RDD/#source-scala_26","text":"","title":"[source, scala]"},{"location":"rdd/RDD/#isreliablycheckpointed-boolean","text":"isReliablyCheckpointed...FIXME isReliablyCheckpointed is used when...FIXME == [[getCheckpointFile]] getCheckpointFile Method","title":"isReliablyCheckpointed: Boolean"},{"location":"rdd/RDD/#source-scala_27","text":"","title":"[source, scala]"},{"location":"rdd/RDD/#getcheckpointfile-optionstring","text":"getCheckpointFile...FIXME getCheckpointFile is used when...FIXME","title":"getCheckpointFile: Option[String]"},{"location":"rdd/RDDCheckpointData/","text":"= RDDCheckpointData RDDCheckpointData is an abstraction of information related to RDD checkpointing. == [[implementations]] Available RDDCheckpointDatas [cols=\"30,70\",options=\"header\",width=\"100%\"] |=== | RDDCheckpointData | Description | xref:rdd:LocalRDDCheckpointData.adoc[LocalRDDCheckpointData] | [[LocalRDDCheckpointData]] | xref:rdd:ReliableRDDCheckpointData.adoc[ReliableRDDCheckpointData] | [[ReliableRDDCheckpointData]] xref:ROOT:rdd-checkpointing.adoc#reliable-checkpointing[Reliable Checkpointing] |=== == [[creating-instance]] Creating Instance RDDCheckpointData takes the following to be created: [[rdd]] xref:rdd:RDD.adoc[RDD] == [[Serializable]] RDDCheckpointData as Serializable RDDCheckpointData is java.io.Serializable. == [[cpState]] States [[Initialized]] Initialized [[CheckpointingInProgress]] CheckpointingInProgress [[Checkpointed]] Checkpointed == [[checkpoint]] Checkpointing RDD [source, scala] \u00b6 checkpoint(): CheckpointRDD[T] \u00b6 checkpoint changes the < > to < > only when in < > state. Otherwise, checkpoint does nothing and returns. checkpoint < > that gives an CheckpointRDD (that is the < > internal registry). checkpoint changes the < > to < >. In the end, checkpoint requests the given < > to xref:rdd:RDD.adoc#markCheckpointed[markCheckpointed]. checkpoint is used when RDD is requested to xref:rdd:RDD.adoc#doCheckpoint[doCheckpoint]. == [[doCheckpoint]] doCheckpoint Method [source, scala] \u00b6 doCheckpoint(): CheckpointRDD[T] \u00b6 doCheckpoint is used when RDDCheckpointData is requested to < >.","title":"RDDCheckpointData"},{"location":"rdd/RDDCheckpointData/#source-scala","text":"","title":"[source, scala]"},{"location":"rdd/RDDCheckpointData/#checkpoint-checkpointrddt","text":"checkpoint changes the < > to < > only when in < > state. Otherwise, checkpoint does nothing and returns. checkpoint < > that gives an CheckpointRDD (that is the < > internal registry). checkpoint changes the < > to < >. In the end, checkpoint requests the given < > to xref:rdd:RDD.adoc#markCheckpointed[markCheckpointed]. checkpoint is used when RDD is requested to xref:rdd:RDD.adoc#doCheckpoint[doCheckpoint]. == [[doCheckpoint]] doCheckpoint Method","title":"checkpoint(): CheckpointRDD[T]"},{"location":"rdd/RDDCheckpointData/#source-scala_1","text":"","title":"[source, scala]"},{"location":"rdd/RDDCheckpointData/#docheckpoint-checkpointrddt","text":"doCheckpoint is used when RDDCheckpointData is requested to < >.","title":"doCheckpoint(): CheckpointRDD[T]"},{"location":"rdd/RangePartitioner/","text":"= RangePartitioner RangePartitioner is a xref:rdd:Partitioner.adoc[Partitioner] for...FIXME [[ordering]] RangePartitioner[K : Ordering : ClassTag, V] is a parameterized type of K keys that can be sorted ( ordered ) and V values. RangePartitioner is used for xref:rdd:spark-rdd-OrderedRDDFunctions.adoc#sortByKey[sortByKey] operator. == [[creating-instance]] Creating Instance RangePartitioner takes the following to be created: [[partitions]] Number of partitions [[rdd]] xref:rdd:RDD.adoc[RDD] ( RDD[_ <: Product2[K, V]] ) [[ascending]] ascending flag (default: true ) [[samplePointsPerPartitionHint]] samplePointsPerPartitionHint (default: 20) == [[rangeBounds]] rangeBounds Array RangePartitioner uses rangeBounds registry (of type Array[K] ) when requested for < > and < >, < >. == [[numPartitions]] Number of Partitions [source,scala] \u00b6 numPartitions: Int \u00b6 numPartitions is simply one more than the length of the < > array. numPartitions is part of the xref:rdd:Partitioner.adoc#numPartitions[Partitioner] abstraction. == [[getPartition]] Finding Partition ID for Key [source, scala] \u00b6 getPartition(key: Any): Int \u00b6 getPartition...FIXME getPartition is part of the xref:rdd:Partitioner.adoc#getPartition[Partitioner] abstraction.","title":"RangePartitioner"},{"location":"rdd/RangePartitioner/#sourcescala","text":"","title":"[source,scala]"},{"location":"rdd/RangePartitioner/#numpartitions-int","text":"numPartitions is simply one more than the length of the < > array. numPartitions is part of the xref:rdd:Partitioner.adoc#numPartitions[Partitioner] abstraction. == [[getPartition]] Finding Partition ID for Key","title":"numPartitions: Int"},{"location":"rdd/RangePartitioner/#source-scala","text":"","title":"[source, scala]"},{"location":"rdd/RangePartitioner/#getpartitionkey-any-int","text":"getPartition...FIXME getPartition is part of the xref:rdd:Partitioner.adoc#getPartition[Partitioner] abstraction.","title":"getPartition(key: Any): Int"},{"location":"rdd/ReliableCheckpointRDD/","text":"= ReliableCheckpointRDD ReliableCheckpointRDD is an xref:rdd:CheckpointRDD.adoc[CheckpointRDD]...FIXME == [[creating-instance]] Creating Instance ReliableCheckpointRDD takes the following to be created: [[sc]] xref:ROOT:SparkContext.adoc[] [[checkpointPath]] Checkpoint Directory (on a Hadoop DFS-compatible file system) <<_partitioner, Partitioner>> ReliableCheckpointRDD is created when: ReliableCheckpointRDD utility is used to < >. SparkContext is requested to xref:ROOT:SparkContext.adoc#checkpointFile[checkpointFile] == [[checkpointPartitionerFileName]] Checkpointed Partitioner File ReliableCheckpointRDD uses _partitioner as the name of the file in the < > with the < > serialized to. == [[partitioner]] Partitioner ReliableCheckpointRDD can be given a xref:rdd:Partitioner.adoc[Partitioner] to be created. When xref:rdd:RDD.adoc#partitioner[requested for the Partitioner] (as an RDD), ReliableCheckpointRDD returns the one it was created with or < >. == [[writeRDDToCheckpointDirectory]] Writing RDD to Checkpoint Directory [source, scala] \u00b6 writeRDDToCheckpointDirectory T: ClassTag : ReliableCheckpointRDD[T] writeRDDToCheckpointDirectory...FIXME writeRDDToCheckpointDirectory is used when ReliableRDDCheckpointData is requested to xref:rdd:ReliableRDDCheckpointData.adoc#doCheckpoint[doCheckpoint]. == [[writePartitionerToCheckpointDir]] Writing Partitioner to Checkpoint Directory [source,scala] \u00b6 writePartitionerToCheckpointDir( sc: SparkContext, partitioner: Partitioner, checkpointDirPath: Path): Unit writePartitionerToCheckpointDir creates the < > with the buffer size based on xref:ROOT:configuration-properties.adoc#spark.buffer.size[spark.buffer.size] configuration property. writePartitionerToCheckpointDir requests the xref:core:SparkEnv.adoc#serializer[default Serializer] for a new xref:serializer:Serializer.adoc#newInstance[SerializerInstance]. writePartitionerToCheckpointDir requests the SerializerInstance to xref:serializer:SerializerInstance.adoc#serializeStream[serialize the output stream] and xref:serializer:DeserializationStream.adoc#writeObject[writes] the given Partitioner. In the end, writePartitionerToCheckpointDir prints out the following DEBUG message to the logs: [source,plaintext] \u00b6 Written partitioner to [partitionerFilePath] \u00b6 In case of any non-fatal exception, writePartitionerToCheckpointDir prints out the following DEBUG message to the logs: [source,plaintext] \u00b6 Error writing partitioner [partitioner] to [checkpointDirPath] \u00b6 writePartitionerToCheckpointDir is used when ReliableCheckpointRDD is requested to < >. == [[readCheckpointedPartitionerFile]] Reading Partitioner from Checkpointed Directory [source,scala] \u00b6 readCheckpointedPartitionerFile( sc: SparkContext, checkpointDirPath: String): Option[Partitioner] readCheckpointedPartitionerFile opens the < > with the buffer size based on xref:ROOT:configuration-properties.adoc#spark.buffer.size[spark.buffer.size] configuration property. readCheckpointedPartitionerFile requests the xref:core:SparkEnv.adoc#serializer[default Serializer] for a new xref:serializer:Serializer.adoc#newInstance[SerializerInstance]. readCheckpointedPartitionerFile requests the SerializerInstance to xref:serializer:SerializerInstance.adoc#deserializeStream[deserialize the input stream] and xref:serializer:DeserializationStream.adoc#readObject[read the Partitioner] from the partitioner file. readCheckpointedPartitionerFile prints out the following DEBUG message to the logs and returns the partitioner. [source,plaintext] \u00b6 Read partitioner from [partitionerFilePath] \u00b6 In case of FileNotFoundException or any non-fatal exceptions, readCheckpointedPartitionerFile prints out a corresponding message to the logs and returns None. readCheckpointedPartitionerFile is used when ReliableCheckpointRDD is requested for the < >. == [[logging]] Logging Enable ALL logging level for org.apache.spark.rdd.ReliableCheckpointRDD$ logger to see what happens inside. Add the following line to conf/log4j.properties : [source,plaintext] \u00b6 log4j.logger.org.apache.spark.rdd.ReliableCheckpointRDD$=ALL \u00b6 Refer to xref:ROOT:spark-logging.adoc[Logging].","title":"ReliableCheckpointRDD"},{"location":"rdd/ReliableCheckpointRDD/#source-scala","text":"writeRDDToCheckpointDirectory T: ClassTag : ReliableCheckpointRDD[T] writeRDDToCheckpointDirectory...FIXME writeRDDToCheckpointDirectory is used when ReliableRDDCheckpointData is requested to xref:rdd:ReliableRDDCheckpointData.adoc#doCheckpoint[doCheckpoint]. == [[writePartitionerToCheckpointDir]] Writing Partitioner to Checkpoint Directory","title":"[source, scala]"},{"location":"rdd/ReliableCheckpointRDD/#sourcescala","text":"writePartitionerToCheckpointDir( sc: SparkContext, partitioner: Partitioner, checkpointDirPath: Path): Unit writePartitionerToCheckpointDir creates the < > with the buffer size based on xref:ROOT:configuration-properties.adoc#spark.buffer.size[spark.buffer.size] configuration property. writePartitionerToCheckpointDir requests the xref:core:SparkEnv.adoc#serializer[default Serializer] for a new xref:serializer:Serializer.adoc#newInstance[SerializerInstance]. writePartitionerToCheckpointDir requests the SerializerInstance to xref:serializer:SerializerInstance.adoc#serializeStream[serialize the output stream] and xref:serializer:DeserializationStream.adoc#writeObject[writes] the given Partitioner. In the end, writePartitionerToCheckpointDir prints out the following DEBUG message to the logs:","title":"[source,scala]"},{"location":"rdd/ReliableCheckpointRDD/#sourceplaintext","text":"","title":"[source,plaintext]"},{"location":"rdd/ReliableCheckpointRDD/#written-partitioner-to-partitionerfilepath","text":"In case of any non-fatal exception, writePartitionerToCheckpointDir prints out the following DEBUG message to the logs:","title":"Written partitioner to [partitionerFilePath]"},{"location":"rdd/ReliableCheckpointRDD/#sourceplaintext_1","text":"","title":"[source,plaintext]"},{"location":"rdd/ReliableCheckpointRDD/#error-writing-partitioner-partitioner-to-checkpointdirpath","text":"writePartitionerToCheckpointDir is used when ReliableCheckpointRDD is requested to < >. == [[readCheckpointedPartitionerFile]] Reading Partitioner from Checkpointed Directory","title":"Error writing partitioner [partitioner] to [checkpointDirPath]"},{"location":"rdd/ReliableCheckpointRDD/#sourcescala_1","text":"readCheckpointedPartitionerFile( sc: SparkContext, checkpointDirPath: String): Option[Partitioner] readCheckpointedPartitionerFile opens the < > with the buffer size based on xref:ROOT:configuration-properties.adoc#spark.buffer.size[spark.buffer.size] configuration property. readCheckpointedPartitionerFile requests the xref:core:SparkEnv.adoc#serializer[default Serializer] for a new xref:serializer:Serializer.adoc#newInstance[SerializerInstance]. readCheckpointedPartitionerFile requests the SerializerInstance to xref:serializer:SerializerInstance.adoc#deserializeStream[deserialize the input stream] and xref:serializer:DeserializationStream.adoc#readObject[read the Partitioner] from the partitioner file. readCheckpointedPartitionerFile prints out the following DEBUG message to the logs and returns the partitioner.","title":"[source,scala]"},{"location":"rdd/ReliableCheckpointRDD/#sourceplaintext_2","text":"","title":"[source,plaintext]"},{"location":"rdd/ReliableCheckpointRDD/#read-partitioner-from-partitionerfilepath","text":"In case of FileNotFoundException or any non-fatal exceptions, readCheckpointedPartitionerFile prints out a corresponding message to the logs and returns None. readCheckpointedPartitionerFile is used when ReliableCheckpointRDD is requested for the < >. == [[logging]] Logging Enable ALL logging level for org.apache.spark.rdd.ReliableCheckpointRDD$ logger to see what happens inside. Add the following line to conf/log4j.properties :","title":"Read partitioner from [partitionerFilePath]"},{"location":"rdd/ReliableCheckpointRDD/#sourceplaintext_3","text":"","title":"[source,plaintext]"},{"location":"rdd/ReliableCheckpointRDD/#log4jloggerorgapachesparkrddreliablecheckpointrddall","text":"Refer to xref:ROOT:spark-logging.adoc[Logging].","title":"log4j.logger.org.apache.spark.rdd.ReliableCheckpointRDD$=ALL"},{"location":"rdd/ReliableRDDCheckpointData/","text":"= ReliableRDDCheckpointData ReliableRDDCheckpointData is a xref:rdd:RDDCheckpointData.adoc[RDDCheckpointData] for xref:ROOT:rdd-checkpointing.adoc#reliable-checkpointing[Reliable Checkpointing]. == [[creating-instance]] Creating Instance ReliableRDDCheckpointData takes the following to be created: [[rdd]] xref:rdd:RDD.adoc[++RDD[T]++] ReliableRDDCheckpointData is created for xref:rdd:RDD.adoc#checkpoint[RDD.checkpoint] operator. == [[cpDir]][[checkpointPath]] Checkpoint Directory ReliableRDDCheckpointData creates a subdirectory of the xref:ROOT:SparkContext.adoc#checkpointDir[application-wide checkpoint directory] for < > the given < >. The name of the subdirectory uses the xref:rdd:RDD.adoc#id[unique identifier] of the < >: [source,plaintext] \u00b6 rdd-[id] \u00b6 == [[doCheckpoint]] Checkpointing RDD [source, scala] \u00b6 doCheckpoint(): CheckpointRDD[T] \u00b6 doCheckpoint xref:rdd:ReliableCheckpointRDD.adoc#writeRDDToCheckpointDirectory[writes] the < > to the < > (that creates a new RDD). With xref:ROOT:configuration-properties.adoc#spark.cleaner.referenceTracking.cleanCheckpoints[spark.cleaner.referenceTracking.cleanCheckpoints] configuration property enabled, doCheckpoint requests the xref:ROOT:SparkContext.adoc#cleaner[ContextCleaner] to xref:core:ContextCleaner.adoc#registerRDDCheckpointDataForCleanup[registerRDDCheckpointDataForCleanup] for the new RDD. In the end, doCheckpoint prints out the following INFO message to the logs and returns the new RDD. [source,plaintext] \u00b6 Done checkpointing RDD [id] to [cpDir], new parent is RDD [id] \u00b6 doCheckpoint is part of the xref:rdd:RDDCheckpointData.adoc#doCheckpoint[RDDCheckpointData] abstraction.","title":"ReliableRDDCheckpointData"},{"location":"rdd/ReliableRDDCheckpointData/#sourceplaintext","text":"","title":"[source,plaintext]"},{"location":"rdd/ReliableRDDCheckpointData/#rdd-id","text":"== [[doCheckpoint]] Checkpointing RDD","title":"rdd-[id]"},{"location":"rdd/ReliableRDDCheckpointData/#source-scala","text":"","title":"[source, scala]"},{"location":"rdd/ReliableRDDCheckpointData/#docheckpoint-checkpointrddt","text":"doCheckpoint xref:rdd:ReliableCheckpointRDD.adoc#writeRDDToCheckpointDirectory[writes] the < > to the < > (that creates a new RDD). With xref:ROOT:configuration-properties.adoc#spark.cleaner.referenceTracking.cleanCheckpoints[spark.cleaner.referenceTracking.cleanCheckpoints] configuration property enabled, doCheckpoint requests the xref:ROOT:SparkContext.adoc#cleaner[ContextCleaner] to xref:core:ContextCleaner.adoc#registerRDDCheckpointDataForCleanup[registerRDDCheckpointDataForCleanup] for the new RDD. In the end, doCheckpoint prints out the following INFO message to the logs and returns the new RDD.","title":"doCheckpoint(): CheckpointRDD[T]"},{"location":"rdd/ReliableRDDCheckpointData/#sourceplaintext_1","text":"","title":"[source,plaintext]"},{"location":"rdd/ReliableRDDCheckpointData/#done-checkpointing-rdd-id-to-cpdir-new-parent-is-rdd-id","text":"doCheckpoint is part of the xref:rdd:RDDCheckpointData.adoc#doCheckpoint[RDDCheckpointData] abstraction.","title":"Done checkpointing RDD [id] to [cpDir], new parent is RDD [id]"},{"location":"rdd/ShuffleDependency/","text":"= [[ShuffleDependency]] ShuffleDependency ShuffleDependency is a xref:rdd:spark-rdd-Dependency.adoc[Dependency] on the output of a xref:scheduler:ShuffleMapStage.adoc[ShuffleMapStage] for a < >. ShuffleDependency uses the < > to know the number of (map-side/pre-shuffle) partitions and the < > for the number of (reduce-size/post-shuffle) partitions. ShuffleDependency is created as a dependency of xref:rdd:ShuffledRDD.adoc[ShuffledRDD]. ShuffleDependency can also be created as a dependency of xref:rdd:spark-rdd-CoGroupedRDD.adoc[CoGroupedRDD] and xref:rdd:spark-rdd-SubtractedRDD.adoc[SubtractedRDD]. == [[creating-instance]] Creating Instance ShuffleDependency takes the following to be created: < > of key-value pairs ( RDD[_ <: Product2[K, V]] ) < > [[serializer]] xref:serializer:Serializer.adoc[Serializer] [[keyOrdering]] Ordering for K keys ( Option[Ordering[K]] ) < > ( Option[Aggregator[K, V, C]] ) < > flag (default: false ) When created, ShuffleDependency gets xref:ROOT:SparkContext.adoc#nextShuffleId[shuffle id] (as shuffleId ). NOTE: ShuffleDependency uses the xref:rdd:index.adoc#context[input RDD to access SparkContext ] and so the shuffleId . ShuffleDependency xref:shuffle:ShuffleManager.adoc#registerShuffle[registers itself with ShuffleManager ] and gets a ShuffleHandle (available as < > property). NOTE: ShuffleDependency accesses xref:core:SparkEnv.adoc#shuffleManager[ ShuffleManager using SparkEnv ]. In the end, ShuffleDependency xref:core:ContextCleaner.adoc#registerShuffleForCleanup[registers itself for cleanup with ContextCleaner ]. NOTE: ShuffleDependency accesses the xref:ROOT:SparkContext.adoc#cleaner[optional ContextCleaner through SparkContext ]. NOTE: ShuffleDependency is created when xref:ShuffledRDD.adoc#getDependencies[ShuffledRDD], link:spark-rdd-CoGroupedRDD.adoc#getDependencies[CoGroupedRDD], and link:spark-rdd-SubtractedRDD.adoc#getDependencies[SubtractedRDD] return their RDD dependencies. == [[shuffleId]] Shuffle ID Every ShuffleDependency has a unique application-wide shuffle ID that is assigned when < > (and is used throughout Spark's code to reference a ShuffleDependency). Shuffle IDs are tracked by xref:ROOT:SparkContext.adoc#nextShuffleId[SparkContext]. == [[rdd]] Parent RDD ShuffleDependency is given the parent xref:rdd:RDD.adoc[RDD] of key-value pairs ( RDD[_ <: Product2[K, V]] ). The parent RDD is available as rdd property that is part of the xref:rdd:spark-rdd-Dependency.adoc#rdd[Dependency] abstraction. [source,scala] \u00b6 rdd: RDD[Product2[K, V]] \u00b6 == [[partitioner]] Partitioner ShuffleDependency is given a xref:rdd:Partitioner.adoc[Partitioner] that is used to partition the shuffle output (when xref:shuffle:SortShuffleWriter.adoc[SortShuffleWriter], xref:shuffle:BypassMergeSortShuffleWriter.adoc[BypassMergeSortShuffleWriter] and xref:shuffle:UnsafeShuffleWriter.adoc[UnsafeShuffleWriter] are requested to write). == [[shuffleHandle]] ShuffleHandle [source, scala] \u00b6 shuffleHandle: ShuffleHandle \u00b6 shuffleHandle is the ShuffleHandle of a ShuffleDependency as assigned eagerly when < >. shuffleHandle is used to compute link:spark-rdd-CoGroupedRDD.adoc#compute[CoGroupedRDDs], xref:ShuffledRDD.adoc#compute[ShuffledRDD], link:spark-rdd-SubtractedRDD.adoc#compute[SubtractedRDD], and link:spark-sql-ShuffledRowRDD.adoc[ShuffledRowRDD] (to get a link:spark-shuffle-ShuffleReader.adoc[ShuffleReader] for a ShuffleDependency) and when a xref:scheduler:ShuffleMapTask.adoc#runTask[ ShuffleMapTask runs] (to get a ShuffleWriter for a ShuffleDependency). == [[mapSideCombine]] Map-Size Partial Aggregation Flag ShuffleDependency uses a mapSideCombine flag that controls whether to perform map-side partial aggregation ( map-side combine ) using an < >. mapSideCombine is disabled ( false ) by default and can be enabled ( true ) for some use cases of xref:rdd:ShuffledRDD.adoc#mapSideCombine[ShuffledRDD]. ShuffleDependency requires that the optional < > is defined when the flag is enabled. mapSideCombine is used when: BlockStoreShuffleReader is requested to xref:shuffle:BlockStoreShuffleReader.adoc#read[read combined records for a reduce task] SortShuffleManager is requested to xref:shuffle:SortShuffleManager.adoc#registerShuffle[register a shuffle] SortShuffleWriter is requested to xref:shuffle:SortShuffleWriter.adoc#write[write records] == [[aggregator]] Optional Aggregator [source, scala] \u00b6 aggregator: Option[Aggregator[K, V, C]] = None \u00b6 aggregator is a xref:rdd:Aggregator.adoc[map/reduce-side Aggregator] (for a RDD's shuffle). aggregator is by default undefined (i.e. None ) when < >. NOTE: aggregator is used when xref:shuffle:SortShuffleWriter.adoc#write[ SortShuffleWriter writes records] and xref:shuffle:BlockStoreShuffleReader.adoc#read[ BlockStoreShuffleReader reads combined key-values for a reduce task].","title":"ShuffleDependency"},{"location":"rdd/ShuffleDependency/#sourcescala","text":"","title":"[source,scala]"},{"location":"rdd/ShuffleDependency/#rdd-rddproduct2k-v","text":"== [[partitioner]] Partitioner ShuffleDependency is given a xref:rdd:Partitioner.adoc[Partitioner] that is used to partition the shuffle output (when xref:shuffle:SortShuffleWriter.adoc[SortShuffleWriter], xref:shuffle:BypassMergeSortShuffleWriter.adoc[BypassMergeSortShuffleWriter] and xref:shuffle:UnsafeShuffleWriter.adoc[UnsafeShuffleWriter] are requested to write). == [[shuffleHandle]] ShuffleHandle","title":"rdd: RDD[Product2[K, V]]"},{"location":"rdd/ShuffleDependency/#source-scala","text":"","title":"[source, scala]"},{"location":"rdd/ShuffleDependency/#shufflehandle-shufflehandle","text":"shuffleHandle is the ShuffleHandle of a ShuffleDependency as assigned eagerly when < >. shuffleHandle is used to compute link:spark-rdd-CoGroupedRDD.adoc#compute[CoGroupedRDDs], xref:ShuffledRDD.adoc#compute[ShuffledRDD], link:spark-rdd-SubtractedRDD.adoc#compute[SubtractedRDD], and link:spark-sql-ShuffledRowRDD.adoc[ShuffledRowRDD] (to get a link:spark-shuffle-ShuffleReader.adoc[ShuffleReader] for a ShuffleDependency) and when a xref:scheduler:ShuffleMapTask.adoc#runTask[ ShuffleMapTask runs] (to get a ShuffleWriter for a ShuffleDependency). == [[mapSideCombine]] Map-Size Partial Aggregation Flag ShuffleDependency uses a mapSideCombine flag that controls whether to perform map-side partial aggregation ( map-side combine ) using an < >. mapSideCombine is disabled ( false ) by default and can be enabled ( true ) for some use cases of xref:rdd:ShuffledRDD.adoc#mapSideCombine[ShuffledRDD]. ShuffleDependency requires that the optional < > is defined when the flag is enabled. mapSideCombine is used when: BlockStoreShuffleReader is requested to xref:shuffle:BlockStoreShuffleReader.adoc#read[read combined records for a reduce task] SortShuffleManager is requested to xref:shuffle:SortShuffleManager.adoc#registerShuffle[register a shuffle] SortShuffleWriter is requested to xref:shuffle:SortShuffleWriter.adoc#write[write records] == [[aggregator]] Optional Aggregator","title":"shuffleHandle: ShuffleHandle"},{"location":"rdd/ShuffleDependency/#source-scala_1","text":"","title":"[source, scala]"},{"location":"rdd/ShuffleDependency/#aggregator-optionaggregatork-v-c-none","text":"aggregator is a xref:rdd:Aggregator.adoc[map/reduce-side Aggregator] (for a RDD's shuffle). aggregator is by default undefined (i.e. None ) when < >. NOTE: aggregator is used when xref:shuffle:SortShuffleWriter.adoc#write[ SortShuffleWriter writes records] and xref:shuffle:BlockStoreShuffleReader.adoc#read[ BlockStoreShuffleReader reads combined key-values for a reduce task].","title":"aggregator: Option[Aggregator[K, V, C]] = None"},{"location":"rdd/ShuffledRDD/","text":"= [[ShuffledRDD]] ShuffledRDD ShuffledRDD is an xref:rdd:RDD.adoc[RDD] of key-value pairs that represents a shuffle step in a xref:spark-rdd-lineage.adoc[RDD lineage]. ShuffledRDD is given an < > of key-value pairs of K and V types, respectively, when < > and < > key-value pairs of K and C types, respectively. ShuffledRDD is < > for the following RDD transformations: xref:spark-rdd-OrderedRDDFunctions.adoc#sortByKey[OrderedRDDFunctions.sortByKey] and xref:spark-rdd-OrderedRDDFunctions.adoc#repartitionAndSortWithinPartitions[OrderedRDDFunctions.repartitionAndSortWithinPartitions] xref:rdd:PairRDDFunctions.adoc#combineByKeyWithClassTag[PairRDDFunctions.combineByKeyWithClassTag] and xref:rdd:PairRDDFunctions.adoc#partitionBy[PairRDDFunctions.partitionBy] xref:spark-rdd-transformations.adoc#coalesce[RDD.coalesce] (with shuffle flag enabled) ShuffledRDD uses custom < > partitions. [[isBarrier]] ShuffledRDD has xref:rdd:RDD.adoc#isBarrier[isBarrier] flag always disabled ( false ). == [[creating-instance]] Creating Instance ShuffledRDD takes the following to be created: [[prev]] Previous xref:rdd:RDD.adoc[RDD] of key-value pairs ( RDD[_ <: Product2[K, V]] ) [[part]] xref:rdd:Partitioner.adoc[Partitioner] == [[mapSideCombine]][[setMapSideCombine]] Map-Side Combine Flag ShuffledRDD uses a map-side combine flag to create a xref:rdd:ShuffleDependency.adoc[ShuffleDependency] when requested for the < > (there is always only one). The flag is disabled ( false ) by default and can be changed using setMapSideCombine method. [source,scala] \u00b6 setMapSideCombine( mapSideCombine: Boolean): ShuffledRDD[K, V, C] setMapSideCombine is used for xref:rdd:PairRDDFunctions.adoc#combineByKeyWithClassTag[PairRDDFunctions.combineByKeyWithClassTag] transformation (which defaults to the flag enabled). == [[compute]] Computing Partition [source, scala] \u00b6 compute( split: Partition, context: TaskContext): Iterator[(K, C)] compute requests the only xref:rdd:RDD.adoc#dependencies[dependency] (that is assumed a xref:rdd:ShuffleDependency.adoc[ShuffleDependency]) for the xref:rdd:ShuffleDependency.adoc#shuffleHandle[ShuffleHandle]. compute uses the xref:core:SparkEnv.adoc[SparkEnv] to access the xref:core:SparkEnv.adoc#shuffleManager[ShuffleManager]. compute requests the xref:shuffle:ShuffleManager.adoc#shuffleManager[ShuffleManager] for the xref:shuffle:ShuffleManager.adoc#getReader[ShuffleReader] (for the ShuffleHandle, the xref:rdd:spark-rdd-Partition.adoc[partition]). In the end, compute requests the ShuffleReader to xref:shuffle:spark-shuffle-ShuffleReader.adoc#read[read] the combined key-value pairs (of type (K, C) ). compute is part of the xref:rdd:RDD.adoc#compute[RDD] abstraction. == [[getPreferredLocations]] Placement Preferences of Partition [source, scala] \u00b6 getPreferredLocations( partition: Partition): Seq[String] getPreferredLocations requests MapOutputTrackerMaster for the xref:scheduler:MapOutputTrackerMaster.adoc#getPreferredLocationsForShuffle[preferred locations] of the given xref:rdd:spark-rdd-Partition.adoc[partition] (xref:storage:BlockManager.adoc[BlockManagers] with the most map outputs). getPreferredLocations uses SparkEnv to access the current xref:core:SparkEnv.adoc#mapOutputTracker[MapOutputTrackerMaster]. getPreferredLocations is part of the xref:rdd:RDD.adoc#compute[RDD] abstraction. == [[getDependencies]] Dependencies [source, scala] \u00b6 getDependencies: Seq[Dependency[_]] \u00b6 getDependencies uses the < > if defined or requests the current xref:serializer:SerializerManager.adoc[SerializerManager] for xref:serializer:SerializerManager.adoc#getSerializer[one]. getDependencies uses the < > internal flag for the types of the keys and values (i.e. K and C or K and V when the flag is enabled or not, respectively). In the end, getDependencies returns a single xref:rdd:ShuffleDependency.adoc[ShuffleDependency] (with the < >, the < >, and the Serializer). getDependencies is part of the xref:rdd:RDD.adoc#getDependencies[RDD] abstraction. == [[ShuffledRDDPartition]] ShuffledRDDPartition ShuffledRDDPartition gets an index to be created (that in turn is the index of partitions as calculated by the xref:rdd:Partitioner.adoc[Partitioner] of a < >). == Demos === Demo: ShuffledRDD and coalesce [source,plaintext] \u00b6 val data = sc.parallelize(0 to 9) val coalesced = data.coalesce(numPartitions = 4, shuffle = true) scala> println(coalesced.toDebugString) (4) MapPartitionsRDD[9] at coalesce at :75 [] | CoalescedRDD[8] at coalesce at :75 [] | ShuffledRDD[7] at coalesce at :75 [] +-(16) MapPartitionsRDD[6] at coalesce at :75 [] | ParallelCollectionRDD[5] at parallelize at :74 [] === Demo: ShuffledRDD and sortByKey [source,plaintext] \u00b6 val data = sc.parallelize(0 to 9) val grouped = rdd.groupBy(_ % 2) val sorted = grouped.sortByKey(numPartitions = 2) scala> println(sorted.toDebugString) (2) ShuffledRDD[15] at sortByKey at :74 [] +-(4) ShuffledRDD[12] at groupBy at :74 [] +-(4) MapPartitionsRDD[11] at groupBy at :74 [] | MapPartitionsRDD[9] at coalesce at :75 [] | CoalescedRDD[8] at coalesce at :75 [] | ShuffledRDD[7] at coalesce at :75 [] +-(16) MapPartitionsRDD[6] at coalesce at :75 [] | ParallelCollectionRDD[5] at parallelize at :74 [] == [[internal-properties]] Internal Properties [cols=\"30m,70\",options=\"header\",width=\"100%\"] |=== | Name | Description | userSpecifiedSerializer a| [[userSpecifiedSerializer]] User-specified xref:serializer:Serializer.adoc[Serializer] for the single xref:rdd:ShuffleDependency.adoc[ShuffleDependency] dependency [source, scala] \u00b6 userSpecifiedSerializer: Option[Serializer] = None \u00b6 userSpecifiedSerializer is undefined ( None ) by default and can be changed using setSerializer method (that is used for xref:rdd:PairRDDFunctions.adoc#combineByKeyWithClassTag[PairRDDFunctions.combineByKeyWithClassTag] transformation). |===","title":"ShuffledRDD"},{"location":"rdd/ShuffledRDD/#sourcescala","text":"setMapSideCombine( mapSideCombine: Boolean): ShuffledRDD[K, V, C] setMapSideCombine is used for xref:rdd:PairRDDFunctions.adoc#combineByKeyWithClassTag[PairRDDFunctions.combineByKeyWithClassTag] transformation (which defaults to the flag enabled). == [[compute]] Computing Partition","title":"[source,scala]"},{"location":"rdd/ShuffledRDD/#source-scala","text":"compute( split: Partition, context: TaskContext): Iterator[(K, C)] compute requests the only xref:rdd:RDD.adoc#dependencies[dependency] (that is assumed a xref:rdd:ShuffleDependency.adoc[ShuffleDependency]) for the xref:rdd:ShuffleDependency.adoc#shuffleHandle[ShuffleHandle]. compute uses the xref:core:SparkEnv.adoc[SparkEnv] to access the xref:core:SparkEnv.adoc#shuffleManager[ShuffleManager]. compute requests the xref:shuffle:ShuffleManager.adoc#shuffleManager[ShuffleManager] for the xref:shuffle:ShuffleManager.adoc#getReader[ShuffleReader] (for the ShuffleHandle, the xref:rdd:spark-rdd-Partition.adoc[partition]). In the end, compute requests the ShuffleReader to xref:shuffle:spark-shuffle-ShuffleReader.adoc#read[read] the combined key-value pairs (of type (K, C) ). compute is part of the xref:rdd:RDD.adoc#compute[RDD] abstraction. == [[getPreferredLocations]] Placement Preferences of Partition","title":"[source, scala]"},{"location":"rdd/ShuffledRDD/#source-scala_1","text":"getPreferredLocations( partition: Partition): Seq[String] getPreferredLocations requests MapOutputTrackerMaster for the xref:scheduler:MapOutputTrackerMaster.adoc#getPreferredLocationsForShuffle[preferred locations] of the given xref:rdd:spark-rdd-Partition.adoc[partition] (xref:storage:BlockManager.adoc[BlockManagers] with the most map outputs). getPreferredLocations uses SparkEnv to access the current xref:core:SparkEnv.adoc#mapOutputTracker[MapOutputTrackerMaster]. getPreferredLocations is part of the xref:rdd:RDD.adoc#compute[RDD] abstraction. == [[getDependencies]] Dependencies","title":"[source, scala]"},{"location":"rdd/ShuffledRDD/#source-scala_2","text":"","title":"[source, scala]"},{"location":"rdd/ShuffledRDD/#getdependencies-seqdependency_","text":"getDependencies uses the < > if defined or requests the current xref:serializer:SerializerManager.adoc[SerializerManager] for xref:serializer:SerializerManager.adoc#getSerializer[one]. getDependencies uses the < > internal flag for the types of the keys and values (i.e. K and C or K and V when the flag is enabled or not, respectively). In the end, getDependencies returns a single xref:rdd:ShuffleDependency.adoc[ShuffleDependency] (with the < >, the < >, and the Serializer). getDependencies is part of the xref:rdd:RDD.adoc#getDependencies[RDD] abstraction. == [[ShuffledRDDPartition]] ShuffledRDDPartition ShuffledRDDPartition gets an index to be created (that in turn is the index of partitions as calculated by the xref:rdd:Partitioner.adoc[Partitioner] of a < >). == Demos === Demo: ShuffledRDD and coalesce","title":"getDependencies: Seq[Dependency[_]]"},{"location":"rdd/ShuffledRDD/#sourceplaintext","text":"val data = sc.parallelize(0 to 9) val coalesced = data.coalesce(numPartitions = 4, shuffle = true) scala> println(coalesced.toDebugString) (4) MapPartitionsRDD[9] at coalesce at :75 [] | CoalescedRDD[8] at coalesce at :75 [] | ShuffledRDD[7] at coalesce at :75 [] +-(16) MapPartitionsRDD[6] at coalesce at :75 [] | ParallelCollectionRDD[5] at parallelize at :74 [] === Demo: ShuffledRDD and sortByKey","title":"[source,plaintext]"},{"location":"rdd/ShuffledRDD/#sourceplaintext_1","text":"val data = sc.parallelize(0 to 9) val grouped = rdd.groupBy(_ % 2) val sorted = grouped.sortByKey(numPartitions = 2) scala> println(sorted.toDebugString) (2) ShuffledRDD[15] at sortByKey at :74 [] +-(4) ShuffledRDD[12] at groupBy at :74 [] +-(4) MapPartitionsRDD[11] at groupBy at :74 [] | MapPartitionsRDD[9] at coalesce at :75 [] | CoalescedRDD[8] at coalesce at :75 [] | ShuffledRDD[7] at coalesce at :75 [] +-(16) MapPartitionsRDD[6] at coalesce at :75 [] | ParallelCollectionRDD[5] at parallelize at :74 [] == [[internal-properties]] Internal Properties [cols=\"30m,70\",options=\"header\",width=\"100%\"] |=== | Name | Description | userSpecifiedSerializer a| [[userSpecifiedSerializer]] User-specified xref:serializer:Serializer.adoc[Serializer] for the single xref:rdd:ShuffleDependency.adoc[ShuffleDependency] dependency","title":"[source,plaintext]"},{"location":"rdd/ShuffledRDD/#source-scala_3","text":"","title":"[source, scala]"},{"location":"rdd/ShuffledRDD/#userspecifiedserializer-optionserializer-none","text":"userSpecifiedSerializer is undefined ( None ) by default and can be changed using setSerializer method (that is used for xref:rdd:PairRDDFunctions.adoc#combineByKeyWithClassTag[PairRDDFunctions.combineByKeyWithClassTag] transformation). |===","title":"userSpecifiedSerializer: Option[Serializer] = None"},{"location":"rdd/spark-rdd-CoGroupedRDD/","text":"= CoGroupedRDD A RDD that cogroups its pair RDD parents. For each key k in parent RDDs, the resulting RDD contains a tuple with the list of values for that key. Use RDD.cogroup(...) to create one. == [[getDependencies]] getDependencies Method CAUTION: FIXME == [[compute]] Computing Partition (in TaskContext) [source, scala] \u00b6 compute(s: Partition, context: TaskContext): Iterator[(K, Array[Iterable[_]])] \u00b6 compute...FIXME compute is part of xref:rdd:RDD.adoc#compute[RDD] abstraction.","title":"CoGroupedRDD"},{"location":"rdd/spark-rdd-CoGroupedRDD/#source-scala","text":"","title":"[source, scala]"},{"location":"rdd/spark-rdd-CoGroupedRDD/#computes-partition-context-taskcontext-iteratork-arrayiterable_","text":"compute...FIXME compute is part of xref:rdd:RDD.adoc#compute[RDD] abstraction.","title":"compute(s: Partition, context: TaskContext): Iterator[(K, Array[Iterable[_]])]"},{"location":"rdd/spark-rdd-Dependency/","text":"== [[Dependency]] RDD Dependencies Dependency class is the base (abstract) class to model a dependency relationship between two or more RDDs. [[rdd]] Dependency has a single method rdd to access the RDD that is behind a dependency. [source, scala] \u00b6 def rdd: RDD[T] \u00b6 Whenever you apply a link:spark-rdd-transformations.adoc[transformation] (e.g. map , flatMap ) to a RDD you build the so-called link:spark-rdd-lineage.adoc[RDD lineage graph]. Dependency -ies represent the edges in a lineage graph. NOTE: link:spark-rdd-NarrowDependency.adoc[NarrowDependency] and xref:rdd:ShuffleDependency.adoc[ShuffleDependency] are the two top-level subclasses of Dependency abstract class. .Kinds of Dependencies [cols=\"1,2\",options=\"header\",width=\"100%\"] |=== | Name | Description | link:spark-rdd-NarrowDependency.adoc[NarrowDependency] | | xref:rdd:ShuffleDependency.adoc[ShuffleDependency] | | link:spark-rdd-NarrowDependency.adoc#OneToOneDependency[OneToOneDependency] | | link:spark-rdd-NarrowDependency.adoc#PruneDependency[PruneDependency] | | link:spark-rdd-NarrowDependency.adoc#RangeDependency[RangeDependency] | |=== [NOTE] \u00b6 The dependencies of a RDD are available using xref:rdd:index.adoc#dependencies[dependencies] method. // A demo RDD scala> val myRdd = sc.parallelize(0 to 9).groupBy(_ % 2) myRdd: org.apache.spark.rdd.RDD[(Int, Iterable[Int])] = ShuffledRDD[8] at groupBy at <console>:24 scala> myRdd.foreach(println) (0,CompactBuffer(0, 2, 4, 6, 8)) (1,CompactBuffer(1, 3, 5, 7, 9)) scala> myRdd.dependencies res5: Seq[org.apache.spark.Dependency[_]] = List(org.apache.spark.ShuffleDependency@27ace619) // Access all RDDs in the demo RDD lineage scala> myRdd.dependencies.map(_.rdd).foreach(println) MapPartitionsRDD[7] at groupBy at <console>:24 You use link:spark-rdd-lineage.adoc#toDebugString[toDebugString] method to print out the RDD lineage in a user-friendly way. scala> myRdd.toDebugString res6: String = (8) ShuffledRDD[8] at groupBy at <console>:24 [] +-(8) MapPartitionsRDD[7] at groupBy at <console>:24 [] | ParallelCollectionRDD[6] at parallelize at <console>:24 [] \u00b6","title":"Dependencies"},{"location":"rdd/spark-rdd-Dependency/#source-scala","text":"","title":"[source, scala]"},{"location":"rdd/spark-rdd-Dependency/#def-rdd-rddt","text":"Whenever you apply a link:spark-rdd-transformations.adoc[transformation] (e.g. map , flatMap ) to a RDD you build the so-called link:spark-rdd-lineage.adoc[RDD lineage graph]. Dependency -ies represent the edges in a lineage graph. NOTE: link:spark-rdd-NarrowDependency.adoc[NarrowDependency] and xref:rdd:ShuffleDependency.adoc[ShuffleDependency] are the two top-level subclasses of Dependency abstract class. .Kinds of Dependencies [cols=\"1,2\",options=\"header\",width=\"100%\"] |=== | Name | Description | link:spark-rdd-NarrowDependency.adoc[NarrowDependency] | | xref:rdd:ShuffleDependency.adoc[ShuffleDependency] | | link:spark-rdd-NarrowDependency.adoc#OneToOneDependency[OneToOneDependency] | | link:spark-rdd-NarrowDependency.adoc#PruneDependency[PruneDependency] | | link:spark-rdd-NarrowDependency.adoc#RangeDependency[RangeDependency] | |===","title":"def rdd: RDD[T]"},{"location":"rdd/spark-rdd-Dependency/#note","text":"The dependencies of a RDD are available using xref:rdd:index.adoc#dependencies[dependencies] method. // A demo RDD scala> val myRdd = sc.parallelize(0 to 9).groupBy(_ % 2) myRdd: org.apache.spark.rdd.RDD[(Int, Iterable[Int])] = ShuffledRDD[8] at groupBy at <console>:24 scala> myRdd.foreach(println) (0,CompactBuffer(0, 2, 4, 6, 8)) (1,CompactBuffer(1, 3, 5, 7, 9)) scala> myRdd.dependencies res5: Seq[org.apache.spark.Dependency[_]] = List(org.apache.spark.ShuffleDependency@27ace619) // Access all RDDs in the demo RDD lineage scala> myRdd.dependencies.map(_.rdd).foreach(println) MapPartitionsRDD[7] at groupBy at <console>:24 You use link:spark-rdd-lineage.adoc#toDebugString[toDebugString] method to print out the RDD lineage in a user-friendly way.","title":"[NOTE]"},{"location":"rdd/spark-rdd-Dependency/#scala-myrddtodebugstring-res6-string-8-shuffledrdd8-at-groupby-at-console24-8-mappartitionsrdd7-at-groupby-at-console24-parallelcollectionrdd6-at-parallelize-at-console24","text":"","title":"scala&gt; myRdd.toDebugString\nres6: String =\n(8) ShuffledRDD[8] at groupBy at &lt;console&gt;:24 []\n +-(8) MapPartitionsRDD[7] at groupBy at &lt;console&gt;:24 []\n    |  ParallelCollectionRDD[6] at parallelize at &lt;console&gt;:24 []\n"},{"location":"rdd/spark-rdd-HadoopRDD/","text":"== HadoopRDD https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.rdd.HadoopRDD[HadoopRDD ] is an RDD that provides core functionality for reading data stored in HDFS, a local file system (available on all nodes), or any Hadoop-supported file system URI using the older MapReduce API ( https://hadoop.apache.org/docs/current/api/org/apache/hadoop/mapred/package-summary.html[org.apache.hadoop.mapred ]). HadoopRDD is created as a result of calling the following methods in xref:ROOT:SparkContext.adoc[]: hadoopFile textFile (the most often used in examples!) sequenceFile Partitions are of type HadoopPartition . When an HadoopRDD is computed, i.e. an action is called, you should see the INFO message Input split: in the logs. scala> sc.textFile(\"README.md\").count ... 15/10/10 18:03:21 INFO HadoopRDD: Input split: file:/Users/jacek/dev/oss/spark/README.md:0+1784 15/10/10 18:03:21 INFO HadoopRDD: Input split: file:/Users/jacek/dev/oss/spark/README.md:1784+1784 ... The following properties are set upon partition execution: mapred.tip.id - task id of this task's attempt mapred.task.id - task attempt's id mapred.task.is.map as true mapred.task.partition - split id mapred.job.id Spark settings for HadoopRDD : spark.hadoop.cloneConf (default: false ) - shouldCloneJobConf - should a Hadoop job configuration JobConf object be cloned before spawning a Hadoop job. Refer to https://issues.apache.org/jira/browse/SPARK-2546[[SPARK-2546 ] Configuration object thread safety issue]. When true , you should see a DEBUG message Cloning Hadoop Configuration . You can register callbacks on link:spark-TaskContext.adoc[TaskContext]. HadoopRDDs are not checkpointed. They do nothing when checkpoint() is called. [CAUTION] \u00b6 FIXME What are InputMetrics ? What is JobConf ? What are the InputSplits: FileSplit and CombineFileSplit ? * What are InputFormat and Configurable subtypes? What's InputFormat's RecordReader? It creates a key and a value. What are they? What's Hadoop Split? input splits for Hadoop reads? See InputFormat.getSplits \u00b6 === [[getPreferredLocations]] getPreferredLocations Method CAUTION: FIXME === [[getPartitions]] getPartitions Method The number of partition for HadoopRDD, i.e. the return value of getPartitions , is calculated using InputFormat.getSplits(jobConf, minPartitions) where minPartitions is only a hint of how many partitions one may want at minimum. As a hint it does not mean the number of partitions will be exactly the number given. For SparkContext.textFile the input format class is https://hadoop.apache.org/docs/current/api/org/apache/hadoop/mapred/TextInputFormat.html[org.apache.hadoop.mapred.TextInputFormat ]. The https://hadoop.apache.org/docs/current/api/org/apache/hadoop/mapred/FileInputFormat.html[javadoc of org.apache.hadoop.mapred.FileInputFormat] says: FileInputFormat is the base class for all file-based InputFormats. This provides a generic implementation of getSplits(JobConf, int). Subclasses of FileInputFormat can also override the isSplitable(FileSystem, Path) method to ensure input-files are not split-up and are processed as a whole by Mappers. TIP: You may find https://github.com/apache/hadoop/blob/trunk/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/FileInputFormat.java#L319[the sources of org.apache.hadoop.mapred.FileInputFormat.getSplits] enlightening.","title":"HadoopRDD"},{"location":"rdd/spark-rdd-HadoopRDD/#caution","text":"FIXME What are InputMetrics ? What is JobConf ? What are the InputSplits: FileSplit and CombineFileSplit ? * What are InputFormat and Configurable subtypes? What's InputFormat's RecordReader? It creates a key and a value. What are they?","title":"[CAUTION]"},{"location":"rdd/spark-rdd-HadoopRDD/#whats-hadoop-split-input-splits-for-hadoop-reads-see-inputformatgetsplits","text":"=== [[getPreferredLocations]] getPreferredLocations Method CAUTION: FIXME === [[getPartitions]] getPartitions Method The number of partition for HadoopRDD, i.e. the return value of getPartitions , is calculated using InputFormat.getSplits(jobConf, minPartitions) where minPartitions is only a hint of how many partitions one may want at minimum. As a hint it does not mean the number of partitions will be exactly the number given. For SparkContext.textFile the input format class is https://hadoop.apache.org/docs/current/api/org/apache/hadoop/mapred/TextInputFormat.html[org.apache.hadoop.mapred.TextInputFormat ]. The https://hadoop.apache.org/docs/current/api/org/apache/hadoop/mapred/FileInputFormat.html[javadoc of org.apache.hadoop.mapred.FileInputFormat] says: FileInputFormat is the base class for all file-based InputFormats. This provides a generic implementation of getSplits(JobConf, int). Subclasses of FileInputFormat can also override the isSplitable(FileSystem, Path) method to ensure input-files are not split-up and are processed as a whole by Mappers. TIP: You may find https://github.com/apache/hadoop/blob/trunk/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/FileInputFormat.java#L319[the sources of org.apache.hadoop.mapred.FileInputFormat.getSplits] enlightening.","title":"What's Hadoop Split? input splits for Hadoop reads? See InputFormat.getSplits"},{"location":"rdd/spark-rdd-MapPartitionsRDD/","text":"== [[MapPartitionsRDD]] MapPartitionsRDD MapPartitionsRDD is an xref:rdd:RDD.adoc[RDD] that has exactly xref:rdd:spark-rdd-NarrowDependency.adoc#OneToOneDependency[one-to-one narrow dependency] on the < > and \"describes\" a distributed computation of the given < > to every RDD partition. MapPartitionsRDD is < > when: PairRDDFunctions ( RDD[(K, V)] ) is requested to xref:rdd:PairRDDFunctions.adoc#mapValues[mapValues] and xref:rdd:PairRDDFunctions.adoc#flatMapValues[flatMapValues] (with the < > flag enabled) RDD[T] is requested to < >, < >, < >, < >, < >, < >, < >, and < > RDDBarrier[T] is requested to < > (with the < > flag enabled) By default, it does not preserve partitioning -- the last input parameter preservesPartitioning is false . If it is true , it retains the original RDD's partitioning. MapPartitionsRDD is the result of the following transformations: filter glom link:spark-rdd-transformations.adoc#mapPartitions[mapPartitions] mapPartitionsWithIndex xref:rdd:PairRDDFunctions.adoc#mapValues[PairRDDFunctions.mapValues] xref:rdd:PairRDDFunctions.adoc#flatMapValues[PairRDDFunctions.flatMapValues] [[isBarrier_]] When requested for the xref:rdd:RDD.adoc#isBarrier_[isBarrier_] flag, MapPartitionsRDD gives the < > flag or check whether any of the RDDs of the xref:rdd:RDD.adoc#dependencies[RDD dependencies] are xref:rdd:RDD.adoc#isBarrier[barrier-enabled]. === [[creating-instance]] Creating MapPartitionsRDD Instance MapPartitionsRDD takes the following to be created: [[prev]] Parent xref:rdd:RDD.adoc[RDD] ( RDD[T] ) [[f]] Function to execute on partitions + (TaskContext, partitionID, Iterator[T]) => Iterator[U] [[preservesPartitioning]] preservesPartitioning flag (default: false ) [[isFromBarrier]] isFromBarrier flag for < > (default: false ) [[isOrderSensitive]] isOrderSensitive flag (default: false )","title":"MapPartitionsRDD"},{"location":"rdd/spark-rdd-NarrowDependency/","text":"== [[NarrowDependency]] NarrowDependency -- Narrow Dependencies NarrowDependency is a base (abstract) link:spark-rdd-Dependency.adoc[Dependency] with narrow (limited) number of link:spark-rdd-Partition.adoc[partitions] of the parent RDD that are required to compute a partition of the child RDD. NOTE: Narrow dependencies allow for pipelined execution. .Concrete NarrowDependency -ies [cols=\"1,2\",options=\"header\",width=\"100%\"] |=== | Name | Description | < > | | < > | | < > | |=== === [[contract]] NarrowDependency Contract NarrowDependency contract assumes that extensions implement getParents method. [source, scala] \u00b6 def getParents(partitionId: Int): Seq[Int] \u00b6 getParents returns the partitions of the parent RDD that the input partitionId depends on. === [[OneToOneDependency]] OneToOneDependency OneToOneDependency is a narrow dependency that represents a one-to-one dependency between partitions of the parent and child RDDs. scala> val r1 = sc.parallelize(0 to 9) r1: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[13] at parallelize at <console>:18 scala> val r3 = r1.map((_, 1)) r3: org.apache.spark.rdd.RDD[(Int, Int)] = MapPartitionsRDD[19] at map at <console>:20 scala> r3.dependencies res32: Seq[org.apache.spark.Dependency[_]] = List(org.apache.spark.OneToOneDependency@7353a0fb) scala> r3.toDebugString res33: String = (8) MapPartitionsRDD[19] at map at <console>:20 [] | ParallelCollectionRDD[13] at parallelize at <console>:18 [] === [[PruneDependency]] PruneDependency PruneDependency is a narrow dependency that represents a dependency between the PartitionPruningRDD and its parent RDD. === [[RangeDependency]] RangeDependency RangeDependency is a narrow dependency that represents a one-to-one dependency between ranges of partitions in the parent and child RDDs. It is used in UnionRDD for SparkContext.union , RDD.union transformation to list only a few. scala> val r1 = sc.parallelize(0 to 9) r1: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[13] at parallelize at <console>:18 scala> val r2 = sc.parallelize(10 to 19) r2: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[14] at parallelize at <console>:18 scala> val unioned = sc.union(r1, r2) unioned: org.apache.spark.rdd.RDD[Int] = UnionRDD[16] at union at <console>:22 scala> unioned.dependencies res19: Seq[org.apache.spark.Dependency[_]] = ArrayBuffer(org.apache.spark.RangeDependency@28408ad7, org.apache.spark.RangeDependency@6e1d2e9f) scala> unioned.toDebugString res18: String = (16) UnionRDD[16] at union at <console>:22 [] | ParallelCollectionRDD[13] at parallelize at <console>:18 [] | ParallelCollectionRDD[14] at parallelize at <console>:18 []","title":"NarrowDependency"},{"location":"rdd/spark-rdd-NarrowDependency/#source-scala","text":"","title":"[source, scala]"},{"location":"rdd/spark-rdd-NarrowDependency/#def-getparentspartitionid-int-seqint","text":"getParents returns the partitions of the parent RDD that the input partitionId depends on. === [[OneToOneDependency]] OneToOneDependency OneToOneDependency is a narrow dependency that represents a one-to-one dependency between partitions of the parent and child RDDs. scala> val r1 = sc.parallelize(0 to 9) r1: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[13] at parallelize at <console>:18 scala> val r3 = r1.map((_, 1)) r3: org.apache.spark.rdd.RDD[(Int, Int)] = MapPartitionsRDD[19] at map at <console>:20 scala> r3.dependencies res32: Seq[org.apache.spark.Dependency[_]] = List(org.apache.spark.OneToOneDependency@7353a0fb) scala> r3.toDebugString res33: String = (8) MapPartitionsRDD[19] at map at <console>:20 [] | ParallelCollectionRDD[13] at parallelize at <console>:18 [] === [[PruneDependency]] PruneDependency PruneDependency is a narrow dependency that represents a dependency between the PartitionPruningRDD and its parent RDD. === [[RangeDependency]] RangeDependency RangeDependency is a narrow dependency that represents a one-to-one dependency between ranges of partitions in the parent and child RDDs. It is used in UnionRDD for SparkContext.union , RDD.union transformation to list only a few. scala> val r1 = sc.parallelize(0 to 9) r1: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[13] at parallelize at <console>:18 scala> val r2 = sc.parallelize(10 to 19) r2: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[14] at parallelize at <console>:18 scala> val unioned = sc.union(r1, r2) unioned: org.apache.spark.rdd.RDD[Int] = UnionRDD[16] at union at <console>:22 scala> unioned.dependencies res19: Seq[org.apache.spark.Dependency[_]] = ArrayBuffer(org.apache.spark.RangeDependency@28408ad7, org.apache.spark.RangeDependency@6e1d2e9f) scala> unioned.toDebugString res18: String = (16) UnionRDD[16] at union at <console>:22 [] | ParallelCollectionRDD[13] at parallelize at <console>:18 [] | ParallelCollectionRDD[14] at parallelize at <console>:18 []","title":"def getParents(partitionId: Int): Seq[Int]"},{"location":"rdd/spark-rdd-NewHadoopRDD/","text":"== [[NewHadoopRDD]] NewHadoopRDD NewHadoopRDD is an xref:rdd:index.adoc[RDD] of K keys and V values. < NewHadoopRDD is created>> when: SparkContext.newAPIHadoopFile SparkContext.newAPIHadoopRDD (indirectly) SparkContext.binaryFiles (indirectly) SparkContext.wholeTextFiles NOTE: NewHadoopRDD is the base RDD of BinaryFileRDD and WholeTextFileRDD . === [[getPreferredLocations]] getPreferredLocations Method CAUTION: FIXME === [[creating-instance]] Creating NewHadoopRDD Instance NewHadoopRDD takes the following when created: [[sc]] xref:ROOT:SparkContext.adoc[] [[inputFormatClass]] HDFS' InputFormat[K, V] [[keyClass]] K class name [[valueClass]] V class name [[_conf]] transient HDFS' Configuration NewHadoopRDD initializes the < >.","title":"NewHadoopRDD"},{"location":"rdd/spark-rdd-OrderedRDDFunctions/","text":"== [[OrderedRDDFunctions]] OrderedRDDFunctions === [[repartitionAndSortWithinPartitions]] repartitionAndSortWithinPartitions Operator CAUTION: FIXME === [[sortByKey]] sortByKey Operator CAUTION: FIXME","title":"OrderedRDDFunctions"},{"location":"rdd/spark-rdd-ParallelCollectionRDD/","text":"== ParallelCollectionRDD ParallelCollectionRDD is an RDD of a collection of elements with numSlices partitions and optional locationPrefs . ParallelCollectionRDD is the result of SparkContext.parallelize and SparkContext.makeRDD methods. The data collection is split on to numSlices slices. It uses ParallelCollectionPartition .","title":"ParallelCollectionRDD"},{"location":"rdd/spark-rdd-Partition/","text":"== [[Partition]] Partition Partition is a < > of a < > of a RDD. NOTE: A partition is missing when it has not be computed yet. [[contract]] [[index]] Partition is identified by an partition index that is a unique identifier of a partition of a RDD. [source, scala] \u00b6 index: Int \u00b6","title":"Partition"},{"location":"rdd/spark-rdd-Partition/#source-scala","text":"","title":"[source, scala]"},{"location":"rdd/spark-rdd-Partition/#index-int","text":"","title":"index: Int"},{"location":"rdd/spark-rdd-SubtractedRDD/","text":"== [[SubtractedRDD]] SubtractedRDD CAUTION: FIXME === [[compute]] Computing Partition (in TaskContext) -- compute Method [source, scala] \u00b6 compute(p: Partition, context: TaskContext): Iterator[(K, V)] \u00b6 NOTE: compute is part of xref:rdd:RDD.adoc#compute[RDD Contract] to compute a link:spark-rdd-Partition.adoc[partition] (in a link:spark-TaskContext.adoc[TaskContext]). compute ...FIXME === [[getDependencies]] getDependencies Method CAUTION: FIXME","title":"SubtractedRDD"},{"location":"rdd/spark-rdd-SubtractedRDD/#source-scala","text":"","title":"[source, scala]"},{"location":"rdd/spark-rdd-SubtractedRDD/#computep-partition-context-taskcontext-iteratork-v","text":"NOTE: compute is part of xref:rdd:RDD.adoc#compute[RDD Contract] to compute a link:spark-rdd-Partition.adoc[partition] (in a link:spark-TaskContext.adoc[TaskContext]). compute ...FIXME === [[getDependencies]] getDependencies Method CAUTION: FIXME","title":"compute(p: Partition, context: TaskContext): Iterator[(K, V)]"},{"location":"rdd/spark-rdd-actions/","text":"== Actions Actions are link:spark-rdd-operations.adoc[RDD operations] that produce non-RDD values. They materialize a value in a Spark program. In other words, a RDD operation that returns a value of any type but RDD[T] is an action. action: RDD => a value NOTE: Actions are synchronous. You can use < > to release a calling thread while calling actions. They trigger execution of < > to return values. Simply put, an action evaluates the link:spark-rdd-lineage.adoc[RDD lineage graph]. You can think of actions as a valve and until action is fired, the data to be processed is not even in the pipes, i.e. transformations. Only actions can materialize the entire processing pipeline with real data. Actions are one of two ways to send data from xref:executor:Executor.adoc[executors] to the link:spark-driver.adoc[driver] (the other being link:spark-accumulators.adoc[accumulators]). Actions in http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.rdd.RDD[org.apache.spark.rdd.RDD ]: aggregate collect count countApprox* countByValue* first fold foreach foreachPartition max min reduce link:spark-io.adoc#saving-rdds-to-files[saveAs* actions], e.g. saveAsTextFile , saveAsHadoopFile take takeOrdered takeSample toLocalIterator top treeAggregate treeReduce Actions run link:spark-scheduler-ActiveJob.adoc[jobs] using xref:ROOT:SparkContext.adoc#runJob[SparkContext.runJob] or directly xref:scheduler:DAGScheduler.adoc#runJob[DAGScheduler.runJob]. [source,scala] \u00b6 scala> words.count // <1> res0: Long = 502 <1> words is an RDD of String . TIP: You should cache RDDs you work with when you want to execute two or more actions on it for a better performance. Refer to link:spark-rdd-caching.adoc[RDD Caching and Persistence]. Before calling an action, Spark does closure/function cleaning (using SparkContext.clean ) to make it ready for serialization and sending over the wire to executors. Cleaning can throw a SparkException if the computation cannot be cleaned. NOTE: Spark uses ClosureCleaner to clean closures. === [[AsyncRDDActions]] AsyncRDDActions AsyncRDDActions class offers asynchronous actions that you can use on RDDs (thanks to the implicit conversion rddToAsyncRDDActions in RDD class). The methods return a < >. The following asynchronous methods are available: countAsync collectAsync takeAsync foreachAsync foreachPartitionAsync === [[FutureAction]] FutureActions CAUTION: FIXME","title":"Actions"},{"location":"rdd/spark-rdd-actions/#sourcescala","text":"scala> words.count // <1> res0: Long = 502 <1> words is an RDD of String . TIP: You should cache RDDs you work with when you want to execute two or more actions on it for a better performance. Refer to link:spark-rdd-caching.adoc[RDD Caching and Persistence]. Before calling an action, Spark does closure/function cleaning (using SparkContext.clean ) to make it ready for serialization and sending over the wire to executors. Cleaning can throw a SparkException if the computation cannot be cleaned. NOTE: Spark uses ClosureCleaner to clean closures. === [[AsyncRDDActions]] AsyncRDDActions AsyncRDDActions class offers asynchronous actions that you can use on RDDs (thanks to the implicit conversion rddToAsyncRDDActions in RDD class). The methods return a < >. The following asynchronous methods are available: countAsync collectAsync takeAsync foreachAsync foreachPartitionAsync === [[FutureAction]] FutureActions CAUTION: FIXME","title":"[source,scala]"},{"location":"rdd/spark-rdd-caching/","text":"== RDD Caching and Persistence Caching or persistence are optimisation techniques for (iterative and interactive) Spark computations. They help saving interim partial results so they can be reused in subsequent stages. These interim results as RDDs are thus kept in memory (default) or more solid storages like disk and/or replicated. RDDs can be cached using < > operation. They can also be persisted using < > operation. The difference between cache and persist operations is purely syntactic. cache is a synonym of persist or persist(MEMORY_ONLY) , i.e. cache is merely persist with the default storage level MEMORY_ONLY . NOTE: Due to the very small and purely syntactic difference between caching and persistence of RDDs the two terms are often used interchangeably and I will follow the \"pattern\" here. RDDs can also be < > to remove RDD from a permanent storage like memory and/or disk. === [[cache]] Caching RDD -- cache Method [source, scala] \u00b6 cache(): this.type = persist() \u00b6 cache is a synonym of < > with xref:storage:StorageLevel.adoc[ MEMORY_ONLY storage level]. === [[persist]] Persisting RDD -- persist Methods [source, scala] \u00b6 persist(): this.type persist(newLevel: StorageLevel): this.type persist marks a RDD for persistence using newLevel xref:storage:StorageLevel.adoc[storage level]. You can only change the storage level once or persist reports an UnsupportedOperationException : Cannot change storage level of an RDD after it was already assigned a level NOTE: You can pretend to change the storage level of an RDD with already-assigned storage level only if the storage level is the same as it is currently assigned. If the RDD is marked as persistent the first time, the RDD is xref:core:ContextCleaner.adoc#registerRDDForCleanup[registered to ContextCleaner ] (if available) and xref:ROOT:SparkContext.adoc#persistRDD[ SparkContext ]. The internal storageLevel attribute is set to the input newLevel storage level. === [[unpersist]] Unpersisting RDDs (Clearing Blocks) -- unpersist Method [source, scala] \u00b6 unpersist(blocking: Boolean = true): this.type \u00b6 When called, unpersist prints the following INFO message to the logs: INFO [RddName]: Removing RDD [id] from persistence list It then calls xref:ROOT:SparkContext.adoc#unpersist[SparkContext.unpersistRDD(id, blocking)] and sets xref:storage:StorageLevel.adoc[ NONE storage level] as the current storage level.","title":"Caching and Persistence"},{"location":"rdd/spark-rdd-caching/#source-scala","text":"","title":"[source, scala]"},{"location":"rdd/spark-rdd-caching/#cache-thistype-persist","text":"cache is a synonym of < > with xref:storage:StorageLevel.adoc[ MEMORY_ONLY storage level]. === [[persist]] Persisting RDD -- persist Methods","title":"cache(): this.type = persist()"},{"location":"rdd/spark-rdd-caching/#source-scala_1","text":"persist(): this.type persist(newLevel: StorageLevel): this.type persist marks a RDD for persistence using newLevel xref:storage:StorageLevel.adoc[storage level]. You can only change the storage level once or persist reports an UnsupportedOperationException : Cannot change storage level of an RDD after it was already assigned a level NOTE: You can pretend to change the storage level of an RDD with already-assigned storage level only if the storage level is the same as it is currently assigned. If the RDD is marked as persistent the first time, the RDD is xref:core:ContextCleaner.adoc#registerRDDForCleanup[registered to ContextCleaner ] (if available) and xref:ROOT:SparkContext.adoc#persistRDD[ SparkContext ]. The internal storageLevel attribute is set to the input newLevel storage level. === [[unpersist]] Unpersisting RDDs (Clearing Blocks) -- unpersist Method","title":"[source, scala]"},{"location":"rdd/spark-rdd-caching/#source-scala_2","text":"","title":"[source, scala]"},{"location":"rdd/spark-rdd-caching/#unpersistblocking-boolean-true-thistype","text":"When called, unpersist prints the following INFO message to the logs: INFO [RddName]: Removing RDD [id] from persistence list It then calls xref:ROOT:SparkContext.adoc#unpersist[SparkContext.unpersistRDD(id, blocking)] and sets xref:storage:StorageLevel.adoc[ NONE storage level] as the current storage level.","title":"unpersist(blocking: Boolean = true): this.type"},{"location":"rdd/spark-rdd-lineage/","text":"== RDD Lineage -- Logical Execution Plan RDD Lineage (aka RDD operator graph or RDD dependency graph ) is a graph of all the parent RDDs of a RDD. It is built as a result of applying transformations to the RDD and creates a < >. NOTE: The execution DAG or physical execution plan is the xref:scheduler:DAGScheduler.adoc[DAG of stages]. NOTE: The following diagram uses cartesian or zip for learning purposes only. You may use other operators to build a RDD graph. .RDD lineage image::rdd-lineage.png[align=\"center\"] The above RDD graph could be the result of the following series of transformations: val r00 = sc.parallelize(0 to 9) val r01 = sc.parallelize(0 to 90 by 10) val r10 = r00 cartesian r01 val r11 = r00.map(n => (n, n)) val r12 = r00 zip r01 val r13 = r01.keyBy(_ / 20) val r20 = Seq(r11, r12, r13).foldLeft(r10)(_ union _) A RDD lineage graph is hence a graph of what transformations need to be executed after an action has been called. You can learn about a RDD lineage graph using < > method. === [[logical-execution-plan]] Logical Execution Plan Logical Execution Plan starts with the earliest RDDs (those with no dependencies on other RDDs or reference cached data) and ends with the RDD that produces the result of the action that has been called to execute. NOTE: A logical plan, i.e. a DAG, is materialized and executed when xref:ROOT:SparkContext.adoc#runJob[ SparkContext is requested to run a Spark job]. === [[toDebugString]] Getting RDD Lineage Graph -- toDebugString Method [source, scala] \u00b6 toDebugString: String \u00b6 You can learn about a < > using toDebugString method. scala> val wordCount = sc.textFile(\"README.md\").flatMap(_.split(\"\\\\s+\")).map((_, 1)).reduceByKey(_ + _) wordCount: org.apache.spark.rdd.RDD[(String, Int)] = ShuffledRDD[21] at reduceByKey at <console>:24 scala> wordCount.toDebugString res13: String = (2) ShuffledRDD[21] at reduceByKey at <console>:24 [] +-(2) MapPartitionsRDD[20] at map at <console>:24 [] | MapPartitionsRDD[19] at flatMap at <console>:24 [] | README.md MapPartitionsRDD[18] at textFile at <console>:24 [] | README.md HadoopRDD[17] at textFile at <console>:24 [] toDebugString uses indentations to indicate a shuffle boundary. The numbers in round brackets show the level of parallelism at each stage, e.g. (2) in the above output. scala> wordCount.getNumPartitions res14: Int = 2 With < > property enabled, toDebugString is included when executing an action. $ ./bin/spark-shell --conf spark.logLineage=true scala> sc.textFile(\"README.md\", 4).count ... 15/10/17 14:46:42 INFO SparkContext: Starting job: count at <console>:25 15/10/17 14:46:42 INFO SparkContext: RDD's recursive dependencies: (4) MapPartitionsRDD[1] at textFile at <console>:25 [] | README.md HadoopRDD[0] at textFile at <console>:25 [] === [[settings]] Settings .Spark Properties [cols=\"1,1,2\",options=\"header\",width=\"100%\"] |=== | Spark Property | Default Value | Description | [[spark_logLineage]] spark.logLineage | false | When enabled (i.e. true ), executing an action (and hence xref:ROOT:SparkContext.adoc#runJob[running a job]) will also print out the RDD lineage graph using < >. |===","title":"RDD Lineage"},{"location":"rdd/spark-rdd-lineage/#source-scala","text":"","title":"[source, scala]"},{"location":"rdd/spark-rdd-lineage/#todebugstring-string","text":"You can learn about a < > using toDebugString method. scala> val wordCount = sc.textFile(\"README.md\").flatMap(_.split(\"\\\\s+\")).map((_, 1)).reduceByKey(_ + _) wordCount: org.apache.spark.rdd.RDD[(String, Int)] = ShuffledRDD[21] at reduceByKey at <console>:24 scala> wordCount.toDebugString res13: String = (2) ShuffledRDD[21] at reduceByKey at <console>:24 [] +-(2) MapPartitionsRDD[20] at map at <console>:24 [] | MapPartitionsRDD[19] at flatMap at <console>:24 [] | README.md MapPartitionsRDD[18] at textFile at <console>:24 [] | README.md HadoopRDD[17] at textFile at <console>:24 [] toDebugString uses indentations to indicate a shuffle boundary. The numbers in round brackets show the level of parallelism at each stage, e.g. (2) in the above output. scala> wordCount.getNumPartitions res14: Int = 2 With < > property enabled, toDebugString is included when executing an action. $ ./bin/spark-shell --conf spark.logLineage=true scala> sc.textFile(\"README.md\", 4).count ... 15/10/17 14:46:42 INFO SparkContext: Starting job: count at <console>:25 15/10/17 14:46:42 INFO SparkContext: RDD's recursive dependencies: (4) MapPartitionsRDD[1] at textFile at <console>:25 [] | README.md HadoopRDD[0] at textFile at <console>:25 [] === [[settings]] Settings .Spark Properties [cols=\"1,1,2\",options=\"header\",width=\"100%\"] |=== | Spark Property | Default Value | Description | [[spark_logLineage]] spark.logLineage | false | When enabled (i.e. true ), executing an action (and hence xref:ROOT:SparkContext.adoc#runJob[running a job]) will also print out the RDD lineage graph using < >. |===","title":"toDebugString: String"},{"location":"rdd/spark-rdd-operations/","text":"== Operators - Transformations and Actions RDDs have two types of operations: link:spark-rdd-transformations.adoc[transformations] and link:spark-rdd-actions.adoc[actions]. NOTE: Operators are also called operations . === Gotchas - things to watch for Even if you don't access it explicitly it cannot be referenced inside a closure as it is serialized and carried around across executors. See https://issues.apache.org/jira/browse/SPARK-5063","title":"Operators"},{"location":"rdd/spark-rdd-partitions/","text":"== Partitions and Partitioning === Introduction Depending on how you look at Spark (programmer, devop, admin), an RDD is about the content (developer's and data scientist's perspective) or how it gets spread out over a cluster (performance), i.e. how many partitions an RDD represents. A partition (aka split ) is a logical chunk of a large distributed data set. [CAUTION] \u00b6 FIXME How does the number of partitions map to the number of tasks? How to verify it? How does the mapping between partitions and tasks correspond to data locality if any? \u00b6 Spark manages data using partitions that helps parallelize distributed data processing with minimal network traffic for sending data between executors. By default, Spark tries to read data into an RDD from the nodes that are close to it. Since Spark usually accesses distributed partitioned data, to optimize transformation operations it creates partitions to hold the data chunks. There is a one-to-one correspondence between how data is laid out in data storage like HDFS or Cassandra (it is partitioned for the same reasons). Features: size number partitioning scheme node distribution repartitioning [TIP] \u00b6 Read the following documentations to learn what experts say on the topic: https://databricks.gitbooks.io/databricks-spark-knowledge-base/content/performance_optimization/how_many_partitions_does_an_rdd_have.html[How Many Partitions Does An RDD Have?] https://spark.apache.org/docs/latest/tuning.html[Tuning Spark] (the official documentation of Spark) \u00b6 By default, a partition is created for each HDFS partition, which by default is 64MB (from http://spark.apache.org/docs/latest/programming-guide.html#external-datasets[Spark's Programming Guide]). RDDs get partitioned automatically without programmer intervention. However, there are times when you'd like to adjust the size and number of partitions or the partitioning scheme according to the needs of your application. You use def getPartitions: Array[Partition] method on a RDD to know the set of partitions in this RDD. As noted in https://github.com/databricks/spark-knowledgebase/blob/master/performance_optimization/how_many_partitions_does_an_rdd_have.md#view-task-execution-against-partitions-using-the-ui[View Task Execution Against Partitions Using the UI]: When a stage executes, you can see the number of partitions for a given stage in the Spark UI. Start spark-shell and see it yourself! scala> sc.parallelize(1 to 100).count res0: Long = 100 When you execute the Spark job, i.e. sc.parallelize(1 to 100).count , you should see the following in http://localhost:4040/jobs[Spark shell application UI]. .The number of partition as Total tasks in UI image::spark-partitions-ui-stages.png[align=\"center\"] The reason for 8 Tasks in Total is that I'm on a 8-core laptop and by default the number of partitions is the number of all available cores. $ sysctl -n hw.ncpu 8 You can request for the minimum number of partitions, using the second input parameter to many transformations. scala> sc.parallelize(1 to 100, 2).count res1: Long = 100 .Total tasks in UI shows 2 partitions image::spark-partitions-ui-stages-2-partitions.png[align=\"center\"] You can always ask for the number of partitions using partitions method of a RDD: scala> val ints = sc.parallelize(1 to 100, 4) ints: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[1] at parallelize at <console>:24 scala> ints.partitions.size res2: Int = 4 In general, smaller/more numerous partitions allow work to be distributed among more workers, but larger/fewer partitions allow work to be done in larger chunks, which may result in the work getting done more quickly as long as all workers are kept busy, due to reduced overhead. Increasing partitions count will make each partition to have less data (or not at all!) Spark can only run 1 concurrent task for every partition of an RDD, up to the number of cores in your cluster. So if you have a cluster with 50 cores, you want your RDDs to at least have 50 partitions (and probably http://spark.apache.org/docs/latest/tuning.html#level-of-parallelism[2-3x times that]). As far as choosing a \"good\" number of partitions, you generally want at least as many as the number of executors for parallelism. You can get this computed value by calling sc.defaultParallelism . Also, the number of partitions determines how many files get generated by actions that save RDDs to files. The maximum size of a partition is ultimately limited by the available memory of an executor. In the first RDD transformation, e.g. reading from a file using sc.textFile(path, partition) , the partition parameter will be applied to all further transformations and actions on this RDD. Partitions get redistributed among nodes whenever shuffle occurs. Repartitioning may cause shuffle to occur in some situations, but it is not guaranteed to occur in all cases. And it usually happens during action stage. When creating an RDD by reading a file using rdd = SparkContext().textFile(\"hdfs://.../file.txt\") the number of partitions may be smaller. Ideally, you would get the same number of blocks as you see in HDFS, but if the lines in your file are too long (longer than the block size), there will be fewer partitions. Preferred way to set up the number of partitions for an RDD is to directly pass it as the second input parameter in the call like rdd = sc.textFile(\"hdfs://.../file.txt\", 400) , where 400 is the number of partitions. In this case, the partitioning makes for 400 splits that would be done by the Hadoop's TextInputFormat , not Spark and it would work much faster. It's also that the code spawns 400 concurrent tasks to try to load file.txt directly into 400 partitions. It will only work as described for uncompressed files. When using textFile with compressed files ( file.txt.gz not file.txt or similar), Spark disables splitting that makes for an RDD with only 1 partition (as reads against gzipped files cannot be parallelized). In this case, to change the number of partitions you should do < >. Some operations, e.g. map , flatMap , filter , don't preserve partitioning. map , flatMap , filter operations apply a function to every partition. === [[repartitioning]][[repartition]] Repartitioning RDD -- repartition Transformation [source, scala] \u00b6 repartition(numPartitions: Int)(implicit ord: Ordering[T] = null): RDD[T] \u00b6 repartition is < > with numPartitions and shuffle enabled. With the following computation you can see that repartition(5) causes 5 tasks to be started using NODE_LOCAL link:spark-data-locality.adoc[data locality]. scala> lines.repartition(5).count ... 15/10/07 08:10:00 INFO DAGScheduler: Submitting 5 missing tasks from ResultStage 7 (MapPartitionsRDD[19] at repartition at <console>:27) 15/10/07 08:10:00 INFO TaskSchedulerImpl: Adding task set 7.0 with 5 tasks 15/10/07 08:10:00 INFO TaskSetManager: Starting task 0.0 in stage 7.0 (TID 17, localhost, partition 0,NODE_LOCAL, 2089 bytes) 15/10/07 08:10:00 INFO TaskSetManager: Starting task 1.0 in stage 7.0 (TID 18, localhost, partition 1,NODE_LOCAL, 2089 bytes) 15/10/07 08:10:00 INFO TaskSetManager: Starting task 2.0 in stage 7.0 (TID 19, localhost, partition 2,NODE_LOCAL, 2089 bytes) 15/10/07 08:10:00 INFO TaskSetManager: Starting task 3.0 in stage 7.0 (TID 20, localhost, partition 3,NODE_LOCAL, 2089 bytes) 15/10/07 08:10:00 INFO TaskSetManager: Starting task 4.0 in stage 7.0 (TID 21, localhost, partition 4,NODE_LOCAL, 2089 bytes) ... You can see a change after executing repartition(1) causes 2 tasks to be started using PROCESS_LOCAL link:spark-data-locality.adoc[data locality]. scala> lines.repartition(1).count ... 15/10/07 08:14:09 INFO DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 8 (MapPartitionsRDD[20] at repartition at <console>:27) 15/10/07 08:14:09 INFO TaskSchedulerImpl: Adding task set 8.0 with 2 tasks 15/10/07 08:14:09 INFO TaskSetManager: Starting task 0.0 in stage 8.0 (TID 22, localhost, partition 0,PROCESS_LOCAL, 2058 bytes) 15/10/07 08:14:09 INFO TaskSetManager: Starting task 1.0 in stage 8.0 (TID 23, localhost, partition 1,PROCESS_LOCAL, 2058 bytes) ... Please note that Spark disables splitting for compressed files and creates RDDs with only 1 partition. In such cases, it's helpful to use sc.textFile('demo.gz') and do repartitioning using rdd.repartition(100) as follows: rdd = sc.textFile('demo.gz') rdd = rdd.repartition(100) With the lines, you end up with rdd to be exactly 100 partitions of roughly equal in size. rdd.repartition(N) does a shuffle to split data to match N ** partitioning is done on round robin basis TIP: If partitioning scheme doesn't work for you, you can write your own custom partitioner. TIP: It's useful to get familiar with https://hadoop.apache.org/docs/current/api/org/apache/hadoop/mapred/TextInputFormat.html[Hadoop's TextInputFormat]. === [[coalesce]] coalesce Transformation [source, scala] \u00b6 coalesce(numPartitions: Int, shuffle: Boolean = false)(implicit ord: Ordering[T] = null): RDD[T] \u00b6 The coalesce transformation is used to change the number of partitions. It can trigger link:spark-rdd-shuffle.adoc[RDD shuffling] depending on the shuffle flag (disabled by default, i.e. false ). In the following sample, you parallelize a local 10-number sequence and coalesce it first without and then with shuffling (note the shuffle parameter being false and true , respectively). TIP: Use link:spark-rdd-lineage.adoc#toDebugString[toDebugString] to check out the link:spark-rdd-lineage.adoc[RDD lineage graph]. scala> val rdd = sc.parallelize(0 to 10, 8) rdd: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[0] at parallelize at <console>:24 scala> rdd.partitions.size res0: Int = 8 scala> rdd.coalesce(numPartitions=8, shuffle=false) // <1> res1: org.apache.spark.rdd.RDD[Int] = CoalescedRDD[1] at coalesce at <console>:27 scala> res1.toDebugString res2: String = (8) CoalescedRDD[1] at coalesce at <console>:27 [] | ParallelCollectionRDD[0] at parallelize at <console>:24 [] scala> rdd.coalesce(numPartitions=8, shuffle=true) res3: org.apache.spark.rdd.RDD[Int] = MapPartitionsRDD[5] at coalesce at <console>:27 scala> res3.toDebugString res4: String = (8) MapPartitionsRDD[5] at coalesce at <console>:27 [] | CoalescedRDD[4] at coalesce at <console>:27 [] | ShuffledRDD[3] at coalesce at <console>:27 [] +-(8) MapPartitionsRDD[2] at coalesce at <console>:27 [] | ParallelCollectionRDD[0] at parallelize at <console>:24 [] <1> shuffle is false by default and it's explicitly used here for demo purposes. Note the number of partitions that remains the same as the number of partitions in the source RDD rdd .","title":"Partitions and Partitioning"},{"location":"rdd/spark-rdd-partitions/#caution","text":"FIXME How does the number of partitions map to the number of tasks? How to verify it?","title":"[CAUTION]"},{"location":"rdd/spark-rdd-partitions/#how-does-the-mapping-between-partitions-and-tasks-correspond-to-data-locality-if-any","text":"Spark manages data using partitions that helps parallelize distributed data processing with minimal network traffic for sending data between executors. By default, Spark tries to read data into an RDD from the nodes that are close to it. Since Spark usually accesses distributed partitioned data, to optimize transformation operations it creates partitions to hold the data chunks. There is a one-to-one correspondence between how data is laid out in data storage like HDFS or Cassandra (it is partitioned for the same reasons). Features: size number partitioning scheme node distribution repartitioning","title":"How does the mapping between partitions and tasks correspond to data locality if any?"},{"location":"rdd/spark-rdd-partitions/#tip","text":"Read the following documentations to learn what experts say on the topic: https://databricks.gitbooks.io/databricks-spark-knowledge-base/content/performance_optimization/how_many_partitions_does_an_rdd_have.html[How Many Partitions Does An RDD Have?]","title":"[TIP]"},{"location":"rdd/spark-rdd-partitions/#httpssparkapacheorgdocslatesttuninghtmltuning-spark-the-official-documentation-of-spark","text":"By default, a partition is created for each HDFS partition, which by default is 64MB (from http://spark.apache.org/docs/latest/programming-guide.html#external-datasets[Spark's Programming Guide]). RDDs get partitioned automatically without programmer intervention. However, there are times when you'd like to adjust the size and number of partitions or the partitioning scheme according to the needs of your application. You use def getPartitions: Array[Partition] method on a RDD to know the set of partitions in this RDD. As noted in https://github.com/databricks/spark-knowledgebase/blob/master/performance_optimization/how_many_partitions_does_an_rdd_have.md#view-task-execution-against-partitions-using-the-ui[View Task Execution Against Partitions Using the UI]: When a stage executes, you can see the number of partitions for a given stage in the Spark UI. Start spark-shell and see it yourself! scala> sc.parallelize(1 to 100).count res0: Long = 100 When you execute the Spark job, i.e. sc.parallelize(1 to 100).count , you should see the following in http://localhost:4040/jobs[Spark shell application UI]. .The number of partition as Total tasks in UI image::spark-partitions-ui-stages.png[align=\"center\"] The reason for 8 Tasks in Total is that I'm on a 8-core laptop and by default the number of partitions is the number of all available cores. $ sysctl -n hw.ncpu 8 You can request for the minimum number of partitions, using the second input parameter to many transformations. scala> sc.parallelize(1 to 100, 2).count res1: Long = 100 .Total tasks in UI shows 2 partitions image::spark-partitions-ui-stages-2-partitions.png[align=\"center\"] You can always ask for the number of partitions using partitions method of a RDD: scala> val ints = sc.parallelize(1 to 100, 4) ints: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[1] at parallelize at <console>:24 scala> ints.partitions.size res2: Int = 4 In general, smaller/more numerous partitions allow work to be distributed among more workers, but larger/fewer partitions allow work to be done in larger chunks, which may result in the work getting done more quickly as long as all workers are kept busy, due to reduced overhead. Increasing partitions count will make each partition to have less data (or not at all!) Spark can only run 1 concurrent task for every partition of an RDD, up to the number of cores in your cluster. So if you have a cluster with 50 cores, you want your RDDs to at least have 50 partitions (and probably http://spark.apache.org/docs/latest/tuning.html#level-of-parallelism[2-3x times that]). As far as choosing a \"good\" number of partitions, you generally want at least as many as the number of executors for parallelism. You can get this computed value by calling sc.defaultParallelism . Also, the number of partitions determines how many files get generated by actions that save RDDs to files. The maximum size of a partition is ultimately limited by the available memory of an executor. In the first RDD transformation, e.g. reading from a file using sc.textFile(path, partition) , the partition parameter will be applied to all further transformations and actions on this RDD. Partitions get redistributed among nodes whenever shuffle occurs. Repartitioning may cause shuffle to occur in some situations, but it is not guaranteed to occur in all cases. And it usually happens during action stage. When creating an RDD by reading a file using rdd = SparkContext().textFile(\"hdfs://.../file.txt\") the number of partitions may be smaller. Ideally, you would get the same number of blocks as you see in HDFS, but if the lines in your file are too long (longer than the block size), there will be fewer partitions. Preferred way to set up the number of partitions for an RDD is to directly pass it as the second input parameter in the call like rdd = sc.textFile(\"hdfs://.../file.txt\", 400) , where 400 is the number of partitions. In this case, the partitioning makes for 400 splits that would be done by the Hadoop's TextInputFormat , not Spark and it would work much faster. It's also that the code spawns 400 concurrent tasks to try to load file.txt directly into 400 partitions. It will only work as described for uncompressed files. When using textFile with compressed files ( file.txt.gz not file.txt or similar), Spark disables splitting that makes for an RDD with only 1 partition (as reads against gzipped files cannot be parallelized). In this case, to change the number of partitions you should do < >. Some operations, e.g. map , flatMap , filter , don't preserve partitioning. map , flatMap , filter operations apply a function to every partition. === [[repartitioning]][[repartition]] Repartitioning RDD -- repartition Transformation","title":"https://spark.apache.org/docs/latest/tuning.html[Tuning Spark] (the official documentation of Spark)"},{"location":"rdd/spark-rdd-partitions/#source-scala","text":"","title":"[source, scala]"},{"location":"rdd/spark-rdd-partitions/#repartitionnumpartitions-intimplicit-ord-orderingt-null-rddt","text":"repartition is < > with numPartitions and shuffle enabled. With the following computation you can see that repartition(5) causes 5 tasks to be started using NODE_LOCAL link:spark-data-locality.adoc[data locality]. scala> lines.repartition(5).count ... 15/10/07 08:10:00 INFO DAGScheduler: Submitting 5 missing tasks from ResultStage 7 (MapPartitionsRDD[19] at repartition at <console>:27) 15/10/07 08:10:00 INFO TaskSchedulerImpl: Adding task set 7.0 with 5 tasks 15/10/07 08:10:00 INFO TaskSetManager: Starting task 0.0 in stage 7.0 (TID 17, localhost, partition 0,NODE_LOCAL, 2089 bytes) 15/10/07 08:10:00 INFO TaskSetManager: Starting task 1.0 in stage 7.0 (TID 18, localhost, partition 1,NODE_LOCAL, 2089 bytes) 15/10/07 08:10:00 INFO TaskSetManager: Starting task 2.0 in stage 7.0 (TID 19, localhost, partition 2,NODE_LOCAL, 2089 bytes) 15/10/07 08:10:00 INFO TaskSetManager: Starting task 3.0 in stage 7.0 (TID 20, localhost, partition 3,NODE_LOCAL, 2089 bytes) 15/10/07 08:10:00 INFO TaskSetManager: Starting task 4.0 in stage 7.0 (TID 21, localhost, partition 4,NODE_LOCAL, 2089 bytes) ... You can see a change after executing repartition(1) causes 2 tasks to be started using PROCESS_LOCAL link:spark-data-locality.adoc[data locality]. scala> lines.repartition(1).count ... 15/10/07 08:14:09 INFO DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 8 (MapPartitionsRDD[20] at repartition at <console>:27) 15/10/07 08:14:09 INFO TaskSchedulerImpl: Adding task set 8.0 with 2 tasks 15/10/07 08:14:09 INFO TaskSetManager: Starting task 0.0 in stage 8.0 (TID 22, localhost, partition 0,PROCESS_LOCAL, 2058 bytes) 15/10/07 08:14:09 INFO TaskSetManager: Starting task 1.0 in stage 8.0 (TID 23, localhost, partition 1,PROCESS_LOCAL, 2058 bytes) ... Please note that Spark disables splitting for compressed files and creates RDDs with only 1 partition. In such cases, it's helpful to use sc.textFile('demo.gz') and do repartitioning using rdd.repartition(100) as follows: rdd = sc.textFile('demo.gz') rdd = rdd.repartition(100) With the lines, you end up with rdd to be exactly 100 partitions of roughly equal in size. rdd.repartition(N) does a shuffle to split data to match N ** partitioning is done on round robin basis TIP: If partitioning scheme doesn't work for you, you can write your own custom partitioner. TIP: It's useful to get familiar with https://hadoop.apache.org/docs/current/api/org/apache/hadoop/mapred/TextInputFormat.html[Hadoop's TextInputFormat]. === [[coalesce]] coalesce Transformation","title":"repartition(numPartitions: Int)(implicit ord: Ordering[T] = null): RDD[T]"},{"location":"rdd/spark-rdd-partitions/#source-scala_1","text":"","title":"[source, scala]"},{"location":"rdd/spark-rdd-partitions/#coalescenumpartitions-int-shuffle-boolean-falseimplicit-ord-orderingt-null-rddt","text":"The coalesce transformation is used to change the number of partitions. It can trigger link:spark-rdd-shuffle.adoc[RDD shuffling] depending on the shuffle flag (disabled by default, i.e. false ). In the following sample, you parallelize a local 10-number sequence and coalesce it first without and then with shuffling (note the shuffle parameter being false and true , respectively). TIP: Use link:spark-rdd-lineage.adoc#toDebugString[toDebugString] to check out the link:spark-rdd-lineage.adoc[RDD lineage graph]. scala> val rdd = sc.parallelize(0 to 10, 8) rdd: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[0] at parallelize at <console>:24 scala> rdd.partitions.size res0: Int = 8 scala> rdd.coalesce(numPartitions=8, shuffle=false) // <1> res1: org.apache.spark.rdd.RDD[Int] = CoalescedRDD[1] at coalesce at <console>:27 scala> res1.toDebugString res2: String = (8) CoalescedRDD[1] at coalesce at <console>:27 [] | ParallelCollectionRDD[0] at parallelize at <console>:24 [] scala> rdd.coalesce(numPartitions=8, shuffle=true) res3: org.apache.spark.rdd.RDD[Int] = MapPartitionsRDD[5] at coalesce at <console>:27 scala> res3.toDebugString res4: String = (8) MapPartitionsRDD[5] at coalesce at <console>:27 [] | CoalescedRDD[4] at coalesce at <console>:27 [] | ShuffledRDD[3] at coalesce at <console>:27 [] +-(8) MapPartitionsRDD[2] at coalesce at <console>:27 [] | ParallelCollectionRDD[0] at parallelize at <console>:24 [] <1> shuffle is false by default and it's explicitly used here for demo purposes. Note the number of partitions that remains the same as the number of partitions in the source RDD rdd .","title":"coalesce(numPartitions: Int, shuffle: Boolean = false)(implicit ord: Ordering[T] = null): RDD[T]"},{"location":"rdd/spark-rdd-shuffle/","text":"= RDD shuffling :url-spark-docs: https://spark.apache.org/docs/{spark-version } TIP: Read the official documentation about the topic {url-spark-docs}/rdd-programming-guide.html#shuffle-operations[Shuffle operations]. It is still better than this page. Shuffling is a process of link:spark-rdd-partitions.adoc[redistributing data across partitions] (aka repartitioning ) that may or may not cause moving data across JVM processes or even over the wire (between executors on separate machines). Shuffling is the process of data transfer between stages. TIP: Avoid shuffling at all cost. Think about ways to leverage existing partitions. Leverage partial aggregation to reduce data transfer. By default, shuffling doesn't change the number of partitions, but their content. Avoid groupByKey and use reduceByKey or combineByKey instead. ** groupByKey shuffles all the data, which is slow. ** reduceByKey shuffles only the results of sub-aggregations in each partition of the data. == Example - join PairRDD offers http://people.apache.org/~pwendell/spark-nightly/spark-master-docs/latest/programming-guide.html#JoinLink[join ] transformation that (quoting the official documentation): When called on datasets of type (K, V) and (K, W), returns a dataset of (K, (V, W)) pairs with all pairs of elements for each key. Let's have a look at an example and see how it works under the covers: scala> val kv = (0 to 5) zip Stream.continually(5) kv: scala.collection.immutable.IndexedSeq[(Int, Int)] = Vector((0,5), (1,5), (2,5), (3,5), (4,5), (5,5)) scala> val kw = (0 to 5) zip Stream.continually(10) kw: scala.collection.immutable.IndexedSeq[(Int, Int)] = Vector((0,10), (1,10), (2,10), (3,10), (4,10), (5,10)) scala> val kvR = sc.parallelize(kv) kvR: org.apache.spark.rdd.RDD[(Int, Int)] = ParallelCollectionRDD[3] at parallelize at <console>:26 scala> val kwR = sc.parallelize(kw) kwR: org.apache.spark.rdd.RDD[(Int, Int)] = ParallelCollectionRDD[4] at parallelize at <console>:26 scala> val joined = kvR join kwR joined: org.apache.spark.rdd.RDD[(Int, (Int, Int))] = MapPartitionsRDD[10] at join at <console>:32 scala> joined.toDebugString res7: String = (8) MapPartitionsRDD[10] at join at <console>:32 [] | MapPartitionsRDD[9] at join at <console>:32 [] | CoGroupedRDD[8] at join at <console>:32 [] +-(8) ParallelCollectionRDD[3] at parallelize at <console>:26 [] +-(8) ParallelCollectionRDD[4] at parallelize at <console>:26 [] It doesn't look good when there is an \"angle\" between \"nodes\" in an operation graph. It appears before the join operation so shuffle is expected. Here is how the job of executing joined.count looks in Web UI. .Executing joined.count image::spark-shuffle-join-webui.png[align=\"center\"] The screenshot of Web UI shows 3 stages with two parallelize to Shuffle Write and count to Shuffle Read. It means shuffling has indeed happened. CAUTION: FIXME Just learnt about sc.range(0, 5) as a shorter version of sc.parallelize(0 to 5) join operation is one of the cogroup operations that uses defaultPartitioner , i.e. walks through link:spark-rdd-lineage.adoc[the RDD lineage graph] (sorted by the number of partitions decreasing) and picks the partitioner with positive number of output partitions. Otherwise, it checks xref:ROOT:configuration-properties.adoc#spark.default.parallelism[spark.default.parallelism] configuration and if defined picks xref:rdd:HashPartitioner.adoc[HashPartitioner] with the default parallelism of the xref:scheduler:SchedulerBackend.adoc[SchedulerBackend]. join is almost CoGroupedRDD.mapValues . CAUTION: FIXME the default parallelism of scheduler backend","title":"Shuffling"},{"location":"rdd/spark-rdd-transformations/","text":"== Transformations -- Lazy Operations on RDD (to Create One or More RDDs) Transformations are lazy operations on an xref:rdd:RDD.adoc[RDD] that create one or many new RDDs. // T and U are Scala types transformation: RDD[T] => RDD[U] transformation: RDD[T] => Seq[RDD[U]] In other words, transformations are functions that take an RDD as the input and produce one or many RDDs as the output. Transformations do not change the input RDD (since xref:rdd:index.adoc#introduction[RDDs are immutable] and hence cannot be modified), but produce one or more new RDDs by applying the computations they represent. [[methods]] .(Subset of) RDD Transformations (Public API) [cols=\"1m,3\",options=\"header\",width=\"100%\"] |=== | Method | Description | aggregate a| [[aggregate]] [source, scala] \u00b6 aggregate U ( seqOp: (U, T) => U, combOp: (U, U) => U): U | barrier a| [[barrier]] [source, scala] \u00b6 barrier(): RDDBarrier[T] \u00b6 ( New in 2.4.0 ) Marks the current stage as a < > in < >, where Spark must launch all tasks together Internally, barrier creates a < > over the RDD | cache a| [[cache]] [source, scala] \u00b6 cache(): this.type \u00b6 Persists the RDD with the xref:storage:StorageLevel.adoc#MEMORY_ONLY[MEMORY_ONLY] storage level Synonym of < > | coalesce a| [[coalesce]] [source, scala] \u00b6 coalesce( numPartitions: Int, shuffle: Boolean = false, partitionCoalescer: Option[PartitionCoalescer] = Option.empty) (implicit ord: Ordering[T] = null): RDD[T] | filter a| [[filter]] [source, scala] \u00b6 filter(f: T => Boolean): RDD[T] \u00b6 | flatMap a| [[flatMap]] [source, scala] \u00b6 flatMap U : RDD[U] \u00b6 | map a| [[map]] [source, scala] \u00b6 map U : RDD[U] \u00b6 | mapPartitions a| [[mapPartitions]] [source, scala] \u00b6 mapPartitions U : RDD[U] | mapPartitionsWithIndex a| [[mapPartitionsWithIndex]] [source, scala] \u00b6 mapPartitionsWithIndex U : RDD[U] | randomSplit a| [[randomSplit]] [source, scala] \u00b6 randomSplit( weights: Array[Double], seed: Long = Utils.random.nextLong): Array[RDD[T]] | union a| [[union]] [source, scala] \u00b6 ++(other: RDD[T]): RDD[T] union(other: RDD[T]): RDD[T] | persist a| [[persist]] [source, scala] \u00b6 persist(): this.type persist(newLevel: StorageLevel): this.type |=== By applying transformations you incrementally build a link:spark-rdd-lineage.adoc[RDD lineage] with all the parent RDDs of the final RDD(s). Transformations are lazy, i.e. are not executed immediately. Only after calling an action are transformations executed. After executing a transformation, the result RDD(s) will always be different from their parents and can be smaller (e.g. filter , count , distinct , sample ), bigger (e.g. flatMap , union , cartesian ) or the same size (e.g. map ). CAUTION: There are transformations that may trigger jobs, e.g. sortBy , < >, etc. .From SparkContext by transformations to the result image::rdd-sparkcontext-transformations-action.png[align=\"center\"] Certain transformations can be pipelined which is an optimization that Spark uses to improve performance of computations. [source,scala] \u00b6 scala> val file = sc.textFile(\"README.md\") file: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[54] at textFile at :24 scala> val allWords = file.flatMap(_.split(\"\\W+\")) allWords: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[55] at flatMap at :26 scala> val words = allWords.filter(!_.isEmpty) words: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[56] at filter at :28 scala> val pairs = words.map((_,1)) pairs: org.apache.spark.rdd.RDD[(String, Int)] = MapPartitionsRDD[57] at map at :30 scala> val reducedByKey = pairs.reduceByKey(_ + _) reducedByKey: org.apache.spark.rdd.RDD[(String, Int)] = ShuffledRDD[59] at reduceByKey at :32 scala> val top10words = reducedByKey.takeOrdered(10)(Ordering[Int].reverse.on(_._2)) INFO SparkContext: Starting job: takeOrdered at :34 ... INFO DAGScheduler: Job 18 finished: takeOrdered at :34, took 0.074386 s top10words: Array[(String, Int)] = Array((the,21), (to,14), (Spark,13), (for,11), (and,10), (##,8), (a,8), (run,7), (can,6), (is,6)) There are two kinds of transformations: < > < > === [[narrow-transformations]] Narrow Transformations Narrow transformations are the result of map , filter and such that is from the data from a single partition only, i.e. it is self-sustained. An output RDD has partitions with records that originate from a single partition in the parent RDD. Only a limited subset of partitions used to calculate the result. Spark groups narrow transformations as a stage which is called pipelining . === [[wide-transformations]] Wide Transformations Wide transformations are the result of groupByKey and reduceByKey . The data required to compute the records in a single partition may reside in many partitions of the parent RDD. NOTE: Wide transformations are also called shuffle transformations as they may or may not depend on a shuffle. All of the tuples with the same key must end up in the same partition, processed by the same task. To satisfy these operations, Spark must execute link:spark-rdd-shuffle.adoc[RDD shuffle], which transfers data across cluster and results in a new stage with a new set of partitions. === [[zipWithIndex]] zipWithIndex [source, scala] \u00b6 zipWithIndex(): RDD[(T, Long)] \u00b6 zipWithIndex zips this RDD[T] with its element indices. [CAUTION] \u00b6 If the number of partitions of the source RDD is greater than 1, it will submit an additional job to calculate start indices. [source, scala] \u00b6 val onePartition = sc.parallelize(0 to 9, 1) scala> onePartition.partitions.length res0: Int = 1 // no job submitted onePartition.zipWithIndex val eightPartitions = sc.parallelize(0 to 9, 8) scala> eightPartitions.partitions.length res1: Int = 8 // submits a job eightPartitions.zipWithIndex .Spark job submitted by zipWithIndex transformation image::spark-transformations-zipWithIndex-webui.png[align=\"center\"] ====","title":"Transformations"},{"location":"rdd/spark-rdd-transformations/#source-scala","text":"aggregate U ( seqOp: (U, T) => U, combOp: (U, U) => U): U | barrier a| [[barrier]]","title":"[source, scala]"},{"location":"rdd/spark-rdd-transformations/#source-scala_1","text":"","title":"[source, scala]"},{"location":"rdd/spark-rdd-transformations/#barrier-rddbarriert","text":"( New in 2.4.0 ) Marks the current stage as a < > in < >, where Spark must launch all tasks together Internally, barrier creates a < > over the RDD | cache a| [[cache]]","title":"barrier(): RDDBarrier[T]"},{"location":"rdd/spark-rdd-transformations/#source-scala_2","text":"","title":"[source, scala]"},{"location":"rdd/spark-rdd-transformations/#cache-thistype","text":"Persists the RDD with the xref:storage:StorageLevel.adoc#MEMORY_ONLY[MEMORY_ONLY] storage level Synonym of < > | coalesce a| [[coalesce]]","title":"cache(): this.type"},{"location":"rdd/spark-rdd-transformations/#source-scala_3","text":"coalesce( numPartitions: Int, shuffle: Boolean = false, partitionCoalescer: Option[PartitionCoalescer] = Option.empty) (implicit ord: Ordering[T] = null): RDD[T] | filter a| [[filter]]","title":"[source, scala]"},{"location":"rdd/spark-rdd-transformations/#source-scala_4","text":"","title":"[source, scala]"},{"location":"rdd/spark-rdd-transformations/#filterf-t-boolean-rddt","text":"| flatMap a| [[flatMap]]","title":"filter(f: T =&gt; Boolean): RDD[T]"},{"location":"rdd/spark-rdd-transformations/#source-scala_5","text":"","title":"[source, scala]"},{"location":"rdd/spark-rdd-transformations/#flatmapu-rddu","text":"| map a| [[map]]","title":"flatMapU: RDD[U]"},{"location":"rdd/spark-rdd-transformations/#source-scala_6","text":"","title":"[source, scala]"},{"location":"rdd/spark-rdd-transformations/#mapu-rddu","text":"| mapPartitions a| [[mapPartitions]]","title":"mapU: RDD[U]"},{"location":"rdd/spark-rdd-transformations/#source-scala_7","text":"mapPartitions U : RDD[U] | mapPartitionsWithIndex a| [[mapPartitionsWithIndex]]","title":"[source, scala]"},{"location":"rdd/spark-rdd-transformations/#source-scala_8","text":"mapPartitionsWithIndex U : RDD[U] | randomSplit a| [[randomSplit]]","title":"[source, scala]"},{"location":"rdd/spark-rdd-transformations/#source-scala_9","text":"randomSplit( weights: Array[Double], seed: Long = Utils.random.nextLong): Array[RDD[T]] | union a| [[union]]","title":"[source, scala]"},{"location":"rdd/spark-rdd-transformations/#source-scala_10","text":"++(other: RDD[T]): RDD[T] union(other: RDD[T]): RDD[T] | persist a| [[persist]]","title":"[source, scala]"},{"location":"rdd/spark-rdd-transformations/#source-scala_11","text":"persist(): this.type persist(newLevel: StorageLevel): this.type |=== By applying transformations you incrementally build a link:spark-rdd-lineage.adoc[RDD lineage] with all the parent RDDs of the final RDD(s). Transformations are lazy, i.e. are not executed immediately. Only after calling an action are transformations executed. After executing a transformation, the result RDD(s) will always be different from their parents and can be smaller (e.g. filter , count , distinct , sample ), bigger (e.g. flatMap , union , cartesian ) or the same size (e.g. map ). CAUTION: There are transformations that may trigger jobs, e.g. sortBy , < >, etc. .From SparkContext by transformations to the result image::rdd-sparkcontext-transformations-action.png[align=\"center\"] Certain transformations can be pipelined which is an optimization that Spark uses to improve performance of computations.","title":"[source, scala]"},{"location":"rdd/spark-rdd-transformations/#sourcescala","text":"scala> val file = sc.textFile(\"README.md\") file: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[54] at textFile at :24 scala> val allWords = file.flatMap(_.split(\"\\W+\")) allWords: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[55] at flatMap at :26 scala> val words = allWords.filter(!_.isEmpty) words: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[56] at filter at :28 scala> val pairs = words.map((_,1)) pairs: org.apache.spark.rdd.RDD[(String, Int)] = MapPartitionsRDD[57] at map at :30 scala> val reducedByKey = pairs.reduceByKey(_ + _) reducedByKey: org.apache.spark.rdd.RDD[(String, Int)] = ShuffledRDD[59] at reduceByKey at :32 scala> val top10words = reducedByKey.takeOrdered(10)(Ordering[Int].reverse.on(_._2)) INFO SparkContext: Starting job: takeOrdered at :34 ... INFO DAGScheduler: Job 18 finished: takeOrdered at :34, took 0.074386 s top10words: Array[(String, Int)] = Array((the,21), (to,14), (Spark,13), (for,11), (and,10), (##,8), (a,8), (run,7), (can,6), (is,6)) There are two kinds of transformations: < > < > === [[narrow-transformations]] Narrow Transformations Narrow transformations are the result of map , filter and such that is from the data from a single partition only, i.e. it is self-sustained. An output RDD has partitions with records that originate from a single partition in the parent RDD. Only a limited subset of partitions used to calculate the result. Spark groups narrow transformations as a stage which is called pipelining . === [[wide-transformations]] Wide Transformations Wide transformations are the result of groupByKey and reduceByKey . The data required to compute the records in a single partition may reside in many partitions of the parent RDD. NOTE: Wide transformations are also called shuffle transformations as they may or may not depend on a shuffle. All of the tuples with the same key must end up in the same partition, processed by the same task. To satisfy these operations, Spark must execute link:spark-rdd-shuffle.adoc[RDD shuffle], which transfers data across cluster and results in a new stage with a new set of partitions. === [[zipWithIndex]] zipWithIndex","title":"[source,scala]"},{"location":"rdd/spark-rdd-transformations/#source-scala_12","text":"","title":"[source, scala]"},{"location":"rdd/spark-rdd-transformations/#zipwithindex-rddt-long","text":"zipWithIndex zips this RDD[T] with its element indices.","title":"zipWithIndex(): RDD[(T, Long)]"},{"location":"rdd/spark-rdd-transformations/#caution","text":"If the number of partitions of the source RDD is greater than 1, it will submit an additional job to calculate start indices.","title":"[CAUTION]"},{"location":"rdd/spark-rdd-transformations/#source-scala_13","text":"val onePartition = sc.parallelize(0 to 9, 1) scala> onePartition.partitions.length res0: Int = 1 // no job submitted onePartition.zipWithIndex val eightPartitions = sc.parallelize(0 to 9, 8) scala> eightPartitions.partitions.length res1: Int = 8 // submits a job eightPartitions.zipWithIndex .Spark job submitted by zipWithIndex transformation image::spark-transformations-zipWithIndex-webui.png[align=\"center\"] ====","title":"[source, scala]"},{"location":"scheduler/DAGScheduler/","text":"= [[DAGScheduler]] DAGScheduler [NOTE] \u00b6 The introduction that follows was highly influenced by the scaladoc of https://github.com/apache/spark/blob/master/core/src/main/scala/org/apache/spark/scheduler/DAGScheduler.scala[org.apache.spark.scheduler.DAGScheduler ]. As DAGScheduler is a private class it does not appear in the official API documentation. You are strongly encouraged to read https://github.com/apache/spark/blob/master/core/src/main/scala/org/apache/spark/scheduler/DAGScheduler.scala[the sources] and only then read this and the related pages afterwards. \u00b6 == [[introduction]] Introduction DAGScheduler is the scheduling layer of Apache Spark that implements stage-oriented scheduling . DAGScheduler transforms a logical execution plan (i.e. xref:rdd:spark-rdd-lineage.adoc[RDD lineage] of dependencies built using xref:rdd:spark-rdd-transformations.adoc[RDD transformations]) to a physical execution plan (using xref:scheduler:Stage.adoc[stages]). .DAGScheduler Transforming RDD Lineage Into Stage DAG image::dagscheduler-rdd-lineage-stage-dag.png[align=\"center\"] After an xref:rdd:spark-rdd-actions.adoc[action] has been called, xref:ROOT:SparkContext.adoc[SparkContext] hands over a logical plan to DAGScheduler that it in turn translates to a set of stages that are submitted as xref:scheduler:TaskSet.adoc[TaskSets] for execution. .Executing action leads to new ResultStage and ActiveJob in DAGScheduler image::dagscheduler-rdd-partitions-job-resultstage.png[align=\"center\"] The fundamental concepts of DAGScheduler are jobs and stages (refer to xref:scheduler:spark-scheduler-ActiveJob.adoc[Jobs] and xref:scheduler:Stage.adoc[Stages] respectively) that it tracks through < >. DAGScheduler works solely on the driver and is created as part of xref:ROOT:SparkContext.adoc#creating-instance[SparkContext's initialization] (right after xref:scheduler:TaskScheduler.adoc[TaskScheduler] and xref:scheduler:SchedulerBackend.adoc[SchedulerBackend] are ready). .DAGScheduler as created by SparkContext with other services image::dagscheduler-new-instance.png[align=\"center\"] DAGScheduler does three things in Spark (thorough explanations follow): Computes an execution DAG , i.e. DAG of stages, for a job. Determines the < > to run each task on. Handles failures due to shuffle output files being lost. DAGScheduler computes https://en.wikipedia.org/wiki/Directed_acyclic_graph[a directed acyclic graph (DAG)] of stages for each job, keeps track of which RDDs and stage outputs are materialized, and finds a minimal schedule to run jobs. It then submits stages to xref:scheduler:TaskScheduler.adoc[TaskScheduler]. .DAGScheduler.submitJob image::dagscheduler-submitjob.png[align=\"center\"] In addition to coming up with the execution DAG, DAGScheduler also determines the preferred locations to run each task on, based on the current cache status, and passes the information to xref:scheduler:TaskScheduler.adoc[TaskScheduler]. DAGScheduler tracks which xref:rdd:spark-rdd-caching.adoc[RDDs are cached (or persisted)] to avoid \"recomputing\" them, i.e. redoing the map side of a shuffle. DAGScheduler remembers what xref:scheduler:ShuffleMapStage.adoc[ShuffleMapStage]s have already produced output files (that are stored in xref:storage:BlockManager.adoc[BlockManager]s). DAGScheduler is only interested in cache location coordinates, i.e. host and executor id, per partition of a RDD. Furthermore, it handles failures due to shuffle output files being lost, in which case old stages may need to be resubmitted. Failures within a stage that are not caused by shuffle file loss are handled by the TaskScheduler itself, which will retry each task a small number of times before cancelling the whole stage. DAGScheduler uses an event queue architecture in which a thread can post DAGSchedulerEvent events, e.g. a new job or stage being submitted, that DAGScheduler reads and executes sequentially. See the section < >. DAGScheduler runs stages in topological order. DAGScheduler uses xref:ROOT:SparkContext.adoc[SparkContext], xref:scheduler:TaskScheduler.adoc[TaskScheduler], xref:scheduler:LiveListenerBus.adoc[], xref:scheduler:MapOutputTracker.adoc[MapOutputTracker] and xref:storage:BlockManager.adoc[BlockManager] for its services. However, at the very minimum, DAGScheduler takes a SparkContext only (and requests SparkContext for the other services). When DAGScheduler schedules a job as a result of xref:rdd:index.adoc#actions[executing an action on a RDD] or xref:ROOT:SparkContext.adoc#runJob[calling SparkContext.runJob() method directly], it spawns parallel tasks to compute (partial) results per partition. == [[creating-instance]][[initialization]] Creating Instance DAGScheduler takes the following to be created: [[sc]] xref:ROOT:SparkContext.adoc[] < > [[listenerBus]] xref:scheduler:LiveListenerBus.adoc[] [[mapOutputTracker]] xref:scheduler:MapOutputTrackerMaster.adoc[MapOutputTrackerMaster] [[blockManagerMaster]] xref:storage:BlockManagerMaster.adoc[BlockManagerMaster] [[env]] xref:core:SparkEnv.adoc[] [[clock]] Clock (default: SystemClock) While being created, DAGScheduler xref:scheduler:TaskScheduler.adoc#setDAGScheduler[associates itself] with the < > and starts < >. == [[event-loop]][[eventProcessLoop]] DAGScheduler Event Bus DAGScheduler uses an xref:scheduler:DAGSchedulerEventProcessLoop.adoc[event bus] to process scheduling-related events on a separate thread (one by one and asynchronously). DAGScheduler starts the event bus when created and stops it when requested to < >. DAGScheduler defines < > that allow posting DAGSchedulerEvent events to the event bus. [[event-posting-methods]] .DAGScheduler Event Posting Methods [cols=\"20m,20m,60\",options=\"header\",width=\"100%\"] |=== | Method | Event Posted | Trigger | [[cancelAllJobs]] cancelAllJobs | xref:scheduler:DAGSchedulerEvent.adoc#AllJobsCancelled[AllJobsCancelled] | SparkContext is requested to xref:ROOT:SparkContext.adoc#cancelAllJobs[cancel all running or scheduled Spark jobs] | [[cancelJob]] cancelJob | xref:scheduler:DAGSchedulerEvent.adoc#JobCancelled[JobCancelled] | xref:ROOT:SparkContext.adoc#cancelJob[SparkContext] or xref:scheduler:spark-scheduler-JobWaiter.adoc[JobWaiter] are requested to cancel a Spark job | [[cancelJobGroup]] cancelJobGroup | xref:scheduler:DAGSchedulerEvent.adoc#JobGroupCancelled[JobGroupCancelled] | SparkContext is requested to xref:ROOT:SparkContext.adoc#cancelJobGroup[cancel a job group] | [[cancelStage]] cancelStage | xref:scheduler:DAGSchedulerEvent.adoc#StageCancelled[StageCancelled] | SparkContext is requested to xref:ROOT:SparkContext.adoc#cancelStage[cancel a stage] | [[executorAdded]] executorAdded | xref:scheduler:DAGSchedulerEvent.adoc#ExecutorAdded[ExecutorAdded] | TaskSchedulerImpl is requested to xref:scheduler:TaskSchedulerImpl.adoc#resourceOffers[handle resource offers] (and a new executor is found in the resource offers) | [[executorLost]] executorLost | xref:scheduler:DAGSchedulerEvent.adoc#ExecutorLost[ExecutorLost] | TaskSchedulerImpl is requested to xref:scheduler:TaskSchedulerImpl.adoc#statusUpdate[handle a task status update] (and a task gets lost which is used to indicate that the executor got broken and hence should be considered lost) or xref:scheduler:TaskSchedulerImpl.adoc#executorLost[executorLost] | [[runApproximateJob]] runApproximateJob | xref:scheduler:DAGSchedulerEvent.adoc#JobSubmitted[JobSubmitted] | SparkContext is requested to xref:ROOT:SparkContext.adoc#runApproximateJob[run an approximate job] | [[speculativeTaskSubmitted]] speculativeTaskSubmitted | xref:scheduler:DAGSchedulerEvent.adoc#SpeculativeTaskSubmitted[SpeculativeTaskSubmitted] | | [[submitJob]] submitJob | xref:scheduler:DAGSchedulerEvent.adoc#JobSubmitted[JobSubmitted] a| SparkContext is requested to xref:ROOT:SparkContext.adoc#submitJob[submits a job] DAGScheduler is requested to < > | [[submitMapStage]] submitMapStage | xref:scheduler:DAGSchedulerEvent.adoc#MapStageSubmitted[MapStageSubmitted] | SparkContext is requested to xref:ROOT:SparkContext.adoc#submitMapStage[submit a MapStage for execution]. | [[taskEnded]] taskEnded | xref:scheduler:DAGSchedulerEvent.adoc#CompletionEvent[CompletionEvent] | TaskSetManager is requested to xref:scheduler:TaskSetManager.adoc#handleSuccessfulTask[handleSuccessfulTask], xref:scheduler:TaskSetManager.adoc#handleFailedTask[handleFailedTask], and xref:scheduler:TaskSetManager.adoc#executorLost[executorLost] | [[taskGettingResult]] taskGettingResult | xref:scheduler:DAGSchedulerEvent.adoc#GettingResultEvent[GettingResultEvent] | TaskSetManager is requested to xref:scheduler:TaskSetManager.adoc#handleTaskGettingResult[handle a task fetching result] | [[taskSetFailed]] taskSetFailed | xref:scheduler:DAGSchedulerEvent.adoc#TaskSetFailed[TaskSetFailed] | TaskSetManager is requested to xref:scheduler:TaskSetManager.adoc#abort[abort] | [[taskStarted]] taskStarted | xref:scheduler:DAGSchedulerEvent.adoc#BeginEvent[BeginEvent] | TaskSetManager is requested to xref:scheduler:TaskSetManager.adoc#resourceOffer[start a task] | [[workerRemoved]] workerRemoved | xref:scheduler:DAGSchedulerEvent.adoc#WorkerRemoved[WorkerRemoved] | TaskSchedulerImpl is requested to xref:scheduler:TaskSchedulerImpl.adoc#workerRemoved[handle a removed worker event] |=== == [[taskScheduler]] DAGScheduler and TaskScheduler DAGScheduler is given a xref:scheduler:TaskScheduler.adoc[TaskScheduler] when < >. DAGScheduler uses the TaskScheduler for the following: < > < > < > < > < > == [[runJob]] Running Job [source, scala] \u00b6 runJob T, U : Unit runJob submits an action job to the DAGScheduler and waits for a result. Internally, runJob executes < > and then waits until a result comes using xref:scheduler:spark-scheduler-JobWaiter.adoc[JobWaiter]. When the job succeeds, you should see the following INFO message in the logs: Job [jobId] finished: [callSite], took [time] s When the job fails, you should see the following INFO message in the logs and the exception (that led to the failure) is thrown. Job [jobId] failed: [callSite], took [time] s runJob is used when SparkContext is requested to xref:ROOT:SparkContext.adoc#runJob[run a job]. == [[cacheLocs]][[clearCacheLocs]] Partition Placement Preferences DAGScheduler keeps track of block locations per RDD and partition. DAGScheduler uses xref:scheduler:TaskLocation.adoc[TaskLocation] that includes a host name and an executor id on that host (as ExecutorCacheTaskLocation ). The keys are RDDs (their ids) and the values are arrays indexed by partition numbers. Each entry is a set of block locations where a RDD partition is cached, i.e. the xref:storage:BlockManager.adoc[BlockManager]s of the blocks. Initialized empty when < >. Used when DAGScheduler is requested for the < > or < >. == [[activeJobs]] ActiveJobs DAGScheduler tracks xref:scheduler:spark-scheduler-ActiveJob.adoc[ActiveJobs]: Adds a new ActiveJob when requested to handle < > or < > events Removes an ActiveJob when requested to < >. Removes all ActiveJobs when requested to < >. DAGScheduler uses ActiveJobs registry when requested to handle < > or < > events, to < > and to < >. The number of ActiveJobs is available using xref:metrics:spark-scheduler-DAGSchedulerSource.adoc#job.activeJobs[job.activeJobs] performance metric. == [[createResultStage]] Creating ResultStage for RDD [source, scala] \u00b6 createResultStage( rdd: RDD[ ], func: (TaskContext, Iterator[ ]) => _, partitions: Array[Int], jobId: Int, callSite: CallSite): ResultStage createResultStage...FIXME createResultStage is used when DAGScheduler is requested to < >. == [[createShuffleMapStage]] Creating ShuffleMapStage for ShuffleDependency [source, scala] \u00b6 createShuffleMapStage( shuffleDep: ShuffleDependency[_, _, _], jobId: Int): ShuffleMapStage createShuffleMapStage creates a xref:scheduler:ShuffleMapStage.adoc[ShuffleMapStage] for the given xref:rdd:ShuffleDependency.adoc[ShuffleDependency] as follows: Stage ID is generated based on < > internal counter RDD is taken from the given xref:rdd:ShuffleDependency.adoc#rdd[ShuffleDependency] Number of tasks is the number of xref:rdd:RDD.adoc#partitions[partitions] of the RDD < > < > createShuffleMapStage registers the ShuffleMapStage in the < > and < > internal registries. createShuffleMapStage < >. createShuffleMapStage requests the < > to xref:scheduler:MapOutputTrackerMaster.adoc#containsShuffle[check whether it contains the shuffle ID or not]. If not, createShuffleMapStage prints out the following INFO message to the logs and requests the < > to xref:scheduler:MapOutputTrackerMaster.adoc#registerShuffle[register the shuffle]. [source,plaintext] \u00b6 Registering RDD [id] ([creationSite]) as input to shuffle [shuffleId] \u00b6 .DAGScheduler Asks MapOutputTrackerMaster Whether Shuffle Map Output Is Already Tracked image::DAGScheduler-MapOutputTrackerMaster-containsShuffle.png[align=\"center\"] createShuffleMapStage is used when DAGScheduler is requested to < >. == [[cleanupStateForJobAndIndependentStages]] Cleaning Up After Job and Independent Stages [source, scala] \u00b6 cleanupStateForJobAndIndependentStages( job: ActiveJob): Unit cleanupStateForJobAndIndependentStages cleans up the state for job and any stages that are not part of any other job. cleanupStateForJobAndIndependentStages looks the job up in the internal < > registry. If no stages are found, the following ERROR is printed out to the logs: No stages registered for job [jobId] Oterwise, cleanupStateForJobAndIndependentStages uses < > registry to find the stages (the real objects not ids!). For each stage, cleanupStateForJobAndIndependentStages reads the jobs the stage belongs to. If the job does not belong to the jobs of the stage, the following ERROR is printed out to the logs: Job [jobId] not registered for stage [stageId] even though that stage was registered for the job If the job was the only job for the stage, the stage (and the stage id) gets cleaned up from the registries, i.e. < >, < >, < >, < > and < >. While removing from < >, you should see the following DEBUG message in the logs: Removing running stage [stageId] While removing from < >, you should see the following DEBUG message in the logs: Removing stage [stageId] from waiting set. While removing from < >, you should see the following DEBUG message in the logs: Removing stage [stageId] from failed set. After all cleaning (using < > as the source registry), if the stage belonged to the one and only job , you should see the following DEBUG message in the logs: After removal of stage [stageId], remaining stages = [stageIdToStage.size] The job is removed from < >, < >, < > registries. The final stage of the job is removed, i.e. xref:scheduler:ResultStage.adoc#removeActiveJob[ResultStage] or xref:scheduler:ShuffleMapStage.adoc#removeActiveJob[ShuffleMapStage]. cleanupStateForJobAndIndependentStages is used in xref:scheduler:DAGSchedulerEventProcessLoop.adoc#handleTaskCompletion-Success-ResultTask[handleTaskCompletion when a ResultTask has completed successfully], < > and < >. == [[markMapStageJobAsFinished]] Marking ShuffleMapStage Job Finished [source, scala] \u00b6 markMapStageJobAsFinished( job: ActiveJob, stats: MapOutputStatistics): Unit markMapStageJobAsFinished marks the active job finished and notifies Spark listeners. Internally, markMapStageJobAsFinished marks the zeroth partition finished and increases the number of tasks finished in job . The xref:scheduler:spark-scheduler-JobListener.adoc#taskSucceeded[ job listener is notified about the 0 th task succeeded]. The < job and independent stages are cleaned up>>. Ultimately, xref:ROOT:SparkListener.adoc#SparkListenerJobEnd[SparkListenerJobEnd] is posted to xref:scheduler:LiveListenerBus.adoc[] (as < >) for the job , the current time (in millis) and JobSucceeded job result. markMapStageJobAsFinished is used in xref:scheduler:DAGSchedulerEventProcessLoop.adoc#handleMapStageSubmitted[handleMapStageSubmitted] and xref:scheduler:DAGSchedulerEventProcessLoop.adoc#handleTaskCompletion[handleTaskCompletion]. == [[getOrCreateParentStages]] Finding Or Creating Missing Direct Parent ShuffleMapStages (For ShuffleDependencies) of RDD [source, scala] \u00b6 getOrCreateParentStages( rdd: RDD[_], firstJobId: Int): List[Stage] getOrCreateParentStages < ShuffleDependencies >> of the input rdd and then < ShuffleMapStage stages>> for each xref:rdd:ShuffleDependency.adoc[ShuffleDependency]. getOrCreateParentStages is used when DAGScheduler is requested to create a < > or a < >. == [[markStageAsFinished]] Marking Stage Finished [source, scala] \u00b6 markStageAsFinished( stage: Stage, errorMessage: Option[String] = None, willRetry: Boolean = false): Unit markStageAsFinished...FIXME markStageAsFinished is used when...FIXME == [[getOrCreateShuffleMapStage]] Finding or Creating ShuffleMapStage for ShuffleDependency [source, scala] \u00b6 getOrCreateShuffleMapStage( shuffleDep: ShuffleDependency[_, _, _], firstJobId: Int): ShuffleMapStage getOrCreateShuffleMapStage finds the xref:scheduler:ShuffleMapStage.adoc[ShuffleMapStage] in the < > internal registry and returns it if available. If not found, getOrCreateShuffleMapStage < > and < > (including one for the input ShuffleDependency). getOrCreateShuffleMapStage is used when DAGScheduler is requested to < >, < >, < >, and < >. == [[getMissingAncestorShuffleDependencies]] Finding Missing ShuffleDependencies For RDD [source, scala] \u00b6 getMissingAncestorShuffleDependencies( rdd: RDD[ ]): Stack[ShuffleDependency[ , _, _]] getMissingAncestorShuffleDependencies finds all missing xref:rdd:ShuffleDependency.adoc[shuffle dependencies] for the given xref:rdd:index.adoc[RDD] traversing its xref:rdd:spark-rdd-lineage.adoc[RDD lineage]. NOTE: A missing shuffle dependency of a RDD is a dependency not registered in < shuffleIdToMapStage internal registry>>. Internally, getMissingAncestorShuffleDependencies < >\u2009of the input RDD and collects the ones that are not registered in < shuffleIdToMapStage internal registry>>. It repeats the process for the RDDs of the parent shuffle dependencies. getMissingAncestorShuffleDependencies is used when DAGScheduler is requested to < >. == [[getShuffleDependencies]] Finding Direct Parent Shuffle Dependencies of RDD [source, scala] \u00b6 getShuffleDependencies( rdd: RDD[ ]): HashSet[ShuffleDependency[ , _, _]] getShuffleDependencies finds direct parent xref:rdd:ShuffleDependency.adoc[shuffle dependencies] for the given xref:rdd:index.adoc[RDD]. .getShuffleDependencies Finds Direct Parent ShuffleDependencies (shuffle1 and shuffle2) image::spark-DAGScheduler-getShuffleDependencies.png[align=\"center\"] Internally, getShuffleDependencies takes the direct xref:rdd:index.adoc#dependencies[shuffle dependencies of the input RDD] and direct shuffle dependencies of all the parent non- ShuffleDependencies in the xref:rdd:spark-rdd-lineage.adoc[dependency chain] (aka RDD lineage ). getShuffleDependencies is used when DAGScheduler is requested to < > (for ShuffleDependencies of a RDD) and < >. == [[failJobAndIndependentStages]] Failing Job and Independent Single-Job Stages [source, scala] \u00b6 failJobAndIndependentStages( job: ActiveJob, failureReason: String, exception: Option[Throwable] = None): Unit failJobAndIndependentStages fails the input job and all the stages that are only used by the job. Internally, failJobAndIndependentStages uses < jobIdToStageIds internal registry>> to look up the stages registered for the job. If no stages could be found, you should see the following ERROR message in the logs: No stages registered for job [id] Otherwise, for every stage, failJobAndIndependentStages finds the job ids the stage belongs to. If no stages could be found or the job is not referenced by the stages, you should see the following ERROR message in the logs: Job [id] not registered for stage [id] even though that stage was registered for the job Only when there is exactly one job registered for the stage and the stage is in RUNNING state (in runningStages internal registry), xref:scheduler:TaskScheduler.adoc#contract[ TaskScheduler is requested to cancel the stage's tasks] and < >. NOTE: failJobAndIndependentStages uses < >, < >, and < > internal registries. failJobAndIndependentStages is used when...FIXME == [[abortStage]] Aborting Stage [source, scala] \u00b6 abortStage( failedStage: Stage, reason: String, exception: Option[Throwable]): Unit abortStage is an internal method that finds all the active jobs that depend on the failedStage stage and fails them. Internally, abortStage looks the failedStage stage up in the internal < > registry and exits if there the stage was not registered earlier. If it was, abortStage finds all the active jobs (in the internal < > registry) with the < failedStage stage>>. At this time, the completionTime property (of the failed stage's xref:scheduler:spark-scheduler-StageInfo.adoc[StageInfo]) is assigned to the current time (millis). All the active jobs that depend on the failed stage (as calculated above) and the stages that do not belong to other jobs (aka independent stages ) are < > (with the failure reason being \"Job aborted due to stage failure: [reason]\" and the input exception ). If there are no jobs depending on the failed stage, you should see the following INFO message in the logs: [source,plaintext] \u00b6 Ignoring failure of [failedStage] because all jobs depending on it are done \u00b6 abortStage is used when DAGScheduler is requested to < >, < >, < >, < >. == [[stageDependsOn]] Checking Out Stage Dependency on Given Stage [source, scala] \u00b6 stageDependsOn( stage: Stage, target: Stage): Boolean stageDependsOn compares two stages and returns whether the stage depends on target stage (i.e. true ) or not (i.e. false ). NOTE: A stage A depends on stage B if B is among the ancestors of A . Internally, stageDependsOn walks through the graph of RDDs of the input stage . For every RDD in the RDD's dependencies (using RDD.dependencies ) stageDependsOn adds the RDD of a xref:rdd:spark-rdd-NarrowDependency.adoc[NarrowDependency] to a stack of RDDs to visit while for a xref:rdd:ShuffleDependency.adoc[ShuffleDependency] it < ShuffleMapStage stages for a ShuffleDependency >> for the dependency and the stage 's first job id that it later adds to a stack of RDDs to visit if the map stage is ready, i.e. all the partitions have shuffle outputs. After all the RDDs of the input stage are visited, stageDependsOn checks if the target 's RDD is among the RDDs of the stage , i.e. whether the stage depends on target stage. stageDependsOn is used when DAGScheduler is requested to < >. == [[submitWaitingChildStages]] Submitting Waiting Child Stages for Execution [source, scala] \u00b6 submitWaitingChildStages( parent: Stage): Unit submitWaitingChildStages submits for execution all waiting stages for which the input parent xref:scheduler:Stage.adoc[Stage] is the direct parent. NOTE: Waiting stages are the stages registered in < waitingStages internal registry>>. When executed, you should see the following TRACE messages in the logs: Checking if any dependencies of [parent] are now runnable running: [runningStages] waiting: [waitingStages] failed: [failedStages] submitWaitingChildStages finds child stages of the input parent stage, removes them from waitingStages internal registry, and < > one by one sorted by their job ids. submitWaitingChildStages is used when DAGScheduler is requested to < > and < >. == [[submitStage]] Submitting Stage (with Missing Parents) for Execution [source, scala] \u00b6 submitStage( stage: Stage): Unit submitStage submits the input stage or its missing parents (if there any stages not computed yet before the input stage could). NOTE: submitStage is also used to xref:scheduler:DAGSchedulerEventProcessLoop.adoc#resubmitFailedStages[resubmit failed stages]. submitStage recursively submits any missing parents of the stage . Internally, submitStage first finds the earliest-created job id that needs the stage . NOTE: A stage itself tracks the jobs (their ids) it belongs to (using the internal jobIds registry). The following steps depend on whether there is a job or not. If there are no jobs that require the stage , submitStage < > with the reason: No active job for stage [id] If however there is a job for the stage , you should see the following DEBUG message in the logs: submitStage([stage]) submitStage checks the status of the stage and continues when it was not recorded in < >, < > or < > internal registries. It simply exits otherwise. With the stage ready for submission, submitStage calculates the < stage >> (sorted by their job ids). You should see the following DEBUG message in the logs: missing: [missing] When the stage has no parent stages missing, you should see the following INFO message in the logs: Submitting [stage] ([stage.rdd]), which has no missing parents submitStage < stage >> (with the earliest-created job id) and finishes. If however there are missing parent stages for the stage , submitStage < >, and the stage is recorded in the internal < > registry. submitStage is used recursively for missing parents of the given stage and when DAGScheduler is requested for the following: < > (ResubmitFailedStages event) < > (CompletionEvent event) Handle < >, < > and < > events == [[stage-attempts]] Stage Attempts A single stage can be re-executed in multiple attempts due to fault recovery. The number of attempts is configured (FIXME). If TaskScheduler reports that a task failed because a map output file from a previous stage was lost, the DAGScheduler resubmits the lost stage. This is detected through a xref:scheduler:DAGSchedulerEventProcessLoop.adoc#handleTaskCompletion-FetchFailed[ CompletionEvent with FetchFailed ], or an < > event. DAGScheduler will wait a small amount of time to see whether other nodes or tasks fail, then resubmit TaskSets for any lost stage(s) that compute the missing tasks. Please note that tasks from the old attempts of a stage could still be running. A stage object tracks multiple xref:scheduler:spark-scheduler-StageInfo.adoc[StageInfo] objects to pass to Spark listeners or the web UI. The latest StageInfo for the most recent attempt for a stage is accessible through latestInfo . == [[preferred-locations]] Preferred Locations DAGScheduler computes where to run each task in a stage based on the xref:rdd:index.adoc#getPreferredLocations[preferred locations of its underlying RDDs], or < >. == [[adaptive-query-planning]] Adaptive Query Planning / Adaptive Scheduling See https://issues.apache.org/jira/browse/SPARK-9850[SPARK-9850 Adaptive execution in Spark] for the design document. The work is currently in progress. https://github.com/apache/spark/blob/master/core/src/main/scala/org/apache/spark/scheduler/DAGScheduler.scala#L661[DAGScheduler.submitMapStage ] method is used for adaptive query planning, to run map stages and look at statistics about their outputs before submitting downstream stages. == ScheduledExecutorService daemon services DAGScheduler uses the following ScheduledThreadPoolExecutors (with the policy of removing cancelled tasks from a work queue at time of cancellation): dag-scheduler-message - a daemon thread pool using j.u.c.ScheduledThreadPoolExecutor with core pool size 1 . It is used to post a xref:scheduler:DAGSchedulerEventProcessLoop.adoc#ResubmitFailedStages[ResubmitFailedStages] event when xref:scheduler:DAGSchedulerEventProcessLoop.adoc#handleTaskCompletion-FetchFailed[ FetchFailed is reported]. They are created using ThreadUtils.newDaemonSingleThreadScheduledExecutor method that uses Guava DSL to instantiate a ThreadFactory. == [[getMissingParentStages]] Finding Missing Parent ShuffleMapStages For Stage [source, scala] \u00b6 getMissingParentStages( stage: Stage): List[Stage] getMissingParentStages finds missing parent xref:scheduler:ShuffleMapStage.adoc[ShuffleMapStage]s in the dependency graph of the input stage (using the https://en.wikipedia.org/wiki/Breadth-first_search[breadth-first search algorithm]). Internally, getMissingParentStages starts with the stage 's RDD and walks up the tree of all parent RDDs to find < >. NOTE: A Stage tracks the associated RDD using xref:scheduler:Stage.adoc#rdd[ rdd property]. NOTE: An uncached partition of a RDD is a partition that has Nil in the < > (which results in no RDD blocks in any of the active xref:storage:BlockManager.adoc[BlockManager]s on executors). getMissingParentStages traverses the xref:rdd:index.adoc#dependencies[parent dependencies of the RDD] and acts according to their type, i.e. xref:rdd:ShuffleDependency.adoc[ShuffleDependency] or xref:rdd:spark-rdd-NarrowDependency.adoc[NarrowDependency]. NOTE: xref:rdd:ShuffleDependency.adoc[ShuffleDependency] and xref:rdd:spark-rdd-NarrowDependency.adoc[NarrowDependency] are the main top-level xref:rdd:spark-rdd-Dependency.adoc[Dependencies]. For each NarrowDependency , getMissingParentStages simply marks the corresponding RDD to visit and moves on to a next dependency of a RDD or works on another unvisited parent RDD. NOTE: xref:rdd:spark-rdd-NarrowDependency.adoc[NarrowDependency] is a RDD dependency that allows for pipelined execution. getMissingParentStages focuses on ShuffleDependency dependencies. NOTE: xref:rdd:ShuffleDependency.adoc[ShuffleDependency] is a RDD dependency that represents a dependency on the output of a xref:scheduler:ShuffleMapStage.adoc[ShuffleMapStage], i.e. shuffle map stage . For each ShuffleDependency , getMissingParentStages < ShuffleMapStage stages>>. If the ShuffleMapStage is not available , it is added to the set of missing (map) stages. NOTE: A ShuffleMapStage is available when all its partitions are computed, i.e. results are available (as blocks). CAUTION: FIXME...IMAGE with ShuffleDependencies queried getMissingParentStages is used when DAGScheduler is requested to < > and handle < > and < > events. == [[submitMissingTasks]] Submitting Missing Tasks of Stage [source, scala] \u00b6 submitMissingTasks( stage: Stage, jobId: Int): Unit submitMissingTasks prints out the following DEBUG message to the logs: submitMissingTasks([stage]) submitMissingTasks requests the given xref:scheduler:Stage.adoc[Stage] for the xref:scheduler:Stage.adoc#findMissingPartitions[missing partitions] (partitions that need to be computed). submitMissingTasks adds the stage to the < > internal registry. submitMissingTasks notifies the < > that xref:scheduler:OutputCommitCoordinator.adoc#stageStart[stage execution started]. [[submitMissingTasks-taskIdToLocations]] submitMissingTasks < > ( task locality preferences ) of the missing partitions. submitMissingTasks requests the stage for a xref:scheduler:Stage.adoc#makeNewStageAttempt[new stage attempt]. submitMissingTasks requests the < > to xref:scheduler:LiveListenerBus.adoc#post[post] a xref:ROOT:SparkListener.adoc#SparkListenerStageSubmitted[SparkListenerStageSubmitted] event. submitMissingTasks uses the < > to xref:serializer:Serializer.adoc#serialize[serialize] the stage and create a so-called task binary. submitMissingTasks serializes the RDD (of the stage) and either the ShuffleDependency or the compute function based on the type of the stage, i.e. ShuffleMapStage and ResultStage, respectively. submitMissingTasks creates a xref:ROOT:SparkContext.adoc#broadcast[broadcast variable] for the task binary. NOTE: That shows how important xref:ROOT:Broadcast.adoc[]s are for Spark itself to distribute data among executors in a Spark application in the most efficient way. submitMissingTasks creates xref:scheduler:Task.adoc[tasks] for every missing partition: xref:scheduler:ShuffleMapTask.adoc[ShuffleMapTasks] for a xref:scheduler:ShuffleMapStage.adoc[ShuffleMapStage] xref:scheduler:ResultTask.adoc[ResultTasks] for a xref:scheduler:ResultStage.adoc[ResultStage] If there are tasks to submit for execution (i.e. there are missing partitions in the stage), submitMissingTasks prints out the following INFO message to the logs: Submitting [size] missing tasks from [stage] ([rdd]) (first 15 tasks are for partitions [partitionIds]) submitMissingTasks requests the < > to xref:scheduler:TaskScheduler.adoc#submitTasks[submit the tasks for execution] (as a new xref:scheduler:TaskSet.adoc[TaskSet]). With no tasks to submit for execution, submitMissingTasks < >. submitMissingTasks prints out the following DEBUG messages based on the type of the stage: Stage [stage] is actually done; (available: [isAvailable],available outputs: [numAvailableOutputs],partitions: [numPartitions]) or Stage [stage] is actually done; (partitions: [numPartitions]) for ShuffleMapStage and ResultStage , respectively. In the end, with no tasks to submit for execution, submitMissingTasks < > and exits. submitMissingTasks is used when DAGScheduler is requested to < >. == [[getPreferredLocs]] Finding Preferred Locations for Missing Partitions [source, scala] \u00b6 getPreferredLocs( rdd: RDD[_], partition: Int): Seq[TaskLocation] getPreferredLocs is simply an alias for the internal (recursive) < >. getPreferredLocs is used when...FIXME == [[getCacheLocs]] Finding BlockManagers (Executors) for Cached RDD Partitions (aka Block Location Discovery) [source, scala] \u00b6 getCacheLocs( rdd: RDD[_]): IndexedSeq[Seq[TaskLocation]] getCacheLocs gives xref:scheduler:TaskLocation.adoc[TaskLocations] (block locations) for the partitions of the input rdd . getCacheLocs caches lookup results in < > internal registry. NOTE: The size of the collection from getCacheLocs is exactly the number of partitions in rdd RDD. NOTE: The size of every xref:scheduler:TaskLocation.adoc[TaskLocation] collection (i.e. every entry in the result of getCacheLocs) is exactly the number of blocks managed using xref:storage:BlockManager.adoc[BlockManagers] on executors. Internally, getCacheLocs finds rdd in the < > internal registry (of partition locations per RDD). If rdd is not in < > internal registry, getCacheLocs branches per its xref:storage:StorageLevel.adoc[storage level]. For NONE storage level (i.e. no caching), the result is an empty locations (i.e. no location preference). For other non- NONE storage levels, getCacheLocs xref:storage:BlockManagerMaster.adoc#getLocations-block-array[requests BlockManagerMaster for block locations] that are then mapped to xref:scheduler:TaskLocation.adoc[TaskLocations] with the hostname of the owning BlockManager for a block (of a partition) and the executor id. NOTE: getCacheLocs uses < > that was defined when < >. getCacheLocs records the computed block locations per partition (as xref:scheduler:TaskLocation.adoc[TaskLocation]) in < > internal registry. NOTE: getCacheLocs requests locations from BlockManagerMaster using xref:storage:BlockId.adoc#RDDBlockId[RDDBlockId] with the RDD id and the partition indices (which implies that the order of the partitions matters to request proper blocks). NOTE: DAGScheduler uses xref:scheduler:TaskLocation.adoc[TaskLocations] (with host and executor) while xref:storage:BlockManagerMaster.adoc[BlockManagerMaster] uses xref:storage:BlockManagerId.adoc[] (to track similar information, i.e. block locations). getCacheLocs is used when DAGScheduler is requested to finds < > and < >. == [[getPreferredLocsInternal]] Finding Placement Preferences for RDD Partition (recursively) [source, scala] \u00b6 getPreferredLocsInternal( rdd: RDD[ ], partition: Int, visited: HashSet[(RDD[ ], Int)]): Seq[TaskLocation] getPreferredLocsInternal first < TaskLocations for the partition of the rdd >> (using < > internal cache) and returns them. Otherwise, if not found, getPreferredLocsInternal xref:rdd:index.adoc#preferredLocations[requests rdd for the preferred locations of partition ] and returns them. NOTE: Preferred locations of the partitions of a RDD are also called placement preferences or locality preferences . Otherwise, if not found, getPreferredLocsInternal finds the first parent xref:rdd:spark-rdd-NarrowDependency.adoc[NarrowDependency] and (recursively) < TaskLocations >>. If all the attempts fail to yield any non-empty result, getPreferredLocsInternal returns an empty collection of xref:scheduler:TaskLocation.adoc[TaskLocations]. getPreferredLocsInternal is used when DAGScheduler is requested for the < >. == [[stop]] Stopping DAGScheduler [source, scala] \u00b6 stop(): Unit \u00b6 stop stops the internal dag-scheduler-message thread pool, < >, and xref:scheduler:TaskScheduler.adoc#stop[TaskScheduler]. stop is used when...FIXME == [[updateAccumulators]] Updating Accumulators with Partial Values from Completed Tasks [source, scala] \u00b6 updateAccumulators( event: CompletionEvent): Unit updateAccumulators merges the partial values of accumulators from a completed task into their \"source\" accumulators on the driver. NOTE: It is called by < >. For each xref:ROOT:spark-accumulators.adoc#AccumulableInfo[AccumulableInfo] in the CompletionEvent , a partial value from a task is obtained (from AccumulableInfo.update ) and added to the driver's accumulator (using Accumulable.++= method). For named accumulators with the update value being a non-zero value, i.e. not Accumulable.zero : stage.latestInfo.accumulables for the AccumulableInfo.id is set CompletionEvent.taskInfo.accumulables has a new xref:ROOT:spark-accumulators.adoc#AccumulableInfo[AccumulableInfo] added. CAUTION: FIXME Where are Stage.latestInfo.accumulables and CompletionEvent.taskInfo.accumulables used? updateAccumulators is used when DAGScheduler is requested to < >. == [[checkBarrierStageWithNumSlots]] checkBarrierStageWithNumSlots Method [source, scala] \u00b6 checkBarrierStageWithNumSlots( rdd: RDD[_]): Unit checkBarrierStageWithNumSlots...FIXME checkBarrierStageWithNumSlots is used when DAGScheduler is requested to create < > and < > stages. == [[killTaskAttempt]] Killing Task [source, scala] \u00b6 killTaskAttempt( taskId: Long, interruptThread: Boolean, reason: String): Boolean killTaskAttempt requests the < > to xref:scheduler:TaskScheduler.adoc#killTaskAttempt[kill a task]. killTaskAttempt is used when SparkContext is requested to xref:ROOT:SparkContext.adoc#killTaskAttempt[kill a task]. == [[cleanUpAfterSchedulerStop]] cleanUpAfterSchedulerStop Method [source, scala] \u00b6 cleanUpAfterSchedulerStop(): Unit \u00b6 cleanUpAfterSchedulerStop...FIXME cleanUpAfterSchedulerStop is used when DAGSchedulerEventProcessLoop is requested to xref:scheduler:DAGSchedulerEventProcessLoop.adoc#onStop[onStop]. == [[removeExecutorAndUnregisterOutputs]] removeExecutorAndUnregisterOutputs Method [source, scala] \u00b6 removeExecutorAndUnregisterOutputs( execId: String, fileLost: Boolean, hostToUnregisterOutputs: Option[String], maybeEpoch: Option[Long] = None): Unit removeExecutorAndUnregisterOutputs...FIXME removeExecutorAndUnregisterOutputs is used when DAGScheduler is requested to handle < > (due to a fetch failure) and < > events. == [[markMapStageJobsAsFinished]] markMapStageJobsAsFinished Method [source, scala] \u00b6 markMapStageJobsAsFinished( shuffleStage: ShuffleMapStage): Unit markMapStageJobsAsFinished...FIXME markMapStageJobsAsFinished is used when DAGScheduler is requested to < > (of a ShuffleMapStage that has just been computed) and < > (of a ShuffleMapStage). == [[updateJobIdStageIdMaps]] updateJobIdStageIdMaps Method [source, scala] \u00b6 updateJobIdStageIdMaps( jobId: Int, stage: Stage): Unit updateJobIdStageIdMaps...FIXME updateJobIdStageIdMaps is used when DAGScheduler is requested to create < > and < > stages. == [[executorHeartbeatReceived]] executorHeartbeatReceived Method [source, scala] \u00b6 executorHeartbeatReceived( execId: String, // (taskId, stageId, stageAttemptId, accumUpdates) accumUpdates: Array[(Long, Int, Int, Seq[AccumulableInfo])], blockManagerId: BlockManagerId): Boolean executorHeartbeatReceived posts a xref:ROOT:SparkListener.adoc#SparkListenerExecutorMetricsUpdate[SparkListenerExecutorMetricsUpdate] (to < >) and informs xref:storage:BlockManagerMaster.adoc[BlockManagerMaster] that blockManagerId block manager is alive (by posting xref:storage:BlockManagerMaster.adoc#BlockManagerHeartbeat[BlockManagerHeartbeat]). executorHeartbeatReceived is used when TaskSchedulerImpl is requested to xref:scheduler:TaskSchedulerImpl.adoc#executorHeartbeatReceived[handle an executor heartbeat]. == [[postTaskEnd]] postTaskEnd Method [source, scala] \u00b6 postTaskEnd( event: CompletionEvent): Unit postTaskEnd...FIXME postTaskEnd is used when DAGScheduler is requested to < >. == Event Handlers === [[doCancelAllJobs]] AllJobsCancelled Event Handler [source, scala] \u00b6 doCancelAllJobs(): Unit \u00b6 doCancelAllJobs...FIXME doCancelAllJobs is used when DAGSchedulerEventProcessLoop is requested to handle an xref:scheduler:DAGSchedulerEventProcessLoop.adoc#AllJobsCancelled[AllJobsCancelled] event and xref:scheduler:DAGSchedulerEventProcessLoop.adoc#onError[onError]. === [[handleBeginEvent]] BeginEvent Event Handler [source, scala] \u00b6 handleBeginEvent( task: Task[_], taskInfo: TaskInfo): Unit handleBeginEvent...FIXME handleBeginEvent is used when DAGSchedulerEventProcessLoop is requested to handle a xref:scheduler:DAGSchedulerEvent.adoc#BeginEvent[BeginEvent] event. === [[handleTaskCompletion]] CompletionEvent Event Handler [source, scala] \u00b6 handleTaskCompletion( event: CompletionEvent): Unit handleTaskCompletion...FIXME handleTaskCompletion is used when DAGSchedulerEventProcessLoop is requested to handle a xref:scheduler:DAGSchedulerEvent.adoc#CompletionEvent[CompletionEvent] event. === [[handleExecutorAdded]] ExecutorAdded Event Handler [source, scala] \u00b6 handleExecutorAdded( execId: String, host: String): Unit handleExecutorAdded...FIXME handleExecutorAdded is used when DAGSchedulerEventProcessLoop is requested to handle an xref:scheduler:DAGSchedulerEvent.adoc#ExecutorAdded[ExecutorAdded] event. === [[handleExecutorLost]] ExecutorLost Event Handler [source, scala] \u00b6 handleExecutorLost( execId: String, workerLost: Boolean): Unit handleExecutorLost...FIXME handleExecutorLost is used when DAGSchedulerEventProcessLoop is requested to handle an xref:scheduler:DAGSchedulerEvent.adoc#ExecutorLost[ExecutorLost] event. === [[handleGetTaskResult]] GettingResultEvent Event Handler [source, scala] \u00b6 handleGetTaskResult( taskInfo: TaskInfo): Unit handleGetTaskResult...FIXME handleGetTaskResult is used when DAGSchedulerEventProcessLoop is requested to handle a xref:scheduler:DAGSchedulerEvent.adoc#GettingResultEvent[GettingResultEvent] event. === [[handleJobCancellation]] JobCancelled Event Handler [source, scala] \u00b6 handleJobCancellation( jobId: Int, reason: Option[String]): Unit handleJobCancellation...FIXME handleJobCancellation is used when DAGScheduler is requested to handle a xref:scheduler:DAGSchedulerEvent.adoc#JobCancelled[JobCancelled] event, < >, < >, < >. === [[handleJobGroupCancelled]] JobGroupCancelled Event Handler [source, scala] \u00b6 handleJobGroupCancelled( groupId: String): Unit handleJobGroupCancelled...FIXME handleJobGroupCancelled is used when DAGScheduler is requested to handle xref:scheduler:DAGSchedulerEvent.adoc#JobGroupCancelled[JobGroupCancelled] event. === [[handleJobSubmitted]] JobSubmitted Event Handler [source, scala] \u00b6 handleJobSubmitted( jobId: Int, finalRDD: RDD[ ], func: (TaskContext, Iterator[ ]) => _, partitions: Array[Int], callSite: CallSite, listener: JobListener, properties: Properties): Unit handleJobSubmitted xref:scheduler:DAGScheduler.adoc#createResultStage[creates a new ResultStage ] (as finalStage in the picture below) given the input finalRDD , func , partitions , jobId and callSite . . DAGScheduler.handleJobSubmitted Method image::dagscheduler-handleJobSubmitted.png[align=\"center\"] handleJobSubmitted creates an xref:scheduler:spark-scheduler-ActiveJob.adoc[ActiveJob] (with the input jobId , callSite , listener , properties , and the xref:scheduler:ResultStage.adoc[ResultStage]). handleJobSubmitted xref:scheduler:DAGScheduler.adoc#clearCacheLocs[clears the internal cache of RDD partition locations]. CAUTION: FIXME Why is this clearing here so important? You should see the following INFO messages in the logs: Got job [id] ([callSite]) with [number] output partitions Final stage: [stage] ([name]) Parents of final stage: [parents] Missing parents: [missingStages] handleJobSubmitted then registers the new job in xref:scheduler:DAGScheduler.adoc#jobIdToActiveJob[jobIdToActiveJob] and xref:scheduler:DAGScheduler.adoc#activeJobs[activeJobs] internal registries, and xref:scheduler:ResultStage.adoc#setActiveJob[with the final ResultStage ]. NOTE: ResultStage can only have one ActiveJob registered. handleJobSubmitted xref:scheduler:DAGScheduler.adoc#jobIdToStageIds[finds all the registered stages for the input jobId ] and collects xref:scheduler:Stage.adoc#latestInfo[their latest StageInfo ]. In the end, handleJobSubmitted posts xref:ROOT:SparkListener.adoc#SparkListenerJobStart[SparkListenerJobStart] message to xref:scheduler:LiveListenerBus.adoc[] and xref:scheduler:DAGScheduler.adoc#submitStage[submits the stage]. handleJobSubmitted is used when DAGSchedulerEventProcessLoop is requested to handle a xref:scheduler:DAGSchedulerEvent.adoc#JobSubmitted[JobSubmitted] event. === [[handleMapStageSubmitted]] MapStageSubmitted Event Handler [source, scala] \u00b6 handleMapStageSubmitted( jobId: Int, dependency: ShuffleDependency[_, _, _], callSite: CallSite, listener: JobListener, properties: Properties): Unit handleMapStageSubmitted...FIXME handleMapStageSubmitted is used when DAGSchedulerEventProcessLoop is requested to handle a xref:scheduler:DAGSchedulerEvent.adoc#MapStageSubmitted[MapStageSubmitted] event. === [[resubmitFailedStages]] ResubmitFailedStages Event Handler [source, scala] \u00b6 resubmitFailedStages(): Unit \u00b6 resubmitFailedStages...FIXME resubmitFailedStages is used when DAGSchedulerEventProcessLoop is requested to handle a xref:scheduler:DAGSchedulerEvent.adoc#ResubmitFailedStages[ResubmitFailedStages] event. === [[handleSpeculativeTaskSubmitted]] SpeculativeTaskSubmitted Event Handler [source, scala] \u00b6 handleSpeculativeTaskSubmitted(): Unit \u00b6 handleSpeculativeTaskSubmitted...FIXME handleSpeculativeTaskSubmitted is used when DAGSchedulerEventProcessLoop is requested to handle a xref:scheduler:DAGSchedulerEvent.adoc#SpeculativeTaskSubmitted[SpeculativeTaskSubmitted] event. === [[handleStageCancellation]] StageCancelled Event Handler [source, scala] \u00b6 handleStageCancellation(): Unit \u00b6 handleStageCancellation...FIXME handleStageCancellation is used when DAGSchedulerEventProcessLoop is requested to handle a xref:scheduler:DAGSchedulerEvent.adoc#StageCancelled[StageCancelled] event. === [[handleTaskSetFailed]] TaskSetFailed Event Handler [source, scala] \u00b6 handleTaskSetFailed(): Unit \u00b6 handleTaskSetFailed...FIXME handleTaskSetFailed is used when DAGSchedulerEventProcessLoop is requested to handle a xref:scheduler:DAGSchedulerEvent.adoc#TaskSetFailed[TaskSetFailed] event. === [[handleWorkerRemoved]] WorkerRemoved Event Handler [source, scala] \u00b6 handleWorkerRemoved( workerId: String, host: String, message: String): Unit handleWorkerRemoved...FIXME handleWorkerRemoved is used when DAGSchedulerEventProcessLoop is requested to handle a xref:scheduler:DAGSchedulerEvent.adoc#WorkerRemoved[WorkerRemoved] event. == [[logging]] Logging Enable ALL logging level for org.apache.spark.scheduler.DAGScheduler logger to see what happens inside. Add the following line to conf/log4j.properties : [source] \u00b6 log4j.logger.org.apache.spark.scheduler.DAGScheduler=ALL \u00b6 Refer to xref:ROOT:spark-logging.adoc[Logging]. == [[internal-properties]] Internal Properties [cols=\"30m,70\",options=\"header\",width=\"100%\"] |=== | Name | Description | failedEpoch | [[failedEpoch]] The lookup table of lost executors and the epoch of the event. | failedStages | [[failedStages]] Stages that failed due to fetch failures (when a xref:scheduler:DAGSchedulerEventProcessLoop.adoc#handleTaskCompletion-FetchFailed[task fails with FetchFailed exception]). | jobIdToActiveJob | [[jobIdToActiveJob]] The lookup table of ActiveJob s per job id. | jobIdToStageIds | [[jobIdToStageIds]] The lookup table of all stages per ActiveJob id | metricsSource | [[metricsSource]] xref:metrics:spark-scheduler-DAGSchedulerSource.adoc[DAGSchedulerSource] | nextJobId | [[nextJobId]] The next job id counting from 0 . Used when DAGScheduler < > and < >, and < >. | nextStageId | [[nextStageId]] The next stage id counting from 0 . Used when DAGScheduler creates a < > and a < >. It is the key in < >. | runningStages | [[runningStages]] The set of stages that are currently \"running\". A stage is added when < > gets executed (without first checking if the stage has not already been added). | shuffleIdToMapStage | [[shuffleIdToMapStage]] The lookup table of xref:scheduler:ShuffleMapStage.adoc[ShuffleMapStage]s per xref:rdd:ShuffleDependency.adoc[ShuffleDependency]. | stageIdToStage | [[stageIdToStage]] The lookup table for stages per their ids. Used when DAGScheduler < >, < >, < >, is informed that xref:scheduler:DAGSchedulerEventProcessLoop.adoc#handleBeginEvent[a task is started], xref:scheduler:DAGSchedulerEventProcessLoop.adoc#handleTaskSetFailed[a taskset has failed], xref:scheduler:DAGSchedulerEventProcessLoop.adoc#handleJobSubmitted[a job is submitted (to compute a ResultStage )], xref:scheduler:DAGSchedulerEventProcessLoop.adoc#handleMapStageSubmitted[a map stage was submitted], xref:scheduler:DAGSchedulerEventProcessLoop.adoc#handleTaskCompletion[a task has completed] or xref:scheduler:DAGSchedulerEventProcessLoop.adoc#handleStageCancellation[a stage was cancelled], < >, < > and < >. | waitingStages | [[waitingStages]] The stages with parents to be computed |===","title":"DAGScheduler"},{"location":"scheduler/DAGScheduler/#note","text":"","title":"[NOTE]"},{"location":"scheduler/DAGScheduler/#the-introduction-that-follows-was-highly-influenced-by-the-scaladoc-of-httpsgithubcomapachesparkblobmastercoresrcmainscalaorgapachesparkschedulerdagschedulerscalaorgapachesparkschedulerdagscheduler-as-dagscheduler-is-a-private-class-it-does-not-appear-in-the-official-api-documentation-you-are-strongly-encouraged-to-read-httpsgithubcomapachesparkblobmastercoresrcmainscalaorgapachesparkschedulerdagschedulerscalathe-sources-and-only-then-read-this-and-the-related-pages-afterwards","text":"== [[introduction]] Introduction DAGScheduler is the scheduling layer of Apache Spark that implements stage-oriented scheduling . DAGScheduler transforms a logical execution plan (i.e. xref:rdd:spark-rdd-lineage.adoc[RDD lineage] of dependencies built using xref:rdd:spark-rdd-transformations.adoc[RDD transformations]) to a physical execution plan (using xref:scheduler:Stage.adoc[stages]). .DAGScheduler Transforming RDD Lineage Into Stage DAG image::dagscheduler-rdd-lineage-stage-dag.png[align=\"center\"] After an xref:rdd:spark-rdd-actions.adoc[action] has been called, xref:ROOT:SparkContext.adoc[SparkContext] hands over a logical plan to DAGScheduler that it in turn translates to a set of stages that are submitted as xref:scheduler:TaskSet.adoc[TaskSets] for execution. .Executing action leads to new ResultStage and ActiveJob in DAGScheduler image::dagscheduler-rdd-partitions-job-resultstage.png[align=\"center\"] The fundamental concepts of DAGScheduler are jobs and stages (refer to xref:scheduler:spark-scheduler-ActiveJob.adoc[Jobs] and xref:scheduler:Stage.adoc[Stages] respectively) that it tracks through < >. DAGScheduler works solely on the driver and is created as part of xref:ROOT:SparkContext.adoc#creating-instance[SparkContext's initialization] (right after xref:scheduler:TaskScheduler.adoc[TaskScheduler] and xref:scheduler:SchedulerBackend.adoc[SchedulerBackend] are ready). .DAGScheduler as created by SparkContext with other services image::dagscheduler-new-instance.png[align=\"center\"] DAGScheduler does three things in Spark (thorough explanations follow): Computes an execution DAG , i.e. DAG of stages, for a job. Determines the < > to run each task on. Handles failures due to shuffle output files being lost. DAGScheduler computes https://en.wikipedia.org/wiki/Directed_acyclic_graph[a directed acyclic graph (DAG)] of stages for each job, keeps track of which RDDs and stage outputs are materialized, and finds a minimal schedule to run jobs. It then submits stages to xref:scheduler:TaskScheduler.adoc[TaskScheduler]. .DAGScheduler.submitJob image::dagscheduler-submitjob.png[align=\"center\"] In addition to coming up with the execution DAG, DAGScheduler also determines the preferred locations to run each task on, based on the current cache status, and passes the information to xref:scheduler:TaskScheduler.adoc[TaskScheduler]. DAGScheduler tracks which xref:rdd:spark-rdd-caching.adoc[RDDs are cached (or persisted)] to avoid \"recomputing\" them, i.e. redoing the map side of a shuffle. DAGScheduler remembers what xref:scheduler:ShuffleMapStage.adoc[ShuffleMapStage]s have already produced output files (that are stored in xref:storage:BlockManager.adoc[BlockManager]s). DAGScheduler is only interested in cache location coordinates, i.e. host and executor id, per partition of a RDD. Furthermore, it handles failures due to shuffle output files being lost, in which case old stages may need to be resubmitted. Failures within a stage that are not caused by shuffle file loss are handled by the TaskScheduler itself, which will retry each task a small number of times before cancelling the whole stage. DAGScheduler uses an event queue architecture in which a thread can post DAGSchedulerEvent events, e.g. a new job or stage being submitted, that DAGScheduler reads and executes sequentially. See the section < >. DAGScheduler runs stages in topological order. DAGScheduler uses xref:ROOT:SparkContext.adoc[SparkContext], xref:scheduler:TaskScheduler.adoc[TaskScheduler], xref:scheduler:LiveListenerBus.adoc[], xref:scheduler:MapOutputTracker.adoc[MapOutputTracker] and xref:storage:BlockManager.adoc[BlockManager] for its services. However, at the very minimum, DAGScheduler takes a SparkContext only (and requests SparkContext for the other services). When DAGScheduler schedules a job as a result of xref:rdd:index.adoc#actions[executing an action on a RDD] or xref:ROOT:SparkContext.adoc#runJob[calling SparkContext.runJob() method directly], it spawns parallel tasks to compute (partial) results per partition. == [[creating-instance]][[initialization]] Creating Instance DAGScheduler takes the following to be created: [[sc]] xref:ROOT:SparkContext.adoc[] < > [[listenerBus]] xref:scheduler:LiveListenerBus.adoc[] [[mapOutputTracker]] xref:scheduler:MapOutputTrackerMaster.adoc[MapOutputTrackerMaster] [[blockManagerMaster]] xref:storage:BlockManagerMaster.adoc[BlockManagerMaster] [[env]] xref:core:SparkEnv.adoc[] [[clock]] Clock (default: SystemClock) While being created, DAGScheduler xref:scheduler:TaskScheduler.adoc#setDAGScheduler[associates itself] with the < > and starts < >. == [[event-loop]][[eventProcessLoop]] DAGScheduler Event Bus DAGScheduler uses an xref:scheduler:DAGSchedulerEventProcessLoop.adoc[event bus] to process scheduling-related events on a separate thread (one by one and asynchronously). DAGScheduler starts the event bus when created and stops it when requested to < >. DAGScheduler defines < > that allow posting DAGSchedulerEvent events to the event bus. [[event-posting-methods]] .DAGScheduler Event Posting Methods [cols=\"20m,20m,60\",options=\"header\",width=\"100%\"] |=== | Method | Event Posted | Trigger | [[cancelAllJobs]] cancelAllJobs | xref:scheduler:DAGSchedulerEvent.adoc#AllJobsCancelled[AllJobsCancelled] | SparkContext is requested to xref:ROOT:SparkContext.adoc#cancelAllJobs[cancel all running or scheduled Spark jobs] | [[cancelJob]] cancelJob | xref:scheduler:DAGSchedulerEvent.adoc#JobCancelled[JobCancelled] | xref:ROOT:SparkContext.adoc#cancelJob[SparkContext] or xref:scheduler:spark-scheduler-JobWaiter.adoc[JobWaiter] are requested to cancel a Spark job | [[cancelJobGroup]] cancelJobGroup | xref:scheduler:DAGSchedulerEvent.adoc#JobGroupCancelled[JobGroupCancelled] | SparkContext is requested to xref:ROOT:SparkContext.adoc#cancelJobGroup[cancel a job group] | [[cancelStage]] cancelStage | xref:scheduler:DAGSchedulerEvent.adoc#StageCancelled[StageCancelled] | SparkContext is requested to xref:ROOT:SparkContext.adoc#cancelStage[cancel a stage] | [[executorAdded]] executorAdded | xref:scheduler:DAGSchedulerEvent.adoc#ExecutorAdded[ExecutorAdded] | TaskSchedulerImpl is requested to xref:scheduler:TaskSchedulerImpl.adoc#resourceOffers[handle resource offers] (and a new executor is found in the resource offers) | [[executorLost]] executorLost | xref:scheduler:DAGSchedulerEvent.adoc#ExecutorLost[ExecutorLost] | TaskSchedulerImpl is requested to xref:scheduler:TaskSchedulerImpl.adoc#statusUpdate[handle a task status update] (and a task gets lost which is used to indicate that the executor got broken and hence should be considered lost) or xref:scheduler:TaskSchedulerImpl.adoc#executorLost[executorLost] | [[runApproximateJob]] runApproximateJob | xref:scheduler:DAGSchedulerEvent.adoc#JobSubmitted[JobSubmitted] | SparkContext is requested to xref:ROOT:SparkContext.adoc#runApproximateJob[run an approximate job] | [[speculativeTaskSubmitted]] speculativeTaskSubmitted | xref:scheduler:DAGSchedulerEvent.adoc#SpeculativeTaskSubmitted[SpeculativeTaskSubmitted] | | [[submitJob]] submitJob | xref:scheduler:DAGSchedulerEvent.adoc#JobSubmitted[JobSubmitted] a| SparkContext is requested to xref:ROOT:SparkContext.adoc#submitJob[submits a job] DAGScheduler is requested to < > | [[submitMapStage]] submitMapStage | xref:scheduler:DAGSchedulerEvent.adoc#MapStageSubmitted[MapStageSubmitted] | SparkContext is requested to xref:ROOT:SparkContext.adoc#submitMapStage[submit a MapStage for execution]. | [[taskEnded]] taskEnded | xref:scheduler:DAGSchedulerEvent.adoc#CompletionEvent[CompletionEvent] | TaskSetManager is requested to xref:scheduler:TaskSetManager.adoc#handleSuccessfulTask[handleSuccessfulTask], xref:scheduler:TaskSetManager.adoc#handleFailedTask[handleFailedTask], and xref:scheduler:TaskSetManager.adoc#executorLost[executorLost] | [[taskGettingResult]] taskGettingResult | xref:scheduler:DAGSchedulerEvent.adoc#GettingResultEvent[GettingResultEvent] | TaskSetManager is requested to xref:scheduler:TaskSetManager.adoc#handleTaskGettingResult[handle a task fetching result] | [[taskSetFailed]] taskSetFailed | xref:scheduler:DAGSchedulerEvent.adoc#TaskSetFailed[TaskSetFailed] | TaskSetManager is requested to xref:scheduler:TaskSetManager.adoc#abort[abort] | [[taskStarted]] taskStarted | xref:scheduler:DAGSchedulerEvent.adoc#BeginEvent[BeginEvent] | TaskSetManager is requested to xref:scheduler:TaskSetManager.adoc#resourceOffer[start a task] | [[workerRemoved]] workerRemoved | xref:scheduler:DAGSchedulerEvent.adoc#WorkerRemoved[WorkerRemoved] | TaskSchedulerImpl is requested to xref:scheduler:TaskSchedulerImpl.adoc#workerRemoved[handle a removed worker event] |=== == [[taskScheduler]] DAGScheduler and TaskScheduler DAGScheduler is given a xref:scheduler:TaskScheduler.adoc[TaskScheduler] when < >. DAGScheduler uses the TaskScheduler for the following: < > < > < > < > < > == [[runJob]] Running Job","title":"The introduction that follows was highly influenced by the scaladoc of https://github.com/apache/spark/blob/master/core/src/main/scala/org/apache/spark/scheduler/DAGScheduler.scala[org.apache.spark.scheduler.DAGScheduler]. As DAGScheduler is a private class it does not appear in the official API documentation. You are strongly encouraged to read https://github.com/apache/spark/blob/master/core/src/main/scala/org/apache/spark/scheduler/DAGScheduler.scala[the sources] and only then read this and the related pages afterwards."},{"location":"scheduler/DAGScheduler/#source-scala","text":"runJob T, U : Unit runJob submits an action job to the DAGScheduler and waits for a result. Internally, runJob executes < > and then waits until a result comes using xref:scheduler:spark-scheduler-JobWaiter.adoc[JobWaiter]. When the job succeeds, you should see the following INFO message in the logs: Job [jobId] finished: [callSite], took [time] s When the job fails, you should see the following INFO message in the logs and the exception (that led to the failure) is thrown. Job [jobId] failed: [callSite], took [time] s runJob is used when SparkContext is requested to xref:ROOT:SparkContext.adoc#runJob[run a job]. == [[cacheLocs]][[clearCacheLocs]] Partition Placement Preferences DAGScheduler keeps track of block locations per RDD and partition. DAGScheduler uses xref:scheduler:TaskLocation.adoc[TaskLocation] that includes a host name and an executor id on that host (as ExecutorCacheTaskLocation ). The keys are RDDs (their ids) and the values are arrays indexed by partition numbers. Each entry is a set of block locations where a RDD partition is cached, i.e. the xref:storage:BlockManager.adoc[BlockManager]s of the blocks. Initialized empty when < >. Used when DAGScheduler is requested for the < > or < >. == [[activeJobs]] ActiveJobs DAGScheduler tracks xref:scheduler:spark-scheduler-ActiveJob.adoc[ActiveJobs]: Adds a new ActiveJob when requested to handle < > or < > events Removes an ActiveJob when requested to < >. Removes all ActiveJobs when requested to < >. DAGScheduler uses ActiveJobs registry when requested to handle < > or < > events, to < > and to < >. The number of ActiveJobs is available using xref:metrics:spark-scheduler-DAGSchedulerSource.adoc#job.activeJobs[job.activeJobs] performance metric. == [[createResultStage]] Creating ResultStage for RDD","title":"[source, scala]"},{"location":"scheduler/DAGScheduler/#source-scala_1","text":"createResultStage( rdd: RDD[ ], func: (TaskContext, Iterator[ ]) => _, partitions: Array[Int], jobId: Int, callSite: CallSite): ResultStage createResultStage...FIXME createResultStage is used when DAGScheduler is requested to < >. == [[createShuffleMapStage]] Creating ShuffleMapStage for ShuffleDependency","title":"[source, scala]"},{"location":"scheduler/DAGScheduler/#source-scala_2","text":"createShuffleMapStage( shuffleDep: ShuffleDependency[_, _, _], jobId: Int): ShuffleMapStage createShuffleMapStage creates a xref:scheduler:ShuffleMapStage.adoc[ShuffleMapStage] for the given xref:rdd:ShuffleDependency.adoc[ShuffleDependency] as follows: Stage ID is generated based on < > internal counter RDD is taken from the given xref:rdd:ShuffleDependency.adoc#rdd[ShuffleDependency] Number of tasks is the number of xref:rdd:RDD.adoc#partitions[partitions] of the RDD < > < > createShuffleMapStage registers the ShuffleMapStage in the < > and < > internal registries. createShuffleMapStage < >. createShuffleMapStage requests the < > to xref:scheduler:MapOutputTrackerMaster.adoc#containsShuffle[check whether it contains the shuffle ID or not]. If not, createShuffleMapStage prints out the following INFO message to the logs and requests the < > to xref:scheduler:MapOutputTrackerMaster.adoc#registerShuffle[register the shuffle].","title":"[source, scala]"},{"location":"scheduler/DAGScheduler/#sourceplaintext","text":"","title":"[source,plaintext]"},{"location":"scheduler/DAGScheduler/#registering-rdd-id-creationsite-as-input-to-shuffle-shuffleid","text":".DAGScheduler Asks MapOutputTrackerMaster Whether Shuffle Map Output Is Already Tracked image::DAGScheduler-MapOutputTrackerMaster-containsShuffle.png[align=\"center\"] createShuffleMapStage is used when DAGScheduler is requested to < >. == [[cleanupStateForJobAndIndependentStages]] Cleaning Up After Job and Independent Stages","title":"Registering RDD [id] ([creationSite]) as input to shuffle [shuffleId]"},{"location":"scheduler/DAGScheduler/#source-scala_3","text":"cleanupStateForJobAndIndependentStages( job: ActiveJob): Unit cleanupStateForJobAndIndependentStages cleans up the state for job and any stages that are not part of any other job. cleanupStateForJobAndIndependentStages looks the job up in the internal < > registry. If no stages are found, the following ERROR is printed out to the logs: No stages registered for job [jobId] Oterwise, cleanupStateForJobAndIndependentStages uses < > registry to find the stages (the real objects not ids!). For each stage, cleanupStateForJobAndIndependentStages reads the jobs the stage belongs to. If the job does not belong to the jobs of the stage, the following ERROR is printed out to the logs: Job [jobId] not registered for stage [stageId] even though that stage was registered for the job If the job was the only job for the stage, the stage (and the stage id) gets cleaned up from the registries, i.e. < >, < >, < >, < > and < >. While removing from < >, you should see the following DEBUG message in the logs: Removing running stage [stageId] While removing from < >, you should see the following DEBUG message in the logs: Removing stage [stageId] from waiting set. While removing from < >, you should see the following DEBUG message in the logs: Removing stage [stageId] from failed set. After all cleaning (using < > as the source registry), if the stage belonged to the one and only job , you should see the following DEBUG message in the logs: After removal of stage [stageId], remaining stages = [stageIdToStage.size] The job is removed from < >, < >, < > registries. The final stage of the job is removed, i.e. xref:scheduler:ResultStage.adoc#removeActiveJob[ResultStage] or xref:scheduler:ShuffleMapStage.adoc#removeActiveJob[ShuffleMapStage]. cleanupStateForJobAndIndependentStages is used in xref:scheduler:DAGSchedulerEventProcessLoop.adoc#handleTaskCompletion-Success-ResultTask[handleTaskCompletion when a ResultTask has completed successfully], < > and < >. == [[markMapStageJobAsFinished]] Marking ShuffleMapStage Job Finished","title":"[source, scala]"},{"location":"scheduler/DAGScheduler/#source-scala_4","text":"markMapStageJobAsFinished( job: ActiveJob, stats: MapOutputStatistics): Unit markMapStageJobAsFinished marks the active job finished and notifies Spark listeners. Internally, markMapStageJobAsFinished marks the zeroth partition finished and increases the number of tasks finished in job . The xref:scheduler:spark-scheduler-JobListener.adoc#taskSucceeded[ job listener is notified about the 0 th task succeeded]. The < job and independent stages are cleaned up>>. Ultimately, xref:ROOT:SparkListener.adoc#SparkListenerJobEnd[SparkListenerJobEnd] is posted to xref:scheduler:LiveListenerBus.adoc[] (as < >) for the job , the current time (in millis) and JobSucceeded job result. markMapStageJobAsFinished is used in xref:scheduler:DAGSchedulerEventProcessLoop.adoc#handleMapStageSubmitted[handleMapStageSubmitted] and xref:scheduler:DAGSchedulerEventProcessLoop.adoc#handleTaskCompletion[handleTaskCompletion]. == [[getOrCreateParentStages]] Finding Or Creating Missing Direct Parent ShuffleMapStages (For ShuffleDependencies) of RDD","title":"[source, scala]"},{"location":"scheduler/DAGScheduler/#source-scala_5","text":"getOrCreateParentStages( rdd: RDD[_], firstJobId: Int): List[Stage] getOrCreateParentStages < ShuffleDependencies >> of the input rdd and then < ShuffleMapStage stages>> for each xref:rdd:ShuffleDependency.adoc[ShuffleDependency]. getOrCreateParentStages is used when DAGScheduler is requested to create a < > or a < >. == [[markStageAsFinished]] Marking Stage Finished","title":"[source, scala]"},{"location":"scheduler/DAGScheduler/#source-scala_6","text":"markStageAsFinished( stage: Stage, errorMessage: Option[String] = None, willRetry: Boolean = false): Unit markStageAsFinished...FIXME markStageAsFinished is used when...FIXME == [[getOrCreateShuffleMapStage]] Finding or Creating ShuffleMapStage for ShuffleDependency","title":"[source, scala]"},{"location":"scheduler/DAGScheduler/#source-scala_7","text":"getOrCreateShuffleMapStage( shuffleDep: ShuffleDependency[_, _, _], firstJobId: Int): ShuffleMapStage getOrCreateShuffleMapStage finds the xref:scheduler:ShuffleMapStage.adoc[ShuffleMapStage] in the < > internal registry and returns it if available. If not found, getOrCreateShuffleMapStage < > and < > (including one for the input ShuffleDependency). getOrCreateShuffleMapStage is used when DAGScheduler is requested to < >, < >, < >, and < >. == [[getMissingAncestorShuffleDependencies]] Finding Missing ShuffleDependencies For RDD","title":"[source, scala]"},{"location":"scheduler/DAGScheduler/#source-scala_8","text":"getMissingAncestorShuffleDependencies( rdd: RDD[ ]): Stack[ShuffleDependency[ , _, _]] getMissingAncestorShuffleDependencies finds all missing xref:rdd:ShuffleDependency.adoc[shuffle dependencies] for the given xref:rdd:index.adoc[RDD] traversing its xref:rdd:spark-rdd-lineage.adoc[RDD lineage]. NOTE: A missing shuffle dependency of a RDD is a dependency not registered in < shuffleIdToMapStage internal registry>>. Internally, getMissingAncestorShuffleDependencies < >\u2009of the input RDD and collects the ones that are not registered in < shuffleIdToMapStage internal registry>>. It repeats the process for the RDDs of the parent shuffle dependencies. getMissingAncestorShuffleDependencies is used when DAGScheduler is requested to < >. == [[getShuffleDependencies]] Finding Direct Parent Shuffle Dependencies of RDD","title":"[source, scala]"},{"location":"scheduler/DAGScheduler/#source-scala_9","text":"getShuffleDependencies( rdd: RDD[ ]): HashSet[ShuffleDependency[ , _, _]] getShuffleDependencies finds direct parent xref:rdd:ShuffleDependency.adoc[shuffle dependencies] for the given xref:rdd:index.adoc[RDD]. .getShuffleDependencies Finds Direct Parent ShuffleDependencies (shuffle1 and shuffle2) image::spark-DAGScheduler-getShuffleDependencies.png[align=\"center\"] Internally, getShuffleDependencies takes the direct xref:rdd:index.adoc#dependencies[shuffle dependencies of the input RDD] and direct shuffle dependencies of all the parent non- ShuffleDependencies in the xref:rdd:spark-rdd-lineage.adoc[dependency chain] (aka RDD lineage ). getShuffleDependencies is used when DAGScheduler is requested to < > (for ShuffleDependencies of a RDD) and < >. == [[failJobAndIndependentStages]] Failing Job and Independent Single-Job Stages","title":"[source, scala]"},{"location":"scheduler/DAGScheduler/#source-scala_10","text":"failJobAndIndependentStages( job: ActiveJob, failureReason: String, exception: Option[Throwable] = None): Unit failJobAndIndependentStages fails the input job and all the stages that are only used by the job. Internally, failJobAndIndependentStages uses < jobIdToStageIds internal registry>> to look up the stages registered for the job. If no stages could be found, you should see the following ERROR message in the logs: No stages registered for job [id] Otherwise, for every stage, failJobAndIndependentStages finds the job ids the stage belongs to. If no stages could be found or the job is not referenced by the stages, you should see the following ERROR message in the logs: Job [id] not registered for stage [id] even though that stage was registered for the job Only when there is exactly one job registered for the stage and the stage is in RUNNING state (in runningStages internal registry), xref:scheduler:TaskScheduler.adoc#contract[ TaskScheduler is requested to cancel the stage's tasks] and < >. NOTE: failJobAndIndependentStages uses < >, < >, and < > internal registries. failJobAndIndependentStages is used when...FIXME == [[abortStage]] Aborting Stage","title":"[source, scala]"},{"location":"scheduler/DAGScheduler/#source-scala_11","text":"abortStage( failedStage: Stage, reason: String, exception: Option[Throwable]): Unit abortStage is an internal method that finds all the active jobs that depend on the failedStage stage and fails them. Internally, abortStage looks the failedStage stage up in the internal < > registry and exits if there the stage was not registered earlier. If it was, abortStage finds all the active jobs (in the internal < > registry) with the < failedStage stage>>. At this time, the completionTime property (of the failed stage's xref:scheduler:spark-scheduler-StageInfo.adoc[StageInfo]) is assigned to the current time (millis). All the active jobs that depend on the failed stage (as calculated above) and the stages that do not belong to other jobs (aka independent stages ) are < > (with the failure reason being \"Job aborted due to stage failure: [reason]\" and the input exception ). If there are no jobs depending on the failed stage, you should see the following INFO message in the logs:","title":"[source, scala]"},{"location":"scheduler/DAGScheduler/#sourceplaintext_1","text":"","title":"[source,plaintext]"},{"location":"scheduler/DAGScheduler/#ignoring-failure-of-failedstage-because-all-jobs-depending-on-it-are-done","text":"abortStage is used when DAGScheduler is requested to < >, < >, < >, < >. == [[stageDependsOn]] Checking Out Stage Dependency on Given Stage","title":"Ignoring failure of [failedStage] because all jobs depending on it are done"},{"location":"scheduler/DAGScheduler/#source-scala_12","text":"stageDependsOn( stage: Stage, target: Stage): Boolean stageDependsOn compares two stages and returns whether the stage depends on target stage (i.e. true ) or not (i.e. false ). NOTE: A stage A depends on stage B if B is among the ancestors of A . Internally, stageDependsOn walks through the graph of RDDs of the input stage . For every RDD in the RDD's dependencies (using RDD.dependencies ) stageDependsOn adds the RDD of a xref:rdd:spark-rdd-NarrowDependency.adoc[NarrowDependency] to a stack of RDDs to visit while for a xref:rdd:ShuffleDependency.adoc[ShuffleDependency] it < ShuffleMapStage stages for a ShuffleDependency >> for the dependency and the stage 's first job id that it later adds to a stack of RDDs to visit if the map stage is ready, i.e. all the partitions have shuffle outputs. After all the RDDs of the input stage are visited, stageDependsOn checks if the target 's RDD is among the RDDs of the stage , i.e. whether the stage depends on target stage. stageDependsOn is used when DAGScheduler is requested to < >. == [[submitWaitingChildStages]] Submitting Waiting Child Stages for Execution","title":"[source, scala]"},{"location":"scheduler/DAGScheduler/#source-scala_13","text":"submitWaitingChildStages( parent: Stage): Unit submitWaitingChildStages submits for execution all waiting stages for which the input parent xref:scheduler:Stage.adoc[Stage] is the direct parent. NOTE: Waiting stages are the stages registered in < waitingStages internal registry>>. When executed, you should see the following TRACE messages in the logs: Checking if any dependencies of [parent] are now runnable running: [runningStages] waiting: [waitingStages] failed: [failedStages] submitWaitingChildStages finds child stages of the input parent stage, removes them from waitingStages internal registry, and < > one by one sorted by their job ids. submitWaitingChildStages is used when DAGScheduler is requested to < > and < >. == [[submitStage]] Submitting Stage (with Missing Parents) for Execution","title":"[source, scala]"},{"location":"scheduler/DAGScheduler/#source-scala_14","text":"submitStage( stage: Stage): Unit submitStage submits the input stage or its missing parents (if there any stages not computed yet before the input stage could). NOTE: submitStage is also used to xref:scheduler:DAGSchedulerEventProcessLoop.adoc#resubmitFailedStages[resubmit failed stages]. submitStage recursively submits any missing parents of the stage . Internally, submitStage first finds the earliest-created job id that needs the stage . NOTE: A stage itself tracks the jobs (their ids) it belongs to (using the internal jobIds registry). The following steps depend on whether there is a job or not. If there are no jobs that require the stage , submitStage < > with the reason: No active job for stage [id] If however there is a job for the stage , you should see the following DEBUG message in the logs: submitStage([stage]) submitStage checks the status of the stage and continues when it was not recorded in < >, < > or < > internal registries. It simply exits otherwise. With the stage ready for submission, submitStage calculates the < stage >> (sorted by their job ids). You should see the following DEBUG message in the logs: missing: [missing] When the stage has no parent stages missing, you should see the following INFO message in the logs: Submitting [stage] ([stage.rdd]), which has no missing parents submitStage < stage >> (with the earliest-created job id) and finishes. If however there are missing parent stages for the stage , submitStage < >, and the stage is recorded in the internal < > registry. submitStage is used recursively for missing parents of the given stage and when DAGScheduler is requested for the following: < > (ResubmitFailedStages event) < > (CompletionEvent event) Handle < >, < > and < > events == [[stage-attempts]] Stage Attempts A single stage can be re-executed in multiple attempts due to fault recovery. The number of attempts is configured (FIXME). If TaskScheduler reports that a task failed because a map output file from a previous stage was lost, the DAGScheduler resubmits the lost stage. This is detected through a xref:scheduler:DAGSchedulerEventProcessLoop.adoc#handleTaskCompletion-FetchFailed[ CompletionEvent with FetchFailed ], or an < > event. DAGScheduler will wait a small amount of time to see whether other nodes or tasks fail, then resubmit TaskSets for any lost stage(s) that compute the missing tasks. Please note that tasks from the old attempts of a stage could still be running. A stage object tracks multiple xref:scheduler:spark-scheduler-StageInfo.adoc[StageInfo] objects to pass to Spark listeners or the web UI. The latest StageInfo for the most recent attempt for a stage is accessible through latestInfo . == [[preferred-locations]] Preferred Locations DAGScheduler computes where to run each task in a stage based on the xref:rdd:index.adoc#getPreferredLocations[preferred locations of its underlying RDDs], or < >. == [[adaptive-query-planning]] Adaptive Query Planning / Adaptive Scheduling See https://issues.apache.org/jira/browse/SPARK-9850[SPARK-9850 Adaptive execution in Spark] for the design document. The work is currently in progress. https://github.com/apache/spark/blob/master/core/src/main/scala/org/apache/spark/scheduler/DAGScheduler.scala#L661[DAGScheduler.submitMapStage ] method is used for adaptive query planning, to run map stages and look at statistics about their outputs before submitting downstream stages. == ScheduledExecutorService daemon services DAGScheduler uses the following ScheduledThreadPoolExecutors (with the policy of removing cancelled tasks from a work queue at time of cancellation): dag-scheduler-message - a daemon thread pool using j.u.c.ScheduledThreadPoolExecutor with core pool size 1 . It is used to post a xref:scheduler:DAGSchedulerEventProcessLoop.adoc#ResubmitFailedStages[ResubmitFailedStages] event when xref:scheduler:DAGSchedulerEventProcessLoop.adoc#handleTaskCompletion-FetchFailed[ FetchFailed is reported]. They are created using ThreadUtils.newDaemonSingleThreadScheduledExecutor method that uses Guava DSL to instantiate a ThreadFactory. == [[getMissingParentStages]] Finding Missing Parent ShuffleMapStages For Stage","title":"[source, scala]"},{"location":"scheduler/DAGScheduler/#source-scala_15","text":"getMissingParentStages( stage: Stage): List[Stage] getMissingParentStages finds missing parent xref:scheduler:ShuffleMapStage.adoc[ShuffleMapStage]s in the dependency graph of the input stage (using the https://en.wikipedia.org/wiki/Breadth-first_search[breadth-first search algorithm]). Internally, getMissingParentStages starts with the stage 's RDD and walks up the tree of all parent RDDs to find < >. NOTE: A Stage tracks the associated RDD using xref:scheduler:Stage.adoc#rdd[ rdd property]. NOTE: An uncached partition of a RDD is a partition that has Nil in the < > (which results in no RDD blocks in any of the active xref:storage:BlockManager.adoc[BlockManager]s on executors). getMissingParentStages traverses the xref:rdd:index.adoc#dependencies[parent dependencies of the RDD] and acts according to their type, i.e. xref:rdd:ShuffleDependency.adoc[ShuffleDependency] or xref:rdd:spark-rdd-NarrowDependency.adoc[NarrowDependency]. NOTE: xref:rdd:ShuffleDependency.adoc[ShuffleDependency] and xref:rdd:spark-rdd-NarrowDependency.adoc[NarrowDependency] are the main top-level xref:rdd:spark-rdd-Dependency.adoc[Dependencies]. For each NarrowDependency , getMissingParentStages simply marks the corresponding RDD to visit and moves on to a next dependency of a RDD or works on another unvisited parent RDD. NOTE: xref:rdd:spark-rdd-NarrowDependency.adoc[NarrowDependency] is a RDD dependency that allows for pipelined execution. getMissingParentStages focuses on ShuffleDependency dependencies. NOTE: xref:rdd:ShuffleDependency.adoc[ShuffleDependency] is a RDD dependency that represents a dependency on the output of a xref:scheduler:ShuffleMapStage.adoc[ShuffleMapStage], i.e. shuffle map stage . For each ShuffleDependency , getMissingParentStages < ShuffleMapStage stages>>. If the ShuffleMapStage is not available , it is added to the set of missing (map) stages. NOTE: A ShuffleMapStage is available when all its partitions are computed, i.e. results are available (as blocks). CAUTION: FIXME...IMAGE with ShuffleDependencies queried getMissingParentStages is used when DAGScheduler is requested to < > and handle < > and < > events. == [[submitMissingTasks]] Submitting Missing Tasks of Stage","title":"[source, scala]"},{"location":"scheduler/DAGScheduler/#source-scala_16","text":"submitMissingTasks( stage: Stage, jobId: Int): Unit submitMissingTasks prints out the following DEBUG message to the logs: submitMissingTasks([stage]) submitMissingTasks requests the given xref:scheduler:Stage.adoc[Stage] for the xref:scheduler:Stage.adoc#findMissingPartitions[missing partitions] (partitions that need to be computed). submitMissingTasks adds the stage to the < > internal registry. submitMissingTasks notifies the < > that xref:scheduler:OutputCommitCoordinator.adoc#stageStart[stage execution started]. [[submitMissingTasks-taskIdToLocations]] submitMissingTasks < > ( task locality preferences ) of the missing partitions. submitMissingTasks requests the stage for a xref:scheduler:Stage.adoc#makeNewStageAttempt[new stage attempt]. submitMissingTasks requests the < > to xref:scheduler:LiveListenerBus.adoc#post[post] a xref:ROOT:SparkListener.adoc#SparkListenerStageSubmitted[SparkListenerStageSubmitted] event. submitMissingTasks uses the < > to xref:serializer:Serializer.adoc#serialize[serialize] the stage and create a so-called task binary. submitMissingTasks serializes the RDD (of the stage) and either the ShuffleDependency or the compute function based on the type of the stage, i.e. ShuffleMapStage and ResultStage, respectively. submitMissingTasks creates a xref:ROOT:SparkContext.adoc#broadcast[broadcast variable] for the task binary. NOTE: That shows how important xref:ROOT:Broadcast.adoc[]s are for Spark itself to distribute data among executors in a Spark application in the most efficient way. submitMissingTasks creates xref:scheduler:Task.adoc[tasks] for every missing partition: xref:scheduler:ShuffleMapTask.adoc[ShuffleMapTasks] for a xref:scheduler:ShuffleMapStage.adoc[ShuffleMapStage] xref:scheduler:ResultTask.adoc[ResultTasks] for a xref:scheduler:ResultStage.adoc[ResultStage] If there are tasks to submit for execution (i.e. there are missing partitions in the stage), submitMissingTasks prints out the following INFO message to the logs: Submitting [size] missing tasks from [stage] ([rdd]) (first 15 tasks are for partitions [partitionIds]) submitMissingTasks requests the < > to xref:scheduler:TaskScheduler.adoc#submitTasks[submit the tasks for execution] (as a new xref:scheduler:TaskSet.adoc[TaskSet]). With no tasks to submit for execution, submitMissingTasks < >. submitMissingTasks prints out the following DEBUG messages based on the type of the stage: Stage [stage] is actually done; (available: [isAvailable],available outputs: [numAvailableOutputs],partitions: [numPartitions]) or Stage [stage] is actually done; (partitions: [numPartitions]) for ShuffleMapStage and ResultStage , respectively. In the end, with no tasks to submit for execution, submitMissingTasks < > and exits. submitMissingTasks is used when DAGScheduler is requested to < >. == [[getPreferredLocs]] Finding Preferred Locations for Missing Partitions","title":"[source, scala]"},{"location":"scheduler/DAGScheduler/#source-scala_17","text":"getPreferredLocs( rdd: RDD[_], partition: Int): Seq[TaskLocation] getPreferredLocs is simply an alias for the internal (recursive) < >. getPreferredLocs is used when...FIXME == [[getCacheLocs]] Finding BlockManagers (Executors) for Cached RDD Partitions (aka Block Location Discovery)","title":"[source, scala]"},{"location":"scheduler/DAGScheduler/#source-scala_18","text":"getCacheLocs( rdd: RDD[_]): IndexedSeq[Seq[TaskLocation]] getCacheLocs gives xref:scheduler:TaskLocation.adoc[TaskLocations] (block locations) for the partitions of the input rdd . getCacheLocs caches lookup results in < > internal registry. NOTE: The size of the collection from getCacheLocs is exactly the number of partitions in rdd RDD. NOTE: The size of every xref:scheduler:TaskLocation.adoc[TaskLocation] collection (i.e. every entry in the result of getCacheLocs) is exactly the number of blocks managed using xref:storage:BlockManager.adoc[BlockManagers] on executors. Internally, getCacheLocs finds rdd in the < > internal registry (of partition locations per RDD). If rdd is not in < > internal registry, getCacheLocs branches per its xref:storage:StorageLevel.adoc[storage level]. For NONE storage level (i.e. no caching), the result is an empty locations (i.e. no location preference). For other non- NONE storage levels, getCacheLocs xref:storage:BlockManagerMaster.adoc#getLocations-block-array[requests BlockManagerMaster for block locations] that are then mapped to xref:scheduler:TaskLocation.adoc[TaskLocations] with the hostname of the owning BlockManager for a block (of a partition) and the executor id. NOTE: getCacheLocs uses < > that was defined when < >. getCacheLocs records the computed block locations per partition (as xref:scheduler:TaskLocation.adoc[TaskLocation]) in < > internal registry. NOTE: getCacheLocs requests locations from BlockManagerMaster using xref:storage:BlockId.adoc#RDDBlockId[RDDBlockId] with the RDD id and the partition indices (which implies that the order of the partitions matters to request proper blocks). NOTE: DAGScheduler uses xref:scheduler:TaskLocation.adoc[TaskLocations] (with host and executor) while xref:storage:BlockManagerMaster.adoc[BlockManagerMaster] uses xref:storage:BlockManagerId.adoc[] (to track similar information, i.e. block locations). getCacheLocs is used when DAGScheduler is requested to finds < > and < >. == [[getPreferredLocsInternal]] Finding Placement Preferences for RDD Partition (recursively)","title":"[source, scala]"},{"location":"scheduler/DAGScheduler/#source-scala_19","text":"getPreferredLocsInternal( rdd: RDD[ ], partition: Int, visited: HashSet[(RDD[ ], Int)]): Seq[TaskLocation] getPreferredLocsInternal first < TaskLocations for the partition of the rdd >> (using < > internal cache) and returns them. Otherwise, if not found, getPreferredLocsInternal xref:rdd:index.adoc#preferredLocations[requests rdd for the preferred locations of partition ] and returns them. NOTE: Preferred locations of the partitions of a RDD are also called placement preferences or locality preferences . Otherwise, if not found, getPreferredLocsInternal finds the first parent xref:rdd:spark-rdd-NarrowDependency.adoc[NarrowDependency] and (recursively) < TaskLocations >>. If all the attempts fail to yield any non-empty result, getPreferredLocsInternal returns an empty collection of xref:scheduler:TaskLocation.adoc[TaskLocations]. getPreferredLocsInternal is used when DAGScheduler is requested for the < >. == [[stop]] Stopping DAGScheduler","title":"[source, scala]"},{"location":"scheduler/DAGScheduler/#source-scala_20","text":"","title":"[source, scala]"},{"location":"scheduler/DAGScheduler/#stop-unit","text":"stop stops the internal dag-scheduler-message thread pool, < >, and xref:scheduler:TaskScheduler.adoc#stop[TaskScheduler]. stop is used when...FIXME == [[updateAccumulators]] Updating Accumulators with Partial Values from Completed Tasks","title":"stop(): Unit"},{"location":"scheduler/DAGScheduler/#source-scala_21","text":"updateAccumulators( event: CompletionEvent): Unit updateAccumulators merges the partial values of accumulators from a completed task into their \"source\" accumulators on the driver. NOTE: It is called by < >. For each xref:ROOT:spark-accumulators.adoc#AccumulableInfo[AccumulableInfo] in the CompletionEvent , a partial value from a task is obtained (from AccumulableInfo.update ) and added to the driver's accumulator (using Accumulable.++= method). For named accumulators with the update value being a non-zero value, i.e. not Accumulable.zero : stage.latestInfo.accumulables for the AccumulableInfo.id is set CompletionEvent.taskInfo.accumulables has a new xref:ROOT:spark-accumulators.adoc#AccumulableInfo[AccumulableInfo] added. CAUTION: FIXME Where are Stage.latestInfo.accumulables and CompletionEvent.taskInfo.accumulables used? updateAccumulators is used when DAGScheduler is requested to < >. == [[checkBarrierStageWithNumSlots]] checkBarrierStageWithNumSlots Method","title":"[source, scala]"},{"location":"scheduler/DAGScheduler/#source-scala_22","text":"checkBarrierStageWithNumSlots( rdd: RDD[_]): Unit checkBarrierStageWithNumSlots...FIXME checkBarrierStageWithNumSlots is used when DAGScheduler is requested to create < > and < > stages. == [[killTaskAttempt]] Killing Task","title":"[source, scala]"},{"location":"scheduler/DAGScheduler/#source-scala_23","text":"killTaskAttempt( taskId: Long, interruptThread: Boolean, reason: String): Boolean killTaskAttempt requests the < > to xref:scheduler:TaskScheduler.adoc#killTaskAttempt[kill a task]. killTaskAttempt is used when SparkContext is requested to xref:ROOT:SparkContext.adoc#killTaskAttempt[kill a task]. == [[cleanUpAfterSchedulerStop]] cleanUpAfterSchedulerStop Method","title":"[source, scala]"},{"location":"scheduler/DAGScheduler/#source-scala_24","text":"","title":"[source, scala]"},{"location":"scheduler/DAGScheduler/#cleanupafterschedulerstop-unit","text":"cleanUpAfterSchedulerStop...FIXME cleanUpAfterSchedulerStop is used when DAGSchedulerEventProcessLoop is requested to xref:scheduler:DAGSchedulerEventProcessLoop.adoc#onStop[onStop]. == [[removeExecutorAndUnregisterOutputs]] removeExecutorAndUnregisterOutputs Method","title":"cleanUpAfterSchedulerStop(): Unit"},{"location":"scheduler/DAGScheduler/#source-scala_25","text":"removeExecutorAndUnregisterOutputs( execId: String, fileLost: Boolean, hostToUnregisterOutputs: Option[String], maybeEpoch: Option[Long] = None): Unit removeExecutorAndUnregisterOutputs...FIXME removeExecutorAndUnregisterOutputs is used when DAGScheduler is requested to handle < > (due to a fetch failure) and < > events. == [[markMapStageJobsAsFinished]] markMapStageJobsAsFinished Method","title":"[source, scala]"},{"location":"scheduler/DAGScheduler/#source-scala_26","text":"markMapStageJobsAsFinished( shuffleStage: ShuffleMapStage): Unit markMapStageJobsAsFinished...FIXME markMapStageJobsAsFinished is used when DAGScheduler is requested to < > (of a ShuffleMapStage that has just been computed) and < > (of a ShuffleMapStage). == [[updateJobIdStageIdMaps]] updateJobIdStageIdMaps Method","title":"[source, scala]"},{"location":"scheduler/DAGScheduler/#source-scala_27","text":"updateJobIdStageIdMaps( jobId: Int, stage: Stage): Unit updateJobIdStageIdMaps...FIXME updateJobIdStageIdMaps is used when DAGScheduler is requested to create < > and < > stages. == [[executorHeartbeatReceived]] executorHeartbeatReceived Method","title":"[source, scala]"},{"location":"scheduler/DAGScheduler/#source-scala_28","text":"executorHeartbeatReceived( execId: String, // (taskId, stageId, stageAttemptId, accumUpdates) accumUpdates: Array[(Long, Int, Int, Seq[AccumulableInfo])], blockManagerId: BlockManagerId): Boolean executorHeartbeatReceived posts a xref:ROOT:SparkListener.adoc#SparkListenerExecutorMetricsUpdate[SparkListenerExecutorMetricsUpdate] (to < >) and informs xref:storage:BlockManagerMaster.adoc[BlockManagerMaster] that blockManagerId block manager is alive (by posting xref:storage:BlockManagerMaster.adoc#BlockManagerHeartbeat[BlockManagerHeartbeat]). executorHeartbeatReceived is used when TaskSchedulerImpl is requested to xref:scheduler:TaskSchedulerImpl.adoc#executorHeartbeatReceived[handle an executor heartbeat]. == [[postTaskEnd]] postTaskEnd Method","title":"[source, scala]"},{"location":"scheduler/DAGScheduler/#source-scala_29","text":"postTaskEnd( event: CompletionEvent): Unit postTaskEnd...FIXME postTaskEnd is used when DAGScheduler is requested to < >. == Event Handlers === [[doCancelAllJobs]] AllJobsCancelled Event Handler","title":"[source, scala]"},{"location":"scheduler/DAGScheduler/#source-scala_30","text":"","title":"[source, scala]"},{"location":"scheduler/DAGScheduler/#docancelalljobs-unit","text":"doCancelAllJobs...FIXME doCancelAllJobs is used when DAGSchedulerEventProcessLoop is requested to handle an xref:scheduler:DAGSchedulerEventProcessLoop.adoc#AllJobsCancelled[AllJobsCancelled] event and xref:scheduler:DAGSchedulerEventProcessLoop.adoc#onError[onError]. === [[handleBeginEvent]] BeginEvent Event Handler","title":"doCancelAllJobs(): Unit"},{"location":"scheduler/DAGScheduler/#source-scala_31","text":"handleBeginEvent( task: Task[_], taskInfo: TaskInfo): Unit handleBeginEvent...FIXME handleBeginEvent is used when DAGSchedulerEventProcessLoop is requested to handle a xref:scheduler:DAGSchedulerEvent.adoc#BeginEvent[BeginEvent] event. === [[handleTaskCompletion]] CompletionEvent Event Handler","title":"[source, scala]"},{"location":"scheduler/DAGScheduler/#source-scala_32","text":"handleTaskCompletion( event: CompletionEvent): Unit handleTaskCompletion...FIXME handleTaskCompletion is used when DAGSchedulerEventProcessLoop is requested to handle a xref:scheduler:DAGSchedulerEvent.adoc#CompletionEvent[CompletionEvent] event. === [[handleExecutorAdded]] ExecutorAdded Event Handler","title":"[source, scala]"},{"location":"scheduler/DAGScheduler/#source-scala_33","text":"handleExecutorAdded( execId: String, host: String): Unit handleExecutorAdded...FIXME handleExecutorAdded is used when DAGSchedulerEventProcessLoop is requested to handle an xref:scheduler:DAGSchedulerEvent.adoc#ExecutorAdded[ExecutorAdded] event. === [[handleExecutorLost]] ExecutorLost Event Handler","title":"[source, scala]"},{"location":"scheduler/DAGScheduler/#source-scala_34","text":"handleExecutorLost( execId: String, workerLost: Boolean): Unit handleExecutorLost...FIXME handleExecutorLost is used when DAGSchedulerEventProcessLoop is requested to handle an xref:scheduler:DAGSchedulerEvent.adoc#ExecutorLost[ExecutorLost] event. === [[handleGetTaskResult]] GettingResultEvent Event Handler","title":"[source, scala]"},{"location":"scheduler/DAGScheduler/#source-scala_35","text":"handleGetTaskResult( taskInfo: TaskInfo): Unit handleGetTaskResult...FIXME handleGetTaskResult is used when DAGSchedulerEventProcessLoop is requested to handle a xref:scheduler:DAGSchedulerEvent.adoc#GettingResultEvent[GettingResultEvent] event. === [[handleJobCancellation]] JobCancelled Event Handler","title":"[source, scala]"},{"location":"scheduler/DAGScheduler/#source-scala_36","text":"handleJobCancellation( jobId: Int, reason: Option[String]): Unit handleJobCancellation...FIXME handleJobCancellation is used when DAGScheduler is requested to handle a xref:scheduler:DAGSchedulerEvent.adoc#JobCancelled[JobCancelled] event, < >, < >, < >. === [[handleJobGroupCancelled]] JobGroupCancelled Event Handler","title":"[source, scala]"},{"location":"scheduler/DAGScheduler/#source-scala_37","text":"handleJobGroupCancelled( groupId: String): Unit handleJobGroupCancelled...FIXME handleJobGroupCancelled is used when DAGScheduler is requested to handle xref:scheduler:DAGSchedulerEvent.adoc#JobGroupCancelled[JobGroupCancelled] event. === [[handleJobSubmitted]] JobSubmitted Event Handler","title":"[source, scala]"},{"location":"scheduler/DAGScheduler/#source-scala_38","text":"handleJobSubmitted( jobId: Int, finalRDD: RDD[ ], func: (TaskContext, Iterator[ ]) => _, partitions: Array[Int], callSite: CallSite, listener: JobListener, properties: Properties): Unit handleJobSubmitted xref:scheduler:DAGScheduler.adoc#createResultStage[creates a new ResultStage ] (as finalStage in the picture below) given the input finalRDD , func , partitions , jobId and callSite . . DAGScheduler.handleJobSubmitted Method image::dagscheduler-handleJobSubmitted.png[align=\"center\"] handleJobSubmitted creates an xref:scheduler:spark-scheduler-ActiveJob.adoc[ActiveJob] (with the input jobId , callSite , listener , properties , and the xref:scheduler:ResultStage.adoc[ResultStage]). handleJobSubmitted xref:scheduler:DAGScheduler.adoc#clearCacheLocs[clears the internal cache of RDD partition locations]. CAUTION: FIXME Why is this clearing here so important? You should see the following INFO messages in the logs: Got job [id] ([callSite]) with [number] output partitions Final stage: [stage] ([name]) Parents of final stage: [parents] Missing parents: [missingStages] handleJobSubmitted then registers the new job in xref:scheduler:DAGScheduler.adoc#jobIdToActiveJob[jobIdToActiveJob] and xref:scheduler:DAGScheduler.adoc#activeJobs[activeJobs] internal registries, and xref:scheduler:ResultStage.adoc#setActiveJob[with the final ResultStage ]. NOTE: ResultStage can only have one ActiveJob registered. handleJobSubmitted xref:scheduler:DAGScheduler.adoc#jobIdToStageIds[finds all the registered stages for the input jobId ] and collects xref:scheduler:Stage.adoc#latestInfo[their latest StageInfo ]. In the end, handleJobSubmitted posts xref:ROOT:SparkListener.adoc#SparkListenerJobStart[SparkListenerJobStart] message to xref:scheduler:LiveListenerBus.adoc[] and xref:scheduler:DAGScheduler.adoc#submitStage[submits the stage]. handleJobSubmitted is used when DAGSchedulerEventProcessLoop is requested to handle a xref:scheduler:DAGSchedulerEvent.adoc#JobSubmitted[JobSubmitted] event. === [[handleMapStageSubmitted]] MapStageSubmitted Event Handler","title":"[source, scala]"},{"location":"scheduler/DAGScheduler/#source-scala_39","text":"handleMapStageSubmitted( jobId: Int, dependency: ShuffleDependency[_, _, _], callSite: CallSite, listener: JobListener, properties: Properties): Unit handleMapStageSubmitted...FIXME handleMapStageSubmitted is used when DAGSchedulerEventProcessLoop is requested to handle a xref:scheduler:DAGSchedulerEvent.adoc#MapStageSubmitted[MapStageSubmitted] event. === [[resubmitFailedStages]] ResubmitFailedStages Event Handler","title":"[source, scala]"},{"location":"scheduler/DAGScheduler/#source-scala_40","text":"","title":"[source, scala]"},{"location":"scheduler/DAGScheduler/#resubmitfailedstages-unit","text":"resubmitFailedStages...FIXME resubmitFailedStages is used when DAGSchedulerEventProcessLoop is requested to handle a xref:scheduler:DAGSchedulerEvent.adoc#ResubmitFailedStages[ResubmitFailedStages] event. === [[handleSpeculativeTaskSubmitted]] SpeculativeTaskSubmitted Event Handler","title":"resubmitFailedStages(): Unit"},{"location":"scheduler/DAGScheduler/#source-scala_41","text":"","title":"[source, scala]"},{"location":"scheduler/DAGScheduler/#handlespeculativetasksubmitted-unit","text":"handleSpeculativeTaskSubmitted...FIXME handleSpeculativeTaskSubmitted is used when DAGSchedulerEventProcessLoop is requested to handle a xref:scheduler:DAGSchedulerEvent.adoc#SpeculativeTaskSubmitted[SpeculativeTaskSubmitted] event. === [[handleStageCancellation]] StageCancelled Event Handler","title":"handleSpeculativeTaskSubmitted(): Unit"},{"location":"scheduler/DAGScheduler/#source-scala_42","text":"","title":"[source, scala]"},{"location":"scheduler/DAGScheduler/#handlestagecancellation-unit","text":"handleStageCancellation...FIXME handleStageCancellation is used when DAGSchedulerEventProcessLoop is requested to handle a xref:scheduler:DAGSchedulerEvent.adoc#StageCancelled[StageCancelled] event. === [[handleTaskSetFailed]] TaskSetFailed Event Handler","title":"handleStageCancellation(): Unit"},{"location":"scheduler/DAGScheduler/#source-scala_43","text":"","title":"[source, scala]"},{"location":"scheduler/DAGScheduler/#handletasksetfailed-unit","text":"handleTaskSetFailed...FIXME handleTaskSetFailed is used when DAGSchedulerEventProcessLoop is requested to handle a xref:scheduler:DAGSchedulerEvent.adoc#TaskSetFailed[TaskSetFailed] event. === [[handleWorkerRemoved]] WorkerRemoved Event Handler","title":"handleTaskSetFailed(): Unit"},{"location":"scheduler/DAGScheduler/#source-scala_44","text":"handleWorkerRemoved( workerId: String, host: String, message: String): Unit handleWorkerRemoved...FIXME handleWorkerRemoved is used when DAGSchedulerEventProcessLoop is requested to handle a xref:scheduler:DAGSchedulerEvent.adoc#WorkerRemoved[WorkerRemoved] event. == [[logging]] Logging Enable ALL logging level for org.apache.spark.scheduler.DAGScheduler logger to see what happens inside. Add the following line to conf/log4j.properties :","title":"[source, scala]"},{"location":"scheduler/DAGScheduler/#source","text":"","title":"[source]"},{"location":"scheduler/DAGScheduler/#log4jloggerorgapachesparkschedulerdagschedulerall","text":"Refer to xref:ROOT:spark-logging.adoc[Logging]. == [[internal-properties]] Internal Properties [cols=\"30m,70\",options=\"header\",width=\"100%\"] |=== | Name | Description | failedEpoch | [[failedEpoch]] The lookup table of lost executors and the epoch of the event. | failedStages | [[failedStages]] Stages that failed due to fetch failures (when a xref:scheduler:DAGSchedulerEventProcessLoop.adoc#handleTaskCompletion-FetchFailed[task fails with FetchFailed exception]). | jobIdToActiveJob | [[jobIdToActiveJob]] The lookup table of ActiveJob s per job id. | jobIdToStageIds | [[jobIdToStageIds]] The lookup table of all stages per ActiveJob id | metricsSource | [[metricsSource]] xref:metrics:spark-scheduler-DAGSchedulerSource.adoc[DAGSchedulerSource] | nextJobId | [[nextJobId]] The next job id counting from 0 . Used when DAGScheduler < > and < >, and < >. | nextStageId | [[nextStageId]] The next stage id counting from 0 . Used when DAGScheduler creates a < > and a < >. It is the key in < >. | runningStages | [[runningStages]] The set of stages that are currently \"running\". A stage is added when < > gets executed (without first checking if the stage has not already been added). | shuffleIdToMapStage | [[shuffleIdToMapStage]] The lookup table of xref:scheduler:ShuffleMapStage.adoc[ShuffleMapStage]s per xref:rdd:ShuffleDependency.adoc[ShuffleDependency]. | stageIdToStage | [[stageIdToStage]] The lookup table for stages per their ids. Used when DAGScheduler < >, < >, < >, is informed that xref:scheduler:DAGSchedulerEventProcessLoop.adoc#handleBeginEvent[a task is started], xref:scheduler:DAGSchedulerEventProcessLoop.adoc#handleTaskSetFailed[a taskset has failed], xref:scheduler:DAGSchedulerEventProcessLoop.adoc#handleJobSubmitted[a job is submitted (to compute a ResultStage )], xref:scheduler:DAGSchedulerEventProcessLoop.adoc#handleMapStageSubmitted[a map stage was submitted], xref:scheduler:DAGSchedulerEventProcessLoop.adoc#handleTaskCompletion[a task has completed] or xref:scheduler:DAGSchedulerEventProcessLoop.adoc#handleStageCancellation[a stage was cancelled], < >, < > and < >. | waitingStages | [[waitingStages]] The stages with parents to be computed |===","title":"log4j.logger.org.apache.spark.scheduler.DAGScheduler=ALL"},{"location":"tools/spark-shell/","text":"== [[spark-shell]] Spark Shell -- spark-shell shell script Spark shell is an interactive environment where you can learn how to make the most out of Apache Spark quickly and conveniently. TIP: Spark shell is particularly helpful for fast interactive prototyping. Under the covers, Spark shell is a standalone Spark application written in Scala that offers environment with auto-completion (using TAB key) where you can run ad-hoc queries and get familiar with the features of Spark (that help you in developing your own standalone Spark applications). It is a very convenient tool to explore the many things available in Spark with immediate feedback. It is one of the many reasons why link:spark-overview.adoc#why-spark[Spark is so helpful for tasks to process datasets of any size]. There are variants of Spark shell for different languages: spark-shell for Scala, pyspark for Python and sparkR for R. NOTE: This document (and the book in general) uses spark-shell for Scala only. You can start Spark shell using < spark-shell script>>. $ ./bin/spark-shell scala> spark-shell is an extension of Scala REPL with automatic instantiation of link:spark-sql-SparkSession.adoc[SparkSession] as spark (and xref:ROOT:SparkContext.adoc[] as sc ). [source, scala] \u00b6 scala> :type spark org.apache.spark.sql.SparkSession // Learn the current version of Spark in use scala> spark.version res0: String = 2.1.0-SNAPSHOT spark-shell also imports link:spark-sql-SparkSession.adoc#implicits[Scala SQL's implicits] and link:spark-sql-SparkSession.adoc#sql[ sql method]. [source, scala] \u00b6 scala> :imports 1) import spark.implicits._ (59 terms, 38 are implicit) 2) import spark.sql (1 terms) [NOTE] \u00b6 When you execute spark-shell you actually execute link:spark-submit.adoc[Spark submit] as follows: [options=\"wrap\"] \u00b6 org.apache.spark.deploy.SparkSubmit --class org.apache.spark.repl.Main --name Spark shell spark-shell \u00b6 Set SPARK_PRINT_LAUNCH_COMMAND to see the entire command to be executed. Refer to link:spark-tips-and-tricks.adoc#SPARK_PRINT_LAUNCH_COMMAND[Print Launch Command of Spark Scripts]. \u00b6 === [[using-spark-shell]] Using Spark shell You start Spark shell using spark-shell script (available in bin directory). $ ./bin/spark-shell Setting default log level to \"WARN\". To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel). WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable WARN ObjectStore: Failed to get database global_temp, returning NoSuchObjectException Spark context Web UI available at http://10.47.71.138:4040 Spark context available as 'sc' (master = local[*], app id = local-1477858597347). Spark session available as 'spark'. Welcome to ____ __ / __/__ ___ _____/ /__ _\\ \\/ _ \\/ _ `/ __/ '_/ /___/ .__/\\_,_/_/ /_/\\_\\ version 2.1.0-SNAPSHOT /_/ Using Scala version 2.11.8 (Java HotSpot(TM) 64-Bit Server VM, Java 1.8.0_112) Type in expressions to have them evaluated. Type :help for more information. scala> Spark shell creates an instance of link:spark-sql-SparkSession.adoc[SparkSession] under the name spark for you (so you don't have to know the details how to do it yourself on day 1). scala> :type spark org.apache.spark.sql.SparkSession Besides, there is also sc value created which is an instance of xref:ROOT:SparkContext.adoc[]. scala> :type sc org.apache.spark.SparkContext To close Spark shell, you press Ctrl+D or type in :q (or any subset of :quit ). scala> :q === [[settings]] Settings .Spark Properties [cols=\"1,1,2\",options=\"header\",width=\"100%\"] |=== | Spark Property | Default Value | Description | [[spark_repl_class_uri]] spark.repl.class.uri | null | Used in spark-shell to create REPL ClassLoader to load new classes defined in the Scala REPL as a user types code. Enable INFO logging level for xref:executor:Executor.adoc[org.apache.spark.executor.Executor] logger to have the value printed out to the logs: INFO Using REPL class URI: [classUri] |===","title":"spark-shell"},{"location":"tools/spark-shell/#source-scala","text":"scala> :type spark org.apache.spark.sql.SparkSession // Learn the current version of Spark in use scala> spark.version res0: String = 2.1.0-SNAPSHOT spark-shell also imports link:spark-sql-SparkSession.adoc#implicits[Scala SQL's implicits] and link:spark-sql-SparkSession.adoc#sql[ sql method].","title":"[source, scala]"},{"location":"tools/spark-shell/#source-scala_1","text":"scala> :imports 1) import spark.implicits._ (59 terms, 38 are implicit) 2) import spark.sql (1 terms)","title":"[source, scala]"},{"location":"tools/spark-shell/#note","text":"When you execute spark-shell you actually execute link:spark-submit.adoc[Spark submit] as follows:","title":"[NOTE]"},{"location":"tools/spark-shell/#optionswrap","text":"","title":"[options=\"wrap\"]"},{"location":"tools/spark-shell/#orgapachesparkdeploysparksubmit-class-orgapachesparkreplmain-name-spark-shell-spark-shell","text":"","title":"org.apache.spark.deploy.SparkSubmit --class org.apache.spark.repl.Main --name Spark shell spark-shell"},{"location":"tools/spark-shell/#set-spark_print_launch_command-to-see-the-entire-command-to-be-executed-refer-to-linkspark-tips-and-tricksadocspark_print_launch_commandprint-launch-command-of-spark-scripts","text":"=== [[using-spark-shell]] Using Spark shell You start Spark shell using spark-shell script (available in bin directory). $ ./bin/spark-shell Setting default log level to \"WARN\". To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel). WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable WARN ObjectStore: Failed to get database global_temp, returning NoSuchObjectException Spark context Web UI available at http://10.47.71.138:4040 Spark context available as 'sc' (master = local[*], app id = local-1477858597347). Spark session available as 'spark'. Welcome to ____ __ / __/__ ___ _____/ /__ _\\ \\/ _ \\/ _ `/ __/ '_/ /___/ .__/\\_,_/_/ /_/\\_\\ version 2.1.0-SNAPSHOT /_/ Using Scala version 2.11.8 (Java HotSpot(TM) 64-Bit Server VM, Java 1.8.0_112) Type in expressions to have them evaluated. Type :help for more information. scala> Spark shell creates an instance of link:spark-sql-SparkSession.adoc[SparkSession] under the name spark for you (so you don't have to know the details how to do it yourself on day 1). scala> :type spark org.apache.spark.sql.SparkSession Besides, there is also sc value created which is an instance of xref:ROOT:SparkContext.adoc[]. scala> :type sc org.apache.spark.SparkContext To close Spark shell, you press Ctrl+D or type in :q (or any subset of :quit ). scala> :q === [[settings]] Settings .Spark Properties [cols=\"1,1,2\",options=\"header\",width=\"100%\"] |=== | Spark Property | Default Value | Description | [[spark_repl_class_uri]] spark.repl.class.uri | null | Used in spark-shell to create REPL ClassLoader to load new classes defined in the Scala REPL as a user types code. Enable INFO logging level for xref:executor:Executor.adoc[org.apache.spark.executor.Executor] logger to have the value printed out to the logs: INFO Using REPL class URI: [classUri] |===","title":"Set SPARK_PRINT_LAUNCH_COMMAND to see the entire command to be executed. Refer to link:spark-tips-and-tricks.adoc#SPARK_PRINT_LAUNCH_COMMAND[Print Launch Command of Spark Scripts]."}]}