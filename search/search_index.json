{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"The Internals of Apache Spark 3.0.1 \u00b6 Welcome to The Internals of Apache Spark online book! I'm Jacek Laskowski , a Seasoned IT Professional specializing in Apache Spark , Delta Lake , Apache Kafka and Kafka Streams . I'm very excited to have you here and hope you will enjoy exploring the internals of Apache Spark as much as I have. Flannery O'Connor I write to discover what I know. \"The Internals Of\" series I'm also writing other online books in the \"The Internals Of\" series. Please visit \"The Internals Of\" Online Books home page. Expect text and code snippets from a variety of public sources. Attribution follows. Now, let me introduce you to Apache Spark .","title":"Welcome"},{"location":"#the-internals-of-apache-spark-301","text":"Welcome to The Internals of Apache Spark online book! I'm Jacek Laskowski , a Seasoned IT Professional specializing in Apache Spark , Delta Lake , Apache Kafka and Kafka Streams . I'm very excited to have you here and hope you will enjoy exploring the internals of Apache Spark as much as I have. Flannery O'Connor I write to discover what I know. \"The Internals Of\" series I'm also writing other online books in the \"The Internals Of\" series. Please visit \"The Internals Of\" Online Books home page. Expect text and code snippets from a variety of public sources. Attribution follows. Now, let me introduce you to Apache Spark .","title":"The Internals of Apache Spark 3.0.1"},{"location":"overview/","text":"Apache Spark \u00b6 Apache Spark is an open-source distributed general-purpose cluster computing framework with (mostly) in-memory data processing engine that can do ETL, analytics, machine learning and graph processing on large volumes of data at rest (batch processing) or in motion (streaming processing) with rich concise high-level APIs for the programming languages: Scala, Python, Java, R, and SQL. You could also describe Spark as a distributed, data processing engine for batch and streaming modes featuring SQL queries, graph processing, and machine learning. In contrast to Hadoop\u2019s two-stage disk-based MapReduce computation engine, Spark's multi-stage (mostly) in-memory computing engine allows for running most computations in memory, and hence most of the time provides better performance for certain applications, e.g. iterative algorithms or interactive data mining (read Spark officially sets a new record in large-scale sorting ). Spark aims at speed, ease of use, extensibility and interactive analytics. Spark is a distributed platform for executing complex multi-stage applications , like machine learning algorithms , and interactive ad hoc queries . Spark provides an efficient abstraction for in-memory cluster computing called Resilient Distributed Dataset . Using Spark Application Frameworks, Spark simplifies access to machine learning and predictive analytics at scale. Spark is mainly written in http://scala-lang.org/[Scala ], but provides developer API for languages like Java, Python, and R. NOTE: Microsoft's https://github.com/Microsoft/Mobius[Mobius project] provides C# API for Spark \"enabling the implementation of Spark driver program and data processing operations in the languages supported in the .NET framework like C# or F#.\" If you have large amounts of data that requires low latency processing that a typical MapReduce program cannot provide, Spark is a viable alternative. Access any data type across any data source. Huge demand for storage and data processing. The Apache Spark project is an umbrella for https://jaceklaskowski.gitbooks.io/mastering-spark-sql/[SQL ] (with Datasets), https://jaceklaskowski.gitbooks.io/spark-structured-streaming/[streaming ], http://spark.apache.org/mllib/[machine learning] (pipelines) and http://spark.apache.org/graphx/[graph ] processing engines built on top of the Spark Core. You can run them all in a single application using a consistent API. Spark runs locally as well as in clusters, on-premises or in cloud. It runs on top of Hadoop YARN, Apache Mesos, standalone or in the cloud (Amazon EC2 or IBM Bluemix). Apache Spark's https://jaceklaskowski.gitbooks.io/spark-structured-streaming/[Structured Streaming] and https://jaceklaskowski.gitbooks.io/mastering-spark-sql/[SQL ] programming models with MLlib and GraphX make it easier for developers and data scientists to build applications that exploit machine learning and graph analytics. At a high level, any Spark application creates RDDs out of some input, run xref:rdd:index.adoc[(lazy) transformations] of these RDDs to some other form (shape), and finally perform xref:rdd:index.adoc[actions] to collect or store data. Not much, huh? You can look at Spark from programmer's, data engineer's and administrator's point of view. And to be honest, all three types of people will spend quite a lot of their time with Spark to finally reach the point where they exploit all the available features. Programmers use language-specific APIs (and work at the level of RDDs using transformations and actions), data engineers use higher-level abstractions like DataFrames or Pipelines APIs or external tools (that connect to Spark), and finally it all can only be possible to run because administrators set up Spark clusters to deploy Spark applications to. It is Spark's goal to be a general-purpose computing platform with various specialized applications frameworks on top of a single unified engine. NOTE: When you hear \"Apache Spark\" it can be two things -- the Spark engine aka Spark Core or the Apache Spark open source project which is an \"umbrella\" term for Spark Core and the accompanying Spark Application Frameworks, i.e. Spark SQL, link:spark-streaming/spark-streaming.adoc[Spark Streaming], link:spark-mllib/spark-mllib.adoc[Spark MLlib] and link:spark-graphx.adoc[Spark GraphX] that sit on top of Spark Core and the main data abstraction in Spark called xref:rdd:index.adoc[RDD - Resilient Distributed Dataset]. == [[why-spark]] Why Spark Let's list a few of the many reasons for Spark. We are doing it first, and then comes the overview that lends a more technical helping hand. === Easy to Get Started Spark offers link:spark-shell.adoc[spark-shell] that makes for a very easy head start to writing and running Spark applications on the command line on your laptop. You could then use link:spark-standalone.adoc[Spark Standalone] built-in cluster manager to deploy your Spark applications to a production-grade cluster to run on a full dataset. === Unified Engine for Diverse Workloads As said by Matei Zaharia - the author of Apache Spark - in https://youtu.be/49Hr5xZyTEA[Introduction to AmpLab Spark Internals video] (quoting with few changes): One of the Spark project goals was to deliver a platform that supports a very wide array of diverse workflows - not only MapReduce batch jobs (there were available in Hadoop already at that time), but also iterative computations like graph algorithms or Machine Learning. And also different scales of workloads from sub-second interactive jobs to jobs that run for many hours. Spark combines batch, interactive, and streaming workloads under one rich concise API. Spark supports near real-time streaming workloads via link:spark-streaming/spark-streaming.adoc[Spark Streaming] application framework. ETL workloads and Analytics workloads are different, however Spark attempts to offer a unified platform for a wide variety of workloads. Graph and Machine Learning algorithms are iterative by nature and less saves to disk or transfers over network means better performance. There is also support for interactive workloads using Spark shell. You should watch the video https://youtu.be/SxAxAhn-BDU[What is Apache Spark?] by Mike Olson, Chief Strategy Officer and Co-Founder at Cloudera, who provides a very exceptional overview of Apache Spark, its rise in popularity in the open source community, and how Spark is primed to replace MapReduce as the general processing engine in Hadoop. === Leverages the Best in distributed batch data processing When you think about distributed batch data processing , link:varia/spark-hadoop.adoc[Hadoop] naturally comes to mind as a viable solution. Spark draws many ideas out of Hadoop MapReduce. They work together well - Spark on YARN and HDFS - while improving on the performance and simplicity of the distributed computing engine. For many, Spark is Hadoop++, i.e. MapReduce done in a better way. And it should not come as a surprise, without Hadoop MapReduce (its advances and deficiencies), Spark would not have been born at all. === RDD - Distributed Parallel Scala Collections As a Scala developer, you may find Spark's RDD API very similar (if not identical) to http://www.scala-lang.org/docu/files/collections-api/collections.html[Scala's Collections API]. It is also exposed in Java, Python and R (as well as SQL, i.e. SparkSQL, in a sense). So, when you have a need for distributed Collections API in Scala, Spark with RDD API should be a serious contender. === [[rich-standard-library]] Rich Standard Library Not only can you use map and reduce (as in Hadoop MapReduce jobs) in Spark, but also a vast array of other higher-level operators to ease your Spark queries and application development. It expanded on the available computation styles beyond the only map-and-reduce available in Hadoop MapReduce. === Unified development and deployment environment for all Regardless of the Spark tools you use - the Spark API for the many programming languages supported - Scala, Java, Python, R, or link:spark-shell.adoc[the Spark shell], or the many Spark Application Frameworks leveraging the concept of xref:rdd:index.adoc[RDD], i.e. Spark SQL, link:spark-streaming/spark-streaming.adoc[Spark Streaming], link:spark-mllib/spark-mllib.adoc[Spark MLlib] and link:spark-graphx.adoc[Spark GraphX], you still use the same development and deployment environment to for large data sets to yield a result, be it a prediction (link:spark-mllib/spark-mllib.adoc[Spark MLlib]), a structured data queries (Spark SQL) or just a large distributed batch (Spark Core) or streaming (Spark Streaming) computation. It's also very productive of Spark that teams can exploit the different skills the team members have acquired so far. Data analysts, data scientists, Python programmers, or Java, or Scala, or R, can all use the same Spark platform using tailor-made API. It makes for bringing skilled people with their expertise in different programming languages together to a Spark project. === Interactive Exploration / Exploratory Analytics It is also called ad hoc queries . Using link:spark-shell.adoc[the Spark shell] you can execute computations to process large amount of data ( The Big Data ). It's all interactive and very useful to explore the data before final production release. Also, using the Spark shell you can access any link:spark-cluster.adoc[Spark cluster] as if it was your local machine. Just point the Spark shell to a 20-node of 10TB RAM memory in total (using --master ) and use all the components (and their abstractions) like Spark SQL, Spark MLlib, link:spark-streaming/spark-streaming.adoc[Spark Streaming], and Spark GraphX. Depending on your needs and skills, you may see a better fit for SQL vs programming APIs or apply machine learning algorithms (Spark MLlib) from data in graph data structures (Spark GraphX). === Single Environment Regardless of which programming language you are good at, be it Scala, Java, Python, R or SQL, you can use the same single clustered runtime environment for prototyping, ad hoc queries, and deploying your applications leveraging the many ingestion data points offered by the Spark platform. You can be as low-level as using RDD API directly or leverage higher-level APIs of Spark SQL (Datasets), Spark MLlib (ML Pipelines), Spark GraphX (Graphs) or link:spark-streaming/spark-streaming.adoc[Spark Streaming] (DStreams). Or use them all in a single application. The single programming model and execution engine for different kinds of workloads simplify development and deployment architectures. === Data Integration Toolkit with Rich Set of Supported Data Sources Spark can read from many types of data sources -- relational, NoSQL, file systems, etc. -- using many types of data formats - Parquet, Avro, CSV, JSON. Both, input and output data sources, allow programmers and data engineers use Spark as the platform with the large amount of data that is read from or saved to for processing, interactively (using Spark shell) or in applications. === Tools unavailable then, at your fingertips now As much and often as it's recommended http://c2.com/cgi/wiki?PickTheRightToolForTheJob[to pick the right tool for the job], it's not always feasible. Time, personal preference, operating system you work on are all factors to decide what is right at a time (and using a hammer can be a reasonable choice). Spark embraces many concepts in a single unified development and runtime environment. Machine learning that is so tool- and feature-rich in Python, e.g. SciKit library, can now be used by Scala developers (as Pipeline API in Spark MLlib or calling pipe() ). DataFrames from R are available in Scala, Java, Python, R APIs. Single node computations in machine learning algorithms are migrated to their distributed versions in Spark MLlib. This single platform gives plenty of opportunities for Python, Scala, Java, and R programmers as well as data engineers (SparkR) and scientists (using proprietary enterprise data warehouses with link:spark-sql-thrift-server.adoc[Thrift JDBC/ODBC Server] in Spark SQL). Mind the proverb https://en.wiktionary.org/wiki/if_all_you_have_is_a_hammer,_everything_looks_like_a_nail[if all you have is a hammer, everything looks like a nail], too. === Low-level Optimizations Apache Spark uses a xref:scheduler:DAGScheduler.adoc[directed acyclic graph (DAG) of computation stages] (aka execution DAG ). It postpones any processing until really required for actions. Spark's lazy evaluation gives plenty of opportunities to induce low-level optimizations (so users have to know less to do more). Mind the proverb https://en.wiktionary.org/wiki/less_is_more[less is more]. === Excels at low-latency iterative workloads Spark supports diverse workloads, but successfully targets low-latency iterative ones. They are often used in Machine Learning and graph algorithms. Many Machine Learning algorithms require plenty of iterations before the result models get optimal, like logistic regression. The same applies to graph algorithms to traverse all the nodes and edges when needed. Such computations can increase their performance when the interim partial results are stored in memory or at very fast solid state drives. Spark can link:spark-rdd-caching.adoc[cache intermediate data in memory for faster model building and training]. Once the data is loaded to memory (as an initial step), reusing it multiple times incurs no performance slowdowns. Also, graph algorithms can traverse graphs one connection per iteration with the partial result in memory. Less disk access and network can make a huge difference when you need to process lots of data, esp. when it is a BIG Data. === ETL done easier Spark gives Extract, Transform and Load (ETL) a new look with the many programming languages supported - Scala, Java, Python (less likely R). You can use them all or pick the best for a problem. Scala in Spark, especially, makes for a much less boiler-plate code (comparing to other languages and approaches like MapReduce in Java). === [[unified-api]] Unified Concise High-Level API Spark offers a unified, concise, high-level APIs for batch analytics (RDD API), SQL queries (Dataset API), real-time analysis (DStream API), machine learning (ML Pipeline API) and graph processing (Graph API). Developers no longer have to learn many different processing engines and platforms, and let the time be spent on mastering framework APIs per use case (atop a single computation engine Spark). === Different kinds of data processing using unified API Spark offers three kinds of data processing using batch , interactive , and stream processing with the unified API and data structures. === Little to no disk use for better performance In the no-so-long-ago times, when the most prevalent distributed computing framework was link:varia/spark-hadoop.adoc[Hadoop MapReduce], you could reuse a data between computation (even partial ones!) only after you've written it to an external storage like link:varia/spark-hadoop.adoc[Hadoop Distributed Filesystem (HDFS)]. It can cost you a lot of time to compute even very basic multi-stage computations. It simply suffers from IO (and perhaps network) overhead. One of the many motivations to build Spark was to have a framework that is good at data reuse. Spark cuts it out in a way to keep as much data as possible in memory and keep it there until a job is finished. It doesn't matter how many stages belong to a job. What does matter is the available memory and how effective you are in using Spark API (so xref:rdd:index.adoc[no shuffle occur]). The less network and disk IO, the better performance, and Spark tries hard to find ways to minimize both. === Fault Tolerance included Faults are not considered a special case in Spark, but obvious consequence of being a parallel and distributed system. Spark handles and recovers from faults by default without particularly complex logic to deal with them. === Small Codebase Invites Contributors Spark's design is fairly simple and the code that comes out of it is not huge comparing to the features it offers. The reasonably small codebase of Spark invites project contributors - programmers who extend the platform and fix bugs in a more steady pace. == [[i-want-more]] Further reading or watching (video) https://youtu.be/L029ZNBG7bk[Keynote : Spark 2.0 - Matei Zaharia, Apache Spark Creator and CTO of Databricks]","title":"Overview"},{"location":"overview/#apache-spark","text":"Apache Spark is an open-source distributed general-purpose cluster computing framework with (mostly) in-memory data processing engine that can do ETL, analytics, machine learning and graph processing on large volumes of data at rest (batch processing) or in motion (streaming processing) with rich concise high-level APIs for the programming languages: Scala, Python, Java, R, and SQL. You could also describe Spark as a distributed, data processing engine for batch and streaming modes featuring SQL queries, graph processing, and machine learning. In contrast to Hadoop\u2019s two-stage disk-based MapReduce computation engine, Spark's multi-stage (mostly) in-memory computing engine allows for running most computations in memory, and hence most of the time provides better performance for certain applications, e.g. iterative algorithms or interactive data mining (read Spark officially sets a new record in large-scale sorting ). Spark aims at speed, ease of use, extensibility and interactive analytics. Spark is a distributed platform for executing complex multi-stage applications , like machine learning algorithms , and interactive ad hoc queries . Spark provides an efficient abstraction for in-memory cluster computing called Resilient Distributed Dataset . Using Spark Application Frameworks, Spark simplifies access to machine learning and predictive analytics at scale. Spark is mainly written in http://scala-lang.org/[Scala ], but provides developer API for languages like Java, Python, and R. NOTE: Microsoft's https://github.com/Microsoft/Mobius[Mobius project] provides C# API for Spark \"enabling the implementation of Spark driver program and data processing operations in the languages supported in the .NET framework like C# or F#.\" If you have large amounts of data that requires low latency processing that a typical MapReduce program cannot provide, Spark is a viable alternative. Access any data type across any data source. Huge demand for storage and data processing. The Apache Spark project is an umbrella for https://jaceklaskowski.gitbooks.io/mastering-spark-sql/[SQL ] (with Datasets), https://jaceklaskowski.gitbooks.io/spark-structured-streaming/[streaming ], http://spark.apache.org/mllib/[machine learning] (pipelines) and http://spark.apache.org/graphx/[graph ] processing engines built on top of the Spark Core. You can run them all in a single application using a consistent API. Spark runs locally as well as in clusters, on-premises or in cloud. It runs on top of Hadoop YARN, Apache Mesos, standalone or in the cloud (Amazon EC2 or IBM Bluemix). Apache Spark's https://jaceklaskowski.gitbooks.io/spark-structured-streaming/[Structured Streaming] and https://jaceklaskowski.gitbooks.io/mastering-spark-sql/[SQL ] programming models with MLlib and GraphX make it easier for developers and data scientists to build applications that exploit machine learning and graph analytics. At a high level, any Spark application creates RDDs out of some input, run xref:rdd:index.adoc[(lazy) transformations] of these RDDs to some other form (shape), and finally perform xref:rdd:index.adoc[actions] to collect or store data. Not much, huh? You can look at Spark from programmer's, data engineer's and administrator's point of view. And to be honest, all three types of people will spend quite a lot of their time with Spark to finally reach the point where they exploit all the available features. Programmers use language-specific APIs (and work at the level of RDDs using transformations and actions), data engineers use higher-level abstractions like DataFrames or Pipelines APIs or external tools (that connect to Spark), and finally it all can only be possible to run because administrators set up Spark clusters to deploy Spark applications to. It is Spark's goal to be a general-purpose computing platform with various specialized applications frameworks on top of a single unified engine. NOTE: When you hear \"Apache Spark\" it can be two things -- the Spark engine aka Spark Core or the Apache Spark open source project which is an \"umbrella\" term for Spark Core and the accompanying Spark Application Frameworks, i.e. Spark SQL, link:spark-streaming/spark-streaming.adoc[Spark Streaming], link:spark-mllib/spark-mllib.adoc[Spark MLlib] and link:spark-graphx.adoc[Spark GraphX] that sit on top of Spark Core and the main data abstraction in Spark called xref:rdd:index.adoc[RDD - Resilient Distributed Dataset]. == [[why-spark]] Why Spark Let's list a few of the many reasons for Spark. We are doing it first, and then comes the overview that lends a more technical helping hand. === Easy to Get Started Spark offers link:spark-shell.adoc[spark-shell] that makes for a very easy head start to writing and running Spark applications on the command line on your laptop. You could then use link:spark-standalone.adoc[Spark Standalone] built-in cluster manager to deploy your Spark applications to a production-grade cluster to run on a full dataset. === Unified Engine for Diverse Workloads As said by Matei Zaharia - the author of Apache Spark - in https://youtu.be/49Hr5xZyTEA[Introduction to AmpLab Spark Internals video] (quoting with few changes): One of the Spark project goals was to deliver a platform that supports a very wide array of diverse workflows - not only MapReduce batch jobs (there were available in Hadoop already at that time), but also iterative computations like graph algorithms or Machine Learning. And also different scales of workloads from sub-second interactive jobs to jobs that run for many hours. Spark combines batch, interactive, and streaming workloads under one rich concise API. Spark supports near real-time streaming workloads via link:spark-streaming/spark-streaming.adoc[Spark Streaming] application framework. ETL workloads and Analytics workloads are different, however Spark attempts to offer a unified platform for a wide variety of workloads. Graph and Machine Learning algorithms are iterative by nature and less saves to disk or transfers over network means better performance. There is also support for interactive workloads using Spark shell. You should watch the video https://youtu.be/SxAxAhn-BDU[What is Apache Spark?] by Mike Olson, Chief Strategy Officer and Co-Founder at Cloudera, who provides a very exceptional overview of Apache Spark, its rise in popularity in the open source community, and how Spark is primed to replace MapReduce as the general processing engine in Hadoop. === Leverages the Best in distributed batch data processing When you think about distributed batch data processing , link:varia/spark-hadoop.adoc[Hadoop] naturally comes to mind as a viable solution. Spark draws many ideas out of Hadoop MapReduce. They work together well - Spark on YARN and HDFS - while improving on the performance and simplicity of the distributed computing engine. For many, Spark is Hadoop++, i.e. MapReduce done in a better way. And it should not come as a surprise, without Hadoop MapReduce (its advances and deficiencies), Spark would not have been born at all. === RDD - Distributed Parallel Scala Collections As a Scala developer, you may find Spark's RDD API very similar (if not identical) to http://www.scala-lang.org/docu/files/collections-api/collections.html[Scala's Collections API]. It is also exposed in Java, Python and R (as well as SQL, i.e. SparkSQL, in a sense). So, when you have a need for distributed Collections API in Scala, Spark with RDD API should be a serious contender. === [[rich-standard-library]] Rich Standard Library Not only can you use map and reduce (as in Hadoop MapReduce jobs) in Spark, but also a vast array of other higher-level operators to ease your Spark queries and application development. It expanded on the available computation styles beyond the only map-and-reduce available in Hadoop MapReduce. === Unified development and deployment environment for all Regardless of the Spark tools you use - the Spark API for the many programming languages supported - Scala, Java, Python, R, or link:spark-shell.adoc[the Spark shell], or the many Spark Application Frameworks leveraging the concept of xref:rdd:index.adoc[RDD], i.e. Spark SQL, link:spark-streaming/spark-streaming.adoc[Spark Streaming], link:spark-mllib/spark-mllib.adoc[Spark MLlib] and link:spark-graphx.adoc[Spark GraphX], you still use the same development and deployment environment to for large data sets to yield a result, be it a prediction (link:spark-mllib/spark-mllib.adoc[Spark MLlib]), a structured data queries (Spark SQL) or just a large distributed batch (Spark Core) or streaming (Spark Streaming) computation. It's also very productive of Spark that teams can exploit the different skills the team members have acquired so far. Data analysts, data scientists, Python programmers, or Java, or Scala, or R, can all use the same Spark platform using tailor-made API. It makes for bringing skilled people with their expertise in different programming languages together to a Spark project. === Interactive Exploration / Exploratory Analytics It is also called ad hoc queries . Using link:spark-shell.adoc[the Spark shell] you can execute computations to process large amount of data ( The Big Data ). It's all interactive and very useful to explore the data before final production release. Also, using the Spark shell you can access any link:spark-cluster.adoc[Spark cluster] as if it was your local machine. Just point the Spark shell to a 20-node of 10TB RAM memory in total (using --master ) and use all the components (and their abstractions) like Spark SQL, Spark MLlib, link:spark-streaming/spark-streaming.adoc[Spark Streaming], and Spark GraphX. Depending on your needs and skills, you may see a better fit for SQL vs programming APIs or apply machine learning algorithms (Spark MLlib) from data in graph data structures (Spark GraphX). === Single Environment Regardless of which programming language you are good at, be it Scala, Java, Python, R or SQL, you can use the same single clustered runtime environment for prototyping, ad hoc queries, and deploying your applications leveraging the many ingestion data points offered by the Spark platform. You can be as low-level as using RDD API directly or leverage higher-level APIs of Spark SQL (Datasets), Spark MLlib (ML Pipelines), Spark GraphX (Graphs) or link:spark-streaming/spark-streaming.adoc[Spark Streaming] (DStreams). Or use them all in a single application. The single programming model and execution engine for different kinds of workloads simplify development and deployment architectures. === Data Integration Toolkit with Rich Set of Supported Data Sources Spark can read from many types of data sources -- relational, NoSQL, file systems, etc. -- using many types of data formats - Parquet, Avro, CSV, JSON. Both, input and output data sources, allow programmers and data engineers use Spark as the platform with the large amount of data that is read from or saved to for processing, interactively (using Spark shell) or in applications. === Tools unavailable then, at your fingertips now As much and often as it's recommended http://c2.com/cgi/wiki?PickTheRightToolForTheJob[to pick the right tool for the job], it's not always feasible. Time, personal preference, operating system you work on are all factors to decide what is right at a time (and using a hammer can be a reasonable choice). Spark embraces many concepts in a single unified development and runtime environment. Machine learning that is so tool- and feature-rich in Python, e.g. SciKit library, can now be used by Scala developers (as Pipeline API in Spark MLlib or calling pipe() ). DataFrames from R are available in Scala, Java, Python, R APIs. Single node computations in machine learning algorithms are migrated to their distributed versions in Spark MLlib. This single platform gives plenty of opportunities for Python, Scala, Java, and R programmers as well as data engineers (SparkR) and scientists (using proprietary enterprise data warehouses with link:spark-sql-thrift-server.adoc[Thrift JDBC/ODBC Server] in Spark SQL). Mind the proverb https://en.wiktionary.org/wiki/if_all_you_have_is_a_hammer,_everything_looks_like_a_nail[if all you have is a hammer, everything looks like a nail], too. === Low-level Optimizations Apache Spark uses a xref:scheduler:DAGScheduler.adoc[directed acyclic graph (DAG) of computation stages] (aka execution DAG ). It postpones any processing until really required for actions. Spark's lazy evaluation gives plenty of opportunities to induce low-level optimizations (so users have to know less to do more). Mind the proverb https://en.wiktionary.org/wiki/less_is_more[less is more]. === Excels at low-latency iterative workloads Spark supports diverse workloads, but successfully targets low-latency iterative ones. They are often used in Machine Learning and graph algorithms. Many Machine Learning algorithms require plenty of iterations before the result models get optimal, like logistic regression. The same applies to graph algorithms to traverse all the nodes and edges when needed. Such computations can increase their performance when the interim partial results are stored in memory or at very fast solid state drives. Spark can link:spark-rdd-caching.adoc[cache intermediate data in memory for faster model building and training]. Once the data is loaded to memory (as an initial step), reusing it multiple times incurs no performance slowdowns. Also, graph algorithms can traverse graphs one connection per iteration with the partial result in memory. Less disk access and network can make a huge difference when you need to process lots of data, esp. when it is a BIG Data. === ETL done easier Spark gives Extract, Transform and Load (ETL) a new look with the many programming languages supported - Scala, Java, Python (less likely R). You can use them all or pick the best for a problem. Scala in Spark, especially, makes for a much less boiler-plate code (comparing to other languages and approaches like MapReduce in Java). === [[unified-api]] Unified Concise High-Level API Spark offers a unified, concise, high-level APIs for batch analytics (RDD API), SQL queries (Dataset API), real-time analysis (DStream API), machine learning (ML Pipeline API) and graph processing (Graph API). Developers no longer have to learn many different processing engines and platforms, and let the time be spent on mastering framework APIs per use case (atop a single computation engine Spark). === Different kinds of data processing using unified API Spark offers three kinds of data processing using batch , interactive , and stream processing with the unified API and data structures. === Little to no disk use for better performance In the no-so-long-ago times, when the most prevalent distributed computing framework was link:varia/spark-hadoop.adoc[Hadoop MapReduce], you could reuse a data between computation (even partial ones!) only after you've written it to an external storage like link:varia/spark-hadoop.adoc[Hadoop Distributed Filesystem (HDFS)]. It can cost you a lot of time to compute even very basic multi-stage computations. It simply suffers from IO (and perhaps network) overhead. One of the many motivations to build Spark was to have a framework that is good at data reuse. Spark cuts it out in a way to keep as much data as possible in memory and keep it there until a job is finished. It doesn't matter how many stages belong to a job. What does matter is the available memory and how effective you are in using Spark API (so xref:rdd:index.adoc[no shuffle occur]). The less network and disk IO, the better performance, and Spark tries hard to find ways to minimize both. === Fault Tolerance included Faults are not considered a special case in Spark, but obvious consequence of being a parallel and distributed system. Spark handles and recovers from faults by default without particularly complex logic to deal with them. === Small Codebase Invites Contributors Spark's design is fairly simple and the code that comes out of it is not huge comparing to the features it offers. The reasonably small codebase of Spark invites project contributors - programmers who extend the platform and fix bugs in a more steady pace. == [[i-want-more]] Further reading or watching (video) https://youtu.be/L029ZNBG7bk[Keynote : Spark 2.0 - Matei Zaharia, Apache Spark Creator and CTO of Databricks]","title":"Apache Spark"},{"location":"metrics/","text":"Spark Metrics \u00b6 Spark Metrics gives you execution metrics of Spark subsystems ( metrics instances ), e.g. the driver of a Spark application or the master of a Spark Standalone cluster. Spark Metrics uses Dropwizard Metrics 3.1.0 Java library for the metrics infrastructure. Metrics is a Java library which gives you unparalleled insight into what your code does in production. Metrics provides a powerful toolkit of ways to measure the behavior of critical components in your production environment . MetricsSystem \u00b6 Spark Metrics uses MetricsSystem . MetricsSystem uses Dropwizard Metrics' link:spark-metrics-MetricsSystem.adoc#registry[MetricRegistry] that acts as the integration point between Spark and the metrics library. A Spark subsystem can access the MetricsSystem through the SparkEnv.metricsSystem property. val metricsSystem = SparkEnv.get.metricsSystem MetricsConfig \u00b6 MetricsConfig is the configuration of the link:spark-metrics-MetricsSystem.adoc[MetricsSystem] (i.e. metrics link:spark-metrics-Source.adoc[sources] and link:spark-metrics-Sink.adoc[sinks]). metrics.properties is the default metrics configuration file. It is configured using link:spark-metrics-properties.adoc#spark.metrics.conf[spark.metrics.conf] configuration property. The file is first loaded from the path directly before using Spark's CLASSPATH. MetricsConfig also accepts a metrics configuration using spark.metrics.conf. -prefixed configuration properties. Spark comes with conf/metrics.properties.template file that is a template of metrics configuration. MetricsServlet Metrics Sink \u00b6 Among the metrics sinks is link:spark-metrics-MetricsServlet.adoc[MetricsServlet] that is used when sink.servlet metrics sink is configured in link:spark-metrics-MetricsConfig.adoc[metrics configuration]. CAUTION: FIXME Describe configuration files and properties JmxSink Metrics Sink \u00b6 Enable org.apache.spark.metrics.sink.JmxSink in link:spark-metrics-MetricsConfig.adoc[metrics configuration]. You can then use jconsole to access Spark metrics through JMX. *.sink.jmx.class=org.apache.spark.metrics.sink.JmxSink JSON URI Path \u00b6 Metrics System is available at http://localhost:4040/metrics/json (for the default setup of a Spark application). $ http --follow http://localhost:4040/metrics/json HTTP/1.1 200 OK Cache-Control: no-cache, no-store, must-revalidate Content-Length: 2200 Content-Type: text/json;charset=utf-8 Date: Sat, 25 Feb 2017 14:14:16 GMT Server: Jetty(9.2.z-SNAPSHOT) X-Frame-Options: SAMEORIGIN { \"counters\": { \"app-20170225151406-0000.driver.HiveExternalCatalog.fileCacheHits\": { \"count\": 0 }, \"app-20170225151406-0000.driver.HiveExternalCatalog.filesDiscovered\": { \"count\": 0 }, \"app-20170225151406-0000.driver.HiveExternalCatalog.hiveClientCalls\": { \"count\": 2 }, \"app-20170225151406-0000.driver.HiveExternalCatalog.parallelListingJobCount\": { \"count\": 0 }, \"app-20170225151406-0000.driver.HiveExternalCatalog.partitionsFetched\": { \"count\": 0 } }, \"gauges\": { ... \"timers\": { \"app-20170225151406-0000.driver.DAGScheduler.messageProcessingTime\": { \"count\": 0, \"duration_units\": \"milliseconds\", \"m15_rate\": 0.0, \"m1_rate\": 0.0, \"m5_rate\": 0.0, \"max\": 0.0, \"mean\": 0.0, \"mean_rate\": 0.0, \"min\": 0.0, \"p50\": 0.0, \"p75\": 0.0, \"p95\": 0.0, \"p98\": 0.0, \"p99\": 0.0, \"p999\": 0.0, \"rate_units\": \"calls/second\", \"stddev\": 0.0 } }, \"version\": \"3.0.0\" } NOTE: You can access a Spark subsystem's MetricsSystem using its corresponding \"leading\" port, e.g. 4040 for the driver , 8080 for Spark Standalone's master and applications . NOTE: You have to use the trailing slash ( / ) to have the output. Spark Standalone Master \u00b6 $ http http://192.168.1.4:8080/metrics/master/json/path HTTP/1.1 200 OK Cache-Control: no-cache, no-store, must-revalidate Content-Length: 207 Content-Type: text/json;charset=UTF-8 Server: Jetty(8.y.z-SNAPSHOT) X-Frame-Options: SAMEORIGIN { \"counters\": {}, \"gauges\": { \"master.aliveWorkers\": { \"value\": 0 }, \"master.apps\": { \"value\": 0 }, \"master.waitingApps\": { \"value\": 0 }, \"master.workers\": { \"value\": 0 } }, \"histograms\": {}, \"meters\": {}, \"timers\": {}, \"version\": \"3.0.0\" }","title":"Spark Metrics"},{"location":"metrics/#spark-metrics","text":"Spark Metrics gives you execution metrics of Spark subsystems ( metrics instances ), e.g. the driver of a Spark application or the master of a Spark Standalone cluster. Spark Metrics uses Dropwizard Metrics 3.1.0 Java library for the metrics infrastructure. Metrics is a Java library which gives you unparalleled insight into what your code does in production. Metrics provides a powerful toolkit of ways to measure the behavior of critical components in your production environment .","title":"Spark Metrics"},{"location":"metrics/#metricssystem","text":"Spark Metrics uses MetricsSystem . MetricsSystem uses Dropwizard Metrics' link:spark-metrics-MetricsSystem.adoc#registry[MetricRegistry] that acts as the integration point between Spark and the metrics library. A Spark subsystem can access the MetricsSystem through the SparkEnv.metricsSystem property. val metricsSystem = SparkEnv.get.metricsSystem","title":" MetricsSystem"},{"location":"metrics/#metricsconfig","text":"MetricsConfig is the configuration of the link:spark-metrics-MetricsSystem.adoc[MetricsSystem] (i.e. metrics link:spark-metrics-Source.adoc[sources] and link:spark-metrics-Sink.adoc[sinks]). metrics.properties is the default metrics configuration file. It is configured using link:spark-metrics-properties.adoc#spark.metrics.conf[spark.metrics.conf] configuration property. The file is first loaded from the path directly before using Spark's CLASSPATH. MetricsConfig also accepts a metrics configuration using spark.metrics.conf. -prefixed configuration properties. Spark comes with conf/metrics.properties.template file that is a template of metrics configuration.","title":" MetricsConfig"},{"location":"metrics/#metricsservlet-metrics-sink","text":"Among the metrics sinks is link:spark-metrics-MetricsServlet.adoc[MetricsServlet] that is used when sink.servlet metrics sink is configured in link:spark-metrics-MetricsConfig.adoc[metrics configuration]. CAUTION: FIXME Describe configuration files and properties","title":" MetricsServlet Metrics Sink"},{"location":"metrics/#jmxsink-metrics-sink","text":"Enable org.apache.spark.metrics.sink.JmxSink in link:spark-metrics-MetricsConfig.adoc[metrics configuration]. You can then use jconsole to access Spark metrics through JMX. *.sink.jmx.class=org.apache.spark.metrics.sink.JmxSink","title":" JmxSink Metrics Sink"},{"location":"metrics/#json-uri-path","text":"Metrics System is available at http://localhost:4040/metrics/json (for the default setup of a Spark application). $ http --follow http://localhost:4040/metrics/json HTTP/1.1 200 OK Cache-Control: no-cache, no-store, must-revalidate Content-Length: 2200 Content-Type: text/json;charset=utf-8 Date: Sat, 25 Feb 2017 14:14:16 GMT Server: Jetty(9.2.z-SNAPSHOT) X-Frame-Options: SAMEORIGIN { \"counters\": { \"app-20170225151406-0000.driver.HiveExternalCatalog.fileCacheHits\": { \"count\": 0 }, \"app-20170225151406-0000.driver.HiveExternalCatalog.filesDiscovered\": { \"count\": 0 }, \"app-20170225151406-0000.driver.HiveExternalCatalog.hiveClientCalls\": { \"count\": 2 }, \"app-20170225151406-0000.driver.HiveExternalCatalog.parallelListingJobCount\": { \"count\": 0 }, \"app-20170225151406-0000.driver.HiveExternalCatalog.partitionsFetched\": { \"count\": 0 } }, \"gauges\": { ... \"timers\": { \"app-20170225151406-0000.driver.DAGScheduler.messageProcessingTime\": { \"count\": 0, \"duration_units\": \"milliseconds\", \"m15_rate\": 0.0, \"m1_rate\": 0.0, \"m5_rate\": 0.0, \"max\": 0.0, \"mean\": 0.0, \"mean_rate\": 0.0, \"min\": 0.0, \"p50\": 0.0, \"p75\": 0.0, \"p95\": 0.0, \"p98\": 0.0, \"p99\": 0.0, \"p999\": 0.0, \"rate_units\": \"calls/second\", \"stddev\": 0.0 } }, \"version\": \"3.0.0\" } NOTE: You can access a Spark subsystem's MetricsSystem using its corresponding \"leading\" port, e.g. 4040 for the driver , 8080 for Spark Standalone's master and applications . NOTE: You have to use the trailing slash ( / ) to have the output.","title":"JSON URI Path"},{"location":"metrics/#spark-standalone-master","text":"$ http http://192.168.1.4:8080/metrics/master/json/path HTTP/1.1 200 OK Cache-Control: no-cache, no-store, must-revalidate Content-Length: 207 Content-Type: text/json;charset=UTF-8 Server: Jetty(8.y.z-SNAPSHOT) X-Frame-Options: SAMEORIGIN { \"counters\": {}, \"gauges\": { \"master.aliveWorkers\": { \"value\": 0 }, \"master.apps\": { \"value\": 0 }, \"master.waitingApps\": { \"value\": 0 }, \"master.workers\": { \"value\": 0 } }, \"histograms\": {}, \"meters\": {}, \"timers\": {}, \"version\": \"3.0.0\" }","title":"Spark Standalone Master"},{"location":"metrics/DAGSchedulerSource/","text":"DAGSchedulerSource \u00b6 DAGSchedulerSource is the metrics source of DAGScheduler . DAGScheduler uses Spark Metrics System to report metrics about internal status. The name of the source is DAGScheduler . DAGSchedulerSource emits the following metrics: stage.failedStages - the number of failed stages stage.runningStages - the number of running stages stage.waitingStages - the number of waiting stages job.allJobs - the number of all jobs job.activeJobs - the number of active jobs","title":"DAGSchedulerSource"},{"location":"metrics/DAGSchedulerSource/#dagschedulersource","text":"DAGSchedulerSource is the metrics source of DAGScheduler . DAGScheduler uses Spark Metrics System to report metrics about internal status. The name of the source is DAGScheduler . DAGSchedulerSource emits the following metrics: stage.failedStages - the number of failed stages stage.runningStages - the number of running stages stage.waitingStages - the number of waiting stages job.allJobs - the number of all jobs job.activeJobs - the number of active jobs","title":"DAGSchedulerSource"},{"location":"metrics/JvmSource/","text":"JvmSource \u00b6 JvmSource is a metrics source . The name of the source is jvm . JvmSource registers the build-in Codehale metrics: GarbageCollectorMetricSet MemoryUsageGaugeSet BufferPoolMetricSet Among the metrics is total.committed (from MemoryUsageGaugeSet ) that describes the current usage of the heap and non-heap memories.","title":"JvmSource"},{"location":"metrics/JvmSource/#jvmsource","text":"JvmSource is a metrics source . The name of the source is jvm . JvmSource registers the build-in Codehale metrics: GarbageCollectorMetricSet MemoryUsageGaugeSet BufferPoolMetricSet Among the metrics is total.committed (from MemoryUsageGaugeSet ) that describes the current usage of the heap and non-heap memories.","title":"JvmSource"},{"location":"metrics/MetricsConfig/","text":"MetricsConfig \u00b6 MetricsConfig is the configuration of the MetricsSystem (i.e. metrics sources and sinks ). MetricsConfig is < > when link:spark-metrics-MetricsSystem.adoc#creating-instance[MetricsSystem] is. MetricsConfig uses metrics.properties as the default metrics configuration file. It is configured using link:spark-metrics-properties.adoc#spark.metrics.conf[spark.metrics.conf] configuration property. The file is first loaded from the path directly before using Spark's CLASSPATH. MetricsConfig accepts a metrics configuration using spark.metrics.conf. -prefixed configuration properties. Spark comes with conf/metrics.properties.template file that is a template of metrics configuration. MetricsConfig < > that the < > are always defined. [[default-properties]] .MetricsConfig's Default Metrics Properties [cols=\"1,2\",options=\"header\",width=\"100%\"] |=== | Name | Description | *.sink.servlet.class | org.apache.spark.metrics.sink.MetricsServlet | *.sink.servlet.path | /metrics/json | master.sink.servlet.path | /metrics/master/json | applications.sink.servlet.path | /metrics/applications/json |=== [NOTE] \u00b6 The order of precedence of metrics configuration settings is as follows: . < > . link:spark-metrics-properties.adoc#spark.metrics.conf[spark.metrics.conf] configuration property or metrics.properties configuration file . spark.metrics.conf. -prefixed Spark properties ==== [[creating-instance]] [[conf]] MetricsConfig takes a xref:ROOT:SparkConf.adoc[SparkConf] when created. [[internal-registries]] .MetricsConfig's Internal Registries and Counters [cols=\"1,2\",options=\"header\",width=\"100%\"] |=== | Name | Description | [[properties]] properties | https://docs.oracle.com/javase/8/docs/api/java/util/Properties.html[java.util.Properties ] with metrics properties Used to < > per-subsystem's < >. | [[perInstanceSubProperties]] perInstanceSubProperties | Lookup table of metrics properties per subsystem |=== === [[initialize]] Initializing MetricsConfig -- initialize Method [source, scala] \u00b6 initialize(): Unit \u00b6 initialize < > and < > (that is defined using link:spark-metrics-properties.adoc#spark.metrics.conf[spark.metrics.conf] configuration property). initialize takes all Spark properties that start with spark.metrics.conf. prefix from < > and adds them to < > (without the prefix). In the end, initialize splits < > with the default configuration (denoted as * ) assigned to all subsystems afterwards. NOTE: initialize accepts * (star) for the default configuration or any combination of lower- and upper-case letters for Spark subsystem names. NOTE: initialize is used exclusively when MetricsSystem is link:spark-metrics-MetricsSystem.adoc#creating-instance[created]. === [[setDefaultProperties]] setDefaultProperties Internal Method [source, scala] \u00b6 setDefaultProperties(prop: Properties): Unit \u00b6 setDefaultProperties sets the < > (in the input prop ). NOTE: setDefaultProperties is used exclusively when MetricsConfig < >. === [[loadPropertiesFromFile]] Loading Custom Metrics Configuration File or metrics.properties -- loadPropertiesFromFile Method [source, scala] \u00b6 loadPropertiesFromFile(path: Option[String]): Unit \u00b6 loadPropertiesFromFile tries to open the input path file (if defined) or the default metrics configuration file metrics.properties (on CLASSPATH). If either file is available, loadPropertiesFromFile loads the properties (to < > registry). In case of exceptions, you should see the following ERROR message in the logs followed by the exception. ERROR Error loading configuration file [file] NOTE: loadPropertiesFromFile is used exclusively when MetricsConfig < >. === [[subProperties]] Grouping Properties Per Subsystem -- subProperties Method [source, scala] \u00b6 subProperties(prop: Properties, regex: Regex): mutable.HashMap[String, Properties] \u00b6 subProperties takes prop properties and destructures keys given regex . subProperties takes the matching prefix (of a key per regex ) and uses it as a new key with the value(s) being the matching suffix(es). [source, scala] \u00b6 driver.hello.world => (driver, (hello.world)) \u00b6 NOTE: subProperties is used when MetricsConfig < > (to apply the default metrics configuration) and when MetricsSystem link:spark-metrics-MetricsSystem.adoc#registerSources[registers metrics sources] and link:spark-metrics-MetricsSystem.adoc#registerSinks[sinks]. === [[getInstance]] getInstance Method [source, scala] \u00b6 getInstance(inst: String): Properties \u00b6 getInstance ...FIXME NOTE: getInstance is used when...FIXME","title":"MetricsConfig"},{"location":"metrics/MetricsConfig/#metricsconfig","text":"MetricsConfig is the configuration of the MetricsSystem (i.e. metrics sources and sinks ). MetricsConfig is < > when link:spark-metrics-MetricsSystem.adoc#creating-instance[MetricsSystem] is. MetricsConfig uses metrics.properties as the default metrics configuration file. It is configured using link:spark-metrics-properties.adoc#spark.metrics.conf[spark.metrics.conf] configuration property. The file is first loaded from the path directly before using Spark's CLASSPATH. MetricsConfig accepts a metrics configuration using spark.metrics.conf. -prefixed configuration properties. Spark comes with conf/metrics.properties.template file that is a template of metrics configuration. MetricsConfig < > that the < > are always defined. [[default-properties]] .MetricsConfig's Default Metrics Properties [cols=\"1,2\",options=\"header\",width=\"100%\"] |=== | Name | Description | *.sink.servlet.class | org.apache.spark.metrics.sink.MetricsServlet | *.sink.servlet.path | /metrics/json | master.sink.servlet.path | /metrics/master/json | applications.sink.servlet.path | /metrics/applications/json |===","title":"MetricsConfig"},{"location":"metrics/MetricsConfig/#note","text":"The order of precedence of metrics configuration settings is as follows: . < > . link:spark-metrics-properties.adoc#spark.metrics.conf[spark.metrics.conf] configuration property or metrics.properties configuration file . spark.metrics.conf. -prefixed Spark properties ==== [[creating-instance]] [[conf]] MetricsConfig takes a xref:ROOT:SparkConf.adoc[SparkConf] when created. [[internal-registries]] .MetricsConfig's Internal Registries and Counters [cols=\"1,2\",options=\"header\",width=\"100%\"] |=== | Name | Description | [[properties]] properties | https://docs.oracle.com/javase/8/docs/api/java/util/Properties.html[java.util.Properties ] with metrics properties Used to < > per-subsystem's < >. | [[perInstanceSubProperties]] perInstanceSubProperties | Lookup table of metrics properties per subsystem |=== === [[initialize]] Initializing MetricsConfig -- initialize Method","title":"[NOTE]"},{"location":"metrics/MetricsConfig/#source-scala","text":"","title":"[source, scala]"},{"location":"metrics/MetricsConfig/#initialize-unit","text":"initialize < > and < > (that is defined using link:spark-metrics-properties.adoc#spark.metrics.conf[spark.metrics.conf] configuration property). initialize takes all Spark properties that start with spark.metrics.conf. prefix from < > and adds them to < > (without the prefix). In the end, initialize splits < > with the default configuration (denoted as * ) assigned to all subsystems afterwards. NOTE: initialize accepts * (star) for the default configuration or any combination of lower- and upper-case letters for Spark subsystem names. NOTE: initialize is used exclusively when MetricsSystem is link:spark-metrics-MetricsSystem.adoc#creating-instance[created]. === [[setDefaultProperties]] setDefaultProperties Internal Method","title":"initialize(): Unit"},{"location":"metrics/MetricsConfig/#source-scala_1","text":"","title":"[source, scala]"},{"location":"metrics/MetricsConfig/#setdefaultpropertiesprop-properties-unit","text":"setDefaultProperties sets the < > (in the input prop ). NOTE: setDefaultProperties is used exclusively when MetricsConfig < >. === [[loadPropertiesFromFile]] Loading Custom Metrics Configuration File or metrics.properties -- loadPropertiesFromFile Method","title":"setDefaultProperties(prop: Properties): Unit"},{"location":"metrics/MetricsConfig/#source-scala_2","text":"","title":"[source, scala]"},{"location":"metrics/MetricsConfig/#loadpropertiesfromfilepath-optionstring-unit","text":"loadPropertiesFromFile tries to open the input path file (if defined) or the default metrics configuration file metrics.properties (on CLASSPATH). If either file is available, loadPropertiesFromFile loads the properties (to < > registry). In case of exceptions, you should see the following ERROR message in the logs followed by the exception. ERROR Error loading configuration file [file] NOTE: loadPropertiesFromFile is used exclusively when MetricsConfig < >. === [[subProperties]] Grouping Properties Per Subsystem -- subProperties Method","title":"loadPropertiesFromFile(path: Option[String]): Unit"},{"location":"metrics/MetricsConfig/#source-scala_3","text":"","title":"[source, scala]"},{"location":"metrics/MetricsConfig/#subpropertiesprop-properties-regex-regex-mutablehashmapstring-properties","text":"subProperties takes prop properties and destructures keys given regex . subProperties takes the matching prefix (of a key per regex ) and uses it as a new key with the value(s) being the matching suffix(es).","title":"subProperties(prop: Properties, regex: Regex): mutable.HashMap[String, Properties]"},{"location":"metrics/MetricsConfig/#source-scala_4","text":"","title":"[source, scala]"},{"location":"metrics/MetricsConfig/#driverhelloworld-driver-helloworld","text":"NOTE: subProperties is used when MetricsConfig < > (to apply the default metrics configuration) and when MetricsSystem link:spark-metrics-MetricsSystem.adoc#registerSources[registers metrics sources] and link:spark-metrics-MetricsSystem.adoc#registerSinks[sinks]. === [[getInstance]] getInstance Method","title":"driver.hello.world =&gt; (driver, (hello.world))"},{"location":"metrics/MetricsConfig/#source-scala_5","text":"","title":"[source, scala]"},{"location":"metrics/MetricsConfig/#getinstanceinst-string-properties","text":"getInstance ...FIXME NOTE: getInstance is used when...FIXME","title":"getInstance(inst: String): Properties"},{"location":"metrics/MetricsServlet/","text":"MetricsServlet JSON Metrics Sink \u00b6 MetricsServlet is a metrics sink that gives metrics snapshots in JSON format. MetricsServlet is a \"special\" sink as it is only available to the metrics instances with a web UI: Driver of a Spark application Spark Standalone's Master and Worker You can access the metrics from MetricsServlet at /metrics/json URI by default. The entire URL depends on a metrics instance, e.g. http://localhost:4040/metrics/json/ for a running Spark application. $ http http://localhost:4040/metrics/json/ HTTP/1.1 200 OK Cache-Control: no-cache, no-store, must-revalidate Content-Length: 5005 Content-Type: text/json;charset=utf-8 Date: Mon, 11 Jun 2018 06:29:03 GMT Server: Jetty(9.3.z-SNAPSHOT) X-Content-Type-Options: nosniff X-Frame-Options: SAMEORIGIN X-XSS-Protection: 1; mode=block { \"counters\": { \"local-1528698499919.driver.HiveExternalCatalog.fileCacheHits\": { \"count\": 0 }, \"local-1528698499919.driver.HiveExternalCatalog.filesDiscovered\": { \"count\": 0 }, \"local-1528698499919.driver.HiveExternalCatalog.hiveClientCalls\": { \"count\": 0 }, \"local-1528698499919.driver.HiveExternalCatalog.parallelListingJobCount\": { \"count\": 0 }, \"local-1528698499919.driver.HiveExternalCatalog.partitionsFetched\": { \"count\": 0 }, \"local-1528698499919.driver.LiveListenerBus.numEventsPosted\": { \"count\": 7 }, \"local-1528698499919.driver.LiveListenerBus.queue.appStatus.numDroppedEvents\": { \"count\": 0 }, \"local-1528698499919.driver.LiveListenerBus.queue.executorManagement.numDroppedEvents\": { \"count\": 0 } }, ... MetricsServlet is < > exclusively when MetricsSystem is link:spark-metrics-MetricsSystem.adoc#start[started] (and requested to link:spark-metrics-MetricsSystem.adoc#registerSinks[register metrics sinks]). MetricsServlet can be configured using configuration properties with sink.servlet prefix (in link:spark-metrics-MetricsConfig.adoc[metrics configuration]). That is not required since MetricsConfig link:spark-metrics-MetricsConfig.adoc#setDefaultProperties[makes sure] that MetricsServlet is always configured. MetricsServlet uses https://fasterxml.github.io/jackson-databind/[jackson-databind ], the general data-binding package for Jackson (as < >) with https://metrics.dropwizard.io/3.1.0/[Dropwizard Metrics] library (i.e. registering a Coda Hale MetricsModule ). [[properties]] .MetricsServlet's Configuration Properties [cols=\"1,1,2\",options=\"header\",width=\"100%\"] |=== | Name | Default | Description | path | /metrics/json/ | [[path]] Path URI prefix to bind to | sample | false | [[sample]] Whether to show entire set of samples for histograms |=== [[internal-registries]] .MetricsServlet's Internal Properties (e.g. Registries, Counters and Flags) [cols=\"1,2\",options=\"header\",width=\"100%\"] |=== | Name | Description | mapper | [[mapper]] Jaxson's https://fasterxml.github.io/jackson-databind/javadoc/2.6/com/fasterxml/jackson/databind/ObjectMapper.html[com.fasterxml.jackson.databind.ObjectMapper ] that \"provides functionality for reading and writing JSON, either to and from basic POJOs (Plain Old Java Objects), or to and from a general-purpose JSON Tree Model (JsonNode), as well as related functionality for performing conversions.\" When created, mapper is requested to register a Coda Hale https://metrics.dropwizard.io/3.1.0/apidocs/com/codahale/metrics/json/MetricsModule.html[com.codahale.metrics.json.MetricsModule ]. Used exclusively when MetricsServlet is requested to < >. | servletPath | [[servletPath]] Value of < > configuration property | servletShowSample | [[servletShowSample]] Flag to control whether to show samples ( true ) or not ( false ). servletShowSample is the value of < > configuration property (if defined) or false . Used when < > is requested to register a Coda Hale https://metrics.dropwizard.io/3.1.0/apidocs/com/codahale/metrics/json/MetricsModule.html[com.codahale.metrics.json.MetricsModule ]. |=== === [[creating-instance]] Creating MetricsServlet Instance MetricsServlet takes the following when created: [[property]] Configuration Properties (as Java Properties ) [[registry]] Dropwizard Metrics' https://metrics.dropwizard.io/3.1.0/apidocs/com/codahale/metrics/MetricRegistry.html[MetricRegistry ] [[securityMgr]] SecurityManager MetricsServlet initializes the < >. === [[getMetricsSnapshot]] Requesting Metrics Snapshot -- getMetricsSnapshot Method [source, scala] \u00b6 getMetricsSnapshot(request: HttpServletRequest): String \u00b6 getMetricsSnapshot simply requests the < > to serialize the < > to a JSON string (using link:++ https://fasterxml.github.io/jackson-databind/javadoc/2.6/com/fasterxml/jackson/databind/ObjectMapper.html#writeValueAsString-java.lang.Object-++[ObjectMapper.writeValueAsString ]). NOTE: getMetricsSnapshot is used exclusively when MetricsServlet is requested to < >. === [[getHandlers]] Requesting JSON Servlet Handler -- getHandlers Method [source, scala] \u00b6 getHandlers(conf: SparkConf): Array[ServletContextHandler] \u00b6 getHandlers returns just a single ServletContextHandler (in a collection) that gives < > in JSON format at every request at < > URI path. NOTE: getHandlers is used exclusively when MetricsSystem is requested for link:spark-metrics-MetricsSystem.adoc#getServletHandlers[metrics ServletContextHandlers].","title":"MetricsServlet"},{"location":"metrics/MetricsServlet/#metricsservlet-json-metrics-sink","text":"MetricsServlet is a metrics sink that gives metrics snapshots in JSON format. MetricsServlet is a \"special\" sink as it is only available to the metrics instances with a web UI: Driver of a Spark application Spark Standalone's Master and Worker You can access the metrics from MetricsServlet at /metrics/json URI by default. The entire URL depends on a metrics instance, e.g. http://localhost:4040/metrics/json/ for a running Spark application. $ http http://localhost:4040/metrics/json/ HTTP/1.1 200 OK Cache-Control: no-cache, no-store, must-revalidate Content-Length: 5005 Content-Type: text/json;charset=utf-8 Date: Mon, 11 Jun 2018 06:29:03 GMT Server: Jetty(9.3.z-SNAPSHOT) X-Content-Type-Options: nosniff X-Frame-Options: SAMEORIGIN X-XSS-Protection: 1; mode=block { \"counters\": { \"local-1528698499919.driver.HiveExternalCatalog.fileCacheHits\": { \"count\": 0 }, \"local-1528698499919.driver.HiveExternalCatalog.filesDiscovered\": { \"count\": 0 }, \"local-1528698499919.driver.HiveExternalCatalog.hiveClientCalls\": { \"count\": 0 }, \"local-1528698499919.driver.HiveExternalCatalog.parallelListingJobCount\": { \"count\": 0 }, \"local-1528698499919.driver.HiveExternalCatalog.partitionsFetched\": { \"count\": 0 }, \"local-1528698499919.driver.LiveListenerBus.numEventsPosted\": { \"count\": 7 }, \"local-1528698499919.driver.LiveListenerBus.queue.appStatus.numDroppedEvents\": { \"count\": 0 }, \"local-1528698499919.driver.LiveListenerBus.queue.executorManagement.numDroppedEvents\": { \"count\": 0 } }, ... MetricsServlet is < > exclusively when MetricsSystem is link:spark-metrics-MetricsSystem.adoc#start[started] (and requested to link:spark-metrics-MetricsSystem.adoc#registerSinks[register metrics sinks]). MetricsServlet can be configured using configuration properties with sink.servlet prefix (in link:spark-metrics-MetricsConfig.adoc[metrics configuration]). That is not required since MetricsConfig link:spark-metrics-MetricsConfig.adoc#setDefaultProperties[makes sure] that MetricsServlet is always configured. MetricsServlet uses https://fasterxml.github.io/jackson-databind/[jackson-databind ], the general data-binding package for Jackson (as < >) with https://metrics.dropwizard.io/3.1.0/[Dropwizard Metrics] library (i.e. registering a Coda Hale MetricsModule ). [[properties]] .MetricsServlet's Configuration Properties [cols=\"1,1,2\",options=\"header\",width=\"100%\"] |=== | Name | Default | Description | path | /metrics/json/ | [[path]] Path URI prefix to bind to | sample | false | [[sample]] Whether to show entire set of samples for histograms |=== [[internal-registries]] .MetricsServlet's Internal Properties (e.g. Registries, Counters and Flags) [cols=\"1,2\",options=\"header\",width=\"100%\"] |=== | Name | Description | mapper | [[mapper]] Jaxson's https://fasterxml.github.io/jackson-databind/javadoc/2.6/com/fasterxml/jackson/databind/ObjectMapper.html[com.fasterxml.jackson.databind.ObjectMapper ] that \"provides functionality for reading and writing JSON, either to and from basic POJOs (Plain Old Java Objects), or to and from a general-purpose JSON Tree Model (JsonNode), as well as related functionality for performing conversions.\" When created, mapper is requested to register a Coda Hale https://metrics.dropwizard.io/3.1.0/apidocs/com/codahale/metrics/json/MetricsModule.html[com.codahale.metrics.json.MetricsModule ]. Used exclusively when MetricsServlet is requested to < >. | servletPath | [[servletPath]] Value of < > configuration property | servletShowSample | [[servletShowSample]] Flag to control whether to show samples ( true ) or not ( false ). servletShowSample is the value of < > configuration property (if defined) or false . Used when < > is requested to register a Coda Hale https://metrics.dropwizard.io/3.1.0/apidocs/com/codahale/metrics/json/MetricsModule.html[com.codahale.metrics.json.MetricsModule ]. |=== === [[creating-instance]] Creating MetricsServlet Instance MetricsServlet takes the following when created: [[property]] Configuration Properties (as Java Properties ) [[registry]] Dropwizard Metrics' https://metrics.dropwizard.io/3.1.0/apidocs/com/codahale/metrics/MetricRegistry.html[MetricRegistry ] [[securityMgr]] SecurityManager MetricsServlet initializes the < >. === [[getMetricsSnapshot]] Requesting Metrics Snapshot -- getMetricsSnapshot Method","title":"MetricsServlet JSON Metrics Sink"},{"location":"metrics/MetricsServlet/#source-scala","text":"","title":"[source, scala]"},{"location":"metrics/MetricsServlet/#getmetricssnapshotrequest-httpservletrequest-string","text":"getMetricsSnapshot simply requests the < > to serialize the < > to a JSON string (using link:++ https://fasterxml.github.io/jackson-databind/javadoc/2.6/com/fasterxml/jackson/databind/ObjectMapper.html#writeValueAsString-java.lang.Object-++[ObjectMapper.writeValueAsString ]). NOTE: getMetricsSnapshot is used exclusively when MetricsServlet is requested to < >. === [[getHandlers]] Requesting JSON Servlet Handler -- getHandlers Method","title":"getMetricsSnapshot(request: HttpServletRequest): String"},{"location":"metrics/MetricsServlet/#source-scala_1","text":"","title":"[source, scala]"},{"location":"metrics/MetricsServlet/#gethandlersconf-sparkconf-arrayservletcontexthandler","text":"getHandlers returns just a single ServletContextHandler (in a collection) that gives < > in JSON format at every request at < > URI path. NOTE: getHandlers is used exclusively when MetricsSystem is requested for link:spark-metrics-MetricsSystem.adoc#getServletHandlers[metrics ServletContextHandlers].","title":"getHandlers(conf: SparkConf): Array[ServletContextHandler]"},{"location":"metrics/MetricsSystem/","text":"MetricsSystem \u00b6 MetricsSystem is a registry of metrics sources and sinks of a Spark subsystem (e.g. the driver of a Spark application). MetricsSystem may have at most one < > (which is link:spark-metrics-MetricsConfig.adoc#setDefaultProperties[registered by default]). When < >, MetricsSystem requests < > to link:spark-metrics-MetricsConfig.adoc#initialize[initialize]. .Creating MetricsSystem image::spark-metrics-MetricsSystem.png[align=\"center\"] [[metrics-instances]] [[subsystems]] .Metrics Instances (Subsystems) and MetricsSystems [cols=\"1,2\",options=\"header\",width=\"100%\"] |=== | Name | When Created | applications | Spark Standalone's Master is link:spark-standalone-Master.adoc#creating-instance[created]. | driver | SparkEnv is xref:core:SparkEnv.adoc#create[created] for the driver. | executor | SparkEnv is xref:core:SparkEnv.adoc#create[created] for an executor. | master | Spark Standalone's Master is link:spark-standalone-Master.adoc#creating-instance[created]. | mesos_cluster | Spark on Mesos' MesosClusterScheduler is created. | shuffleService | ExternalShuffleService is xref:deploy:ExternalShuffleService.adoc#creating-instance[created]. | worker | Spark Standalone's Worker is link:spark-standalone-worker.adoc#creating-instance[created]. |=== MetricsSystem uses < > as the integration point to Dropwizard Metrics library. [[internal-registries]] .MetricsSystem's Internal Registries and Counters [cols=\"1,2\",options=\"header\",width=\"100%\"] |=== | Name | Description | [[metricsConfig]] metricsConfig | link:spark-metrics-MetricsConfig.adoc[MetricsConfig] Initialized when MetricsSystem is < >. Used when MetricsSystem registers < > and < >. | [[metricsServlet]] metricsServlet | link:spark-metrics-MetricsServlet.adoc[MetricsServlet JSON metrics sink] that is only available for the < > with a web UI, i.e. the driver of a Spark application and Spark Standalone's Master . Initialized when MetricsSystem registers < > (and finds a configuration entry with servlet sink name). Used exclusively when MetricsSystem is requested for a < >. | [[registry]] registry a| Dropwizard Metrics' https://metrics.dropwizard.io/3.1.0/apidocs/com/codahale/metrics/MetricRegistry.html[MetricRegistry ] Used when MetricsSystem is requested to: < > < > < > (that in turn < >) | [[running]] running | Flag that indicates whether MetricsSystem has been < > ( true ) or not ( false ) Default: false | [[sinks]] sinks | link:spark-metrics-Sink.adoc[Metrics sinks] in a Spark application. Used when MetricsSystem < > and < >. | [[sources]] sources | link:spark-metrics-Source.adoc[Metrics sources] in a Spark application. Used when MetricsSystem < >. |=== [TIP] \u00b6 Enable WARN or ERROR logging levels for org.apache.spark.metrics.MetricsSystem logger to see what happens in MetricsSystem. Add the following line to conf/log4j.properties : log4j.logger.org.apache.spark.metrics.MetricsSystem=WARN Refer to link:spark-logging.adoc[Logging]. \u00b6 == [[StaticSources]] \"Static\" Metrics Sources for Spark SQL -- StaticSources CAUTION: FIXME == [[registerSource]] Registering Metrics Source -- registerSource Method [source, scala] \u00b6 registerSource(source: Source): Unit \u00b6 registerSource adds source to < > internal registry. registerSource < > for the metrics source and registers it with < >. NOTE: registerSource uses Metrics' link:++ http://metrics.dropwizard.io/3.1.0/apidocs/com/codahale/metrics/MetricRegistry.html#register-java.lang.String-T-++[MetricRegistry.register ] to register a metrics source under a given name. When registerSource tries to register a name more than once, you should see the following INFO message in the logs: INFO Metrics already registered [NOTE] \u00b6 registerSource is used when: SparkContext link:spark-SparkContext-creating-instance-internals.adoc#registerSource[registers metrics sources] for: ** xref:scheduler:DAGScheduler.adoc#metricsSource[DAGScheduler] ** link:spark-BlockManager-BlockManagerSource.adoc[BlockManager] ** link:spark-ExecutorAllocationManager.adoc#executorAllocationManagerSource[ExecutorAllocationManager] (for xref:ROOT:spark-dynamic-allocation.adoc[]) MetricsSystem < > (and registers the \"static\" metrics sources -- CodegenMetrics and HiveCatalogMetrics ) and does < >. Executor xref:executor:Executor.adoc#creating-instance[is created] (and registers a xref:executor:ExecutorSource.adoc[]) ExternalShuffleService xref:deploy:ExternalShuffleService.adoc#start[is started] (and registers ExternalShuffleServiceSource ) Spark Structured Streaming's StreamExecution runs batches as data arrives (when metrics are enabled). Spark Streaming's StreamingContext is started (and registers StreamingSource ) Spark Standalone's Master and Worker start (and register their MasterSource and WorkerSource , respectively) Spark Standalone's Master registers a Spark application (and registers a ApplicationSource ) Spark on Mesos' MesosClusterScheduler is started (and registers a MesosClusterSchedulerSource ) \u00b6 == [[buildRegistryName]] Building Metrics Source Identifier -- buildRegistryName Method [source, scala] \u00b6 buildRegistryName(source: Source): String \u00b6 NOTE: buildRegistryName is used to build the metrics source identifiers for a Spark application's driver and executors, but also for other Spark framework's components (e.g. Spark Standalone's master and workers). NOTE: buildRegistryName uses link:spark-metrics-properties.adoc#spark.metrics.namespace[spark.metrics.namespace] and xref:executor:Executor.adoc#spark.executor.id[spark.executor.id] Spark properties to differentiate between a Spark application's driver and executors, and the other Spark framework's components. (only when < > is driver or executor ) buildRegistryName builds metrics source name that is made up of link:spark-metrics-properties.adoc#spark.metrics.namespace[spark.metrics.namespace], xref:executor:Executor.adoc#spark.executor.id[spark.executor.id] and the name of the source . NOTE: buildRegistryName uses Dropwizard Metrics' https://metrics.dropwizard.io/3.1.0/apidocs/com/codahale/metrics/MetricRegistry.html[MetricRegistry ] to build metrics source identifiers. CAUTION: FIXME Finish for the other components. NOTE: buildRegistryName is used when MetricsSystem < > or < > a metrics source. == [[registerSources]] Registering Metrics Sources for Spark Instance -- registerSources Internal Method [source, scala] \u00b6 registerSources(): Unit \u00b6 registerSources finds < > configuration for the < >. NOTE: instance is defined when MetricsSystem < >. registerSources finds the configuration of all the link:spark-metrics-Source.adoc[metrics sources] for the subsystem (as described with source. prefix). For every metrics source, registerSources finds class property, creates an instance, and in the end < >. When registerSources fails, you should see the following ERROR message in the logs followed by the exception. ERROR Source class [classPath] cannot be instantiated NOTE: registerSources is used exclusively when MetricsSystem is < >. == [[getServletHandlers]] Requesting JSON Servlet Handler -- getServletHandlers Method [source, scala] \u00b6 getServletHandlers: Array[ServletContextHandler] \u00b6 If the MetricsSystem is < > and the < > is defined for the metrics system, getServletHandlers simply requests the < > for the link:spark-metrics-MetricsServlet.adoc#getHandlers[JSON servlet handler]. When MetricsSystem is not < > getServletHandlers throws an IllegalArgumentException . Can only call getServletHandlers on a running MetricsSystem [NOTE] \u00b6 getServletHandlers is used when: SparkContext is link:spark-SparkContext-creating-instance-internals.adoc#MetricsSystem-getServletHandlers[created] * Spark Standalone's Master and Worker are requested to start (as onStart ) \u00b6 == [[registerSinks]] Registering Metrics Sinks -- registerSinks Internal Method [source, scala] \u00b6 registerSinks(): Unit \u00b6 registerSinks requests the < > for the link:spark-metrics-MetricsConfig.adoc#getInstance[configuration] of the < >. registerSinks requests the < > for the link:spark-metrics-MetricsConfig.adoc#subProperties[configuration] of all metrics sinks (i.e. configuration entries that match ^sink\\\\.(.+)\\\\.(.+) regular expression). For every metrics sink configuration, registerSinks takes class property and (if defined) creates an instance of the metric sink using an constructor that takes the configuration, < > and < >. For a single servlet metrics sink, registerSinks converts the sink to a link:spark-metrics-MetricsServlet.adoc[MetricsServlet] and sets the < > internal registry. For all other metrics sinks, registerSinks adds the sink to the < > internal registry. In case of an Exception , registerSinks prints out the following ERROR message to the logs: Sink class [classPath] cannot be instantiated NOTE: registerSinks is used exclusively when MetricsSystem is requested to < >. == [[stop]] stop Method [source, scala] \u00b6 stop(): Unit \u00b6 stop ...FIXME NOTE: stop is used when...FIXME == [[getSourcesByName]] getSourcesByName Method [source, scala] \u00b6 getSourcesByName(sourceName: String): Seq[Source] \u00b6 getSourcesByName ...FIXME NOTE: getSourcesByName is used when...FIXME == [[removeSource]] removeSource Method [source, scala] \u00b6 removeSource(source: Source): Unit \u00b6 removeSource ...FIXME NOTE: removeSource is used when...FIXME == [[creating-instance]] Creating MetricsSystem Instance MetricsSystem takes the following when created: [[instance]] Instance name [[conf]] xref:ROOT:SparkConf.adoc[SparkConf] [[securityMgr]] SecurityManager MetricsSystem initializes the < >. When created, MetricsSystem requests < > to link:spark-metrics-MetricsConfig.adoc#initialize[initialize]. NOTE: < > is used to create a new MetricsSystems instance instead. == [[createMetricsSystem]] Creating MetricsSystem Instance For Subsystem -- createMetricsSystem Factory Method [source, scala] \u00b6 createMetricsSystem( instance: String conf: SparkConf securityMgr: SecurityManager): MetricsSystem createMetricsSystem returns a new < >. NOTE: createMetricsSystem is used when a < > is created. == [[report]] Requesting Sinks to Report Metrics -- report Method [source, scala] \u00b6 report(): Unit \u00b6 report simply requests the registered < > to link:spark-metrics-Sink.adoc#report[report metrics]. NOTE: report is used when xref:ROOT:SparkContext.adoc#stop[SparkContext], xref:executor:Executor.adoc#stop[Executor], Spark Standalone's Master and Worker , Spark on Mesos' MesosClusterScheduler are requested to stop == [[start]] Starting MetricsSystem -- start Method [source, scala] \u00b6 start(): Unit \u00b6 start turns < > flag on. NOTE: start can only be called once and < > an IllegalArgumentException when called multiple times. start < > the < > for Spark SQL, i.e. CodegenMetrics and HiveCatalogMetrics . start then registers the configured metrics < > and < > for the < >. In the end, start requests the registered < > to link:spark-metrics-Sink.adoc#start[start]. [[start-IllegalArgumentException]] start throws an IllegalArgumentException when < > flag is on. requirement failed: Attempting to start a MetricsSystem that is already running [NOTE] \u00b6 start is used when: SparkContext is link:spark-SparkContext-creating-instance-internals.adoc#MetricsSystem-start[created] SparkEnv is xref:core:SparkEnv.adoc#create[created] (on executors) ExternalShuffleService is requested to xref:deploy:ExternalShuffleService.adoc#start[start] * Spark Standalone's Master and Worker , and Spark on Mesos' MesosClusterScheduler are requested to start \u00b6","title":"MetricsSystem"},{"location":"metrics/MetricsSystem/#metricssystem","text":"MetricsSystem is a registry of metrics sources and sinks of a Spark subsystem (e.g. the driver of a Spark application). MetricsSystem may have at most one < > (which is link:spark-metrics-MetricsConfig.adoc#setDefaultProperties[registered by default]). When < >, MetricsSystem requests < > to link:spark-metrics-MetricsConfig.adoc#initialize[initialize]. .Creating MetricsSystem image::spark-metrics-MetricsSystem.png[align=\"center\"] [[metrics-instances]] [[subsystems]] .Metrics Instances (Subsystems) and MetricsSystems [cols=\"1,2\",options=\"header\",width=\"100%\"] |=== | Name | When Created | applications | Spark Standalone's Master is link:spark-standalone-Master.adoc#creating-instance[created]. | driver | SparkEnv is xref:core:SparkEnv.adoc#create[created] for the driver. | executor | SparkEnv is xref:core:SparkEnv.adoc#create[created] for an executor. | master | Spark Standalone's Master is link:spark-standalone-Master.adoc#creating-instance[created]. | mesos_cluster | Spark on Mesos' MesosClusterScheduler is created. | shuffleService | ExternalShuffleService is xref:deploy:ExternalShuffleService.adoc#creating-instance[created]. | worker | Spark Standalone's Worker is link:spark-standalone-worker.adoc#creating-instance[created]. |=== MetricsSystem uses < > as the integration point to Dropwizard Metrics library. [[internal-registries]] .MetricsSystem's Internal Registries and Counters [cols=\"1,2\",options=\"header\",width=\"100%\"] |=== | Name | Description | [[metricsConfig]] metricsConfig | link:spark-metrics-MetricsConfig.adoc[MetricsConfig] Initialized when MetricsSystem is < >. Used when MetricsSystem registers < > and < >. | [[metricsServlet]] metricsServlet | link:spark-metrics-MetricsServlet.adoc[MetricsServlet JSON metrics sink] that is only available for the < > with a web UI, i.e. the driver of a Spark application and Spark Standalone's Master . Initialized when MetricsSystem registers < > (and finds a configuration entry with servlet sink name). Used exclusively when MetricsSystem is requested for a < >. | [[registry]] registry a| Dropwizard Metrics' https://metrics.dropwizard.io/3.1.0/apidocs/com/codahale/metrics/MetricRegistry.html[MetricRegistry ] Used when MetricsSystem is requested to: < > < > < > (that in turn < >) | [[running]] running | Flag that indicates whether MetricsSystem has been < > ( true ) or not ( false ) Default: false | [[sinks]] sinks | link:spark-metrics-Sink.adoc[Metrics sinks] in a Spark application. Used when MetricsSystem < > and < >. | [[sources]] sources | link:spark-metrics-Source.adoc[Metrics sources] in a Spark application. Used when MetricsSystem < >. |===","title":"MetricsSystem"},{"location":"metrics/MetricsSystem/#tip","text":"Enable WARN or ERROR logging levels for org.apache.spark.metrics.MetricsSystem logger to see what happens in MetricsSystem. Add the following line to conf/log4j.properties : log4j.logger.org.apache.spark.metrics.MetricsSystem=WARN","title":"[TIP]"},{"location":"metrics/MetricsSystem/#refer-to-linkspark-loggingadoclogging","text":"== [[StaticSources]] \"Static\" Metrics Sources for Spark SQL -- StaticSources CAUTION: FIXME == [[registerSource]] Registering Metrics Source -- registerSource Method","title":"Refer to link:spark-logging.adoc[Logging]."},{"location":"metrics/MetricsSystem/#source-scala","text":"","title":"[source, scala]"},{"location":"metrics/MetricsSystem/#registersourcesource-source-unit","text":"registerSource adds source to < > internal registry. registerSource < > for the metrics source and registers it with < >. NOTE: registerSource uses Metrics' link:++ http://metrics.dropwizard.io/3.1.0/apidocs/com/codahale/metrics/MetricRegistry.html#register-java.lang.String-T-++[MetricRegistry.register ] to register a metrics source under a given name. When registerSource tries to register a name more than once, you should see the following INFO message in the logs: INFO Metrics already registered","title":"registerSource(source: Source): Unit"},{"location":"metrics/MetricsSystem/#note","text":"registerSource is used when: SparkContext link:spark-SparkContext-creating-instance-internals.adoc#registerSource[registers metrics sources] for: ** xref:scheduler:DAGScheduler.adoc#metricsSource[DAGScheduler] ** link:spark-BlockManager-BlockManagerSource.adoc[BlockManager] ** link:spark-ExecutorAllocationManager.adoc#executorAllocationManagerSource[ExecutorAllocationManager] (for xref:ROOT:spark-dynamic-allocation.adoc[]) MetricsSystem < > (and registers the \"static\" metrics sources -- CodegenMetrics and HiveCatalogMetrics ) and does < >. Executor xref:executor:Executor.adoc#creating-instance[is created] (and registers a xref:executor:ExecutorSource.adoc[]) ExternalShuffleService xref:deploy:ExternalShuffleService.adoc#start[is started] (and registers ExternalShuffleServiceSource ) Spark Structured Streaming's StreamExecution runs batches as data arrives (when metrics are enabled). Spark Streaming's StreamingContext is started (and registers StreamingSource ) Spark Standalone's Master and Worker start (and register their MasterSource and WorkerSource , respectively) Spark Standalone's Master registers a Spark application (and registers a ApplicationSource )","title":"[NOTE]"},{"location":"metrics/MetricsSystem/#spark-on-mesos-mesosclusterscheduler-is-started-and-registers-a-mesosclusterschedulersource","text":"== [[buildRegistryName]] Building Metrics Source Identifier -- buildRegistryName Method","title":"Spark on Mesos' MesosClusterScheduler is started (and registers a MesosClusterSchedulerSource)"},{"location":"metrics/MetricsSystem/#source-scala_1","text":"","title":"[source, scala]"},{"location":"metrics/MetricsSystem/#buildregistrynamesource-source-string","text":"NOTE: buildRegistryName is used to build the metrics source identifiers for a Spark application's driver and executors, but also for other Spark framework's components (e.g. Spark Standalone's master and workers). NOTE: buildRegistryName uses link:spark-metrics-properties.adoc#spark.metrics.namespace[spark.metrics.namespace] and xref:executor:Executor.adoc#spark.executor.id[spark.executor.id] Spark properties to differentiate between a Spark application's driver and executors, and the other Spark framework's components. (only when < > is driver or executor ) buildRegistryName builds metrics source name that is made up of link:spark-metrics-properties.adoc#spark.metrics.namespace[spark.metrics.namespace], xref:executor:Executor.adoc#spark.executor.id[spark.executor.id] and the name of the source . NOTE: buildRegistryName uses Dropwizard Metrics' https://metrics.dropwizard.io/3.1.0/apidocs/com/codahale/metrics/MetricRegistry.html[MetricRegistry ] to build metrics source identifiers. CAUTION: FIXME Finish for the other components. NOTE: buildRegistryName is used when MetricsSystem < > or < > a metrics source. == [[registerSources]] Registering Metrics Sources for Spark Instance -- registerSources Internal Method","title":"buildRegistryName(source: Source): String"},{"location":"metrics/MetricsSystem/#source-scala_2","text":"","title":"[source, scala]"},{"location":"metrics/MetricsSystem/#registersources-unit","text":"registerSources finds < > configuration for the < >. NOTE: instance is defined when MetricsSystem < >. registerSources finds the configuration of all the link:spark-metrics-Source.adoc[metrics sources] for the subsystem (as described with source. prefix). For every metrics source, registerSources finds class property, creates an instance, and in the end < >. When registerSources fails, you should see the following ERROR message in the logs followed by the exception. ERROR Source class [classPath] cannot be instantiated NOTE: registerSources is used exclusively when MetricsSystem is < >. == [[getServletHandlers]] Requesting JSON Servlet Handler -- getServletHandlers Method","title":"registerSources(): Unit"},{"location":"metrics/MetricsSystem/#source-scala_3","text":"","title":"[source, scala]"},{"location":"metrics/MetricsSystem/#getservlethandlers-arrayservletcontexthandler","text":"If the MetricsSystem is < > and the < > is defined for the metrics system, getServletHandlers simply requests the < > for the link:spark-metrics-MetricsServlet.adoc#getHandlers[JSON servlet handler]. When MetricsSystem is not < > getServletHandlers throws an IllegalArgumentException . Can only call getServletHandlers on a running MetricsSystem","title":"getServletHandlers: Array[ServletContextHandler]"},{"location":"metrics/MetricsSystem/#note_1","text":"getServletHandlers is used when: SparkContext is link:spark-SparkContext-creating-instance-internals.adoc#MetricsSystem-getServletHandlers[created]","title":"[NOTE]"},{"location":"metrics/MetricsSystem/#spark-standalones-master-and-worker-are-requested-to-start-as-onstart","text":"== [[registerSinks]] Registering Metrics Sinks -- registerSinks Internal Method","title":"* Spark Standalone's Master and Worker are requested to start (as onStart)"},{"location":"metrics/MetricsSystem/#source-scala_4","text":"","title":"[source, scala]"},{"location":"metrics/MetricsSystem/#registersinks-unit","text":"registerSinks requests the < > for the link:spark-metrics-MetricsConfig.adoc#getInstance[configuration] of the < >. registerSinks requests the < > for the link:spark-metrics-MetricsConfig.adoc#subProperties[configuration] of all metrics sinks (i.e. configuration entries that match ^sink\\\\.(.+)\\\\.(.+) regular expression). For every metrics sink configuration, registerSinks takes class property and (if defined) creates an instance of the metric sink using an constructor that takes the configuration, < > and < >. For a single servlet metrics sink, registerSinks converts the sink to a link:spark-metrics-MetricsServlet.adoc[MetricsServlet] and sets the < > internal registry. For all other metrics sinks, registerSinks adds the sink to the < > internal registry. In case of an Exception , registerSinks prints out the following ERROR message to the logs: Sink class [classPath] cannot be instantiated NOTE: registerSinks is used exclusively when MetricsSystem is requested to < >. == [[stop]] stop Method","title":"registerSinks(): Unit"},{"location":"metrics/MetricsSystem/#source-scala_5","text":"","title":"[source, scala]"},{"location":"metrics/MetricsSystem/#stop-unit","text":"stop ...FIXME NOTE: stop is used when...FIXME == [[getSourcesByName]] getSourcesByName Method","title":"stop(): Unit"},{"location":"metrics/MetricsSystem/#source-scala_6","text":"","title":"[source, scala]"},{"location":"metrics/MetricsSystem/#getsourcesbynamesourcename-string-seqsource","text":"getSourcesByName ...FIXME NOTE: getSourcesByName is used when...FIXME == [[removeSource]] removeSource Method","title":"getSourcesByName(sourceName: String): Seq[Source]"},{"location":"metrics/MetricsSystem/#source-scala_7","text":"","title":"[source, scala]"},{"location":"metrics/MetricsSystem/#removesourcesource-source-unit","text":"removeSource ...FIXME NOTE: removeSource is used when...FIXME == [[creating-instance]] Creating MetricsSystem Instance MetricsSystem takes the following when created: [[instance]] Instance name [[conf]] xref:ROOT:SparkConf.adoc[SparkConf] [[securityMgr]] SecurityManager MetricsSystem initializes the < >. When created, MetricsSystem requests < > to link:spark-metrics-MetricsConfig.adoc#initialize[initialize]. NOTE: < > is used to create a new MetricsSystems instance instead. == [[createMetricsSystem]] Creating MetricsSystem Instance For Subsystem -- createMetricsSystem Factory Method","title":"removeSource(source: Source): Unit"},{"location":"metrics/MetricsSystem/#source-scala_8","text":"createMetricsSystem( instance: String conf: SparkConf securityMgr: SecurityManager): MetricsSystem createMetricsSystem returns a new < >. NOTE: createMetricsSystem is used when a < > is created. == [[report]] Requesting Sinks to Report Metrics -- report Method","title":"[source, scala]"},{"location":"metrics/MetricsSystem/#source-scala_9","text":"","title":"[source, scala]"},{"location":"metrics/MetricsSystem/#report-unit","text":"report simply requests the registered < > to link:spark-metrics-Sink.adoc#report[report metrics]. NOTE: report is used when xref:ROOT:SparkContext.adoc#stop[SparkContext], xref:executor:Executor.adoc#stop[Executor], Spark Standalone's Master and Worker , Spark on Mesos' MesosClusterScheduler are requested to stop == [[start]] Starting MetricsSystem -- start Method","title":"report(): Unit"},{"location":"metrics/MetricsSystem/#source-scala_10","text":"","title":"[source, scala]"},{"location":"metrics/MetricsSystem/#start-unit","text":"start turns < > flag on. NOTE: start can only be called once and < > an IllegalArgumentException when called multiple times. start < > the < > for Spark SQL, i.e. CodegenMetrics and HiveCatalogMetrics . start then registers the configured metrics < > and < > for the < >. In the end, start requests the registered < > to link:spark-metrics-Sink.adoc#start[start]. [[start-IllegalArgumentException]] start throws an IllegalArgumentException when < > flag is on. requirement failed: Attempting to start a MetricsSystem that is already running","title":"start(): Unit"},{"location":"metrics/MetricsSystem/#note_2","text":"start is used when: SparkContext is link:spark-SparkContext-creating-instance-internals.adoc#MetricsSystem-start[created] SparkEnv is xref:core:SparkEnv.adoc#create[created] (on executors) ExternalShuffleService is requested to xref:deploy:ExternalShuffleService.adoc#start[start]","title":"[NOTE]"},{"location":"metrics/MetricsSystem/#spark-standalones-master-and-worker-and-spark-on-mesos-mesosclusterscheduler-are-requested-to-start","text":"","title":"* Spark Standalone's Master and Worker, and Spark on Mesos' MesosClusterScheduler are requested to start"},{"location":"metrics/Sink/","text":"Sink \u00b6 Sink is a < > of metrics sinks . [[contract]] [source, scala] package org.apache.spark.metrics.sink trait Sink { def start(): Unit def stop(): Unit def report(): Unit } NOTE: Sink is a private[spark] contract. .Sink Contract [cols=\"1,2\",options=\"header\",width=\"100%\"] |=== | Method | Description | start | [[start]] Used when...FIXME | stop | [[stop]] Used when...FIXME | report | [[report]] Used when...FIXME |=== [[implementations]] .Sinks [cols=\"1,2\",options=\"header\",width=\"100%\"] |=== | Sink | Description | ConsoleSink | [[ConsoleSink]] | CsvSink | [[CsvSink]] | GraphiteSink | [[GraphiteSink]] | JmxSink | [[JmxSink]] | link:spark-metrics-MetricsServlet.adoc[MetricsServlet] | [[MetricsServlet]] | Slf4jSink | [[Slf4jSink]] | StatsdSink | [[StatsdSink]] |=== NOTE: All known < > in Spark 2.3 are in org.apache.spark.metrics.sink Scala package.","title":"Sink"},{"location":"metrics/Sink/#sink","text":"Sink is a < > of metrics sinks . [[contract]] [source, scala] package org.apache.spark.metrics.sink trait Sink { def start(): Unit def stop(): Unit def report(): Unit } NOTE: Sink is a private[spark] contract. .Sink Contract [cols=\"1,2\",options=\"header\",width=\"100%\"] |=== | Method | Description | start | [[start]] Used when...FIXME | stop | [[stop]] Used when...FIXME | report | [[report]] Used when...FIXME |=== [[implementations]] .Sinks [cols=\"1,2\",options=\"header\",width=\"100%\"] |=== | Sink | Description | ConsoleSink | [[ConsoleSink]] | CsvSink | [[CsvSink]] | GraphiteSink | [[GraphiteSink]] | JmxSink | [[JmxSink]] | link:spark-metrics-MetricsServlet.adoc[MetricsServlet] | [[MetricsServlet]] | Slf4jSink | [[Slf4jSink]] | StatsdSink | [[StatsdSink]] |=== NOTE: All known < > in Spark 2.3 are in org.apache.spark.metrics.sink Scala package.","title":"Sink"},{"location":"metrics/Source/","text":"== [[Source]] Source -- Contract of Metrics Sources Source is a < > of metrics sources . [[contract]] [source, scala] package org.apache.spark.metrics.source trait Source { def sourceName: String def metricRegistry: MetricRegistry } NOTE: Source is a private[spark] contract. .Source Contract [cols=\"1,2\",options=\"header\",width=\"100%\"] |=== | Method | Description | sourceName | [[sourceName]] Used when...FIXME | metricRegistry | [[metricRegistry]] Dropwizard Metrics' https://metrics.dropwizard.io/3.1.0/apidocs/com/codahale/metrics/MetricRegistry.html[MetricRegistry ] Used when...FIXME |=== [[implementations]] .Sources [cols=\"1,2\",options=\"header\",width=\"100%\"] |=== | Source | Description | ApplicationSource | [[ApplicationSource]] | xref:storage:spark-BlockManager-BlockManagerSource.adoc[BlockManagerSource] | [[BlockManagerSource]] | CacheMetrics | [[CacheMetrics]] | CodegenMetrics | [[CodegenMetrics]] | xref:metrics:spark-scheduler-DAGSchedulerSource.adoc[DAGSchedulerSource] | [[DAGSchedulerSource]] | xref:ROOT:spark-service-ExecutorAllocationManagerSource.adoc[ExecutorAllocationManagerSource] | [[ExecutorAllocationManagerSource]] | xref:executor:ExecutorSource.adoc[] | [[ExecutorSource]] | ExternalShuffleServiceSource | [[ExternalShuffleServiceSource]] | HiveCatalogMetrics | [[HiveCatalogMetrics]] | xref:metrics:JvmSource.adoc[JvmSource] | [[JvmSource]] | LiveListenerBusMetrics | [[LiveListenerBusMetrics]] | MasterSource | [[MasterSource]] | MesosClusterSchedulerSource | [[MesosClusterSchedulerSource]] | xref:storage:ShuffleMetricsSource.adoc[] | [[ShuffleMetricsSource]] | StreamingSource | [[StreamingSource]] | WorkerSource | [[WorkerSource]] |===","title":"Source"},{"location":"metrics/configuration-properties/","text":"Configuration Properties \u00b6 spark.metrics.conf \u00b6 The metrics configuration file Default: metrics.properties spark.metrics.namespace \u00b6 Root namespace for metrics reporting Default: Spark Application ID (i.e. spark.app.id configuration property) Since a Spark application's ID changes with every execution of a Spark application, a custom namespace can be specified for an easier metrics reporting. Used when MetricsSystem is requested for a metrics source identifier ( metrics namespace )","title":"Configuration Properties"},{"location":"metrics/configuration-properties/#configuration-properties","text":"","title":"Configuration Properties"},{"location":"metrics/configuration-properties/#sparkmetricsconf","text":"The metrics configuration file Default: metrics.properties","title":" spark.metrics.conf"},{"location":"metrics/configuration-properties/#sparkmetricsnamespace","text":"Root namespace for metrics reporting Default: Spark Application ID (i.e. spark.app.id configuration property) Since a Spark application's ID changes with every execution of a Spark application, a custom namespace can be specified for an easier metrics reporting. Used when MetricsSystem is requested for a metrics source identifier ( metrics namespace )","title":" spark.metrics.namespace"},{"location":"rdd/","text":"Resilient Distributed Dataset (RDD) \u00b6 Resilient Distributed Dataset (aka RDD ) is the primary data abstraction in Apache Spark and the core of Spark (that I often refer to as \"Spark Core\"). .The origins of RDD The original paper that gave birth to the concept of RDD is https://cs.stanford.edu/~matei/papers/2012/nsdi_spark.pdf[Resilient Distributed Datasets: A Fault-Tolerant Abstraction for In-Memory Cluster Computing] by Matei Zaharia, et al. An RDD is a description of a fault-tolerant and resilient computation over a distributed collection of records (spread over < >). NOTE: One could compare RDDs to collections in Scala, i.e. a RDD is computed on many JVMs while a Scala collection lives on a single JVM. Using RDD Spark hides data partitioning and so distribution that in turn allowed them to design parallel computational framework with a higher-level programming interface (API) for four mainstream programming languages. The features of RDDs (decomposing the name): Resilient , i.e. fault-tolerant with the help of < > and so able to recompute missing or damaged partitions due to node failures. Distributed with data residing on multiple nodes in a link:spark-cluster.adoc[cluster]. Dataset is a collection of link:spark-rdd-partitions.adoc[partitioned data] with primitive values or values of values, e.g. tuples or other objects (that represent records of the data you work with). .RDDs image::spark-rdds.png[align=\"center\"] From the scaladoc of http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.rdd.RDD[org.apache.spark.rdd.RDD ]: A Resilient Distributed Dataset (RDD), the basic abstraction in Spark. Represents an immutable, partitioned collection of elements that can be operated on in parallel. From the original paper about RDD - https://cs.stanford.edu/~matei/papers/2012/nsdi_spark.pdf[Resilient Distributed Datasets: A Fault-Tolerant Abstraction for In-Memory Cluster Computing]: Resilient Distributed Datasets (RDDs) are a distributed memory abstraction that lets programmers perform in-memory computations on large clusters in a fault-tolerant manner. Beside the above traits (that are directly embedded in the name of the data abstraction - RDD) it has the following additional traits: In-Memory , i.e. data inside RDD is stored in memory as much (size) and long (time) as possible. Immutable or Read-Only , i.e. it does not change once created and can only be transformed using transformations to new RDDs. Lazy evaluated , i.e. the data inside RDD is not available or transformed until an action is executed that triggers the execution. Cacheable , i.e. you can hold all the data in a persistent \"storage\" like memory (default and the most preferred) or disk (the least preferred due to access speed). Parallel , i.e. process data in parallel. Typed -- RDD records have types, e.g. Long in RDD[Long] or (Int, String) in RDD[(Int, String)] . Partitioned -- records are partitioned (split into logical partitions) and distributed across nodes in a cluster. Location-Stickiness -- RDD can define < > to compute partitions (as close to the records as possible). NOTE: Preferred location (aka locality preferences or placement preferences or locality info ) is information about the locations of RDD records (that Spark's xref:scheduler:DAGScheduler.adoc#preferred-locations[DAGScheduler] uses to place computing partitions on to have the tasks as close to the data as possible). Computing partitions in a RDD is a distributed process by design and to achieve even data distribution as well as leverage link:spark-data-locality.adoc[data locality] (in distributed systems like HDFS or Cassandra in which data is partitioned by default), they are partitioned to a fixed number of link:spark-rdd-partitions.adoc[partitions] - logical chunks (parts) of data. The logical division is for processing only and internally it is not divided whatsoever. Each partition comprises of records . .RDDs image::spark-rdd-partitioned-distributed.png[align=\"center\"] link:spark-rdd-partitions.adoc[Partitions are the units of parallelism]. You can control the number of partitions of a RDD using link:spark-rdd-partitions.adoc#repartition[repartition] or link:spark-rdd-partitions.adoc#coalesce[coalesce] transformations. Spark tries to be as close to data as possible without wasting time to send data across network by means of link:spark-rdd-shuffle.adoc[RDD shuffling], and creates as many partitions as required to follow the storage layout and thus optimize data access. It leads to a one-to-one mapping between (physical) data in distributed data storage, e.g. HDFS or Cassandra, and partitions. RDDs support two kinds of operations: < > - lazy operations that return another RDD. < > - operations that trigger computation and return values. The motivation to create RDD were ( https://www.cs.berkeley.edu/~matei/papers/2012/nsdi_spark.pdf[after the authors]) two types of applications that current computing frameworks handle inefficiently: iterative algorithms in machine learning and graph computations. interactive data mining tools as ad-hoc queries on the same dataset. The goal is to reuse intermediate in-memory results across multiple data-intensive workloads with no need for copying large amounts of data over the network. Technically, RDDs follow the < > defined by the five main intrinsic properties: [[dependencies]] Parent RDDs (aka xref:rdd:RDD.adoc#dependencies[RDD dependencies]) An array of link:spark-rdd-partitions.adoc[partitions] that a dataset is divided to. A xref:rdd:RDD.adoc#compute[compute] function to do a computation on partitions. An optional xref:rdd:Partitioner.adoc[Partitioner] that defines how keys are hashed, and the pairs partitioned (for key-value RDDs) Optional < > (aka locality info ), i.e. hosts for a partition where the records live or are the closest to read from. This RDD abstraction supports an expressive set of operations without having to modify scheduler for each one. [[context]] An RDD is a named (by name ) and uniquely identified (by id ) entity in a xref:ROOT:SparkContext.adoc[] (available as context property). RDDs live in one and only one xref:ROOT:SparkContext.adoc[] that creates a logical boundary. NOTE: RDDs cannot be shared between SparkContexts (see xref:ROOT:SparkContext.adoc#sparkcontext-and-rdd[SparkContext and RDDs]). An RDD can optionally have a friendly name accessible using name that can be changed using = : scala> val ns = sc.parallelize(0 to 10) ns: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[2] at parallelize at <console>:24 scala> ns.id res0: Int = 2 scala> ns.name res1: String = null scala> ns.name = \"Friendly name\" ns.name: String = Friendly name scala> ns.name res2: String = Friendly name scala> ns.toDebugString res3: String = (8) Friendly name ParallelCollectionRDD[2] at parallelize at <console>:24 [] RDDs are a container of instructions on how to materialize big (arrays of) distributed data, and how to split it into partitions so Spark (using xref:executor:Executor.adoc[executors]) can hold some of them. In general data distribution can help executing processing in parallel so a task processes a chunk of data that it could eventually keep in memory. Spark does jobs in parallel, and RDDs are split into partitions to be processed and written in parallel. Inside a partition, data is processed sequentially. Saving partitions results in part-files instead of one single file (unless there is a single partition). == [[transformations]] Transformations A transformation is a lazy operation on a RDD that returns another RDD, e.g. map , flatMap , filter , reduceByKey , join , cogroup , etc. Find out more in xref:rdd:spark-rdd-transformations.adoc[Transformations]. == [[actions]] Actions An action is an operation that triggers execution of < > and returns a value (to a Spark driver - the user program). TIP: Go in-depth in the section link:spark-rdd-actions.adoc[Actions]. == [[creating-rdds]] Creating RDDs === SparkContext.parallelize One way to create a RDD is with SparkContext.parallelize method. It accepts a collection of elements as shown below ( sc is a SparkContext instance): scala> val rdd = sc.parallelize(1 to 1000) rdd: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[0] at parallelize at <console>:25 You may also want to randomize the sample data: scala> val data = Seq.fill(10)(util.Random.nextInt) data: Seq[Int] = List(-964985204, 1662791, -1820544313, -383666422, -111039198, 310967683, 1114081267, 1244509086, 1797452433, 124035586) scala> val rdd = sc.parallelize(data) rdd: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[0] at parallelize at <console>:29 Given the reason to use Spark to process more data than your own laptop could handle, SparkContext.parallelize is mainly used to learn Spark in the Spark shell. SparkContext.parallelize requires all the data to be available on a single machine - the Spark driver - that eventually hits the limits of your laptop. === SparkContext.makeRDD CAUTION: FIXME What's the use case for makeRDD ? scala> sc.makeRDD(0 to 1000) res0: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[1] at makeRDD at <console>:25 === SparkContext.textFile One of the easiest ways to create an RDD is to use SparkContext.textFile to read files. You can use the local README.md file (and then flatMap over the lines inside to have an RDD of words): scala> val words = sc.textFile(\"README.md\").flatMap(_.split(\"\\\\W+\")).cache words: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[27] at flatMap at <console>:24 NOTE: You link:spark-rdd-caching.adoc[cache] it so the computation is not performed every time you work with words . == [[creating-rdds-from-input]] Creating RDDs from Input Refer to link:spark-io.adoc[Using Input and Output (I/O)] to learn about the IO API to create RDDs. === Transformations RDD transformations by definition transform an RDD into another RDD and hence are the way to create new ones. Refer to < > section to learn more. == RDDs in Web UI It is quite informative to look at RDDs in the Web UI that is at http://localhost:4040 for link:spark-shell.adoc[Spark shell]. Execute the following Spark application (type all the lines in spark-shell ): [source,scala] \u00b6 val ints = sc.parallelize(1 to 100) // <1> ints.setName(\"Hundred ints\") // <2> ints.cache // <3> ints.count // <4> <1> Creates an RDD with hundred of numbers (with as many partitions as possible) <2> Sets the name of the RDD <3> Caches the RDD for performance reasons that also makes it visible in Storage tab in the web UI <4> Executes action (and materializes the RDD) With the above executed, you should see the following in the Web UI: .RDD with custom name image::spark-ui-rdd-name.png[align=\"center\"] Click the name of the RDD (under RDD Name ) and you will get the details of how the RDD is cached. .RDD Storage Info image::spark-ui-storage-hundred-ints.png[align=\"center\"] Execute the following Spark job and you will see how the number of partitions decreases. ints.repartition(2).count .Number of tasks after repartition image::spark-ui-repartition-2.png[align=\"center\"]","title":"Resilient Distributed Dataset"},{"location":"rdd/#resilient-distributed-dataset-rdd","text":"Resilient Distributed Dataset (aka RDD ) is the primary data abstraction in Apache Spark and the core of Spark (that I often refer to as \"Spark Core\"). .The origins of RDD The original paper that gave birth to the concept of RDD is https://cs.stanford.edu/~matei/papers/2012/nsdi_spark.pdf[Resilient Distributed Datasets: A Fault-Tolerant Abstraction for In-Memory Cluster Computing] by Matei Zaharia, et al. An RDD is a description of a fault-tolerant and resilient computation over a distributed collection of records (spread over < >). NOTE: One could compare RDDs to collections in Scala, i.e. a RDD is computed on many JVMs while a Scala collection lives on a single JVM. Using RDD Spark hides data partitioning and so distribution that in turn allowed them to design parallel computational framework with a higher-level programming interface (API) for four mainstream programming languages. The features of RDDs (decomposing the name): Resilient , i.e. fault-tolerant with the help of < > and so able to recompute missing or damaged partitions due to node failures. Distributed with data residing on multiple nodes in a link:spark-cluster.adoc[cluster]. Dataset is a collection of link:spark-rdd-partitions.adoc[partitioned data] with primitive values or values of values, e.g. tuples or other objects (that represent records of the data you work with). .RDDs image::spark-rdds.png[align=\"center\"] From the scaladoc of http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.rdd.RDD[org.apache.spark.rdd.RDD ]: A Resilient Distributed Dataset (RDD), the basic abstraction in Spark. Represents an immutable, partitioned collection of elements that can be operated on in parallel. From the original paper about RDD - https://cs.stanford.edu/~matei/papers/2012/nsdi_spark.pdf[Resilient Distributed Datasets: A Fault-Tolerant Abstraction for In-Memory Cluster Computing]: Resilient Distributed Datasets (RDDs) are a distributed memory abstraction that lets programmers perform in-memory computations on large clusters in a fault-tolerant manner. Beside the above traits (that are directly embedded in the name of the data abstraction - RDD) it has the following additional traits: In-Memory , i.e. data inside RDD is stored in memory as much (size) and long (time) as possible. Immutable or Read-Only , i.e. it does not change once created and can only be transformed using transformations to new RDDs. Lazy evaluated , i.e. the data inside RDD is not available or transformed until an action is executed that triggers the execution. Cacheable , i.e. you can hold all the data in a persistent \"storage\" like memory (default and the most preferred) or disk (the least preferred due to access speed). Parallel , i.e. process data in parallel. Typed -- RDD records have types, e.g. Long in RDD[Long] or (Int, String) in RDD[(Int, String)] . Partitioned -- records are partitioned (split into logical partitions) and distributed across nodes in a cluster. Location-Stickiness -- RDD can define < > to compute partitions (as close to the records as possible). NOTE: Preferred location (aka locality preferences or placement preferences or locality info ) is information about the locations of RDD records (that Spark's xref:scheduler:DAGScheduler.adoc#preferred-locations[DAGScheduler] uses to place computing partitions on to have the tasks as close to the data as possible). Computing partitions in a RDD is a distributed process by design and to achieve even data distribution as well as leverage link:spark-data-locality.adoc[data locality] (in distributed systems like HDFS or Cassandra in which data is partitioned by default), they are partitioned to a fixed number of link:spark-rdd-partitions.adoc[partitions] - logical chunks (parts) of data. The logical division is for processing only and internally it is not divided whatsoever. Each partition comprises of records . .RDDs image::spark-rdd-partitioned-distributed.png[align=\"center\"] link:spark-rdd-partitions.adoc[Partitions are the units of parallelism]. You can control the number of partitions of a RDD using link:spark-rdd-partitions.adoc#repartition[repartition] or link:spark-rdd-partitions.adoc#coalesce[coalesce] transformations. Spark tries to be as close to data as possible without wasting time to send data across network by means of link:spark-rdd-shuffle.adoc[RDD shuffling], and creates as many partitions as required to follow the storage layout and thus optimize data access. It leads to a one-to-one mapping between (physical) data in distributed data storage, e.g. HDFS or Cassandra, and partitions. RDDs support two kinds of operations: < > - lazy operations that return another RDD. < > - operations that trigger computation and return values. The motivation to create RDD were ( https://www.cs.berkeley.edu/~matei/papers/2012/nsdi_spark.pdf[after the authors]) two types of applications that current computing frameworks handle inefficiently: iterative algorithms in machine learning and graph computations. interactive data mining tools as ad-hoc queries on the same dataset. The goal is to reuse intermediate in-memory results across multiple data-intensive workloads with no need for copying large amounts of data over the network. Technically, RDDs follow the < > defined by the five main intrinsic properties: [[dependencies]] Parent RDDs (aka xref:rdd:RDD.adoc#dependencies[RDD dependencies]) An array of link:spark-rdd-partitions.adoc[partitions] that a dataset is divided to. A xref:rdd:RDD.adoc#compute[compute] function to do a computation on partitions. An optional xref:rdd:Partitioner.adoc[Partitioner] that defines how keys are hashed, and the pairs partitioned (for key-value RDDs) Optional < > (aka locality info ), i.e. hosts for a partition where the records live or are the closest to read from. This RDD abstraction supports an expressive set of operations without having to modify scheduler for each one. [[context]] An RDD is a named (by name ) and uniquely identified (by id ) entity in a xref:ROOT:SparkContext.adoc[] (available as context property). RDDs live in one and only one xref:ROOT:SparkContext.adoc[] that creates a logical boundary. NOTE: RDDs cannot be shared between SparkContexts (see xref:ROOT:SparkContext.adoc#sparkcontext-and-rdd[SparkContext and RDDs]). An RDD can optionally have a friendly name accessible using name that can be changed using = : scala> val ns = sc.parallelize(0 to 10) ns: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[2] at parallelize at <console>:24 scala> ns.id res0: Int = 2 scala> ns.name res1: String = null scala> ns.name = \"Friendly name\" ns.name: String = Friendly name scala> ns.name res2: String = Friendly name scala> ns.toDebugString res3: String = (8) Friendly name ParallelCollectionRDD[2] at parallelize at <console>:24 [] RDDs are a container of instructions on how to materialize big (arrays of) distributed data, and how to split it into partitions so Spark (using xref:executor:Executor.adoc[executors]) can hold some of them. In general data distribution can help executing processing in parallel so a task processes a chunk of data that it could eventually keep in memory. Spark does jobs in parallel, and RDDs are split into partitions to be processed and written in parallel. Inside a partition, data is processed sequentially. Saving partitions results in part-files instead of one single file (unless there is a single partition). == [[transformations]] Transformations A transformation is a lazy operation on a RDD that returns another RDD, e.g. map , flatMap , filter , reduceByKey , join , cogroup , etc. Find out more in xref:rdd:spark-rdd-transformations.adoc[Transformations]. == [[actions]] Actions An action is an operation that triggers execution of < > and returns a value (to a Spark driver - the user program). TIP: Go in-depth in the section link:spark-rdd-actions.adoc[Actions]. == [[creating-rdds]] Creating RDDs === SparkContext.parallelize One way to create a RDD is with SparkContext.parallelize method. It accepts a collection of elements as shown below ( sc is a SparkContext instance): scala> val rdd = sc.parallelize(1 to 1000) rdd: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[0] at parallelize at <console>:25 You may also want to randomize the sample data: scala> val data = Seq.fill(10)(util.Random.nextInt) data: Seq[Int] = List(-964985204, 1662791, -1820544313, -383666422, -111039198, 310967683, 1114081267, 1244509086, 1797452433, 124035586) scala> val rdd = sc.parallelize(data) rdd: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[0] at parallelize at <console>:29 Given the reason to use Spark to process more data than your own laptop could handle, SparkContext.parallelize is mainly used to learn Spark in the Spark shell. SparkContext.parallelize requires all the data to be available on a single machine - the Spark driver - that eventually hits the limits of your laptop. === SparkContext.makeRDD CAUTION: FIXME What's the use case for makeRDD ? scala> sc.makeRDD(0 to 1000) res0: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[1] at makeRDD at <console>:25 === SparkContext.textFile One of the easiest ways to create an RDD is to use SparkContext.textFile to read files. You can use the local README.md file (and then flatMap over the lines inside to have an RDD of words): scala> val words = sc.textFile(\"README.md\").flatMap(_.split(\"\\\\W+\")).cache words: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[27] at flatMap at <console>:24 NOTE: You link:spark-rdd-caching.adoc[cache] it so the computation is not performed every time you work with words . == [[creating-rdds-from-input]] Creating RDDs from Input Refer to link:spark-io.adoc[Using Input and Output (I/O)] to learn about the IO API to create RDDs. === Transformations RDD transformations by definition transform an RDD into another RDD and hence are the way to create new ones. Refer to < > section to learn more. == RDDs in Web UI It is quite informative to look at RDDs in the Web UI that is at http://localhost:4040 for link:spark-shell.adoc[Spark shell]. Execute the following Spark application (type all the lines in spark-shell ):","title":"Resilient Distributed Dataset (RDD)"},{"location":"rdd/#sourcescala","text":"val ints = sc.parallelize(1 to 100) // <1> ints.setName(\"Hundred ints\") // <2> ints.cache // <3> ints.count // <4> <1> Creates an RDD with hundred of numbers (with as many partitions as possible) <2> Sets the name of the RDD <3> Caches the RDD for performance reasons that also makes it visible in Storage tab in the web UI <4> Executes action (and materializes the RDD) With the above executed, you should see the following in the Web UI: .RDD with custom name image::spark-ui-rdd-name.png[align=\"center\"] Click the name of the RDD (under RDD Name ) and you will get the details of how the RDD is cached. .RDD Storage Info image::spark-ui-storage-hundred-ints.png[align=\"center\"] Execute the following Spark job and you will see how the number of partitions decreases. ints.repartition(2).count .Number of tasks after repartition image::spark-ui-repartition-2.png[align=\"center\"]","title":"[source,scala]"},{"location":"rdd/Aggregator/","text":"= [[Aggregator]] Aggregator Aggregator is a set of < > used to aggregate data using xref:rdd:PairRDDFunctions.adoc#combineByKeyWithClassTag[PairRDDFunctions.combineByKeyWithClassTag] transformation. Aggregator[K, V, C] is a parameterized type of K keys, V values, and C combiner (partial) values. [[creating-instance]][[aggregation-functions]] Aggregator transforms an RDD[(K, V)] into an RDD[(K, C)] (for a \"combined type\" C) using the functions: [[createCombiner]] createCombiner: V => C [[mergeValue]] mergeValue: (C, V) => C [[mergeCombiners]] mergeCombiners: (C, C) => C Aggregator is used to create a xref:rdd:ShuffleDependency.adoc[ShuffleDependency] and xref:shuffle:ExternalSorter.adoc[ExternalSorter]. == [[combineValuesByKey]] combineValuesByKey Method [source, scala] \u00b6 combineValuesByKey( iter: Iterator[_ <: Product2[K, V]], context: TaskContext): Iterator[(K, C)] combineValuesByKey creates a new xref:shuffle:ExternalAppendOnlyMap.adoc[ExternalAppendOnlyMap] (with the < >). combineValuesByKey requests the ExternalAppendOnlyMap to xref:shuffle:ExternalAppendOnlyMap.adoc#insertAll[insert all key-value pairs] from the given iterator (that is the values of a partition). combineValuesByKey < >. In the end, combineValuesByKey requests the ExternalAppendOnlyMap for an xref:shuffle:ExternalAppendOnlyMap.adoc#iterator[iterator of \"combined\" pairs]. combineValuesByKey is used when: xref:rdd:PairRDDFunctions.adoc#combineByKeyWithClassTag[PairRDDFunctions.combineByKeyWithClassTag] transformation is used (with the same Partitioner as the RDD's) BlockStoreShuffleReader is requested to xref:shuffle:BlockStoreShuffleReader.adoc#read[read combined records for a reduce task] (with the xref:rdd:ShuffleDependency.adoc#mapSideCombine[Map-Size Partial Aggregation Flag] off) == [[combineCombinersByKey]] combineCombinersByKey Method [source, scala] \u00b6 combineCombinersByKey( iter: Iterator[_ <: Product2[K, C]], context: TaskContext): Iterator[(K, C)] combineCombinersByKey...FIXME combineCombinersByKey is used when BlockStoreShuffleReader is requested to xref:shuffle:BlockStoreShuffleReader.adoc#read[read combined records for a reduce task] (with the xref:rdd:ShuffleDependency.adoc#mapSideCombine[Map-Size Partial Aggregation Flag] on). == [[updateMetrics]] Updating Task Metrics [source, scala] \u00b6 updateMetrics( context: TaskContext, map: ExternalAppendOnlyMap[_, _, _]): Unit updateMetrics requests the input xref:scheduler:spark-TaskContext.adoc[TaskContext] for the xref:scheduler:spark-TaskContext.adoc#taskMetrics[TaskMetrics] to update the metrics based on the metrics of the input xref:shuffle:ExternalAppendOnlyMap.adoc[ExternalAppendOnlyMap]: xref:executor:TaskMetrics.adoc#incMemoryBytesSpilled[Increment memory bytes spilled] xref:executor:TaskMetrics.adoc#incDiskBytesSpilled[Increment disk bytes spilled] xref:executor:TaskMetrics.adoc#incPeakExecutionMemory[Increment peak execution memory] updateMetrics is used when Aggregator is requested to < > and < >.","title":"Aggregator"},{"location":"rdd/Aggregator/#source-scala","text":"combineValuesByKey( iter: Iterator[_ <: Product2[K, V]], context: TaskContext): Iterator[(K, C)] combineValuesByKey creates a new xref:shuffle:ExternalAppendOnlyMap.adoc[ExternalAppendOnlyMap] (with the < >). combineValuesByKey requests the ExternalAppendOnlyMap to xref:shuffle:ExternalAppendOnlyMap.adoc#insertAll[insert all key-value pairs] from the given iterator (that is the values of a partition). combineValuesByKey < >. In the end, combineValuesByKey requests the ExternalAppendOnlyMap for an xref:shuffle:ExternalAppendOnlyMap.adoc#iterator[iterator of \"combined\" pairs]. combineValuesByKey is used when: xref:rdd:PairRDDFunctions.adoc#combineByKeyWithClassTag[PairRDDFunctions.combineByKeyWithClassTag] transformation is used (with the same Partitioner as the RDD's) BlockStoreShuffleReader is requested to xref:shuffle:BlockStoreShuffleReader.adoc#read[read combined records for a reduce task] (with the xref:rdd:ShuffleDependency.adoc#mapSideCombine[Map-Size Partial Aggregation Flag] off) == [[combineCombinersByKey]] combineCombinersByKey Method","title":"[source, scala]"},{"location":"rdd/Aggregator/#source-scala_1","text":"combineCombinersByKey( iter: Iterator[_ <: Product2[K, C]], context: TaskContext): Iterator[(K, C)] combineCombinersByKey...FIXME combineCombinersByKey is used when BlockStoreShuffleReader is requested to xref:shuffle:BlockStoreShuffleReader.adoc#read[read combined records for a reduce task] (with the xref:rdd:ShuffleDependency.adoc#mapSideCombine[Map-Size Partial Aggregation Flag] on). == [[updateMetrics]] Updating Task Metrics","title":"[source, scala]"},{"location":"rdd/Aggregator/#source-scala_2","text":"updateMetrics( context: TaskContext, map: ExternalAppendOnlyMap[_, _, _]): Unit updateMetrics requests the input xref:scheduler:spark-TaskContext.adoc[TaskContext] for the xref:scheduler:spark-TaskContext.adoc#taskMetrics[TaskMetrics] to update the metrics based on the metrics of the input xref:shuffle:ExternalAppendOnlyMap.adoc[ExternalAppendOnlyMap]: xref:executor:TaskMetrics.adoc#incMemoryBytesSpilled[Increment memory bytes spilled] xref:executor:TaskMetrics.adoc#incDiskBytesSpilled[Increment disk bytes spilled] xref:executor:TaskMetrics.adoc#incPeakExecutionMemory[Increment peak execution memory] updateMetrics is used when Aggregator is requested to < > and < >.","title":"[source, scala]"},{"location":"rdd/CheckpointRDD/","text":"= CheckpointRDD CheckpointRDD is...FIXME","title":"CheckpointRDD"},{"location":"rdd/HashPartitioner/","text":"= HashPartitioner HashPartitioner is a xref:rdd:Partitioner.adoc[Partitioner] for hash-based partitioning. HashPartitioner is used as the default Partitioner. == [[partitions]][[numPartitions]] Number of Partitions HashPartitioner takes a number of partitions to be created. HashPartitioner uses the number of partitions to find the < > (of a key-value record). == [[getPartition]] Finding Partition ID for Key [source, scala] \u00b6 getPartition(key: Any): Int \u00b6 getPartition returns 0 as the partition ID for null keys. For non- null keys, getPartition uses the key's {java-javadoc-url}/java/lang/Object.html#++hashCode--++[Object.hashCode] modulo the configured < >. For a negative result, getPartition adds the < > (used for the modulo operator) to make it positive. getPartition is part of the xref:rdd:Partitioner.adoc#getPartition[Partitioner] abstraction. == [[equals]] equals Method [source, scala] \u00b6 equals(other: Any): Boolean \u00b6 Two HashPartitioners are considered equal when the < > are the same. == [[hashCode]] hashCode Method [source, scala] \u00b6 hashCode: Int \u00b6 hashCode is the < >.","title":"HashPartitioner"},{"location":"rdd/HashPartitioner/#source-scala","text":"","title":"[source, scala]"},{"location":"rdd/HashPartitioner/#getpartitionkey-any-int","text":"getPartition returns 0 as the partition ID for null keys. For non- null keys, getPartition uses the key's {java-javadoc-url}/java/lang/Object.html#++hashCode--++[Object.hashCode] modulo the configured < >. For a negative result, getPartition adds the < > (used for the modulo operator) to make it positive. getPartition is part of the xref:rdd:Partitioner.adoc#getPartition[Partitioner] abstraction. == [[equals]] equals Method","title":"getPartition(key: Any): Int"},{"location":"rdd/HashPartitioner/#source-scala_1","text":"","title":"[source, scala]"},{"location":"rdd/HashPartitioner/#equalsother-any-boolean","text":"Two HashPartitioners are considered equal when the < > are the same. == [[hashCode]] hashCode Method","title":"equals(other: Any): Boolean"},{"location":"rdd/HashPartitioner/#source-scala_2","text":"","title":"[source, scala]"},{"location":"rdd/HashPartitioner/#hashcode-int","text":"hashCode is the < >.","title":"hashCode: Int"},{"location":"rdd/LocalRDDCheckpointData/","text":"= LocalRDDCheckpointData LocalRDDCheckpointData is...FIXME","title":"LocalRDDCheckpointData"},{"location":"rdd/PairRDDFunctions/","text":"= [[PairRDDFunctions]] PairRDDFunctions :page-toctitle: Transformations PairRDDFunctions is an extension of RDD API to provide additional < > for RDDs of key-value pairs ( RDD[(K, V)] ). PairRDDFunctions is available in RDDs of key-value pairs via Scala implicit conversion. [[transformations]] .PairRDDFunctions' Transformations [cols=\"30m,70\",options=\"header\",width=\"100%\"] |=== | Method | Description | aggregateByKey a| [[aggregateByKey]] [source, scala] \u00b6 aggregateByKey U: ClassTag ( seqOp: (U, V) => U, combOp: (U, U) => U): RDD[(K, U)] aggregateByKey U: ClassTag ( seqOp: (U, V) => U, combOp: (U, U) => U): RDD[(K, U)] aggregateByKey U: ClassTag ( seqOp: (U, V) => U, combOp: (U, U) => U): RDD[(K, U)] | combineByKey a| [[combineByKey]] [source, scala] \u00b6 combineByKey C : RDD[(K, C)] combineByKey C : RDD[(K, C)] combineByKey C : RDD[(K, C)] | countApproxDistinctByKey a| [[countApproxDistinctByKey]] [source, scala] \u00b6 countApproxDistinctByKey( relativeSD: Double = 0.05): RDD[(K, Long)] countApproxDistinctByKey( relativeSD: Double, numPartitions: Int): RDD[(K, Long)] countApproxDistinctByKey( relativeSD: Double, partitioner: Partitioner): RDD[(K, Long)] countApproxDistinctByKey( p: Int, sp: Int, partitioner: Partitioner): RDD[(K, Long)] | flatMapValues a| [[flatMapValues]] [source, scala] \u00b6 flatMapValues U : RDD[(K, U)] | foldByKey a| [[foldByKey]] [source, scala] \u00b6 foldByKey( zeroValue: V)( func: (V, V) => V): RDD[(K, V)] foldByKey( zeroValue: V, numPartitions: Int)( func: (V, V) => V): RDD[(K, V)] foldByKey( zeroValue: V, partitioner: Partitioner)( func: (V, V) => V): RDD[(K, V)] | mapValues a| [[mapValues]] [source, scala] \u00b6 mapValues U : RDD[(K, U)] | partitionBy a| [[partitionBy]] [source, scala] \u00b6 partitionBy( partitioner: Partitioner): RDD[(K, V)] | saveAsHadoopDataset a| [[saveAsHadoopDataset]] [source, scala] \u00b6 saveAsHadoopDataset( conf: JobConf): Unit saveAsHadoopDataset uses the SparkHadoopWriter utility to < > with a < > (for the given Hadoop https://hadoop.apache.org/docs/r2.7.3/api/org/apache/hadoop/mapred/JobConf.html[JobConf ]) | saveAsHadoopFile a| [[saveAsHadoopFile]] [source, scala] \u00b6 saveAsHadoopFile( path: String, keyClass: Class[ ], valueClass: Class[ ], outputFormatClass: Class[_ <: OutputFormat[ , _]], codec: Class[ <: CompressionCodec]): Unit saveAsHadoopFile( path: String, keyClass: Class[ ], valueClass: Class[ ], outputFormatClass: Class[_ <: OutputFormat[ , _]], conf: JobConf = new JobConf(self.context.hadoopConfiguration), codec: Option[Class[ <: CompressionCodec]] = None): Unit saveAsHadoopFile F <: OutputFormat[K, V] (implicit fm: ClassTag[F]): Unit saveAsHadoopFile F <: OutputFormat[K, V] (implicit fm: ClassTag[F]): Unit | saveAsNewAPIHadoopDataset a| [[saveAsNewAPIHadoopDataset]] [source, scala] \u00b6 saveAsNewAPIHadoopDataset( conf: Configuration): Unit Saves this RDD of key-value pairs ( RDD[K,V] ) to any Hadoop-supported storage system with new Hadoop API (using a Hadoop https://hadoop.apache.org/docs/r2.7.3/api/org/apache/hadoop/conf/Configuration.html[Configuration ] object for that storage system). The configuration should set relevant output params (an https://hadoop.apache.org/docs/r2.7.3/api/org/apache/hadoop/mapreduce/OutputFormat.html[output format], output paths, e.g. a table name to write to) in the same way as it would be configured for a Hadoop MapReduce job. saveAsNewAPIHadoopDataset uses the SparkHadoopWriter utility to < > with a < > (for the given Hadoop https://hadoop.apache.org/docs/r2.7.3/api/org/apache/hadoop/conf/Configuration.html[Configuration ]) | saveAsNewAPIHadoopFile a| [[saveAsNewAPIHadoopFile]] [source, scala] \u00b6 saveAsNewAPIHadoopFile( path: String, keyClass: Class[ ], valueClass: Class[ ], outputFormatClass: Class[_ <: NewOutputFormat[_, _]], conf: Configuration = self.context.hadoopConfiguration): Unit saveAsNewAPIHadoopFile F <: NewOutputFormat[K, V] (implicit fm: ClassTag[F]): Unit |=== == [[reduceByKey]][[groupByKey]] groupByKey and reduceByKey reduceByKey is sort of a particular case of < >. You may want to look at the number of partitions from another angle. It may often not be important to have a given number of partitions upfront (at RDD creation time upon link:spark-data-sources.adoc[loading data from data sources]), so only \"regrouping\" the data by key after it is an RDD might be...the key ( pun not intended ). You can use groupByKey or another PairRDDFunctions method to have a key in one processing flow. You could use partitionBy that is available for RDDs to be RDDs of tuples, i.e. PairRDD : rdd.keyBy(_.kind) .partitionBy(new HashPartitioner(PARTITIONS)) .foreachPartition(...) Think of situations where kind has low cardinality or highly skewed distribution and using the technique for partitioning might be not an optimal solution. You could do as follows: rdd.keyBy(_.kind).reduceByKey(....) or mapValues or plenty of other solutions. FIXME, man . == [[combineByKeyWithClassTag]] combineByKeyWithClassTag [source, scala] \u00b6 combineByKeyWithClassTag C (implicit ct: ClassTag[C]): RDD[(K, C)] // <1> combineByKeyWithClassTag C (implicit ct: ClassTag[C]): RDD[(K, C)] // <2> combineByKeyWithClassTag C (implicit ct: ClassTag[C]): RDD[(K, C)] <1> Uses the xref:rdd:Partitioner.adoc#defaultPartitioner[default partitioner] <2> Uses a xref:rdd:HashPartitioner.adoc[HashPartitioner] with the given number of partitions combineByKeyWithClassTag creates an xref:rdd:Aggregator.adoc[Aggregator] for the given aggregation functions. combineByKeyWithClassTag branches off per the given xref:rdd:Partitioner.adoc[Partitioner]. If the input partitioner and the RDD's are the same, combineByKeyWithClassTag simply xref:rdd:spark-rdd-transformations.adoc#mapPartitions[mapPartitions] on the RDD with the following arguments: Iterator of the xref:rdd:Aggregator.adoc#combineValuesByKey[Aggregator] preservesPartitioning flag turned on If the input partitioner is different than the RDD's, combineByKeyWithClassTag creates a xref:rdd:ShuffledRDD.adoc[ShuffledRDD] (with the Serializer, the Aggregator, and the mapSideCombine flag). === [[combineByKeyWithClassTag-usage]] Usage combineByKeyWithClassTag lays the foundation for the following transformations: < > < > < > < > < > < > === [[combineByKeyWithClassTag-requirements]] Requirements combineByKeyWithClassTag requires that the mergeCombiners is defined (not- null ) or throws an IllegalArgumentException: [source,plaintext] \u00b6 mergeCombiners must be defined \u00b6 combineByKeyWithClassTag throws a SparkException for the keys being of type array with the mapSideCombine flag enabled: [source,plaintext] \u00b6 Cannot use map-side combining with array keys. \u00b6 combineByKeyWithClassTag throws a SparkException for the keys being of type array with the partitioner being a xref:rdd:HashPartitioner.adoc[HashPartitioner]: [source,plaintext] \u00b6 HashPartitioner cannot partition array keys. \u00b6 === [[combineByKeyWithClassTag-example]] Example [source,scala] \u00b6 val nums = sc.parallelize(0 to 9, numSlices = 4) val groups = nums.keyBy(_ % 2) def createCombiner(n: Int) = { println(s\"createCombiner( n)\") n } def mergeValue(n1: Int, n2: Int) = { println(s\"mergeValue( n)\") n } def mergeValue(n1: Int, n2: Int) = { println(s\"mergeValue( n1, n2)\") n1 + n2 } def mergeCombiners(c1: Int, c2: Int) = { println(s\"mergeCombiners( n2)\") n1 + n2 } def mergeCombiners(c1: Int, c2: Int) = { println(s\"mergeCombiners( c1, $c2)\") c1 + c2 } val countByGroup = groups.combineByKeyWithClassTag( createCombiner, mergeValue, mergeCombiners) println(countByGroup.toDebugString) /* (4) ShuffledRDD[3] at combineByKeyWithClassTag at :31 [] +-(4) MapPartitionsRDD[1] at keyBy at :25 [] | ParallelCollectionRDD[0] at parallelize at :24 [] */","title":"PairRDDFunctions"},{"location":"rdd/PairRDDFunctions/#source-scala","text":"aggregateByKey U: ClassTag ( seqOp: (U, V) => U, combOp: (U, U) => U): RDD[(K, U)] aggregateByKey U: ClassTag ( seqOp: (U, V) => U, combOp: (U, U) => U): RDD[(K, U)] aggregateByKey U: ClassTag ( seqOp: (U, V) => U, combOp: (U, U) => U): RDD[(K, U)] | combineByKey a| [[combineByKey]]","title":"[source, scala]"},{"location":"rdd/PairRDDFunctions/#source-scala_1","text":"combineByKey C : RDD[(K, C)] combineByKey C : RDD[(K, C)] combineByKey C : RDD[(K, C)] | countApproxDistinctByKey a| [[countApproxDistinctByKey]]","title":"[source, scala]"},{"location":"rdd/PairRDDFunctions/#source-scala_2","text":"countApproxDistinctByKey( relativeSD: Double = 0.05): RDD[(K, Long)] countApproxDistinctByKey( relativeSD: Double, numPartitions: Int): RDD[(K, Long)] countApproxDistinctByKey( relativeSD: Double, partitioner: Partitioner): RDD[(K, Long)] countApproxDistinctByKey( p: Int, sp: Int, partitioner: Partitioner): RDD[(K, Long)] | flatMapValues a| [[flatMapValues]]","title":"[source, scala]"},{"location":"rdd/PairRDDFunctions/#source-scala_3","text":"flatMapValues U : RDD[(K, U)] | foldByKey a| [[foldByKey]]","title":"[source, scala]"},{"location":"rdd/PairRDDFunctions/#source-scala_4","text":"foldByKey( zeroValue: V)( func: (V, V) => V): RDD[(K, V)] foldByKey( zeroValue: V, numPartitions: Int)( func: (V, V) => V): RDD[(K, V)] foldByKey( zeroValue: V, partitioner: Partitioner)( func: (V, V) => V): RDD[(K, V)] | mapValues a| [[mapValues]]","title":"[source, scala]"},{"location":"rdd/PairRDDFunctions/#source-scala_5","text":"mapValues U : RDD[(K, U)] | partitionBy a| [[partitionBy]]","title":"[source, scala]"},{"location":"rdd/PairRDDFunctions/#source-scala_6","text":"partitionBy( partitioner: Partitioner): RDD[(K, V)] | saveAsHadoopDataset a| [[saveAsHadoopDataset]]","title":"[source, scala]"},{"location":"rdd/PairRDDFunctions/#source-scala_7","text":"saveAsHadoopDataset( conf: JobConf): Unit saveAsHadoopDataset uses the SparkHadoopWriter utility to < > with a < > (for the given Hadoop https://hadoop.apache.org/docs/r2.7.3/api/org/apache/hadoop/mapred/JobConf.html[JobConf ]) | saveAsHadoopFile a| [[saveAsHadoopFile]]","title":"[source, scala]"},{"location":"rdd/PairRDDFunctions/#source-scala_8","text":"saveAsHadoopFile( path: String, keyClass: Class[ ], valueClass: Class[ ], outputFormatClass: Class[_ <: OutputFormat[ , _]], codec: Class[ <: CompressionCodec]): Unit saveAsHadoopFile( path: String, keyClass: Class[ ], valueClass: Class[ ], outputFormatClass: Class[_ <: OutputFormat[ , _]], conf: JobConf = new JobConf(self.context.hadoopConfiguration), codec: Option[Class[ <: CompressionCodec]] = None): Unit saveAsHadoopFile F <: OutputFormat[K, V] (implicit fm: ClassTag[F]): Unit saveAsHadoopFile F <: OutputFormat[K, V] (implicit fm: ClassTag[F]): Unit | saveAsNewAPIHadoopDataset a| [[saveAsNewAPIHadoopDataset]]","title":"[source, scala]"},{"location":"rdd/PairRDDFunctions/#source-scala_9","text":"saveAsNewAPIHadoopDataset( conf: Configuration): Unit Saves this RDD of key-value pairs ( RDD[K,V] ) to any Hadoop-supported storage system with new Hadoop API (using a Hadoop https://hadoop.apache.org/docs/r2.7.3/api/org/apache/hadoop/conf/Configuration.html[Configuration ] object for that storage system). The configuration should set relevant output params (an https://hadoop.apache.org/docs/r2.7.3/api/org/apache/hadoop/mapreduce/OutputFormat.html[output format], output paths, e.g. a table name to write to) in the same way as it would be configured for a Hadoop MapReduce job. saveAsNewAPIHadoopDataset uses the SparkHadoopWriter utility to < > with a < > (for the given Hadoop https://hadoop.apache.org/docs/r2.7.3/api/org/apache/hadoop/conf/Configuration.html[Configuration ]) | saveAsNewAPIHadoopFile a| [[saveAsNewAPIHadoopFile]]","title":"[source, scala]"},{"location":"rdd/PairRDDFunctions/#source-scala_10","text":"saveAsNewAPIHadoopFile( path: String, keyClass: Class[ ], valueClass: Class[ ], outputFormatClass: Class[_ <: NewOutputFormat[_, _]], conf: Configuration = self.context.hadoopConfiguration): Unit saveAsNewAPIHadoopFile F <: NewOutputFormat[K, V] (implicit fm: ClassTag[F]): Unit |=== == [[reduceByKey]][[groupByKey]] groupByKey and reduceByKey reduceByKey is sort of a particular case of < >. You may want to look at the number of partitions from another angle. It may often not be important to have a given number of partitions upfront (at RDD creation time upon link:spark-data-sources.adoc[loading data from data sources]), so only \"regrouping\" the data by key after it is an RDD might be...the key ( pun not intended ). You can use groupByKey or another PairRDDFunctions method to have a key in one processing flow. You could use partitionBy that is available for RDDs to be RDDs of tuples, i.e. PairRDD : rdd.keyBy(_.kind) .partitionBy(new HashPartitioner(PARTITIONS)) .foreachPartition(...) Think of situations where kind has low cardinality or highly skewed distribution and using the technique for partitioning might be not an optimal solution. You could do as follows: rdd.keyBy(_.kind).reduceByKey(....) or mapValues or plenty of other solutions. FIXME, man . == [[combineByKeyWithClassTag]] combineByKeyWithClassTag","title":"[source, scala]"},{"location":"rdd/PairRDDFunctions/#source-scala_11","text":"combineByKeyWithClassTag C (implicit ct: ClassTag[C]): RDD[(K, C)] // <1> combineByKeyWithClassTag C (implicit ct: ClassTag[C]): RDD[(K, C)] // <2> combineByKeyWithClassTag C (implicit ct: ClassTag[C]): RDD[(K, C)] <1> Uses the xref:rdd:Partitioner.adoc#defaultPartitioner[default partitioner] <2> Uses a xref:rdd:HashPartitioner.adoc[HashPartitioner] with the given number of partitions combineByKeyWithClassTag creates an xref:rdd:Aggregator.adoc[Aggregator] for the given aggregation functions. combineByKeyWithClassTag branches off per the given xref:rdd:Partitioner.adoc[Partitioner]. If the input partitioner and the RDD's are the same, combineByKeyWithClassTag simply xref:rdd:spark-rdd-transformations.adoc#mapPartitions[mapPartitions] on the RDD with the following arguments: Iterator of the xref:rdd:Aggregator.adoc#combineValuesByKey[Aggregator] preservesPartitioning flag turned on If the input partitioner is different than the RDD's, combineByKeyWithClassTag creates a xref:rdd:ShuffledRDD.adoc[ShuffledRDD] (with the Serializer, the Aggregator, and the mapSideCombine flag). === [[combineByKeyWithClassTag-usage]] Usage combineByKeyWithClassTag lays the foundation for the following transformations: < > < > < > < > < > < > === [[combineByKeyWithClassTag-requirements]] Requirements combineByKeyWithClassTag requires that the mergeCombiners is defined (not- null ) or throws an IllegalArgumentException:","title":"[source, scala]"},{"location":"rdd/PairRDDFunctions/#sourceplaintext","text":"","title":"[source,plaintext]"},{"location":"rdd/PairRDDFunctions/#mergecombiners-must-be-defined","text":"combineByKeyWithClassTag throws a SparkException for the keys being of type array with the mapSideCombine flag enabled:","title":"mergeCombiners must be defined"},{"location":"rdd/PairRDDFunctions/#sourceplaintext_1","text":"","title":"[source,plaintext]"},{"location":"rdd/PairRDDFunctions/#cannot-use-map-side-combining-with-array-keys","text":"combineByKeyWithClassTag throws a SparkException for the keys being of type array with the partitioner being a xref:rdd:HashPartitioner.adoc[HashPartitioner]:","title":"Cannot use map-side combining with array keys."},{"location":"rdd/PairRDDFunctions/#sourceplaintext_2","text":"","title":"[source,plaintext]"},{"location":"rdd/PairRDDFunctions/#hashpartitioner-cannot-partition-array-keys","text":"=== [[combineByKeyWithClassTag-example]] Example","title":"HashPartitioner cannot partition array keys."},{"location":"rdd/PairRDDFunctions/#sourcescala","text":"val nums = sc.parallelize(0 to 9, numSlices = 4) val groups = nums.keyBy(_ % 2) def createCombiner(n: Int) = { println(s\"createCombiner( n)\") n } def mergeValue(n1: Int, n2: Int) = { println(s\"mergeValue( n)\") n } def mergeValue(n1: Int, n2: Int) = { println(s\"mergeValue( n1, n2)\") n1 + n2 } def mergeCombiners(c1: Int, c2: Int) = { println(s\"mergeCombiners( n2)\") n1 + n2 } def mergeCombiners(c1: Int, c2: Int) = { println(s\"mergeCombiners( c1, $c2)\") c1 + c2 } val countByGroup = groups.combineByKeyWithClassTag( createCombiner, mergeValue, mergeCombiners) println(countByGroup.toDebugString) /* (4) ShuffledRDD[3] at combineByKeyWithClassTag at :31 [] +-(4) MapPartitionsRDD[1] at keyBy at :25 [] | ParallelCollectionRDD[0] at parallelize at :24 [] */","title":"[source,scala]"},{"location":"rdd/Partitioner/","text":"= Partitioner Partitioner is an abstraction to define how the elements in a key-value pair RDD are partitioned by key. Partitioner < > (from 0 to < > - 1). Partitioner is used to ensure that records for a given key have to reside on a single partition. == [[implementations]] Available Partitioners [cols=\"30,70\",options=\"header\",width=\"100%\"] |=== | Partitioner | Description | xref:rdd:HashPartitioner.adoc[HashPartitioner] | [[HashPartitioner]] Hash-based partitioning | xref:rdd:RangePartitioner.adoc[RangePartitioner] | [[RangePartitioner]] |=== == [[numPartitions]] numPartitions Method [source, scala] \u00b6 numPartitions: Int \u00b6 numPartitions is the number of partition to use for < >. numPartitions is used when...FIXME == [[getPartition]] getPartition Method [source, scala] \u00b6 getPartition(key: Any): Int \u00b6 getPartition maps a given key to a partition ID (from 0 to < > - 1) getPartition is used when...FIXME == [[defaultPartitioner]] defaultPartitioner Method [source, scala] \u00b6 defaultPartitioner( rdd: RDD[ ], others: RDD[ ]*): Partitioner defaultPartitioner...FIXME defaultPartitioner is used when...FIXME","title":"Partitioner"},{"location":"rdd/Partitioner/#source-scala","text":"","title":"[source, scala]"},{"location":"rdd/Partitioner/#numpartitions-int","text":"numPartitions is the number of partition to use for < >. numPartitions is used when...FIXME == [[getPartition]] getPartition Method","title":"numPartitions: Int"},{"location":"rdd/Partitioner/#source-scala_1","text":"","title":"[source, scala]"},{"location":"rdd/Partitioner/#getpartitionkey-any-int","text":"getPartition maps a given key to a partition ID (from 0 to < > - 1) getPartition is used when...FIXME == [[defaultPartitioner]] defaultPartitioner Method","title":"getPartition(key: Any): Int"},{"location":"rdd/Partitioner/#source-scala_2","text":"defaultPartitioner( rdd: RDD[ ], others: RDD[ ]*): Partitioner defaultPartitioner...FIXME defaultPartitioner is used when...FIXME","title":"[source, scala]"},{"location":"rdd/RDD/","text":"= [[RDD]] RDD -- Description of Distributed Computation :navtitle: RDD [[T]] RDD is a description of a fault-tolerant and resilient computation over a possibly distributed collection of records (of type T ). == [[contract]] RDD Contract === [[compute]] Computing Partition (in TaskContext) [source, scala] \u00b6 compute( split: Partition, context: TaskContext): Iterator[T] compute computes the input split xref:rdd:spark-rdd-partitions.adoc[partition] in the xref:scheduler:spark-TaskContext.adoc[TaskContext] to produce a collection of values (of type T ). compute is implemented by any type of RDD in Spark and is called every time the records are requested unless RDD is xref:rdd:spark-rdd-caching.adoc[cached] or xref:ROOT:rdd-checkpointing.adoc[checkpointed] (and the records can be read from an external storage, but this time closer to the compute node). When an RDD is xref:rdd:spark-rdd-caching.adoc[cached], for specified xref:storage:StorageLevel.adoc[storage levels] (i.e. all but NONE )...FIXME compute runs on the xref:ROOT:spark-driver.adoc[driver]. compute is used when RDD is requested to < >. === [[getPartitions]] Partitions [source, scala] \u00b6 getPartitions: Array[Partition] \u00b6 getPartitions is used when RDD is requested for the < > (called only once as the value is cached afterwards). === [[getDependencies]] Dependencies [source, scala] \u00b6 getDependencies: Seq[Dependency[_]] \u00b6 getDependencies is used when RDD is requested for the < > (called only once as the value is cached afterwards). === [[getPreferredLocations]] Preferred Locations (Placement Preferences) [source, scala] \u00b6 getPreferredLocations( split: Partition): Seq[String] = Nil getPreferredLocations is used when RDD is requested for the < > of a given xref:rdd:spark-rdd-Partition.adoc[partition]. === [[partitioner]] Partitioner [source, scala] \u00b6 partitioner: Option[Partitioner] = None \u00b6 RDD can have a xref:rdd:Partitioner.adoc[Partitioner] defined. == [[extensions]][[implementations]] (Subset of) Available RDDs [cols=\"30,70\",options=\"header\",width=\"100%\"] |=== | RDD | Description | xref:rdd:spark-rdd-CoGroupedRDD.adoc[CoGroupedRDD] | [[CoGroupedRDD]] | CoalescedRDD | [[CoalescedRDD]] Result of xref:rdd:spark-rdd-partitions.adoc#repartition[repartition] or xref:rdd:spark-rdd-partitions.adoc#coalesce[coalesce] transformations | xref:rdd:spark-rdd-HadoopRDD.adoc[HadoopRDD] | [[HadoopRDD]] Allows for reading data stored in HDFS using the older MapReduce API. The most notable use case is the return RDD of SparkContext.textFile . | xref:rdd:spark-rdd-MapPartitionsRDD.adoc[MapPartitionsRDD] | [[MapPartitionsRDD]] Result of calling map-like operations (e.g. map , flatMap , filter , xref:rdd:spark-rdd-transformations.adoc#mapPartitions[mapPartitions]) | xref:rdd:spark-rdd-ParallelCollectionRDD.adoc[ParallelCollectionRDD] | [[ParallelCollectionRDD]] | xref:rdd:ShuffledRDD.adoc[ShuffledRDD] | [[ShuffledRDD]] Result of \"shuffle\" operators (e.g. xref:rdd:spark-rdd-partitions.adoc#repartition[repartition] or xref:rdd:spark-rdd-partitions.adoc#coalesce[coalesce]) |=== == [[creating-instance]] Creating Instance RDD takes the following to be created: [[_sc]] xref:ROOT:SparkContext.adoc[] [[deps]] Parent RDDs , i.e. xref:rdd:spark-rdd-Dependency.adoc[Dependencies] (that have to be all computed successfully before this RDD) RDD is an abstract class and cannot be created directly. It is created indirectly for the < >. == [[storageLevel]][[getStorageLevel]] StorageLevel RDD can have a xref:storage:StorageLevel.adoc[StorageLevel] specified. The default StorageLevel is xref:storage:StorageLevel.adoc#NONE[NONE]. storageLevel can be specified using < > method. storageLevel becomes NONE again after < >. The current StorageLevel is available using getStorageLevel method. [source, scala] \u00b6 getStorageLevel: StorageLevel \u00b6 == [[id]] Unique Identifier [source, scala] \u00b6 id: Int \u00b6 id is an unique identifier (aka RDD ID ) in the given <<_sc, SparkContext>>. id requests the < > for xref:ROOT:SparkContext.adoc#newRddId[newRddId] right when RDD is created. == [[isBarrier_]][[isBarrier]] Barrier Stage An RDD can be part of a xref:ROOT:spark-barrier-execution-mode.adoc#barrier-stage[barrier stage]. By default, isBarrier flag is enabled ( true ) when: . There are no xref:rdd:ShuffleDependency.adoc[ShuffleDependencies] among the < > . There is at least one xref:rdd:spark-rdd-Dependency.adoc#rdd[parent RDD] that has the flag enabled xref:rdd:ShuffledRDD.adoc[ShuffledRDD] has the flag always disabled. xref:rdd:spark-rdd-MapPartitionsRDD.adoc[MapPartitionsRDD] is the only one RDD that can have the flag enabled. == [[getOrCompute]] Getting Or Computing RDD Partition [source, scala] \u00b6 getOrCompute( partition: Partition, context: TaskContext): Iterator[T] getOrCompute creates a xref:storage:BlockId.adoc#RDDBlockId[RDDBlockId] for the < > and the link:spark-rdd-Partition.adoc#index[partition index]. getOrCompute requests the BlockManager to xref:storage:BlockManager.adoc#getOrElseUpdate[getOrElseUpdate] for the block ID (with the < > and the makeIterator function). NOTE: getOrCompute uses xref:core:SparkEnv.adoc#get[SparkEnv] to access the current xref:core:SparkEnv.adoc#blockManager[BlockManager]. [[getOrCompute-readCachedBlock]] getOrCompute records whether...FIXME (readCachedBlock) getOrCompute branches off per the response from the xref:storage:BlockManager.adoc#getOrElseUpdate[BlockManager] and whether the internal readCachedBlock flag is now on or still off. In either case, getOrCompute creates an link:spark-InterruptibleIterator.adoc[InterruptibleIterator]. NOTE: link:spark-InterruptibleIterator.adoc[InterruptibleIterator] simply delegates to a wrapped internal Iterator , but allows for link:spark-TaskContext.adoc#isInterrupted[task killing functionality]. For a BlockResult available and readCachedBlock flag on, getOrCompute ...FIXME For a BlockResult available and readCachedBlock flag off, getOrCompute ...FIXME NOTE: The BlockResult could be found in a local block manager or fetched from a remote block manager. It may also have been stored (persisted) just now. In either case, the BlockResult is available (and xref:storage:BlockManager.adoc#getOrElseUpdate[BlockManager.getOrElseUpdate] gives a Left value with the BlockResult ). For Right(iter) (regardless of the value of readCachedBlock flag since...FIXME), getOrCompute ...FIXME NOTE: xref:storage:BlockManager.adoc#getOrElseUpdate[BlockManager.getOrElseUpdate] gives a Right(iter) value to indicate an error with a block. NOTE: getOrCompute is used on Spark executors. NOTE: getOrCompute is used exclusively when RDD is requested for the < >. == [[dependencies]] RDD Dependencies [source, scala] \u00b6 dependencies: Seq[Dependency[_]] \u00b6 dependencies returns the link:spark-rdd-Dependency.adoc[dependencies of a RDD]. NOTE: dependencies is a final method that no class in Spark can ever override. Internally, dependencies checks out whether the RDD is xref:ROOT:rdd-checkpointing.adoc[checkpointed] and acts accordingly. For a RDD being checkpointed, dependencies returns a single-element collection with a link:spark-rdd-NarrowDependency.adoc#OneToOneDependency[OneToOneDependency]. For a non-checkpointed RDD, dependencies collection is computed using < getDependencies method>>. NOTE: getDependencies method is an abstract method that custom RDDs are required to provide. == [[iterator]] Accessing Records For Partition Lazily [source, scala] \u00b6 iterator( split: Partition, context: TaskContext): Iterator[T] iterator < split partition>> when xref:rdd:spark-rdd-caching.adoc[cached] or < >. == [[checkpointRDD]] Getting CheckpointRDD [source, scala] \u00b6 checkpointRDD: Option[CheckpointRDD[T]] \u00b6 checkpointRDD gives the CheckpointRDD from the < > internal registry if available (if the RDD was checkpointed). checkpointRDD is used when RDD is requested for the < >, < > and < >. == [[isCheckpointedAndMaterialized]] isCheckpointedAndMaterialized Method [source, scala] \u00b6 isCheckpointedAndMaterialized: Boolean \u00b6 isCheckpointedAndMaterialized...FIXME isCheckpointedAndMaterialized is used when RDD is requested to < >, < > and < >. == [[getNarrowAncestors]] getNarrowAncestors Method [source, scala] \u00b6 getNarrowAncestors: Seq[RDD[_]] \u00b6 getNarrowAncestors...FIXME getNarrowAncestors is used when StageInfo is requested to xref:scheduler:spark-scheduler-StageInfo.adoc#fromStage[fromStage]. == [[toLocalIterator]] toLocalIterator Method [source, scala] \u00b6 toLocalIterator: Iterator[T] \u00b6 toLocalIterator...FIXME == [[persist]] Persisting RDD [source, scala] \u00b6 persist(): this.type persist( newLevel: StorageLevel): this.type Refer to xref:rdd:spark-rdd-caching.adoc#persist[Persisting RDD]. == [[persist-internal]] persist Internal Method [source, scala] \u00b6 persist( newLevel: StorageLevel, allowOverride: Boolean): this.type persist...FIXME persist (private) is used when RDD is requested to < > and < >. == [[unpersist]] unpersist Method [source, scala] \u00b6 unpersist(blocking: Boolean = true): this.type \u00b6 unpersist...FIXME == [[localCheckpoint]] localCheckpoint Method [source, scala] \u00b6 localCheckpoint(): this.type \u00b6 localCheckpoint marks this RDD for xref:ROOT:rdd-checkpointing.adoc[local checkpointing] using Spark's caching layer. == [[computeOrReadCheckpoint]] Computing Partition or Reading From Checkpoint [source, scala] \u00b6 computeOrReadCheckpoint( split: Partition, context: TaskContext): Iterator[T] computeOrReadCheckpoint reads split partition from a checkpoint (< >) or < > yourself. computeOrReadCheckpoint is used when RDD is requested to < > or < >. == [[getNumPartitions]] Getting Number of Partitions [source, scala] \u00b6 getNumPartitions: Int \u00b6 getNumPartitions gives the number of partitions of a RDD. [source, scala] \u00b6 scala> sc.textFile(\"README.md\").getNumPartitions res0: Int = 2 scala> sc.textFile(\"README.md\", 5).getNumPartitions res1: Int = 5 == [[preferredLocations]] Defining Placement Preferences of RDD Partition [source, scala] \u00b6 preferredLocations( split: Partition): Seq[String] preferredLocations requests the CheckpointRDD for < > (if the RDD is checkpointed) or < >. preferredLocations is a template method that uses < > that custom RDDs can override to specify placement preferences for a partition. getPreferredLocations defines no placement preferences by default. preferredLocations is mainly used when DAGScheduler is requested to xref:scheduler:DAGScheduler.adoc#getPreferredLocs[compute the preferred locations for missing partitions]. == [[partitions]] Accessing RDD Partitions [source, scala] \u00b6 partitions: Array[Partition] \u00b6 partitions returns the xref:rdd:spark-rdd-partitions.adoc[Partitions] of a RDD . partitions requests CheckpointRDD for the < > (if the RDD is checkpointed) or < > and cache (in < > internal registry that is used next time). Partitions have the property that their internal index should be equal to their position in the owning RDD. == [[markCheckpointed]] markCheckpointed Method [source, scala] \u00b6 markCheckpointed(): Unit \u00b6 markCheckpointed...FIXME markCheckpointed is used when...FIXME == [[doCheckpoint]] doCheckpoint Method [source, scala] \u00b6 doCheckpoint(): Unit \u00b6 doCheckpoint...FIXME doCheckpoint is used when SparkContext is requested to xref:ROOT:SparkContext.adoc#runJob[run a job (synchronously)]. == [[checkpoint]] Reliable Checkpointing -- checkpoint Method [source, scala] \u00b6 checkpoint(): Unit \u00b6 checkpoint...FIXME checkpoint is used when...FIXME == [[isReliablyCheckpointed]] isReliablyCheckpointed Method [source, scala] \u00b6 isReliablyCheckpointed: Boolean \u00b6 isReliablyCheckpointed...FIXME isReliablyCheckpointed is used when...FIXME == [[getCheckpointFile]] getCheckpointFile Method [source, scala] \u00b6 getCheckpointFile: Option[String] \u00b6 getCheckpointFile...FIXME getCheckpointFile is used when...FIXME","title":"RDD"},{"location":"rdd/RDD/#source-scala","text":"compute( split: Partition, context: TaskContext): Iterator[T] compute computes the input split xref:rdd:spark-rdd-partitions.adoc[partition] in the xref:scheduler:spark-TaskContext.adoc[TaskContext] to produce a collection of values (of type T ). compute is implemented by any type of RDD in Spark and is called every time the records are requested unless RDD is xref:rdd:spark-rdd-caching.adoc[cached] or xref:ROOT:rdd-checkpointing.adoc[checkpointed] (and the records can be read from an external storage, but this time closer to the compute node). When an RDD is xref:rdd:spark-rdd-caching.adoc[cached], for specified xref:storage:StorageLevel.adoc[storage levels] (i.e. all but NONE )...FIXME compute runs on the xref:ROOT:spark-driver.adoc[driver]. compute is used when RDD is requested to < >. === [[getPartitions]] Partitions","title":"[source, scala]"},{"location":"rdd/RDD/#source-scala_1","text":"","title":"[source, scala]"},{"location":"rdd/RDD/#getpartitions-arraypartition","text":"getPartitions is used when RDD is requested for the < > (called only once as the value is cached afterwards). === [[getDependencies]] Dependencies","title":"getPartitions: Array[Partition]"},{"location":"rdd/RDD/#source-scala_2","text":"","title":"[source, scala]"},{"location":"rdd/RDD/#getdependencies-seqdependency_","text":"getDependencies is used when RDD is requested for the < > (called only once as the value is cached afterwards). === [[getPreferredLocations]] Preferred Locations (Placement Preferences)","title":"getDependencies: Seq[Dependency[_]]"},{"location":"rdd/RDD/#source-scala_3","text":"getPreferredLocations( split: Partition): Seq[String] = Nil getPreferredLocations is used when RDD is requested for the < > of a given xref:rdd:spark-rdd-Partition.adoc[partition]. === [[partitioner]] Partitioner","title":"[source, scala]"},{"location":"rdd/RDD/#source-scala_4","text":"","title":"[source, scala]"},{"location":"rdd/RDD/#partitioner-optionpartitioner-none","text":"RDD can have a xref:rdd:Partitioner.adoc[Partitioner] defined. == [[extensions]][[implementations]] (Subset of) Available RDDs [cols=\"30,70\",options=\"header\",width=\"100%\"] |=== | RDD | Description | xref:rdd:spark-rdd-CoGroupedRDD.adoc[CoGroupedRDD] | [[CoGroupedRDD]] | CoalescedRDD | [[CoalescedRDD]] Result of xref:rdd:spark-rdd-partitions.adoc#repartition[repartition] or xref:rdd:spark-rdd-partitions.adoc#coalesce[coalesce] transformations | xref:rdd:spark-rdd-HadoopRDD.adoc[HadoopRDD] | [[HadoopRDD]] Allows for reading data stored in HDFS using the older MapReduce API. The most notable use case is the return RDD of SparkContext.textFile . | xref:rdd:spark-rdd-MapPartitionsRDD.adoc[MapPartitionsRDD] | [[MapPartitionsRDD]] Result of calling map-like operations (e.g. map , flatMap , filter , xref:rdd:spark-rdd-transformations.adoc#mapPartitions[mapPartitions]) | xref:rdd:spark-rdd-ParallelCollectionRDD.adoc[ParallelCollectionRDD] | [[ParallelCollectionRDD]] | xref:rdd:ShuffledRDD.adoc[ShuffledRDD] | [[ShuffledRDD]] Result of \"shuffle\" operators (e.g. xref:rdd:spark-rdd-partitions.adoc#repartition[repartition] or xref:rdd:spark-rdd-partitions.adoc#coalesce[coalesce]) |=== == [[creating-instance]] Creating Instance RDD takes the following to be created: [[_sc]] xref:ROOT:SparkContext.adoc[] [[deps]] Parent RDDs , i.e. xref:rdd:spark-rdd-Dependency.adoc[Dependencies] (that have to be all computed successfully before this RDD) RDD is an abstract class and cannot be created directly. It is created indirectly for the < >. == [[storageLevel]][[getStorageLevel]] StorageLevel RDD can have a xref:storage:StorageLevel.adoc[StorageLevel] specified. The default StorageLevel is xref:storage:StorageLevel.adoc#NONE[NONE]. storageLevel can be specified using < > method. storageLevel becomes NONE again after < >. The current StorageLevel is available using getStorageLevel method.","title":"partitioner: Option[Partitioner] = None"},{"location":"rdd/RDD/#source-scala_5","text":"","title":"[source, scala]"},{"location":"rdd/RDD/#getstoragelevel-storagelevel","text":"== [[id]] Unique Identifier","title":"getStorageLevel: StorageLevel"},{"location":"rdd/RDD/#source-scala_6","text":"","title":"[source, scala]"},{"location":"rdd/RDD/#id-int","text":"id is an unique identifier (aka RDD ID ) in the given <<_sc, SparkContext>>. id requests the < > for xref:ROOT:SparkContext.adoc#newRddId[newRddId] right when RDD is created. == [[isBarrier_]][[isBarrier]] Barrier Stage An RDD can be part of a xref:ROOT:spark-barrier-execution-mode.adoc#barrier-stage[barrier stage]. By default, isBarrier flag is enabled ( true ) when: . There are no xref:rdd:ShuffleDependency.adoc[ShuffleDependencies] among the < > . There is at least one xref:rdd:spark-rdd-Dependency.adoc#rdd[parent RDD] that has the flag enabled xref:rdd:ShuffledRDD.adoc[ShuffledRDD] has the flag always disabled. xref:rdd:spark-rdd-MapPartitionsRDD.adoc[MapPartitionsRDD] is the only one RDD that can have the flag enabled. == [[getOrCompute]] Getting Or Computing RDD Partition","title":"id: Int"},{"location":"rdd/RDD/#source-scala_7","text":"getOrCompute( partition: Partition, context: TaskContext): Iterator[T] getOrCompute creates a xref:storage:BlockId.adoc#RDDBlockId[RDDBlockId] for the < > and the link:spark-rdd-Partition.adoc#index[partition index]. getOrCompute requests the BlockManager to xref:storage:BlockManager.adoc#getOrElseUpdate[getOrElseUpdate] for the block ID (with the < > and the makeIterator function). NOTE: getOrCompute uses xref:core:SparkEnv.adoc#get[SparkEnv] to access the current xref:core:SparkEnv.adoc#blockManager[BlockManager]. [[getOrCompute-readCachedBlock]] getOrCompute records whether...FIXME (readCachedBlock) getOrCompute branches off per the response from the xref:storage:BlockManager.adoc#getOrElseUpdate[BlockManager] and whether the internal readCachedBlock flag is now on or still off. In either case, getOrCompute creates an link:spark-InterruptibleIterator.adoc[InterruptibleIterator]. NOTE: link:spark-InterruptibleIterator.adoc[InterruptibleIterator] simply delegates to a wrapped internal Iterator , but allows for link:spark-TaskContext.adoc#isInterrupted[task killing functionality]. For a BlockResult available and readCachedBlock flag on, getOrCompute ...FIXME For a BlockResult available and readCachedBlock flag off, getOrCompute ...FIXME NOTE: The BlockResult could be found in a local block manager or fetched from a remote block manager. It may also have been stored (persisted) just now. In either case, the BlockResult is available (and xref:storage:BlockManager.adoc#getOrElseUpdate[BlockManager.getOrElseUpdate] gives a Left value with the BlockResult ). For Right(iter) (regardless of the value of readCachedBlock flag since...FIXME), getOrCompute ...FIXME NOTE: xref:storage:BlockManager.adoc#getOrElseUpdate[BlockManager.getOrElseUpdate] gives a Right(iter) value to indicate an error with a block. NOTE: getOrCompute is used on Spark executors. NOTE: getOrCompute is used exclusively when RDD is requested for the < >. == [[dependencies]] RDD Dependencies","title":"[source, scala]"},{"location":"rdd/RDD/#source-scala_8","text":"","title":"[source, scala]"},{"location":"rdd/RDD/#dependencies-seqdependency_","text":"dependencies returns the link:spark-rdd-Dependency.adoc[dependencies of a RDD]. NOTE: dependencies is a final method that no class in Spark can ever override. Internally, dependencies checks out whether the RDD is xref:ROOT:rdd-checkpointing.adoc[checkpointed] and acts accordingly. For a RDD being checkpointed, dependencies returns a single-element collection with a link:spark-rdd-NarrowDependency.adoc#OneToOneDependency[OneToOneDependency]. For a non-checkpointed RDD, dependencies collection is computed using < getDependencies method>>. NOTE: getDependencies method is an abstract method that custom RDDs are required to provide. == [[iterator]] Accessing Records For Partition Lazily","title":"dependencies: Seq[Dependency[_]]"},{"location":"rdd/RDD/#source-scala_9","text":"iterator( split: Partition, context: TaskContext): Iterator[T] iterator < split partition>> when xref:rdd:spark-rdd-caching.adoc[cached] or < >. == [[checkpointRDD]] Getting CheckpointRDD","title":"[source, scala]"},{"location":"rdd/RDD/#source-scala_10","text":"","title":"[source, scala]"},{"location":"rdd/RDD/#checkpointrdd-optioncheckpointrddt","text":"checkpointRDD gives the CheckpointRDD from the < > internal registry if available (if the RDD was checkpointed). checkpointRDD is used when RDD is requested for the < >, < > and < >. == [[isCheckpointedAndMaterialized]] isCheckpointedAndMaterialized Method","title":"checkpointRDD: Option[CheckpointRDD[T]]"},{"location":"rdd/RDD/#source-scala_11","text":"","title":"[source, scala]"},{"location":"rdd/RDD/#ischeckpointedandmaterialized-boolean","text":"isCheckpointedAndMaterialized...FIXME isCheckpointedAndMaterialized is used when RDD is requested to < >, < > and < >. == [[getNarrowAncestors]] getNarrowAncestors Method","title":"isCheckpointedAndMaterialized: Boolean"},{"location":"rdd/RDD/#source-scala_12","text":"","title":"[source, scala]"},{"location":"rdd/RDD/#getnarrowancestors-seqrdd_","text":"getNarrowAncestors...FIXME getNarrowAncestors is used when StageInfo is requested to xref:scheduler:spark-scheduler-StageInfo.adoc#fromStage[fromStage]. == [[toLocalIterator]] toLocalIterator Method","title":"getNarrowAncestors: Seq[RDD[_]]"},{"location":"rdd/RDD/#source-scala_13","text":"","title":"[source, scala]"},{"location":"rdd/RDD/#tolocaliterator-iteratort","text":"toLocalIterator...FIXME == [[persist]] Persisting RDD","title":"toLocalIterator: Iterator[T]"},{"location":"rdd/RDD/#source-scala_14","text":"persist(): this.type persist( newLevel: StorageLevel): this.type Refer to xref:rdd:spark-rdd-caching.adoc#persist[Persisting RDD]. == [[persist-internal]] persist Internal Method","title":"[source, scala]"},{"location":"rdd/RDD/#source-scala_15","text":"persist( newLevel: StorageLevel, allowOverride: Boolean): this.type persist...FIXME persist (private) is used when RDD is requested to < > and < >. == [[unpersist]] unpersist Method","title":"[source, scala]"},{"location":"rdd/RDD/#source-scala_16","text":"","title":"[source, scala]"},{"location":"rdd/RDD/#unpersistblocking-boolean-true-thistype","text":"unpersist...FIXME == [[localCheckpoint]] localCheckpoint Method","title":"unpersist(blocking: Boolean = true): this.type"},{"location":"rdd/RDD/#source-scala_17","text":"","title":"[source, scala]"},{"location":"rdd/RDD/#localcheckpoint-thistype","text":"localCheckpoint marks this RDD for xref:ROOT:rdd-checkpointing.adoc[local checkpointing] using Spark's caching layer. == [[computeOrReadCheckpoint]] Computing Partition or Reading From Checkpoint","title":"localCheckpoint(): this.type"},{"location":"rdd/RDD/#source-scala_18","text":"computeOrReadCheckpoint( split: Partition, context: TaskContext): Iterator[T] computeOrReadCheckpoint reads split partition from a checkpoint (< >) or < > yourself. computeOrReadCheckpoint is used when RDD is requested to < > or < >. == [[getNumPartitions]] Getting Number of Partitions","title":"[source, scala]"},{"location":"rdd/RDD/#source-scala_19","text":"","title":"[source, scala]"},{"location":"rdd/RDD/#getnumpartitions-int","text":"getNumPartitions gives the number of partitions of a RDD.","title":"getNumPartitions: Int"},{"location":"rdd/RDD/#source-scala_20","text":"scala> sc.textFile(\"README.md\").getNumPartitions res0: Int = 2 scala> sc.textFile(\"README.md\", 5).getNumPartitions res1: Int = 5 == [[preferredLocations]] Defining Placement Preferences of RDD Partition","title":"[source, scala]"},{"location":"rdd/RDD/#source-scala_21","text":"preferredLocations( split: Partition): Seq[String] preferredLocations requests the CheckpointRDD for < > (if the RDD is checkpointed) or < >. preferredLocations is a template method that uses < > that custom RDDs can override to specify placement preferences for a partition. getPreferredLocations defines no placement preferences by default. preferredLocations is mainly used when DAGScheduler is requested to xref:scheduler:DAGScheduler.adoc#getPreferredLocs[compute the preferred locations for missing partitions]. == [[partitions]] Accessing RDD Partitions","title":"[source, scala]"},{"location":"rdd/RDD/#source-scala_22","text":"","title":"[source, scala]"},{"location":"rdd/RDD/#partitions-arraypartition","text":"partitions returns the xref:rdd:spark-rdd-partitions.adoc[Partitions] of a RDD . partitions requests CheckpointRDD for the < > (if the RDD is checkpointed) or < > and cache (in < > internal registry that is used next time). Partitions have the property that their internal index should be equal to their position in the owning RDD. == [[markCheckpointed]] markCheckpointed Method","title":"partitions: Array[Partition]"},{"location":"rdd/RDD/#source-scala_23","text":"","title":"[source, scala]"},{"location":"rdd/RDD/#markcheckpointed-unit","text":"markCheckpointed...FIXME markCheckpointed is used when...FIXME == [[doCheckpoint]] doCheckpoint Method","title":"markCheckpointed(): Unit"},{"location":"rdd/RDD/#source-scala_24","text":"","title":"[source, scala]"},{"location":"rdd/RDD/#docheckpoint-unit","text":"doCheckpoint...FIXME doCheckpoint is used when SparkContext is requested to xref:ROOT:SparkContext.adoc#runJob[run a job (synchronously)]. == [[checkpoint]] Reliable Checkpointing -- checkpoint Method","title":"doCheckpoint(): Unit"},{"location":"rdd/RDD/#source-scala_25","text":"","title":"[source, scala]"},{"location":"rdd/RDD/#checkpoint-unit","text":"checkpoint...FIXME checkpoint is used when...FIXME == [[isReliablyCheckpointed]] isReliablyCheckpointed Method","title":"checkpoint(): Unit"},{"location":"rdd/RDD/#source-scala_26","text":"","title":"[source, scala]"},{"location":"rdd/RDD/#isreliablycheckpointed-boolean","text":"isReliablyCheckpointed...FIXME isReliablyCheckpointed is used when...FIXME == [[getCheckpointFile]] getCheckpointFile Method","title":"isReliablyCheckpointed: Boolean"},{"location":"rdd/RDD/#source-scala_27","text":"","title":"[source, scala]"},{"location":"rdd/RDD/#getcheckpointfile-optionstring","text":"getCheckpointFile...FIXME getCheckpointFile is used when...FIXME","title":"getCheckpointFile: Option[String]"},{"location":"rdd/RDDCheckpointData/","text":"= RDDCheckpointData RDDCheckpointData is an abstraction of information related to RDD checkpointing. == [[implementations]] Available RDDCheckpointDatas [cols=\"30,70\",options=\"header\",width=\"100%\"] |=== | RDDCheckpointData | Description | xref:rdd:LocalRDDCheckpointData.adoc[LocalRDDCheckpointData] | [[LocalRDDCheckpointData]] | xref:rdd:ReliableRDDCheckpointData.adoc[ReliableRDDCheckpointData] | [[ReliableRDDCheckpointData]] xref:ROOT:rdd-checkpointing.adoc#reliable-checkpointing[Reliable Checkpointing] |=== == [[creating-instance]] Creating Instance RDDCheckpointData takes the following to be created: [[rdd]] xref:rdd:RDD.adoc[RDD] == [[Serializable]] RDDCheckpointData as Serializable RDDCheckpointData is java.io.Serializable. == [[cpState]] States [[Initialized]] Initialized [[CheckpointingInProgress]] CheckpointingInProgress [[Checkpointed]] Checkpointed == [[checkpoint]] Checkpointing RDD [source, scala] \u00b6 checkpoint(): CheckpointRDD[T] \u00b6 checkpoint changes the < > to < > only when in < > state. Otherwise, checkpoint does nothing and returns. checkpoint < > that gives an CheckpointRDD (that is the < > internal registry). checkpoint changes the < > to < >. In the end, checkpoint requests the given < > to xref:rdd:RDD.adoc#markCheckpointed[markCheckpointed]. checkpoint is used when RDD is requested to xref:rdd:RDD.adoc#doCheckpoint[doCheckpoint]. == [[doCheckpoint]] doCheckpoint Method [source, scala] \u00b6 doCheckpoint(): CheckpointRDD[T] \u00b6 doCheckpoint is used when RDDCheckpointData is requested to < >.","title":"RDDCheckpointData"},{"location":"rdd/RDDCheckpointData/#source-scala","text":"","title":"[source, scala]"},{"location":"rdd/RDDCheckpointData/#checkpoint-checkpointrddt","text":"checkpoint changes the < > to < > only when in < > state. Otherwise, checkpoint does nothing and returns. checkpoint < > that gives an CheckpointRDD (that is the < > internal registry). checkpoint changes the < > to < >. In the end, checkpoint requests the given < > to xref:rdd:RDD.adoc#markCheckpointed[markCheckpointed]. checkpoint is used when RDD is requested to xref:rdd:RDD.adoc#doCheckpoint[doCheckpoint]. == [[doCheckpoint]] doCheckpoint Method","title":"checkpoint(): CheckpointRDD[T]"},{"location":"rdd/RDDCheckpointData/#source-scala_1","text":"","title":"[source, scala]"},{"location":"rdd/RDDCheckpointData/#docheckpoint-checkpointrddt","text":"doCheckpoint is used when RDDCheckpointData is requested to < >.","title":"doCheckpoint(): CheckpointRDD[T]"},{"location":"rdd/RangePartitioner/","text":"= RangePartitioner RangePartitioner is a xref:rdd:Partitioner.adoc[Partitioner] for...FIXME [[ordering]] RangePartitioner[K : Ordering : ClassTag, V] is a parameterized type of K keys that can be sorted ( ordered ) and V values. RangePartitioner is used for xref:rdd:spark-rdd-OrderedRDDFunctions.adoc#sortByKey[sortByKey] operator. == [[creating-instance]] Creating Instance RangePartitioner takes the following to be created: [[partitions]] Number of partitions [[rdd]] xref:rdd:RDD.adoc[RDD] ( RDD[_ <: Product2[K, V]] ) [[ascending]] ascending flag (default: true ) [[samplePointsPerPartitionHint]] samplePointsPerPartitionHint (default: 20) == [[rangeBounds]] rangeBounds Array RangePartitioner uses rangeBounds registry (of type Array[K] ) when requested for < > and < >, < >. == [[numPartitions]] Number of Partitions [source,scala] \u00b6 numPartitions: Int \u00b6 numPartitions is simply one more than the length of the < > array. numPartitions is part of the xref:rdd:Partitioner.adoc#numPartitions[Partitioner] abstraction. == [[getPartition]] Finding Partition ID for Key [source, scala] \u00b6 getPartition(key: Any): Int \u00b6 getPartition...FIXME getPartition is part of the xref:rdd:Partitioner.adoc#getPartition[Partitioner] abstraction.","title":"RangePartitioner"},{"location":"rdd/RangePartitioner/#sourcescala","text":"","title":"[source,scala]"},{"location":"rdd/RangePartitioner/#numpartitions-int","text":"numPartitions is simply one more than the length of the < > array. numPartitions is part of the xref:rdd:Partitioner.adoc#numPartitions[Partitioner] abstraction. == [[getPartition]] Finding Partition ID for Key","title":"numPartitions: Int"},{"location":"rdd/RangePartitioner/#source-scala","text":"","title":"[source, scala]"},{"location":"rdd/RangePartitioner/#getpartitionkey-any-int","text":"getPartition...FIXME getPartition is part of the xref:rdd:Partitioner.adoc#getPartition[Partitioner] abstraction.","title":"getPartition(key: Any): Int"},{"location":"rdd/ReliableCheckpointRDD/","text":"= ReliableCheckpointRDD ReliableCheckpointRDD is an xref:rdd:CheckpointRDD.adoc[CheckpointRDD]...FIXME == [[creating-instance]] Creating Instance ReliableCheckpointRDD takes the following to be created: [[sc]] xref:ROOT:SparkContext.adoc[] [[checkpointPath]] Checkpoint Directory (on a Hadoop DFS-compatible file system) <<_partitioner, Partitioner>> ReliableCheckpointRDD is created when: ReliableCheckpointRDD utility is used to < >. SparkContext is requested to xref:ROOT:SparkContext.adoc#checkpointFile[checkpointFile] == [[checkpointPartitionerFileName]] Checkpointed Partitioner File ReliableCheckpointRDD uses _partitioner as the name of the file in the < > with the < > serialized to. == [[partitioner]] Partitioner ReliableCheckpointRDD can be given a xref:rdd:Partitioner.adoc[Partitioner] to be created. When xref:rdd:RDD.adoc#partitioner[requested for the Partitioner] (as an RDD), ReliableCheckpointRDD returns the one it was created with or < >. == [[writeRDDToCheckpointDirectory]] Writing RDD to Checkpoint Directory [source, scala] \u00b6 writeRDDToCheckpointDirectory T: ClassTag : ReliableCheckpointRDD[T] writeRDDToCheckpointDirectory...FIXME writeRDDToCheckpointDirectory is used when ReliableRDDCheckpointData is requested to xref:rdd:ReliableRDDCheckpointData.adoc#doCheckpoint[doCheckpoint]. == [[writePartitionerToCheckpointDir]] Writing Partitioner to Checkpoint Directory [source,scala] \u00b6 writePartitionerToCheckpointDir( sc: SparkContext, partitioner: Partitioner, checkpointDirPath: Path): Unit writePartitionerToCheckpointDir creates the < > with the buffer size based on xref:ROOT:configuration-properties.adoc#spark.buffer.size[spark.buffer.size] configuration property. writePartitionerToCheckpointDir requests the xref:core:SparkEnv.adoc#serializer[default Serializer] for a new xref:serializer:Serializer.adoc#newInstance[SerializerInstance]. writePartitionerToCheckpointDir requests the SerializerInstance to xref:serializer:SerializerInstance.adoc#serializeStream[serialize the output stream] and xref:serializer:DeserializationStream.adoc#writeObject[writes] the given Partitioner. In the end, writePartitionerToCheckpointDir prints out the following DEBUG message to the logs: [source,plaintext] \u00b6 Written partitioner to [partitionerFilePath] \u00b6 In case of any non-fatal exception, writePartitionerToCheckpointDir prints out the following DEBUG message to the logs: [source,plaintext] \u00b6 Error writing partitioner [partitioner] to [checkpointDirPath] \u00b6 writePartitionerToCheckpointDir is used when ReliableCheckpointRDD is requested to < >. == [[readCheckpointedPartitionerFile]] Reading Partitioner from Checkpointed Directory [source,scala] \u00b6 readCheckpointedPartitionerFile( sc: SparkContext, checkpointDirPath: String): Option[Partitioner] readCheckpointedPartitionerFile opens the < > with the buffer size based on xref:ROOT:configuration-properties.adoc#spark.buffer.size[spark.buffer.size] configuration property. readCheckpointedPartitionerFile requests the xref:core:SparkEnv.adoc#serializer[default Serializer] for a new xref:serializer:Serializer.adoc#newInstance[SerializerInstance]. readCheckpointedPartitionerFile requests the SerializerInstance to xref:serializer:SerializerInstance.adoc#deserializeStream[deserialize the input stream] and xref:serializer:DeserializationStream.adoc#readObject[read the Partitioner] from the partitioner file. readCheckpointedPartitionerFile prints out the following DEBUG message to the logs and returns the partitioner. [source,plaintext] \u00b6 Read partitioner from [partitionerFilePath] \u00b6 In case of FileNotFoundException or any non-fatal exceptions, readCheckpointedPartitionerFile prints out a corresponding message to the logs and returns None. readCheckpointedPartitionerFile is used when ReliableCheckpointRDD is requested for the < >. == [[logging]] Logging Enable ALL logging level for org.apache.spark.rdd.ReliableCheckpointRDD$ logger to see what happens inside. Add the following line to conf/log4j.properties : [source,plaintext] \u00b6 log4j.logger.org.apache.spark.rdd.ReliableCheckpointRDD$=ALL \u00b6 Refer to xref:ROOT:spark-logging.adoc[Logging].","title":"ReliableCheckpointRDD"},{"location":"rdd/ReliableCheckpointRDD/#source-scala","text":"writeRDDToCheckpointDirectory T: ClassTag : ReliableCheckpointRDD[T] writeRDDToCheckpointDirectory...FIXME writeRDDToCheckpointDirectory is used when ReliableRDDCheckpointData is requested to xref:rdd:ReliableRDDCheckpointData.adoc#doCheckpoint[doCheckpoint]. == [[writePartitionerToCheckpointDir]] Writing Partitioner to Checkpoint Directory","title":"[source, scala]"},{"location":"rdd/ReliableCheckpointRDD/#sourcescala","text":"writePartitionerToCheckpointDir( sc: SparkContext, partitioner: Partitioner, checkpointDirPath: Path): Unit writePartitionerToCheckpointDir creates the < > with the buffer size based on xref:ROOT:configuration-properties.adoc#spark.buffer.size[spark.buffer.size] configuration property. writePartitionerToCheckpointDir requests the xref:core:SparkEnv.adoc#serializer[default Serializer] for a new xref:serializer:Serializer.adoc#newInstance[SerializerInstance]. writePartitionerToCheckpointDir requests the SerializerInstance to xref:serializer:SerializerInstance.adoc#serializeStream[serialize the output stream] and xref:serializer:DeserializationStream.adoc#writeObject[writes] the given Partitioner. In the end, writePartitionerToCheckpointDir prints out the following DEBUG message to the logs:","title":"[source,scala]"},{"location":"rdd/ReliableCheckpointRDD/#sourceplaintext","text":"","title":"[source,plaintext]"},{"location":"rdd/ReliableCheckpointRDD/#written-partitioner-to-partitionerfilepath","text":"In case of any non-fatal exception, writePartitionerToCheckpointDir prints out the following DEBUG message to the logs:","title":"Written partitioner to [partitionerFilePath]"},{"location":"rdd/ReliableCheckpointRDD/#sourceplaintext_1","text":"","title":"[source,plaintext]"},{"location":"rdd/ReliableCheckpointRDD/#error-writing-partitioner-partitioner-to-checkpointdirpath","text":"writePartitionerToCheckpointDir is used when ReliableCheckpointRDD is requested to < >. == [[readCheckpointedPartitionerFile]] Reading Partitioner from Checkpointed Directory","title":"Error writing partitioner [partitioner] to [checkpointDirPath]"},{"location":"rdd/ReliableCheckpointRDD/#sourcescala_1","text":"readCheckpointedPartitionerFile( sc: SparkContext, checkpointDirPath: String): Option[Partitioner] readCheckpointedPartitionerFile opens the < > with the buffer size based on xref:ROOT:configuration-properties.adoc#spark.buffer.size[spark.buffer.size] configuration property. readCheckpointedPartitionerFile requests the xref:core:SparkEnv.adoc#serializer[default Serializer] for a new xref:serializer:Serializer.adoc#newInstance[SerializerInstance]. readCheckpointedPartitionerFile requests the SerializerInstance to xref:serializer:SerializerInstance.adoc#deserializeStream[deserialize the input stream] and xref:serializer:DeserializationStream.adoc#readObject[read the Partitioner] from the partitioner file. readCheckpointedPartitionerFile prints out the following DEBUG message to the logs and returns the partitioner.","title":"[source,scala]"},{"location":"rdd/ReliableCheckpointRDD/#sourceplaintext_2","text":"","title":"[source,plaintext]"},{"location":"rdd/ReliableCheckpointRDD/#read-partitioner-from-partitionerfilepath","text":"In case of FileNotFoundException or any non-fatal exceptions, readCheckpointedPartitionerFile prints out a corresponding message to the logs and returns None. readCheckpointedPartitionerFile is used when ReliableCheckpointRDD is requested for the < >. == [[logging]] Logging Enable ALL logging level for org.apache.spark.rdd.ReliableCheckpointRDD$ logger to see what happens inside. Add the following line to conf/log4j.properties :","title":"Read partitioner from [partitionerFilePath]"},{"location":"rdd/ReliableCheckpointRDD/#sourceplaintext_3","text":"","title":"[source,plaintext]"},{"location":"rdd/ReliableCheckpointRDD/#log4jloggerorgapachesparkrddreliablecheckpointrddall","text":"Refer to xref:ROOT:spark-logging.adoc[Logging].","title":"log4j.logger.org.apache.spark.rdd.ReliableCheckpointRDD$=ALL"},{"location":"rdd/ReliableRDDCheckpointData/","text":"= ReliableRDDCheckpointData ReliableRDDCheckpointData is a xref:rdd:RDDCheckpointData.adoc[RDDCheckpointData] for xref:ROOT:rdd-checkpointing.adoc#reliable-checkpointing[Reliable Checkpointing]. == [[creating-instance]] Creating Instance ReliableRDDCheckpointData takes the following to be created: [[rdd]] xref:rdd:RDD.adoc[++RDD[T]++] ReliableRDDCheckpointData is created for xref:rdd:RDD.adoc#checkpoint[RDD.checkpoint] operator. == [[cpDir]][[checkpointPath]] Checkpoint Directory ReliableRDDCheckpointData creates a subdirectory of the xref:ROOT:SparkContext.adoc#checkpointDir[application-wide checkpoint directory] for < > the given < >. The name of the subdirectory uses the xref:rdd:RDD.adoc#id[unique identifier] of the < >: [source,plaintext] \u00b6 rdd-[id] \u00b6 == [[doCheckpoint]] Checkpointing RDD [source, scala] \u00b6 doCheckpoint(): CheckpointRDD[T] \u00b6 doCheckpoint xref:rdd:ReliableCheckpointRDD.adoc#writeRDDToCheckpointDirectory[writes] the < > to the < > (that creates a new RDD). With xref:ROOT:configuration-properties.adoc#spark.cleaner.referenceTracking.cleanCheckpoints[spark.cleaner.referenceTracking.cleanCheckpoints] configuration property enabled, doCheckpoint requests the xref:ROOT:SparkContext.adoc#cleaner[ContextCleaner] to xref:core:ContextCleaner.adoc#registerRDDCheckpointDataForCleanup[registerRDDCheckpointDataForCleanup] for the new RDD. In the end, doCheckpoint prints out the following INFO message to the logs and returns the new RDD. [source,plaintext] \u00b6 Done checkpointing RDD [id] to [cpDir], new parent is RDD [id] \u00b6 doCheckpoint is part of the xref:rdd:RDDCheckpointData.adoc#doCheckpoint[RDDCheckpointData] abstraction.","title":"ReliableRDDCheckpointData"},{"location":"rdd/ReliableRDDCheckpointData/#sourceplaintext","text":"","title":"[source,plaintext]"},{"location":"rdd/ReliableRDDCheckpointData/#rdd-id","text":"== [[doCheckpoint]] Checkpointing RDD","title":"rdd-[id]"},{"location":"rdd/ReliableRDDCheckpointData/#source-scala","text":"","title":"[source, scala]"},{"location":"rdd/ReliableRDDCheckpointData/#docheckpoint-checkpointrddt","text":"doCheckpoint xref:rdd:ReliableCheckpointRDD.adoc#writeRDDToCheckpointDirectory[writes] the < > to the < > (that creates a new RDD). With xref:ROOT:configuration-properties.adoc#spark.cleaner.referenceTracking.cleanCheckpoints[spark.cleaner.referenceTracking.cleanCheckpoints] configuration property enabled, doCheckpoint requests the xref:ROOT:SparkContext.adoc#cleaner[ContextCleaner] to xref:core:ContextCleaner.adoc#registerRDDCheckpointDataForCleanup[registerRDDCheckpointDataForCleanup] for the new RDD. In the end, doCheckpoint prints out the following INFO message to the logs and returns the new RDD.","title":"doCheckpoint(): CheckpointRDD[T]"},{"location":"rdd/ReliableRDDCheckpointData/#sourceplaintext_1","text":"","title":"[source,plaintext]"},{"location":"rdd/ReliableRDDCheckpointData/#done-checkpointing-rdd-id-to-cpdir-new-parent-is-rdd-id","text":"doCheckpoint is part of the xref:rdd:RDDCheckpointData.adoc#doCheckpoint[RDDCheckpointData] abstraction.","title":"Done checkpointing RDD [id] to [cpDir], new parent is RDD [id]"},{"location":"rdd/ShuffleDependency/","text":"= [[ShuffleDependency]] ShuffleDependency ShuffleDependency is a xref:rdd:spark-rdd-Dependency.adoc[Dependency] on the output of a xref:scheduler:ShuffleMapStage.adoc[ShuffleMapStage] for a < >. ShuffleDependency uses the < > to know the number of (map-side/pre-shuffle) partitions and the < > for the number of (reduce-size/post-shuffle) partitions. ShuffleDependency is created as a dependency of xref:rdd:ShuffledRDD.adoc[ShuffledRDD]. ShuffleDependency can also be created as a dependency of xref:rdd:spark-rdd-CoGroupedRDD.adoc[CoGroupedRDD] and xref:rdd:spark-rdd-SubtractedRDD.adoc[SubtractedRDD]. == [[creating-instance]] Creating Instance ShuffleDependency takes the following to be created: < > of key-value pairs ( RDD[_ <: Product2[K, V]] ) < > [[serializer]] xref:serializer:Serializer.adoc[Serializer] [[keyOrdering]] Ordering for K keys ( Option[Ordering[K]] ) < > ( Option[Aggregator[K, V, C]] ) < > flag (default: false ) When created, ShuffleDependency gets xref:ROOT:SparkContext.adoc#nextShuffleId[shuffle id] (as shuffleId ). NOTE: ShuffleDependency uses the xref:rdd:index.adoc#context[input RDD to access SparkContext ] and so the shuffleId . ShuffleDependency xref:shuffle:ShuffleManager.adoc#registerShuffle[registers itself with ShuffleManager ] and gets a ShuffleHandle (available as < > property). NOTE: ShuffleDependency accesses xref:core:SparkEnv.adoc#shuffleManager[ ShuffleManager using SparkEnv ]. In the end, ShuffleDependency xref:core:ContextCleaner.adoc#registerShuffleForCleanup[registers itself for cleanup with ContextCleaner ]. NOTE: ShuffleDependency accesses the xref:ROOT:SparkContext.adoc#cleaner[optional ContextCleaner through SparkContext ]. NOTE: ShuffleDependency is created when xref:ShuffledRDD.adoc#getDependencies[ShuffledRDD], link:spark-rdd-CoGroupedRDD.adoc#getDependencies[CoGroupedRDD], and link:spark-rdd-SubtractedRDD.adoc#getDependencies[SubtractedRDD] return their RDD dependencies. == [[shuffleId]] Shuffle ID Every ShuffleDependency has a unique application-wide shuffle ID that is assigned when < > (and is used throughout Spark's code to reference a ShuffleDependency). Shuffle IDs are tracked by xref:ROOT:SparkContext.adoc#nextShuffleId[SparkContext]. == [[rdd]] Parent RDD ShuffleDependency is given the parent xref:rdd:RDD.adoc[RDD] of key-value pairs ( RDD[_ <: Product2[K, V]] ). The parent RDD is available as rdd property that is part of the xref:rdd:spark-rdd-Dependency.adoc#rdd[Dependency] abstraction. [source,scala] \u00b6 rdd: RDD[Product2[K, V]] \u00b6 == [[partitioner]] Partitioner ShuffleDependency is given a xref:rdd:Partitioner.adoc[Partitioner] that is used to partition the shuffle output (when xref:shuffle:SortShuffleWriter.adoc[SortShuffleWriter], xref:shuffle:BypassMergeSortShuffleWriter.adoc[BypassMergeSortShuffleWriter] and xref:shuffle:UnsafeShuffleWriter.adoc[UnsafeShuffleWriter] are requested to write). == [[shuffleHandle]] ShuffleHandle [source, scala] \u00b6 shuffleHandle: ShuffleHandle \u00b6 shuffleHandle is the ShuffleHandle of a ShuffleDependency as assigned eagerly when < >. shuffleHandle is used to compute link:spark-rdd-CoGroupedRDD.adoc#compute[CoGroupedRDDs], xref:ShuffledRDD.adoc#compute[ShuffledRDD], link:spark-rdd-SubtractedRDD.adoc#compute[SubtractedRDD], and link:spark-sql-ShuffledRowRDD.adoc[ShuffledRowRDD] (to get a link:spark-shuffle-ShuffleReader.adoc[ShuffleReader] for a ShuffleDependency) and when a xref:scheduler:ShuffleMapTask.adoc#runTask[ ShuffleMapTask runs] (to get a ShuffleWriter for a ShuffleDependency). == [[mapSideCombine]] Map-Size Partial Aggregation Flag ShuffleDependency uses a mapSideCombine flag that controls whether to perform map-side partial aggregation ( map-side combine ) using an < >. mapSideCombine is disabled ( false ) by default and can be enabled ( true ) for some use cases of xref:rdd:ShuffledRDD.adoc#mapSideCombine[ShuffledRDD]. ShuffleDependency requires that the optional < > is defined when the flag is enabled. mapSideCombine is used when: BlockStoreShuffleReader is requested to xref:shuffle:BlockStoreShuffleReader.adoc#read[read combined records for a reduce task] SortShuffleManager is requested to xref:shuffle:SortShuffleManager.adoc#registerShuffle[register a shuffle] SortShuffleWriter is requested to xref:shuffle:SortShuffleWriter.adoc#write[write records] == [[aggregator]] Optional Aggregator [source, scala] \u00b6 aggregator: Option[Aggregator[K, V, C]] = None \u00b6 aggregator is a xref:rdd:Aggregator.adoc[map/reduce-side Aggregator] (for a RDD's shuffle). aggregator is by default undefined (i.e. None ) when < >. NOTE: aggregator is used when xref:shuffle:SortShuffleWriter.adoc#write[ SortShuffleWriter writes records] and xref:shuffle:BlockStoreShuffleReader.adoc#read[ BlockStoreShuffleReader reads combined key-values for a reduce task].","title":"ShuffleDependency"},{"location":"rdd/ShuffleDependency/#sourcescala","text":"","title":"[source,scala]"},{"location":"rdd/ShuffleDependency/#rdd-rddproduct2k-v","text":"== [[partitioner]] Partitioner ShuffleDependency is given a xref:rdd:Partitioner.adoc[Partitioner] that is used to partition the shuffle output (when xref:shuffle:SortShuffleWriter.adoc[SortShuffleWriter], xref:shuffle:BypassMergeSortShuffleWriter.adoc[BypassMergeSortShuffleWriter] and xref:shuffle:UnsafeShuffleWriter.adoc[UnsafeShuffleWriter] are requested to write). == [[shuffleHandle]] ShuffleHandle","title":"rdd: RDD[Product2[K, V]]"},{"location":"rdd/ShuffleDependency/#source-scala","text":"","title":"[source, scala]"},{"location":"rdd/ShuffleDependency/#shufflehandle-shufflehandle","text":"shuffleHandle is the ShuffleHandle of a ShuffleDependency as assigned eagerly when < >. shuffleHandle is used to compute link:spark-rdd-CoGroupedRDD.adoc#compute[CoGroupedRDDs], xref:ShuffledRDD.adoc#compute[ShuffledRDD], link:spark-rdd-SubtractedRDD.adoc#compute[SubtractedRDD], and link:spark-sql-ShuffledRowRDD.adoc[ShuffledRowRDD] (to get a link:spark-shuffle-ShuffleReader.adoc[ShuffleReader] for a ShuffleDependency) and when a xref:scheduler:ShuffleMapTask.adoc#runTask[ ShuffleMapTask runs] (to get a ShuffleWriter for a ShuffleDependency). == [[mapSideCombine]] Map-Size Partial Aggregation Flag ShuffleDependency uses a mapSideCombine flag that controls whether to perform map-side partial aggregation ( map-side combine ) using an < >. mapSideCombine is disabled ( false ) by default and can be enabled ( true ) for some use cases of xref:rdd:ShuffledRDD.adoc#mapSideCombine[ShuffledRDD]. ShuffleDependency requires that the optional < > is defined when the flag is enabled. mapSideCombine is used when: BlockStoreShuffleReader is requested to xref:shuffle:BlockStoreShuffleReader.adoc#read[read combined records for a reduce task] SortShuffleManager is requested to xref:shuffle:SortShuffleManager.adoc#registerShuffle[register a shuffle] SortShuffleWriter is requested to xref:shuffle:SortShuffleWriter.adoc#write[write records] == [[aggregator]] Optional Aggregator","title":"shuffleHandle: ShuffleHandle"},{"location":"rdd/ShuffleDependency/#source-scala_1","text":"","title":"[source, scala]"},{"location":"rdd/ShuffleDependency/#aggregator-optionaggregatork-v-c-none","text":"aggregator is a xref:rdd:Aggregator.adoc[map/reduce-side Aggregator] (for a RDD's shuffle). aggregator is by default undefined (i.e. None ) when < >. NOTE: aggregator is used when xref:shuffle:SortShuffleWriter.adoc#write[ SortShuffleWriter writes records] and xref:shuffle:BlockStoreShuffleReader.adoc#read[ BlockStoreShuffleReader reads combined key-values for a reduce task].","title":"aggregator: Option[Aggregator[K, V, C]] = None"},{"location":"rdd/ShuffledRDD/","text":"= [[ShuffledRDD]] ShuffledRDD ShuffledRDD is an xref:rdd:RDD.adoc[RDD] of key-value pairs that represents a shuffle step in a xref:spark-rdd-lineage.adoc[RDD lineage]. ShuffledRDD is given an < > of key-value pairs of K and V types, respectively, when < > and < > key-value pairs of K and C types, respectively. ShuffledRDD is < > for the following RDD transformations: xref:spark-rdd-OrderedRDDFunctions.adoc#sortByKey[OrderedRDDFunctions.sortByKey] and xref:spark-rdd-OrderedRDDFunctions.adoc#repartitionAndSortWithinPartitions[OrderedRDDFunctions.repartitionAndSortWithinPartitions] xref:rdd:PairRDDFunctions.adoc#combineByKeyWithClassTag[PairRDDFunctions.combineByKeyWithClassTag] and xref:rdd:PairRDDFunctions.adoc#partitionBy[PairRDDFunctions.partitionBy] xref:spark-rdd-transformations.adoc#coalesce[RDD.coalesce] (with shuffle flag enabled) ShuffledRDD uses custom < > partitions. [[isBarrier]] ShuffledRDD has xref:rdd:RDD.adoc#isBarrier[isBarrier] flag always disabled ( false ). == [[creating-instance]] Creating Instance ShuffledRDD takes the following to be created: [[prev]] Previous xref:rdd:RDD.adoc[RDD] of key-value pairs ( RDD[_ <: Product2[K, V]] ) [[part]] xref:rdd:Partitioner.adoc[Partitioner] == [[mapSideCombine]][[setMapSideCombine]] Map-Side Combine Flag ShuffledRDD uses a map-side combine flag to create a xref:rdd:ShuffleDependency.adoc[ShuffleDependency] when requested for the < > (there is always only one). The flag is disabled ( false ) by default and can be changed using setMapSideCombine method. [source,scala] \u00b6 setMapSideCombine( mapSideCombine: Boolean): ShuffledRDD[K, V, C] setMapSideCombine is used for xref:rdd:PairRDDFunctions.adoc#combineByKeyWithClassTag[PairRDDFunctions.combineByKeyWithClassTag] transformation (which defaults to the flag enabled). == [[compute]] Computing Partition [source, scala] \u00b6 compute( split: Partition, context: TaskContext): Iterator[(K, C)] compute requests the only xref:rdd:RDD.adoc#dependencies[dependency] (that is assumed a xref:rdd:ShuffleDependency.adoc[ShuffleDependency]) for the xref:rdd:ShuffleDependency.adoc#shuffleHandle[ShuffleHandle]. compute uses the xref:core:SparkEnv.adoc[SparkEnv] to access the xref:core:SparkEnv.adoc#shuffleManager[ShuffleManager]. compute requests the xref:shuffle:ShuffleManager.adoc#shuffleManager[ShuffleManager] for the xref:shuffle:ShuffleManager.adoc#getReader[ShuffleReader] (for the ShuffleHandle, the xref:rdd:spark-rdd-Partition.adoc[partition]). In the end, compute requests the ShuffleReader to xref:shuffle:spark-shuffle-ShuffleReader.adoc#read[read] the combined key-value pairs (of type (K, C) ). compute is part of the xref:rdd:RDD.adoc#compute[RDD] abstraction. == [[getPreferredLocations]] Placement Preferences of Partition [source, scala] \u00b6 getPreferredLocations( partition: Partition): Seq[String] getPreferredLocations requests MapOutputTrackerMaster for the xref:scheduler:MapOutputTrackerMaster.adoc#getPreferredLocationsForShuffle[preferred locations] of the given xref:rdd:spark-rdd-Partition.adoc[partition] (xref:storage:BlockManager.adoc[BlockManagers] with the most map outputs). getPreferredLocations uses SparkEnv to access the current xref:core:SparkEnv.adoc#mapOutputTracker[MapOutputTrackerMaster]. getPreferredLocations is part of the xref:rdd:RDD.adoc#compute[RDD] abstraction. == [[getDependencies]] Dependencies [source, scala] \u00b6 getDependencies: Seq[Dependency[_]] \u00b6 getDependencies uses the < > if defined or requests the current xref:serializer:SerializerManager.adoc[SerializerManager] for xref:serializer:SerializerManager.adoc#getSerializer[one]. getDependencies uses the < > internal flag for the types of the keys and values (i.e. K and C or K and V when the flag is enabled or not, respectively). In the end, getDependencies returns a single xref:rdd:ShuffleDependency.adoc[ShuffleDependency] (with the < >, the < >, and the Serializer). getDependencies is part of the xref:rdd:RDD.adoc#getDependencies[RDD] abstraction. == [[ShuffledRDDPartition]] ShuffledRDDPartition ShuffledRDDPartition gets an index to be created (that in turn is the index of partitions as calculated by the xref:rdd:Partitioner.adoc[Partitioner] of a < >). == Demos === Demo: ShuffledRDD and coalesce [source,plaintext] \u00b6 val data = sc.parallelize(0 to 9) val coalesced = data.coalesce(numPartitions = 4, shuffle = true) scala> println(coalesced.toDebugString) (4) MapPartitionsRDD[9] at coalesce at :75 [] | CoalescedRDD[8] at coalesce at :75 [] | ShuffledRDD[7] at coalesce at :75 [] +-(16) MapPartitionsRDD[6] at coalesce at :75 [] | ParallelCollectionRDD[5] at parallelize at :74 [] === Demo: ShuffledRDD and sortByKey [source,plaintext] \u00b6 val data = sc.parallelize(0 to 9) val grouped = rdd.groupBy(_ % 2) val sorted = grouped.sortByKey(numPartitions = 2) scala> println(sorted.toDebugString) (2) ShuffledRDD[15] at sortByKey at :74 [] +-(4) ShuffledRDD[12] at groupBy at :74 [] +-(4) MapPartitionsRDD[11] at groupBy at :74 [] | MapPartitionsRDD[9] at coalesce at :75 [] | CoalescedRDD[8] at coalesce at :75 [] | ShuffledRDD[7] at coalesce at :75 [] +-(16) MapPartitionsRDD[6] at coalesce at :75 [] | ParallelCollectionRDD[5] at parallelize at :74 [] == [[internal-properties]] Internal Properties [cols=\"30m,70\",options=\"header\",width=\"100%\"] |=== | Name | Description | userSpecifiedSerializer a| [[userSpecifiedSerializer]] User-specified xref:serializer:Serializer.adoc[Serializer] for the single xref:rdd:ShuffleDependency.adoc[ShuffleDependency] dependency [source, scala] \u00b6 userSpecifiedSerializer: Option[Serializer] = None \u00b6 userSpecifiedSerializer is undefined ( None ) by default and can be changed using setSerializer method (that is used for xref:rdd:PairRDDFunctions.adoc#combineByKeyWithClassTag[PairRDDFunctions.combineByKeyWithClassTag] transformation). |===","title":"ShuffledRDD"},{"location":"rdd/ShuffledRDD/#sourcescala","text":"setMapSideCombine( mapSideCombine: Boolean): ShuffledRDD[K, V, C] setMapSideCombine is used for xref:rdd:PairRDDFunctions.adoc#combineByKeyWithClassTag[PairRDDFunctions.combineByKeyWithClassTag] transformation (which defaults to the flag enabled). == [[compute]] Computing Partition","title":"[source,scala]"},{"location":"rdd/ShuffledRDD/#source-scala","text":"compute( split: Partition, context: TaskContext): Iterator[(K, C)] compute requests the only xref:rdd:RDD.adoc#dependencies[dependency] (that is assumed a xref:rdd:ShuffleDependency.adoc[ShuffleDependency]) for the xref:rdd:ShuffleDependency.adoc#shuffleHandle[ShuffleHandle]. compute uses the xref:core:SparkEnv.adoc[SparkEnv] to access the xref:core:SparkEnv.adoc#shuffleManager[ShuffleManager]. compute requests the xref:shuffle:ShuffleManager.adoc#shuffleManager[ShuffleManager] for the xref:shuffle:ShuffleManager.adoc#getReader[ShuffleReader] (for the ShuffleHandle, the xref:rdd:spark-rdd-Partition.adoc[partition]). In the end, compute requests the ShuffleReader to xref:shuffle:spark-shuffle-ShuffleReader.adoc#read[read] the combined key-value pairs (of type (K, C) ). compute is part of the xref:rdd:RDD.adoc#compute[RDD] abstraction. == [[getPreferredLocations]] Placement Preferences of Partition","title":"[source, scala]"},{"location":"rdd/ShuffledRDD/#source-scala_1","text":"getPreferredLocations( partition: Partition): Seq[String] getPreferredLocations requests MapOutputTrackerMaster for the xref:scheduler:MapOutputTrackerMaster.adoc#getPreferredLocationsForShuffle[preferred locations] of the given xref:rdd:spark-rdd-Partition.adoc[partition] (xref:storage:BlockManager.adoc[BlockManagers] with the most map outputs). getPreferredLocations uses SparkEnv to access the current xref:core:SparkEnv.adoc#mapOutputTracker[MapOutputTrackerMaster]. getPreferredLocations is part of the xref:rdd:RDD.adoc#compute[RDD] abstraction. == [[getDependencies]] Dependencies","title":"[source, scala]"},{"location":"rdd/ShuffledRDD/#source-scala_2","text":"","title":"[source, scala]"},{"location":"rdd/ShuffledRDD/#getdependencies-seqdependency_","text":"getDependencies uses the < > if defined or requests the current xref:serializer:SerializerManager.adoc[SerializerManager] for xref:serializer:SerializerManager.adoc#getSerializer[one]. getDependencies uses the < > internal flag for the types of the keys and values (i.e. K and C or K and V when the flag is enabled or not, respectively). In the end, getDependencies returns a single xref:rdd:ShuffleDependency.adoc[ShuffleDependency] (with the < >, the < >, and the Serializer). getDependencies is part of the xref:rdd:RDD.adoc#getDependencies[RDD] abstraction. == [[ShuffledRDDPartition]] ShuffledRDDPartition ShuffledRDDPartition gets an index to be created (that in turn is the index of partitions as calculated by the xref:rdd:Partitioner.adoc[Partitioner] of a < >). == Demos === Demo: ShuffledRDD and coalesce","title":"getDependencies: Seq[Dependency[_]]"},{"location":"rdd/ShuffledRDD/#sourceplaintext","text":"val data = sc.parallelize(0 to 9) val coalesced = data.coalesce(numPartitions = 4, shuffle = true) scala> println(coalesced.toDebugString) (4) MapPartitionsRDD[9] at coalesce at :75 [] | CoalescedRDD[8] at coalesce at :75 [] | ShuffledRDD[7] at coalesce at :75 [] +-(16) MapPartitionsRDD[6] at coalesce at :75 [] | ParallelCollectionRDD[5] at parallelize at :74 [] === Demo: ShuffledRDD and sortByKey","title":"[source,plaintext]"},{"location":"rdd/ShuffledRDD/#sourceplaintext_1","text":"val data = sc.parallelize(0 to 9) val grouped = rdd.groupBy(_ % 2) val sorted = grouped.sortByKey(numPartitions = 2) scala> println(sorted.toDebugString) (2) ShuffledRDD[15] at sortByKey at :74 [] +-(4) ShuffledRDD[12] at groupBy at :74 [] +-(4) MapPartitionsRDD[11] at groupBy at :74 [] | MapPartitionsRDD[9] at coalesce at :75 [] | CoalescedRDD[8] at coalesce at :75 [] | ShuffledRDD[7] at coalesce at :75 [] +-(16) MapPartitionsRDD[6] at coalesce at :75 [] | ParallelCollectionRDD[5] at parallelize at :74 [] == [[internal-properties]] Internal Properties [cols=\"30m,70\",options=\"header\",width=\"100%\"] |=== | Name | Description | userSpecifiedSerializer a| [[userSpecifiedSerializer]] User-specified xref:serializer:Serializer.adoc[Serializer] for the single xref:rdd:ShuffleDependency.adoc[ShuffleDependency] dependency","title":"[source,plaintext]"},{"location":"rdd/ShuffledRDD/#source-scala_3","text":"","title":"[source, scala]"},{"location":"rdd/ShuffledRDD/#userspecifiedserializer-optionserializer-none","text":"userSpecifiedSerializer is undefined ( None ) by default and can be changed using setSerializer method (that is used for xref:rdd:PairRDDFunctions.adoc#combineByKeyWithClassTag[PairRDDFunctions.combineByKeyWithClassTag] transformation). |===","title":"userSpecifiedSerializer: Option[Serializer] = None"},{"location":"rdd/spark-rdd-CoGroupedRDD/","text":"= CoGroupedRDD A RDD that cogroups its pair RDD parents. For each key k in parent RDDs, the resulting RDD contains a tuple with the list of values for that key. Use RDD.cogroup(...) to create one. == [[getDependencies]] getDependencies Method CAUTION: FIXME == [[compute]] Computing Partition (in TaskContext) [source, scala] \u00b6 compute(s: Partition, context: TaskContext): Iterator[(K, Array[Iterable[_]])] \u00b6 compute...FIXME compute is part of xref:rdd:RDD.adoc#compute[RDD] abstraction.","title":"CoGroupedRDD"},{"location":"rdd/spark-rdd-CoGroupedRDD/#source-scala","text":"","title":"[source, scala]"},{"location":"rdd/spark-rdd-CoGroupedRDD/#computes-partition-context-taskcontext-iteratork-arrayiterable_","text":"compute...FIXME compute is part of xref:rdd:RDD.adoc#compute[RDD] abstraction.","title":"compute(s: Partition, context: TaskContext): Iterator[(K, Array[Iterable[_]])]"},{"location":"rdd/spark-rdd-Dependency/","text":"== [[Dependency]] RDD Dependencies Dependency class is the base (abstract) class to model a dependency relationship between two or more RDDs. [[rdd]] Dependency has a single method rdd to access the RDD that is behind a dependency. [source, scala] \u00b6 def rdd: RDD[T] \u00b6 Whenever you apply a link:spark-rdd-transformations.adoc[transformation] (e.g. map , flatMap ) to a RDD you build the so-called link:spark-rdd-lineage.adoc[RDD lineage graph]. Dependency -ies represent the edges in a lineage graph. NOTE: link:spark-rdd-NarrowDependency.adoc[NarrowDependency] and xref:rdd:ShuffleDependency.adoc[ShuffleDependency] are the two top-level subclasses of Dependency abstract class. .Kinds of Dependencies [cols=\"1,2\",options=\"header\",width=\"100%\"] |=== | Name | Description | link:spark-rdd-NarrowDependency.adoc[NarrowDependency] | | xref:rdd:ShuffleDependency.adoc[ShuffleDependency] | | link:spark-rdd-NarrowDependency.adoc#OneToOneDependency[OneToOneDependency] | | link:spark-rdd-NarrowDependency.adoc#PruneDependency[PruneDependency] | | link:spark-rdd-NarrowDependency.adoc#RangeDependency[RangeDependency] | |=== [NOTE] \u00b6 The dependencies of a RDD are available using xref:rdd:index.adoc#dependencies[dependencies] method. // A demo RDD scala> val myRdd = sc.parallelize(0 to 9).groupBy(_ % 2) myRdd: org.apache.spark.rdd.RDD[(Int, Iterable[Int])] = ShuffledRDD[8] at groupBy at <console>:24 scala> myRdd.foreach(println) (0,CompactBuffer(0, 2, 4, 6, 8)) (1,CompactBuffer(1, 3, 5, 7, 9)) scala> myRdd.dependencies res5: Seq[org.apache.spark.Dependency[_]] = List(org.apache.spark.ShuffleDependency@27ace619) // Access all RDDs in the demo RDD lineage scala> myRdd.dependencies.map(_.rdd).foreach(println) MapPartitionsRDD[7] at groupBy at <console>:24 You use link:spark-rdd-lineage.adoc#toDebugString[toDebugString] method to print out the RDD lineage in a user-friendly way. scala> myRdd.toDebugString res6: String = (8) ShuffledRDD[8] at groupBy at <console>:24 [] +-(8) MapPartitionsRDD[7] at groupBy at <console>:24 [] | ParallelCollectionRDD[6] at parallelize at <console>:24 [] \u00b6","title":"Dependencies"},{"location":"rdd/spark-rdd-Dependency/#source-scala","text":"","title":"[source, scala]"},{"location":"rdd/spark-rdd-Dependency/#def-rdd-rddt","text":"Whenever you apply a link:spark-rdd-transformations.adoc[transformation] (e.g. map , flatMap ) to a RDD you build the so-called link:spark-rdd-lineage.adoc[RDD lineage graph]. Dependency -ies represent the edges in a lineage graph. NOTE: link:spark-rdd-NarrowDependency.adoc[NarrowDependency] and xref:rdd:ShuffleDependency.adoc[ShuffleDependency] are the two top-level subclasses of Dependency abstract class. .Kinds of Dependencies [cols=\"1,2\",options=\"header\",width=\"100%\"] |=== | Name | Description | link:spark-rdd-NarrowDependency.adoc[NarrowDependency] | | xref:rdd:ShuffleDependency.adoc[ShuffleDependency] | | link:spark-rdd-NarrowDependency.adoc#OneToOneDependency[OneToOneDependency] | | link:spark-rdd-NarrowDependency.adoc#PruneDependency[PruneDependency] | | link:spark-rdd-NarrowDependency.adoc#RangeDependency[RangeDependency] | |===","title":"def rdd: RDD[T]"},{"location":"rdd/spark-rdd-Dependency/#note","text":"The dependencies of a RDD are available using xref:rdd:index.adoc#dependencies[dependencies] method. // A demo RDD scala> val myRdd = sc.parallelize(0 to 9).groupBy(_ % 2) myRdd: org.apache.spark.rdd.RDD[(Int, Iterable[Int])] = ShuffledRDD[8] at groupBy at <console>:24 scala> myRdd.foreach(println) (0,CompactBuffer(0, 2, 4, 6, 8)) (1,CompactBuffer(1, 3, 5, 7, 9)) scala> myRdd.dependencies res5: Seq[org.apache.spark.Dependency[_]] = List(org.apache.spark.ShuffleDependency@27ace619) // Access all RDDs in the demo RDD lineage scala> myRdd.dependencies.map(_.rdd).foreach(println) MapPartitionsRDD[7] at groupBy at <console>:24 You use link:spark-rdd-lineage.adoc#toDebugString[toDebugString] method to print out the RDD lineage in a user-friendly way.","title":"[NOTE]"},{"location":"rdd/spark-rdd-Dependency/#scala-myrddtodebugstring-res6-string-8-shuffledrdd8-at-groupby-at-console24-8-mappartitionsrdd7-at-groupby-at-console24-parallelcollectionrdd6-at-parallelize-at-console24","text":"","title":"scala&gt; myRdd.toDebugString\nres6: String =\n(8) ShuffledRDD[8] at groupBy at &lt;console&gt;:24 []\n +-(8) MapPartitionsRDD[7] at groupBy at &lt;console&gt;:24 []\n    |  ParallelCollectionRDD[6] at parallelize at &lt;console&gt;:24 []\n"},{"location":"rdd/spark-rdd-HadoopRDD/","text":"== HadoopRDD https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.rdd.HadoopRDD[HadoopRDD ] is an RDD that provides core functionality for reading data stored in HDFS, a local file system (available on all nodes), or any Hadoop-supported file system URI using the older MapReduce API ( https://hadoop.apache.org/docs/current/api/org/apache/hadoop/mapred/package-summary.html[org.apache.hadoop.mapred ]). HadoopRDD is created as a result of calling the following methods in xref:ROOT:SparkContext.adoc[]: hadoopFile textFile (the most often used in examples!) sequenceFile Partitions are of type HadoopPartition . When an HadoopRDD is computed, i.e. an action is called, you should see the INFO message Input split: in the logs. scala> sc.textFile(\"README.md\").count ... 15/10/10 18:03:21 INFO HadoopRDD: Input split: file:/Users/jacek/dev/oss/spark/README.md:0+1784 15/10/10 18:03:21 INFO HadoopRDD: Input split: file:/Users/jacek/dev/oss/spark/README.md:1784+1784 ... The following properties are set upon partition execution: mapred.tip.id - task id of this task's attempt mapred.task.id - task attempt's id mapred.task.is.map as true mapred.task.partition - split id mapred.job.id Spark settings for HadoopRDD : spark.hadoop.cloneConf (default: false ) - shouldCloneJobConf - should a Hadoop job configuration JobConf object be cloned before spawning a Hadoop job. Refer to https://issues.apache.org/jira/browse/SPARK-2546[[SPARK-2546 ] Configuration object thread safety issue]. When true , you should see a DEBUG message Cloning Hadoop Configuration . You can register callbacks on link:spark-TaskContext.adoc[TaskContext]. HadoopRDDs are not checkpointed. They do nothing when checkpoint() is called. [CAUTION] \u00b6 FIXME What are InputMetrics ? What is JobConf ? What are the InputSplits: FileSplit and CombineFileSplit ? * What are InputFormat and Configurable subtypes? What's InputFormat's RecordReader? It creates a key and a value. What are they? What's Hadoop Split? input splits for Hadoop reads? See InputFormat.getSplits \u00b6 === [[getPreferredLocations]] getPreferredLocations Method CAUTION: FIXME === [[getPartitions]] getPartitions Method The number of partition for HadoopRDD, i.e. the return value of getPartitions , is calculated using InputFormat.getSplits(jobConf, minPartitions) where minPartitions is only a hint of how many partitions one may want at minimum. As a hint it does not mean the number of partitions will be exactly the number given. For SparkContext.textFile the input format class is https://hadoop.apache.org/docs/current/api/org/apache/hadoop/mapred/TextInputFormat.html[org.apache.hadoop.mapred.TextInputFormat ]. The https://hadoop.apache.org/docs/current/api/org/apache/hadoop/mapred/FileInputFormat.html[javadoc of org.apache.hadoop.mapred.FileInputFormat] says: FileInputFormat is the base class for all file-based InputFormats. This provides a generic implementation of getSplits(JobConf, int). Subclasses of FileInputFormat can also override the isSplitable(FileSystem, Path) method to ensure input-files are not split-up and are processed as a whole by Mappers. TIP: You may find https://github.com/apache/hadoop/blob/trunk/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/FileInputFormat.java#L319[the sources of org.apache.hadoop.mapred.FileInputFormat.getSplits] enlightening.","title":"HadoopRDD"},{"location":"rdd/spark-rdd-HadoopRDD/#caution","text":"FIXME What are InputMetrics ? What is JobConf ? What are the InputSplits: FileSplit and CombineFileSplit ? * What are InputFormat and Configurable subtypes? What's InputFormat's RecordReader? It creates a key and a value. What are they?","title":"[CAUTION]"},{"location":"rdd/spark-rdd-HadoopRDD/#whats-hadoop-split-input-splits-for-hadoop-reads-see-inputformatgetsplits","text":"=== [[getPreferredLocations]] getPreferredLocations Method CAUTION: FIXME === [[getPartitions]] getPartitions Method The number of partition for HadoopRDD, i.e. the return value of getPartitions , is calculated using InputFormat.getSplits(jobConf, minPartitions) where minPartitions is only a hint of how many partitions one may want at minimum. As a hint it does not mean the number of partitions will be exactly the number given. For SparkContext.textFile the input format class is https://hadoop.apache.org/docs/current/api/org/apache/hadoop/mapred/TextInputFormat.html[org.apache.hadoop.mapred.TextInputFormat ]. The https://hadoop.apache.org/docs/current/api/org/apache/hadoop/mapred/FileInputFormat.html[javadoc of org.apache.hadoop.mapred.FileInputFormat] says: FileInputFormat is the base class for all file-based InputFormats. This provides a generic implementation of getSplits(JobConf, int). Subclasses of FileInputFormat can also override the isSplitable(FileSystem, Path) method to ensure input-files are not split-up and are processed as a whole by Mappers. TIP: You may find https://github.com/apache/hadoop/blob/trunk/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/FileInputFormat.java#L319[the sources of org.apache.hadoop.mapred.FileInputFormat.getSplits] enlightening.","title":"What's Hadoop Split? input splits for Hadoop reads? See InputFormat.getSplits"},{"location":"rdd/spark-rdd-MapPartitionsRDD/","text":"== [[MapPartitionsRDD]] MapPartitionsRDD MapPartitionsRDD is an xref:rdd:RDD.adoc[RDD] that has exactly xref:rdd:spark-rdd-NarrowDependency.adoc#OneToOneDependency[one-to-one narrow dependency] on the < > and \"describes\" a distributed computation of the given < > to every RDD partition. MapPartitionsRDD is < > when: PairRDDFunctions ( RDD[(K, V)] ) is requested to xref:rdd:PairRDDFunctions.adoc#mapValues[mapValues] and xref:rdd:PairRDDFunctions.adoc#flatMapValues[flatMapValues] (with the < > flag enabled) RDD[T] is requested to < >, < >, < >, < >, < >, < >, < >, and < > RDDBarrier[T] is requested to < > (with the < > flag enabled) By default, it does not preserve partitioning -- the last input parameter preservesPartitioning is false . If it is true , it retains the original RDD's partitioning. MapPartitionsRDD is the result of the following transformations: filter glom link:spark-rdd-transformations.adoc#mapPartitions[mapPartitions] mapPartitionsWithIndex xref:rdd:PairRDDFunctions.adoc#mapValues[PairRDDFunctions.mapValues] xref:rdd:PairRDDFunctions.adoc#flatMapValues[PairRDDFunctions.flatMapValues] [[isBarrier_]] When requested for the xref:rdd:RDD.adoc#isBarrier_[isBarrier_] flag, MapPartitionsRDD gives the < > flag or check whether any of the RDDs of the xref:rdd:RDD.adoc#dependencies[RDD dependencies] are xref:rdd:RDD.adoc#isBarrier[barrier-enabled]. === [[creating-instance]] Creating MapPartitionsRDD Instance MapPartitionsRDD takes the following to be created: [[prev]] Parent xref:rdd:RDD.adoc[RDD] ( RDD[T] ) [[f]] Function to execute on partitions + (TaskContext, partitionID, Iterator[T]) => Iterator[U] [[preservesPartitioning]] preservesPartitioning flag (default: false ) [[isFromBarrier]] isFromBarrier flag for < > (default: false ) [[isOrderSensitive]] isOrderSensitive flag (default: false )","title":"MapPartitionsRDD"},{"location":"rdd/spark-rdd-NarrowDependency/","text":"== [[NarrowDependency]] NarrowDependency -- Narrow Dependencies NarrowDependency is a base (abstract) link:spark-rdd-Dependency.adoc[Dependency] with narrow (limited) number of link:spark-rdd-Partition.adoc[partitions] of the parent RDD that are required to compute a partition of the child RDD. NOTE: Narrow dependencies allow for pipelined execution. .Concrete NarrowDependency -ies [cols=\"1,2\",options=\"header\",width=\"100%\"] |=== | Name | Description | < > | | < > | | < > | |=== === [[contract]] NarrowDependency Contract NarrowDependency contract assumes that extensions implement getParents method. [source, scala] \u00b6 def getParents(partitionId: Int): Seq[Int] \u00b6 getParents returns the partitions of the parent RDD that the input partitionId depends on. === [[OneToOneDependency]] OneToOneDependency OneToOneDependency is a narrow dependency that represents a one-to-one dependency between partitions of the parent and child RDDs. scala> val r1 = sc.parallelize(0 to 9) r1: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[13] at parallelize at <console>:18 scala> val r3 = r1.map((_, 1)) r3: org.apache.spark.rdd.RDD[(Int, Int)] = MapPartitionsRDD[19] at map at <console>:20 scala> r3.dependencies res32: Seq[org.apache.spark.Dependency[_]] = List(org.apache.spark.OneToOneDependency@7353a0fb) scala> r3.toDebugString res33: String = (8) MapPartitionsRDD[19] at map at <console>:20 [] | ParallelCollectionRDD[13] at parallelize at <console>:18 [] === [[PruneDependency]] PruneDependency PruneDependency is a narrow dependency that represents a dependency between the PartitionPruningRDD and its parent RDD. === [[RangeDependency]] RangeDependency RangeDependency is a narrow dependency that represents a one-to-one dependency between ranges of partitions in the parent and child RDDs. It is used in UnionRDD for SparkContext.union , RDD.union transformation to list only a few. scala> val r1 = sc.parallelize(0 to 9) r1: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[13] at parallelize at <console>:18 scala> val r2 = sc.parallelize(10 to 19) r2: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[14] at parallelize at <console>:18 scala> val unioned = sc.union(r1, r2) unioned: org.apache.spark.rdd.RDD[Int] = UnionRDD[16] at union at <console>:22 scala> unioned.dependencies res19: Seq[org.apache.spark.Dependency[_]] = ArrayBuffer(org.apache.spark.RangeDependency@28408ad7, org.apache.spark.RangeDependency@6e1d2e9f) scala> unioned.toDebugString res18: String = (16) UnionRDD[16] at union at <console>:22 [] | ParallelCollectionRDD[13] at parallelize at <console>:18 [] | ParallelCollectionRDD[14] at parallelize at <console>:18 []","title":"NarrowDependency"},{"location":"rdd/spark-rdd-NarrowDependency/#source-scala","text":"","title":"[source, scala]"},{"location":"rdd/spark-rdd-NarrowDependency/#def-getparentspartitionid-int-seqint","text":"getParents returns the partitions of the parent RDD that the input partitionId depends on. === [[OneToOneDependency]] OneToOneDependency OneToOneDependency is a narrow dependency that represents a one-to-one dependency between partitions of the parent and child RDDs. scala> val r1 = sc.parallelize(0 to 9) r1: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[13] at parallelize at <console>:18 scala> val r3 = r1.map((_, 1)) r3: org.apache.spark.rdd.RDD[(Int, Int)] = MapPartitionsRDD[19] at map at <console>:20 scala> r3.dependencies res32: Seq[org.apache.spark.Dependency[_]] = List(org.apache.spark.OneToOneDependency@7353a0fb) scala> r3.toDebugString res33: String = (8) MapPartitionsRDD[19] at map at <console>:20 [] | ParallelCollectionRDD[13] at parallelize at <console>:18 [] === [[PruneDependency]] PruneDependency PruneDependency is a narrow dependency that represents a dependency between the PartitionPruningRDD and its parent RDD. === [[RangeDependency]] RangeDependency RangeDependency is a narrow dependency that represents a one-to-one dependency between ranges of partitions in the parent and child RDDs. It is used in UnionRDD for SparkContext.union , RDD.union transformation to list only a few. scala> val r1 = sc.parallelize(0 to 9) r1: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[13] at parallelize at <console>:18 scala> val r2 = sc.parallelize(10 to 19) r2: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[14] at parallelize at <console>:18 scala> val unioned = sc.union(r1, r2) unioned: org.apache.spark.rdd.RDD[Int] = UnionRDD[16] at union at <console>:22 scala> unioned.dependencies res19: Seq[org.apache.spark.Dependency[_]] = ArrayBuffer(org.apache.spark.RangeDependency@28408ad7, org.apache.spark.RangeDependency@6e1d2e9f) scala> unioned.toDebugString res18: String = (16) UnionRDD[16] at union at <console>:22 [] | ParallelCollectionRDD[13] at parallelize at <console>:18 [] | ParallelCollectionRDD[14] at parallelize at <console>:18 []","title":"def getParents(partitionId: Int): Seq[Int]"},{"location":"rdd/spark-rdd-NewHadoopRDD/","text":"== [[NewHadoopRDD]] NewHadoopRDD NewHadoopRDD is an xref:rdd:index.adoc[RDD] of K keys and V values. < NewHadoopRDD is created>> when: SparkContext.newAPIHadoopFile SparkContext.newAPIHadoopRDD (indirectly) SparkContext.binaryFiles (indirectly) SparkContext.wholeTextFiles NOTE: NewHadoopRDD is the base RDD of BinaryFileRDD and WholeTextFileRDD . === [[getPreferredLocations]] getPreferredLocations Method CAUTION: FIXME === [[creating-instance]] Creating NewHadoopRDD Instance NewHadoopRDD takes the following when created: [[sc]] xref:ROOT:SparkContext.adoc[] [[inputFormatClass]] HDFS' InputFormat[K, V] [[keyClass]] K class name [[valueClass]] V class name [[_conf]] transient HDFS' Configuration NewHadoopRDD initializes the < >.","title":"NewHadoopRDD"},{"location":"rdd/spark-rdd-OrderedRDDFunctions/","text":"== [[OrderedRDDFunctions]] OrderedRDDFunctions === [[repartitionAndSortWithinPartitions]] repartitionAndSortWithinPartitions Operator CAUTION: FIXME === [[sortByKey]] sortByKey Operator CAUTION: FIXME","title":"OrderedRDDFunctions"},{"location":"rdd/spark-rdd-ParallelCollectionRDD/","text":"== ParallelCollectionRDD ParallelCollectionRDD is an RDD of a collection of elements with numSlices partitions and optional locationPrefs . ParallelCollectionRDD is the result of SparkContext.parallelize and SparkContext.makeRDD methods. The data collection is split on to numSlices slices. It uses ParallelCollectionPartition .","title":"ParallelCollectionRDD"},{"location":"rdd/spark-rdd-Partition/","text":"== [[Partition]] Partition Partition is a < > of a < > of a RDD. NOTE: A partition is missing when it has not be computed yet. [[contract]] [[index]] Partition is identified by an partition index that is a unique identifier of a partition of a RDD. [source, scala] \u00b6 index: Int \u00b6","title":"Partition"},{"location":"rdd/spark-rdd-Partition/#source-scala","text":"","title":"[source, scala]"},{"location":"rdd/spark-rdd-Partition/#index-int","text":"","title":"index: Int"},{"location":"rdd/spark-rdd-SubtractedRDD/","text":"== [[SubtractedRDD]] SubtractedRDD CAUTION: FIXME === [[compute]] Computing Partition (in TaskContext) -- compute Method [source, scala] \u00b6 compute(p: Partition, context: TaskContext): Iterator[(K, V)] \u00b6 NOTE: compute is part of xref:rdd:RDD.adoc#compute[RDD Contract] to compute a link:spark-rdd-Partition.adoc[partition] (in a link:spark-TaskContext.adoc[TaskContext]). compute ...FIXME === [[getDependencies]] getDependencies Method CAUTION: FIXME","title":"SubtractedRDD"},{"location":"rdd/spark-rdd-SubtractedRDD/#source-scala","text":"","title":"[source, scala]"},{"location":"rdd/spark-rdd-SubtractedRDD/#computep-partition-context-taskcontext-iteratork-v","text":"NOTE: compute is part of xref:rdd:RDD.adoc#compute[RDD Contract] to compute a link:spark-rdd-Partition.adoc[partition] (in a link:spark-TaskContext.adoc[TaskContext]). compute ...FIXME === [[getDependencies]] getDependencies Method CAUTION: FIXME","title":"compute(p: Partition, context: TaskContext): Iterator[(K, V)]"},{"location":"rdd/spark-rdd-actions/","text":"== Actions Actions are link:spark-rdd-operations.adoc[RDD operations] that produce non-RDD values. They materialize a value in a Spark program. In other words, a RDD operation that returns a value of any type but RDD[T] is an action. action: RDD => a value NOTE: Actions are synchronous. You can use < > to release a calling thread while calling actions. They trigger execution of < > to return values. Simply put, an action evaluates the link:spark-rdd-lineage.adoc[RDD lineage graph]. You can think of actions as a valve and until action is fired, the data to be processed is not even in the pipes, i.e. transformations. Only actions can materialize the entire processing pipeline with real data. Actions are one of two ways to send data from xref:executor:Executor.adoc[executors] to the link:spark-driver.adoc[driver] (the other being link:spark-accumulators.adoc[accumulators]). Actions in http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.rdd.RDD[org.apache.spark.rdd.RDD ]: aggregate collect count countApprox* countByValue* first fold foreach foreachPartition max min reduce link:spark-io.adoc#saving-rdds-to-files[saveAs* actions], e.g. saveAsTextFile , saveAsHadoopFile take takeOrdered takeSample toLocalIterator top treeAggregate treeReduce Actions run link:spark-scheduler-ActiveJob.adoc[jobs] using xref:ROOT:SparkContext.adoc#runJob[SparkContext.runJob] or directly xref:scheduler:DAGScheduler.adoc#runJob[DAGScheduler.runJob]. [source,scala] \u00b6 scala> words.count // <1> res0: Long = 502 <1> words is an RDD of String . TIP: You should cache RDDs you work with when you want to execute two or more actions on it for a better performance. Refer to link:spark-rdd-caching.adoc[RDD Caching and Persistence]. Before calling an action, Spark does closure/function cleaning (using SparkContext.clean ) to make it ready for serialization and sending over the wire to executors. Cleaning can throw a SparkException if the computation cannot be cleaned. NOTE: Spark uses ClosureCleaner to clean closures. === [[AsyncRDDActions]] AsyncRDDActions AsyncRDDActions class offers asynchronous actions that you can use on RDDs (thanks to the implicit conversion rddToAsyncRDDActions in RDD class). The methods return a < >. The following asynchronous methods are available: countAsync collectAsync takeAsync foreachAsync foreachPartitionAsync === [[FutureAction]] FutureActions CAUTION: FIXME","title":"Actions"},{"location":"rdd/spark-rdd-actions/#sourcescala","text":"scala> words.count // <1> res0: Long = 502 <1> words is an RDD of String . TIP: You should cache RDDs you work with when you want to execute two or more actions on it for a better performance. Refer to link:spark-rdd-caching.adoc[RDD Caching and Persistence]. Before calling an action, Spark does closure/function cleaning (using SparkContext.clean ) to make it ready for serialization and sending over the wire to executors. Cleaning can throw a SparkException if the computation cannot be cleaned. NOTE: Spark uses ClosureCleaner to clean closures. === [[AsyncRDDActions]] AsyncRDDActions AsyncRDDActions class offers asynchronous actions that you can use on RDDs (thanks to the implicit conversion rddToAsyncRDDActions in RDD class). The methods return a < >. The following asynchronous methods are available: countAsync collectAsync takeAsync foreachAsync foreachPartitionAsync === [[FutureAction]] FutureActions CAUTION: FIXME","title":"[source,scala]"},{"location":"rdd/spark-rdd-caching/","text":"== RDD Caching and Persistence Caching or persistence are optimisation techniques for (iterative and interactive) Spark computations. They help saving interim partial results so they can be reused in subsequent stages. These interim results as RDDs are thus kept in memory (default) or more solid storages like disk and/or replicated. RDDs can be cached using < > operation. They can also be persisted using < > operation. The difference between cache and persist operations is purely syntactic. cache is a synonym of persist or persist(MEMORY_ONLY) , i.e. cache is merely persist with the default storage level MEMORY_ONLY . NOTE: Due to the very small and purely syntactic difference between caching and persistence of RDDs the two terms are often used interchangeably and I will follow the \"pattern\" here. RDDs can also be < > to remove RDD from a permanent storage like memory and/or disk. === [[cache]] Caching RDD -- cache Method [source, scala] \u00b6 cache(): this.type = persist() \u00b6 cache is a synonym of < > with xref:storage:StorageLevel.adoc[ MEMORY_ONLY storage level]. === [[persist]] Persisting RDD -- persist Methods [source, scala] \u00b6 persist(): this.type persist(newLevel: StorageLevel): this.type persist marks a RDD for persistence using newLevel xref:storage:StorageLevel.adoc[storage level]. You can only change the storage level once or persist reports an UnsupportedOperationException : Cannot change storage level of an RDD after it was already assigned a level NOTE: You can pretend to change the storage level of an RDD with already-assigned storage level only if the storage level is the same as it is currently assigned. If the RDD is marked as persistent the first time, the RDD is xref:core:ContextCleaner.adoc#registerRDDForCleanup[registered to ContextCleaner ] (if available) and xref:ROOT:SparkContext.adoc#persistRDD[ SparkContext ]. The internal storageLevel attribute is set to the input newLevel storage level. === [[unpersist]] Unpersisting RDDs (Clearing Blocks) -- unpersist Method [source, scala] \u00b6 unpersist(blocking: Boolean = true): this.type \u00b6 When called, unpersist prints the following INFO message to the logs: INFO [RddName]: Removing RDD [id] from persistence list It then calls xref:ROOT:SparkContext.adoc#unpersist[SparkContext.unpersistRDD(id, blocking)] and sets xref:storage:StorageLevel.adoc[ NONE storage level] as the current storage level.","title":"Caching and Persistence"},{"location":"rdd/spark-rdd-caching/#source-scala","text":"","title":"[source, scala]"},{"location":"rdd/spark-rdd-caching/#cache-thistype-persist","text":"cache is a synonym of < > with xref:storage:StorageLevel.adoc[ MEMORY_ONLY storage level]. === [[persist]] Persisting RDD -- persist Methods","title":"cache(): this.type = persist()"},{"location":"rdd/spark-rdd-caching/#source-scala_1","text":"persist(): this.type persist(newLevel: StorageLevel): this.type persist marks a RDD for persistence using newLevel xref:storage:StorageLevel.adoc[storage level]. You can only change the storage level once or persist reports an UnsupportedOperationException : Cannot change storage level of an RDD after it was already assigned a level NOTE: You can pretend to change the storage level of an RDD with already-assigned storage level only if the storage level is the same as it is currently assigned. If the RDD is marked as persistent the first time, the RDD is xref:core:ContextCleaner.adoc#registerRDDForCleanup[registered to ContextCleaner ] (if available) and xref:ROOT:SparkContext.adoc#persistRDD[ SparkContext ]. The internal storageLevel attribute is set to the input newLevel storage level. === [[unpersist]] Unpersisting RDDs (Clearing Blocks) -- unpersist Method","title":"[source, scala]"},{"location":"rdd/spark-rdd-caching/#source-scala_2","text":"","title":"[source, scala]"},{"location":"rdd/spark-rdd-caching/#unpersistblocking-boolean-true-thistype","text":"When called, unpersist prints the following INFO message to the logs: INFO [RddName]: Removing RDD [id] from persistence list It then calls xref:ROOT:SparkContext.adoc#unpersist[SparkContext.unpersistRDD(id, blocking)] and sets xref:storage:StorageLevel.adoc[ NONE storage level] as the current storage level.","title":"unpersist(blocking: Boolean = true): this.type"},{"location":"rdd/spark-rdd-lineage/","text":"== RDD Lineage -- Logical Execution Plan RDD Lineage (aka RDD operator graph or RDD dependency graph ) is a graph of all the parent RDDs of a RDD. It is built as a result of applying transformations to the RDD and creates a < >. NOTE: The execution DAG or physical execution plan is the xref:scheduler:DAGScheduler.adoc[DAG of stages]. NOTE: The following diagram uses cartesian or zip for learning purposes only. You may use other operators to build a RDD graph. .RDD lineage image::rdd-lineage.png[align=\"center\"] The above RDD graph could be the result of the following series of transformations: val r00 = sc.parallelize(0 to 9) val r01 = sc.parallelize(0 to 90 by 10) val r10 = r00 cartesian r01 val r11 = r00.map(n => (n, n)) val r12 = r00 zip r01 val r13 = r01.keyBy(_ / 20) val r20 = Seq(r11, r12, r13).foldLeft(r10)(_ union _) A RDD lineage graph is hence a graph of what transformations need to be executed after an action has been called. You can learn about a RDD lineage graph using < > method. === [[logical-execution-plan]] Logical Execution Plan Logical Execution Plan starts with the earliest RDDs (those with no dependencies on other RDDs or reference cached data) and ends with the RDD that produces the result of the action that has been called to execute. NOTE: A logical plan, i.e. a DAG, is materialized and executed when xref:ROOT:SparkContext.adoc#runJob[ SparkContext is requested to run a Spark job]. === [[toDebugString]] Getting RDD Lineage Graph -- toDebugString Method [source, scala] \u00b6 toDebugString: String \u00b6 You can learn about a < > using toDebugString method. scala> val wordCount = sc.textFile(\"README.md\").flatMap(_.split(\"\\\\s+\")).map((_, 1)).reduceByKey(_ + _) wordCount: org.apache.spark.rdd.RDD[(String, Int)] = ShuffledRDD[21] at reduceByKey at <console>:24 scala> wordCount.toDebugString res13: String = (2) ShuffledRDD[21] at reduceByKey at <console>:24 [] +-(2) MapPartitionsRDD[20] at map at <console>:24 [] | MapPartitionsRDD[19] at flatMap at <console>:24 [] | README.md MapPartitionsRDD[18] at textFile at <console>:24 [] | README.md HadoopRDD[17] at textFile at <console>:24 [] toDebugString uses indentations to indicate a shuffle boundary. The numbers in round brackets show the level of parallelism at each stage, e.g. (2) in the above output. scala> wordCount.getNumPartitions res14: Int = 2 With < > property enabled, toDebugString is included when executing an action. $ ./bin/spark-shell --conf spark.logLineage=true scala> sc.textFile(\"README.md\", 4).count ... 15/10/17 14:46:42 INFO SparkContext: Starting job: count at <console>:25 15/10/17 14:46:42 INFO SparkContext: RDD's recursive dependencies: (4) MapPartitionsRDD[1] at textFile at <console>:25 [] | README.md HadoopRDD[0] at textFile at <console>:25 [] === [[settings]] Settings .Spark Properties [cols=\"1,1,2\",options=\"header\",width=\"100%\"] |=== | Spark Property | Default Value | Description | [[spark_logLineage]] spark.logLineage | false | When enabled (i.e. true ), executing an action (and hence xref:ROOT:SparkContext.adoc#runJob[running a job]) will also print out the RDD lineage graph using < >. |===","title":"RDD Lineage"},{"location":"rdd/spark-rdd-lineage/#source-scala","text":"","title":"[source, scala]"},{"location":"rdd/spark-rdd-lineage/#todebugstring-string","text":"You can learn about a < > using toDebugString method. scala> val wordCount = sc.textFile(\"README.md\").flatMap(_.split(\"\\\\s+\")).map((_, 1)).reduceByKey(_ + _) wordCount: org.apache.spark.rdd.RDD[(String, Int)] = ShuffledRDD[21] at reduceByKey at <console>:24 scala> wordCount.toDebugString res13: String = (2) ShuffledRDD[21] at reduceByKey at <console>:24 [] +-(2) MapPartitionsRDD[20] at map at <console>:24 [] | MapPartitionsRDD[19] at flatMap at <console>:24 [] | README.md MapPartitionsRDD[18] at textFile at <console>:24 [] | README.md HadoopRDD[17] at textFile at <console>:24 [] toDebugString uses indentations to indicate a shuffle boundary. The numbers in round brackets show the level of parallelism at each stage, e.g. (2) in the above output. scala> wordCount.getNumPartitions res14: Int = 2 With < > property enabled, toDebugString is included when executing an action. $ ./bin/spark-shell --conf spark.logLineage=true scala> sc.textFile(\"README.md\", 4).count ... 15/10/17 14:46:42 INFO SparkContext: Starting job: count at <console>:25 15/10/17 14:46:42 INFO SparkContext: RDD's recursive dependencies: (4) MapPartitionsRDD[1] at textFile at <console>:25 [] | README.md HadoopRDD[0] at textFile at <console>:25 [] === [[settings]] Settings .Spark Properties [cols=\"1,1,2\",options=\"header\",width=\"100%\"] |=== | Spark Property | Default Value | Description | [[spark_logLineage]] spark.logLineage | false | When enabled (i.e. true ), executing an action (and hence xref:ROOT:SparkContext.adoc#runJob[running a job]) will also print out the RDD lineage graph using < >. |===","title":"toDebugString: String"},{"location":"rdd/spark-rdd-operations/","text":"== Operators - Transformations and Actions RDDs have two types of operations: link:spark-rdd-transformations.adoc[transformations] and link:spark-rdd-actions.adoc[actions]. NOTE: Operators are also called operations . === Gotchas - things to watch for Even if you don't access it explicitly it cannot be referenced inside a closure as it is serialized and carried around across executors. See https://issues.apache.org/jira/browse/SPARK-5063","title":"Operators"},{"location":"rdd/spark-rdd-partitions/","text":"== Partitions and Partitioning === Introduction Depending on how you look at Spark (programmer, devop, admin), an RDD is about the content (developer's and data scientist's perspective) or how it gets spread out over a cluster (performance), i.e. how many partitions an RDD represents. A partition (aka split ) is a logical chunk of a large distributed data set. [CAUTION] \u00b6 FIXME How does the number of partitions map to the number of tasks? How to verify it? How does the mapping between partitions and tasks correspond to data locality if any? \u00b6 Spark manages data using partitions that helps parallelize distributed data processing with minimal network traffic for sending data between executors. By default, Spark tries to read data into an RDD from the nodes that are close to it. Since Spark usually accesses distributed partitioned data, to optimize transformation operations it creates partitions to hold the data chunks. There is a one-to-one correspondence between how data is laid out in data storage like HDFS or Cassandra (it is partitioned for the same reasons). Features: size number partitioning scheme node distribution repartitioning [TIP] \u00b6 Read the following documentations to learn what experts say on the topic: https://databricks.gitbooks.io/databricks-spark-knowledge-base/content/performance_optimization/how_many_partitions_does_an_rdd_have.html[How Many Partitions Does An RDD Have?] https://spark.apache.org/docs/latest/tuning.html[Tuning Spark] (the official documentation of Spark) \u00b6 By default, a partition is created for each HDFS partition, which by default is 64MB (from http://spark.apache.org/docs/latest/programming-guide.html#external-datasets[Spark's Programming Guide]). RDDs get partitioned automatically without programmer intervention. However, there are times when you'd like to adjust the size and number of partitions or the partitioning scheme according to the needs of your application. You use def getPartitions: Array[Partition] method on a RDD to know the set of partitions in this RDD. As noted in https://github.com/databricks/spark-knowledgebase/blob/master/performance_optimization/how_many_partitions_does_an_rdd_have.md#view-task-execution-against-partitions-using-the-ui[View Task Execution Against Partitions Using the UI]: When a stage executes, you can see the number of partitions for a given stage in the Spark UI. Start spark-shell and see it yourself! scala> sc.parallelize(1 to 100).count res0: Long = 100 When you execute the Spark job, i.e. sc.parallelize(1 to 100).count , you should see the following in http://localhost:4040/jobs[Spark shell application UI]. .The number of partition as Total tasks in UI image::spark-partitions-ui-stages.png[align=\"center\"] The reason for 8 Tasks in Total is that I'm on a 8-core laptop and by default the number of partitions is the number of all available cores. $ sysctl -n hw.ncpu 8 You can request for the minimum number of partitions, using the second input parameter to many transformations. scala> sc.parallelize(1 to 100, 2).count res1: Long = 100 .Total tasks in UI shows 2 partitions image::spark-partitions-ui-stages-2-partitions.png[align=\"center\"] You can always ask for the number of partitions using partitions method of a RDD: scala> val ints = sc.parallelize(1 to 100, 4) ints: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[1] at parallelize at <console>:24 scala> ints.partitions.size res2: Int = 4 In general, smaller/more numerous partitions allow work to be distributed among more workers, but larger/fewer partitions allow work to be done in larger chunks, which may result in the work getting done more quickly as long as all workers are kept busy, due to reduced overhead. Increasing partitions count will make each partition to have less data (or not at all!) Spark can only run 1 concurrent task for every partition of an RDD, up to the number of cores in your cluster. So if you have a cluster with 50 cores, you want your RDDs to at least have 50 partitions (and probably http://spark.apache.org/docs/latest/tuning.html#level-of-parallelism[2-3x times that]). As far as choosing a \"good\" number of partitions, you generally want at least as many as the number of executors for parallelism. You can get this computed value by calling sc.defaultParallelism . Also, the number of partitions determines how many files get generated by actions that save RDDs to files. The maximum size of a partition is ultimately limited by the available memory of an executor. In the first RDD transformation, e.g. reading from a file using sc.textFile(path, partition) , the partition parameter will be applied to all further transformations and actions on this RDD. Partitions get redistributed among nodes whenever shuffle occurs. Repartitioning may cause shuffle to occur in some situations, but it is not guaranteed to occur in all cases. And it usually happens during action stage. When creating an RDD by reading a file using rdd = SparkContext().textFile(\"hdfs://.../file.txt\") the number of partitions may be smaller. Ideally, you would get the same number of blocks as you see in HDFS, but if the lines in your file are too long (longer than the block size), there will be fewer partitions. Preferred way to set up the number of partitions for an RDD is to directly pass it as the second input parameter in the call like rdd = sc.textFile(\"hdfs://.../file.txt\", 400) , where 400 is the number of partitions. In this case, the partitioning makes for 400 splits that would be done by the Hadoop's TextInputFormat , not Spark and it would work much faster. It's also that the code spawns 400 concurrent tasks to try to load file.txt directly into 400 partitions. It will only work as described for uncompressed files. When using textFile with compressed files ( file.txt.gz not file.txt or similar), Spark disables splitting that makes for an RDD with only 1 partition (as reads against gzipped files cannot be parallelized). In this case, to change the number of partitions you should do < >. Some operations, e.g. map , flatMap , filter , don't preserve partitioning. map , flatMap , filter operations apply a function to every partition. === [[repartitioning]][[repartition]] Repartitioning RDD -- repartition Transformation [source, scala] \u00b6 repartition(numPartitions: Int)(implicit ord: Ordering[T] = null): RDD[T] \u00b6 repartition is < > with numPartitions and shuffle enabled. With the following computation you can see that repartition(5) causes 5 tasks to be started using NODE_LOCAL link:spark-data-locality.adoc[data locality]. scala> lines.repartition(5).count ... 15/10/07 08:10:00 INFO DAGScheduler: Submitting 5 missing tasks from ResultStage 7 (MapPartitionsRDD[19] at repartition at <console>:27) 15/10/07 08:10:00 INFO TaskSchedulerImpl: Adding task set 7.0 with 5 tasks 15/10/07 08:10:00 INFO TaskSetManager: Starting task 0.0 in stage 7.0 (TID 17, localhost, partition 0,NODE_LOCAL, 2089 bytes) 15/10/07 08:10:00 INFO TaskSetManager: Starting task 1.0 in stage 7.0 (TID 18, localhost, partition 1,NODE_LOCAL, 2089 bytes) 15/10/07 08:10:00 INFO TaskSetManager: Starting task 2.0 in stage 7.0 (TID 19, localhost, partition 2,NODE_LOCAL, 2089 bytes) 15/10/07 08:10:00 INFO TaskSetManager: Starting task 3.0 in stage 7.0 (TID 20, localhost, partition 3,NODE_LOCAL, 2089 bytes) 15/10/07 08:10:00 INFO TaskSetManager: Starting task 4.0 in stage 7.0 (TID 21, localhost, partition 4,NODE_LOCAL, 2089 bytes) ... You can see a change after executing repartition(1) causes 2 tasks to be started using PROCESS_LOCAL link:spark-data-locality.adoc[data locality]. scala> lines.repartition(1).count ... 15/10/07 08:14:09 INFO DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 8 (MapPartitionsRDD[20] at repartition at <console>:27) 15/10/07 08:14:09 INFO TaskSchedulerImpl: Adding task set 8.0 with 2 tasks 15/10/07 08:14:09 INFO TaskSetManager: Starting task 0.0 in stage 8.0 (TID 22, localhost, partition 0,PROCESS_LOCAL, 2058 bytes) 15/10/07 08:14:09 INFO TaskSetManager: Starting task 1.0 in stage 8.0 (TID 23, localhost, partition 1,PROCESS_LOCAL, 2058 bytes) ... Please note that Spark disables splitting for compressed files and creates RDDs with only 1 partition. In such cases, it's helpful to use sc.textFile('demo.gz') and do repartitioning using rdd.repartition(100) as follows: rdd = sc.textFile('demo.gz') rdd = rdd.repartition(100) With the lines, you end up with rdd to be exactly 100 partitions of roughly equal in size. rdd.repartition(N) does a shuffle to split data to match N ** partitioning is done on round robin basis TIP: If partitioning scheme doesn't work for you, you can write your own custom partitioner. TIP: It's useful to get familiar with https://hadoop.apache.org/docs/current/api/org/apache/hadoop/mapred/TextInputFormat.html[Hadoop's TextInputFormat]. === [[coalesce]] coalesce Transformation [source, scala] \u00b6 coalesce(numPartitions: Int, shuffle: Boolean = false)(implicit ord: Ordering[T] = null): RDD[T] \u00b6 The coalesce transformation is used to change the number of partitions. It can trigger link:spark-rdd-shuffle.adoc[RDD shuffling] depending on the shuffle flag (disabled by default, i.e. false ). In the following sample, you parallelize a local 10-number sequence and coalesce it first without and then with shuffling (note the shuffle parameter being false and true , respectively). TIP: Use link:spark-rdd-lineage.adoc#toDebugString[toDebugString] to check out the link:spark-rdd-lineage.adoc[RDD lineage graph]. scala> val rdd = sc.parallelize(0 to 10, 8) rdd: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[0] at parallelize at <console>:24 scala> rdd.partitions.size res0: Int = 8 scala> rdd.coalesce(numPartitions=8, shuffle=false) // <1> res1: org.apache.spark.rdd.RDD[Int] = CoalescedRDD[1] at coalesce at <console>:27 scala> res1.toDebugString res2: String = (8) CoalescedRDD[1] at coalesce at <console>:27 [] | ParallelCollectionRDD[0] at parallelize at <console>:24 [] scala> rdd.coalesce(numPartitions=8, shuffle=true) res3: org.apache.spark.rdd.RDD[Int] = MapPartitionsRDD[5] at coalesce at <console>:27 scala> res3.toDebugString res4: String = (8) MapPartitionsRDD[5] at coalesce at <console>:27 [] | CoalescedRDD[4] at coalesce at <console>:27 [] | ShuffledRDD[3] at coalesce at <console>:27 [] +-(8) MapPartitionsRDD[2] at coalesce at <console>:27 [] | ParallelCollectionRDD[0] at parallelize at <console>:24 [] <1> shuffle is false by default and it's explicitly used here for demo purposes. Note the number of partitions that remains the same as the number of partitions in the source RDD rdd .","title":"Partitions and Partitioning"},{"location":"rdd/spark-rdd-partitions/#caution","text":"FIXME How does the number of partitions map to the number of tasks? How to verify it?","title":"[CAUTION]"},{"location":"rdd/spark-rdd-partitions/#how-does-the-mapping-between-partitions-and-tasks-correspond-to-data-locality-if-any","text":"Spark manages data using partitions that helps parallelize distributed data processing with minimal network traffic for sending data between executors. By default, Spark tries to read data into an RDD from the nodes that are close to it. Since Spark usually accesses distributed partitioned data, to optimize transformation operations it creates partitions to hold the data chunks. There is a one-to-one correspondence between how data is laid out in data storage like HDFS or Cassandra (it is partitioned for the same reasons). Features: size number partitioning scheme node distribution repartitioning","title":"How does the mapping between partitions and tasks correspond to data locality if any?"},{"location":"rdd/spark-rdd-partitions/#tip","text":"Read the following documentations to learn what experts say on the topic: https://databricks.gitbooks.io/databricks-spark-knowledge-base/content/performance_optimization/how_many_partitions_does_an_rdd_have.html[How Many Partitions Does An RDD Have?]","title":"[TIP]"},{"location":"rdd/spark-rdd-partitions/#httpssparkapacheorgdocslatesttuninghtmltuning-spark-the-official-documentation-of-spark","text":"By default, a partition is created for each HDFS partition, which by default is 64MB (from http://spark.apache.org/docs/latest/programming-guide.html#external-datasets[Spark's Programming Guide]). RDDs get partitioned automatically without programmer intervention. However, there are times when you'd like to adjust the size and number of partitions or the partitioning scheme according to the needs of your application. You use def getPartitions: Array[Partition] method on a RDD to know the set of partitions in this RDD. As noted in https://github.com/databricks/spark-knowledgebase/blob/master/performance_optimization/how_many_partitions_does_an_rdd_have.md#view-task-execution-against-partitions-using-the-ui[View Task Execution Against Partitions Using the UI]: When a stage executes, you can see the number of partitions for a given stage in the Spark UI. Start spark-shell and see it yourself! scala> sc.parallelize(1 to 100).count res0: Long = 100 When you execute the Spark job, i.e. sc.parallelize(1 to 100).count , you should see the following in http://localhost:4040/jobs[Spark shell application UI]. .The number of partition as Total tasks in UI image::spark-partitions-ui-stages.png[align=\"center\"] The reason for 8 Tasks in Total is that I'm on a 8-core laptop and by default the number of partitions is the number of all available cores. $ sysctl -n hw.ncpu 8 You can request for the minimum number of partitions, using the second input parameter to many transformations. scala> sc.parallelize(1 to 100, 2).count res1: Long = 100 .Total tasks in UI shows 2 partitions image::spark-partitions-ui-stages-2-partitions.png[align=\"center\"] You can always ask for the number of partitions using partitions method of a RDD: scala> val ints = sc.parallelize(1 to 100, 4) ints: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[1] at parallelize at <console>:24 scala> ints.partitions.size res2: Int = 4 In general, smaller/more numerous partitions allow work to be distributed among more workers, but larger/fewer partitions allow work to be done in larger chunks, which may result in the work getting done more quickly as long as all workers are kept busy, due to reduced overhead. Increasing partitions count will make each partition to have less data (or not at all!) Spark can only run 1 concurrent task for every partition of an RDD, up to the number of cores in your cluster. So if you have a cluster with 50 cores, you want your RDDs to at least have 50 partitions (and probably http://spark.apache.org/docs/latest/tuning.html#level-of-parallelism[2-3x times that]). As far as choosing a \"good\" number of partitions, you generally want at least as many as the number of executors for parallelism. You can get this computed value by calling sc.defaultParallelism . Also, the number of partitions determines how many files get generated by actions that save RDDs to files. The maximum size of a partition is ultimately limited by the available memory of an executor. In the first RDD transformation, e.g. reading from a file using sc.textFile(path, partition) , the partition parameter will be applied to all further transformations and actions on this RDD. Partitions get redistributed among nodes whenever shuffle occurs. Repartitioning may cause shuffle to occur in some situations, but it is not guaranteed to occur in all cases. And it usually happens during action stage. When creating an RDD by reading a file using rdd = SparkContext().textFile(\"hdfs://.../file.txt\") the number of partitions may be smaller. Ideally, you would get the same number of blocks as you see in HDFS, but if the lines in your file are too long (longer than the block size), there will be fewer partitions. Preferred way to set up the number of partitions for an RDD is to directly pass it as the second input parameter in the call like rdd = sc.textFile(\"hdfs://.../file.txt\", 400) , where 400 is the number of partitions. In this case, the partitioning makes for 400 splits that would be done by the Hadoop's TextInputFormat , not Spark and it would work much faster. It's also that the code spawns 400 concurrent tasks to try to load file.txt directly into 400 partitions. It will only work as described for uncompressed files. When using textFile with compressed files ( file.txt.gz not file.txt or similar), Spark disables splitting that makes for an RDD with only 1 partition (as reads against gzipped files cannot be parallelized). In this case, to change the number of partitions you should do < >. Some operations, e.g. map , flatMap , filter , don't preserve partitioning. map , flatMap , filter operations apply a function to every partition. === [[repartitioning]][[repartition]] Repartitioning RDD -- repartition Transformation","title":"https://spark.apache.org/docs/latest/tuning.html[Tuning Spark] (the official documentation of Spark)"},{"location":"rdd/spark-rdd-partitions/#source-scala","text":"","title":"[source, scala]"},{"location":"rdd/spark-rdd-partitions/#repartitionnumpartitions-intimplicit-ord-orderingt-null-rddt","text":"repartition is < > with numPartitions and shuffle enabled. With the following computation you can see that repartition(5) causes 5 tasks to be started using NODE_LOCAL link:spark-data-locality.adoc[data locality]. scala> lines.repartition(5).count ... 15/10/07 08:10:00 INFO DAGScheduler: Submitting 5 missing tasks from ResultStage 7 (MapPartitionsRDD[19] at repartition at <console>:27) 15/10/07 08:10:00 INFO TaskSchedulerImpl: Adding task set 7.0 with 5 tasks 15/10/07 08:10:00 INFO TaskSetManager: Starting task 0.0 in stage 7.0 (TID 17, localhost, partition 0,NODE_LOCAL, 2089 bytes) 15/10/07 08:10:00 INFO TaskSetManager: Starting task 1.0 in stage 7.0 (TID 18, localhost, partition 1,NODE_LOCAL, 2089 bytes) 15/10/07 08:10:00 INFO TaskSetManager: Starting task 2.0 in stage 7.0 (TID 19, localhost, partition 2,NODE_LOCAL, 2089 bytes) 15/10/07 08:10:00 INFO TaskSetManager: Starting task 3.0 in stage 7.0 (TID 20, localhost, partition 3,NODE_LOCAL, 2089 bytes) 15/10/07 08:10:00 INFO TaskSetManager: Starting task 4.0 in stage 7.0 (TID 21, localhost, partition 4,NODE_LOCAL, 2089 bytes) ... You can see a change after executing repartition(1) causes 2 tasks to be started using PROCESS_LOCAL link:spark-data-locality.adoc[data locality]. scala> lines.repartition(1).count ... 15/10/07 08:14:09 INFO DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 8 (MapPartitionsRDD[20] at repartition at <console>:27) 15/10/07 08:14:09 INFO TaskSchedulerImpl: Adding task set 8.0 with 2 tasks 15/10/07 08:14:09 INFO TaskSetManager: Starting task 0.0 in stage 8.0 (TID 22, localhost, partition 0,PROCESS_LOCAL, 2058 bytes) 15/10/07 08:14:09 INFO TaskSetManager: Starting task 1.0 in stage 8.0 (TID 23, localhost, partition 1,PROCESS_LOCAL, 2058 bytes) ... Please note that Spark disables splitting for compressed files and creates RDDs with only 1 partition. In such cases, it's helpful to use sc.textFile('demo.gz') and do repartitioning using rdd.repartition(100) as follows: rdd = sc.textFile('demo.gz') rdd = rdd.repartition(100) With the lines, you end up with rdd to be exactly 100 partitions of roughly equal in size. rdd.repartition(N) does a shuffle to split data to match N ** partitioning is done on round robin basis TIP: If partitioning scheme doesn't work for you, you can write your own custom partitioner. TIP: It's useful to get familiar with https://hadoop.apache.org/docs/current/api/org/apache/hadoop/mapred/TextInputFormat.html[Hadoop's TextInputFormat]. === [[coalesce]] coalesce Transformation","title":"repartition(numPartitions: Int)(implicit ord: Ordering[T] = null): RDD[T]"},{"location":"rdd/spark-rdd-partitions/#source-scala_1","text":"","title":"[source, scala]"},{"location":"rdd/spark-rdd-partitions/#coalescenumpartitions-int-shuffle-boolean-falseimplicit-ord-orderingt-null-rddt","text":"The coalesce transformation is used to change the number of partitions. It can trigger link:spark-rdd-shuffle.adoc[RDD shuffling] depending on the shuffle flag (disabled by default, i.e. false ). In the following sample, you parallelize a local 10-number sequence and coalesce it first without and then with shuffling (note the shuffle parameter being false and true , respectively). TIP: Use link:spark-rdd-lineage.adoc#toDebugString[toDebugString] to check out the link:spark-rdd-lineage.adoc[RDD lineage graph]. scala> val rdd = sc.parallelize(0 to 10, 8) rdd: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[0] at parallelize at <console>:24 scala> rdd.partitions.size res0: Int = 8 scala> rdd.coalesce(numPartitions=8, shuffle=false) // <1> res1: org.apache.spark.rdd.RDD[Int] = CoalescedRDD[1] at coalesce at <console>:27 scala> res1.toDebugString res2: String = (8) CoalescedRDD[1] at coalesce at <console>:27 [] | ParallelCollectionRDD[0] at parallelize at <console>:24 [] scala> rdd.coalesce(numPartitions=8, shuffle=true) res3: org.apache.spark.rdd.RDD[Int] = MapPartitionsRDD[5] at coalesce at <console>:27 scala> res3.toDebugString res4: String = (8) MapPartitionsRDD[5] at coalesce at <console>:27 [] | CoalescedRDD[4] at coalesce at <console>:27 [] | ShuffledRDD[3] at coalesce at <console>:27 [] +-(8) MapPartitionsRDD[2] at coalesce at <console>:27 [] | ParallelCollectionRDD[0] at parallelize at <console>:24 [] <1> shuffle is false by default and it's explicitly used here for demo purposes. Note the number of partitions that remains the same as the number of partitions in the source RDD rdd .","title":"coalesce(numPartitions: Int, shuffle: Boolean = false)(implicit ord: Ordering[T] = null): RDD[T]"},{"location":"rdd/spark-rdd-shuffle/","text":"= RDD shuffling :url-spark-docs: https://spark.apache.org/docs/{spark-version } TIP: Read the official documentation about the topic {url-spark-docs}/rdd-programming-guide.html#shuffle-operations[Shuffle operations]. It is still better than this page. Shuffling is a process of link:spark-rdd-partitions.adoc[redistributing data across partitions] (aka repartitioning ) that may or may not cause moving data across JVM processes or even over the wire (between executors on separate machines). Shuffling is the process of data transfer between stages. TIP: Avoid shuffling at all cost. Think about ways to leverage existing partitions. Leverage partial aggregation to reduce data transfer. By default, shuffling doesn't change the number of partitions, but their content. Avoid groupByKey and use reduceByKey or combineByKey instead. ** groupByKey shuffles all the data, which is slow. ** reduceByKey shuffles only the results of sub-aggregations in each partition of the data. == Example - join PairRDD offers http://people.apache.org/~pwendell/spark-nightly/spark-master-docs/latest/programming-guide.html#JoinLink[join ] transformation that (quoting the official documentation): When called on datasets of type (K, V) and (K, W), returns a dataset of (K, (V, W)) pairs with all pairs of elements for each key. Let's have a look at an example and see how it works under the covers: scala> val kv = (0 to 5) zip Stream.continually(5) kv: scala.collection.immutable.IndexedSeq[(Int, Int)] = Vector((0,5), (1,5), (2,5), (3,5), (4,5), (5,5)) scala> val kw = (0 to 5) zip Stream.continually(10) kw: scala.collection.immutable.IndexedSeq[(Int, Int)] = Vector((0,10), (1,10), (2,10), (3,10), (4,10), (5,10)) scala> val kvR = sc.parallelize(kv) kvR: org.apache.spark.rdd.RDD[(Int, Int)] = ParallelCollectionRDD[3] at parallelize at <console>:26 scala> val kwR = sc.parallelize(kw) kwR: org.apache.spark.rdd.RDD[(Int, Int)] = ParallelCollectionRDD[4] at parallelize at <console>:26 scala> val joined = kvR join kwR joined: org.apache.spark.rdd.RDD[(Int, (Int, Int))] = MapPartitionsRDD[10] at join at <console>:32 scala> joined.toDebugString res7: String = (8) MapPartitionsRDD[10] at join at <console>:32 [] | MapPartitionsRDD[9] at join at <console>:32 [] | CoGroupedRDD[8] at join at <console>:32 [] +-(8) ParallelCollectionRDD[3] at parallelize at <console>:26 [] +-(8) ParallelCollectionRDD[4] at parallelize at <console>:26 [] It doesn't look good when there is an \"angle\" between \"nodes\" in an operation graph. It appears before the join operation so shuffle is expected. Here is how the job of executing joined.count looks in Web UI. .Executing joined.count image::spark-shuffle-join-webui.png[align=\"center\"] The screenshot of Web UI shows 3 stages with two parallelize to Shuffle Write and count to Shuffle Read. It means shuffling has indeed happened. CAUTION: FIXME Just learnt about sc.range(0, 5) as a shorter version of sc.parallelize(0 to 5) join operation is one of the cogroup operations that uses defaultPartitioner , i.e. walks through link:spark-rdd-lineage.adoc[the RDD lineage graph] (sorted by the number of partitions decreasing) and picks the partitioner with positive number of output partitions. Otherwise, it checks xref:ROOT:configuration-properties.adoc#spark.default.parallelism[spark.default.parallelism] configuration and if defined picks xref:rdd:HashPartitioner.adoc[HashPartitioner] with the default parallelism of the xref:scheduler:SchedulerBackend.adoc[SchedulerBackend]. join is almost CoGroupedRDD.mapValues . CAUTION: FIXME the default parallelism of scheduler backend","title":"Shuffling"},{"location":"rdd/spark-rdd-transformations/","text":"== Transformations -- Lazy Operations on RDD (to Create One or More RDDs) Transformations are lazy operations on an xref:rdd:RDD.adoc[RDD] that create one or many new RDDs. // T and U are Scala types transformation: RDD[T] => RDD[U] transformation: RDD[T] => Seq[RDD[U]] In other words, transformations are functions that take an RDD as the input and produce one or many RDDs as the output. Transformations do not change the input RDD (since xref:rdd:index.adoc#introduction[RDDs are immutable] and hence cannot be modified), but produce one or more new RDDs by applying the computations they represent. [[methods]] .(Subset of) RDD Transformations (Public API) [cols=\"1m,3\",options=\"header\",width=\"100%\"] |=== | Method | Description | aggregate a| [[aggregate]] [source, scala] \u00b6 aggregate U ( seqOp: (U, T) => U, combOp: (U, U) => U): U | barrier a| [[barrier]] [source, scala] \u00b6 barrier(): RDDBarrier[T] \u00b6 ( New in 2.4.0 ) Marks the current stage as a < > in < >, where Spark must launch all tasks together Internally, barrier creates a < > over the RDD | cache a| [[cache]] [source, scala] \u00b6 cache(): this.type \u00b6 Persists the RDD with the xref:storage:StorageLevel.adoc#MEMORY_ONLY[MEMORY_ONLY] storage level Synonym of < > | coalesce a| [[coalesce]] [source, scala] \u00b6 coalesce( numPartitions: Int, shuffle: Boolean = false, partitionCoalescer: Option[PartitionCoalescer] = Option.empty) (implicit ord: Ordering[T] = null): RDD[T] | filter a| [[filter]] [source, scala] \u00b6 filter(f: T => Boolean): RDD[T] \u00b6 | flatMap a| [[flatMap]] [source, scala] \u00b6 flatMap U : RDD[U] \u00b6 | map a| [[map]] [source, scala] \u00b6 map U : RDD[U] \u00b6 | mapPartitions a| [[mapPartitions]] [source, scala] \u00b6 mapPartitions U : RDD[U] | mapPartitionsWithIndex a| [[mapPartitionsWithIndex]] [source, scala] \u00b6 mapPartitionsWithIndex U : RDD[U] | randomSplit a| [[randomSplit]] [source, scala] \u00b6 randomSplit( weights: Array[Double], seed: Long = Utils.random.nextLong): Array[RDD[T]] | union a| [[union]] [source, scala] \u00b6 ++(other: RDD[T]): RDD[T] union(other: RDD[T]): RDD[T] | persist a| [[persist]] [source, scala] \u00b6 persist(): this.type persist(newLevel: StorageLevel): this.type |=== By applying transformations you incrementally build a link:spark-rdd-lineage.adoc[RDD lineage] with all the parent RDDs of the final RDD(s). Transformations are lazy, i.e. are not executed immediately. Only after calling an action are transformations executed. After executing a transformation, the result RDD(s) will always be different from their parents and can be smaller (e.g. filter , count , distinct , sample ), bigger (e.g. flatMap , union , cartesian ) or the same size (e.g. map ). CAUTION: There are transformations that may trigger jobs, e.g. sortBy , < >, etc. .From SparkContext by transformations to the result image::rdd-sparkcontext-transformations-action.png[align=\"center\"] Certain transformations can be pipelined which is an optimization that Spark uses to improve performance of computations. [source,scala] \u00b6 scala> val file = sc.textFile(\"README.md\") file: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[54] at textFile at :24 scala> val allWords = file.flatMap(_.split(\"\\W+\")) allWords: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[55] at flatMap at :26 scala> val words = allWords.filter(!_.isEmpty) words: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[56] at filter at :28 scala> val pairs = words.map((_,1)) pairs: org.apache.spark.rdd.RDD[(String, Int)] = MapPartitionsRDD[57] at map at :30 scala> val reducedByKey = pairs.reduceByKey(_ + _) reducedByKey: org.apache.spark.rdd.RDD[(String, Int)] = ShuffledRDD[59] at reduceByKey at :32 scala> val top10words = reducedByKey.takeOrdered(10)(Ordering[Int].reverse.on(_._2)) INFO SparkContext: Starting job: takeOrdered at :34 ... INFO DAGScheduler: Job 18 finished: takeOrdered at :34, took 0.074386 s top10words: Array[(String, Int)] = Array((the,21), (to,14), (Spark,13), (for,11), (and,10), (##,8), (a,8), (run,7), (can,6), (is,6)) There are two kinds of transformations: < > < > === [[narrow-transformations]] Narrow Transformations Narrow transformations are the result of map , filter and such that is from the data from a single partition only, i.e. it is self-sustained. An output RDD has partitions with records that originate from a single partition in the parent RDD. Only a limited subset of partitions used to calculate the result. Spark groups narrow transformations as a stage which is called pipelining . === [[wide-transformations]] Wide Transformations Wide transformations are the result of groupByKey and reduceByKey . The data required to compute the records in a single partition may reside in many partitions of the parent RDD. NOTE: Wide transformations are also called shuffle transformations as they may or may not depend on a shuffle. All of the tuples with the same key must end up in the same partition, processed by the same task. To satisfy these operations, Spark must execute link:spark-rdd-shuffle.adoc[RDD shuffle], which transfers data across cluster and results in a new stage with a new set of partitions. === [[zipWithIndex]] zipWithIndex [source, scala] \u00b6 zipWithIndex(): RDD[(T, Long)] \u00b6 zipWithIndex zips this RDD[T] with its element indices. [CAUTION] \u00b6 If the number of partitions of the source RDD is greater than 1, it will submit an additional job to calculate start indices. [source, scala] \u00b6 val onePartition = sc.parallelize(0 to 9, 1) scala> onePartition.partitions.length res0: Int = 1 // no job submitted onePartition.zipWithIndex val eightPartitions = sc.parallelize(0 to 9, 8) scala> eightPartitions.partitions.length res1: Int = 8 // submits a job eightPartitions.zipWithIndex .Spark job submitted by zipWithIndex transformation image::spark-transformations-zipWithIndex-webui.png[align=\"center\"] ====","title":"Transformations"},{"location":"rdd/spark-rdd-transformations/#source-scala","text":"aggregate U ( seqOp: (U, T) => U, combOp: (U, U) => U): U | barrier a| [[barrier]]","title":"[source, scala]"},{"location":"rdd/spark-rdd-transformations/#source-scala_1","text":"","title":"[source, scala]"},{"location":"rdd/spark-rdd-transformations/#barrier-rddbarriert","text":"( New in 2.4.0 ) Marks the current stage as a < > in < >, where Spark must launch all tasks together Internally, barrier creates a < > over the RDD | cache a| [[cache]]","title":"barrier(): RDDBarrier[T]"},{"location":"rdd/spark-rdd-transformations/#source-scala_2","text":"","title":"[source, scala]"},{"location":"rdd/spark-rdd-transformations/#cache-thistype","text":"Persists the RDD with the xref:storage:StorageLevel.adoc#MEMORY_ONLY[MEMORY_ONLY] storage level Synonym of < > | coalesce a| [[coalesce]]","title":"cache(): this.type"},{"location":"rdd/spark-rdd-transformations/#source-scala_3","text":"coalesce( numPartitions: Int, shuffle: Boolean = false, partitionCoalescer: Option[PartitionCoalescer] = Option.empty) (implicit ord: Ordering[T] = null): RDD[T] | filter a| [[filter]]","title":"[source, scala]"},{"location":"rdd/spark-rdd-transformations/#source-scala_4","text":"","title":"[source, scala]"},{"location":"rdd/spark-rdd-transformations/#filterf-t-boolean-rddt","text":"| flatMap a| [[flatMap]]","title":"filter(f: T =&gt; Boolean): RDD[T]"},{"location":"rdd/spark-rdd-transformations/#source-scala_5","text":"","title":"[source, scala]"},{"location":"rdd/spark-rdd-transformations/#flatmapu-rddu","text":"| map a| [[map]]","title":"flatMapU: RDD[U]"},{"location":"rdd/spark-rdd-transformations/#source-scala_6","text":"","title":"[source, scala]"},{"location":"rdd/spark-rdd-transformations/#mapu-rddu","text":"| mapPartitions a| [[mapPartitions]]","title":"mapU: RDD[U]"},{"location":"rdd/spark-rdd-transformations/#source-scala_7","text":"mapPartitions U : RDD[U] | mapPartitionsWithIndex a| [[mapPartitionsWithIndex]]","title":"[source, scala]"},{"location":"rdd/spark-rdd-transformations/#source-scala_8","text":"mapPartitionsWithIndex U : RDD[U] | randomSplit a| [[randomSplit]]","title":"[source, scala]"},{"location":"rdd/spark-rdd-transformations/#source-scala_9","text":"randomSplit( weights: Array[Double], seed: Long = Utils.random.nextLong): Array[RDD[T]] | union a| [[union]]","title":"[source, scala]"},{"location":"rdd/spark-rdd-transformations/#source-scala_10","text":"++(other: RDD[T]): RDD[T] union(other: RDD[T]): RDD[T] | persist a| [[persist]]","title":"[source, scala]"},{"location":"rdd/spark-rdd-transformations/#source-scala_11","text":"persist(): this.type persist(newLevel: StorageLevel): this.type |=== By applying transformations you incrementally build a link:spark-rdd-lineage.adoc[RDD lineage] with all the parent RDDs of the final RDD(s). Transformations are lazy, i.e. are not executed immediately. Only after calling an action are transformations executed. After executing a transformation, the result RDD(s) will always be different from their parents and can be smaller (e.g. filter , count , distinct , sample ), bigger (e.g. flatMap , union , cartesian ) or the same size (e.g. map ). CAUTION: There are transformations that may trigger jobs, e.g. sortBy , < >, etc. .From SparkContext by transformations to the result image::rdd-sparkcontext-transformations-action.png[align=\"center\"] Certain transformations can be pipelined which is an optimization that Spark uses to improve performance of computations.","title":"[source, scala]"},{"location":"rdd/spark-rdd-transformations/#sourcescala","text":"scala> val file = sc.textFile(\"README.md\") file: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[54] at textFile at :24 scala> val allWords = file.flatMap(_.split(\"\\W+\")) allWords: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[55] at flatMap at :26 scala> val words = allWords.filter(!_.isEmpty) words: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[56] at filter at :28 scala> val pairs = words.map((_,1)) pairs: org.apache.spark.rdd.RDD[(String, Int)] = MapPartitionsRDD[57] at map at :30 scala> val reducedByKey = pairs.reduceByKey(_ + _) reducedByKey: org.apache.spark.rdd.RDD[(String, Int)] = ShuffledRDD[59] at reduceByKey at :32 scala> val top10words = reducedByKey.takeOrdered(10)(Ordering[Int].reverse.on(_._2)) INFO SparkContext: Starting job: takeOrdered at :34 ... INFO DAGScheduler: Job 18 finished: takeOrdered at :34, took 0.074386 s top10words: Array[(String, Int)] = Array((the,21), (to,14), (Spark,13), (for,11), (and,10), (##,8), (a,8), (run,7), (can,6), (is,6)) There are two kinds of transformations: < > < > === [[narrow-transformations]] Narrow Transformations Narrow transformations are the result of map , filter and such that is from the data from a single partition only, i.e. it is self-sustained. An output RDD has partitions with records that originate from a single partition in the parent RDD. Only a limited subset of partitions used to calculate the result. Spark groups narrow transformations as a stage which is called pipelining . === [[wide-transformations]] Wide Transformations Wide transformations are the result of groupByKey and reduceByKey . The data required to compute the records in a single partition may reside in many partitions of the parent RDD. NOTE: Wide transformations are also called shuffle transformations as they may or may not depend on a shuffle. All of the tuples with the same key must end up in the same partition, processed by the same task. To satisfy these operations, Spark must execute link:spark-rdd-shuffle.adoc[RDD shuffle], which transfers data across cluster and results in a new stage with a new set of partitions. === [[zipWithIndex]] zipWithIndex","title":"[source,scala]"},{"location":"rdd/spark-rdd-transformations/#source-scala_12","text":"","title":"[source, scala]"},{"location":"rdd/spark-rdd-transformations/#zipwithindex-rddt-long","text":"zipWithIndex zips this RDD[T] with its element indices.","title":"zipWithIndex(): RDD[(T, Long)]"},{"location":"rdd/spark-rdd-transformations/#caution","text":"If the number of partitions of the source RDD is greater than 1, it will submit an additional job to calculate start indices.","title":"[CAUTION]"},{"location":"rdd/spark-rdd-transformations/#source-scala_13","text":"val onePartition = sc.parallelize(0 to 9, 1) scala> onePartition.partitions.length res0: Int = 1 // no job submitted onePartition.zipWithIndex val eightPartitions = sc.parallelize(0 to 9, 8) scala> eightPartitions.partitions.length res1: Int = 8 // submits a job eightPartitions.zipWithIndex .Spark job submitted by zipWithIndex transformation image::spark-transformations-zipWithIndex-webui.png[align=\"center\"] ====","title":"[source, scala]"}]}